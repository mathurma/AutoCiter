<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006907</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01793</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Education</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Multivariate analysis</subject><subj-group><subject>Principal component analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Multivariate analysis</subject><subj-group><subject>Principal component analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Algebra</subject><subj-group><subject>Linear algebra</subject><subj-group><subject>Eigenvalues</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Chemical compounds</subject><subj-group><subject>Phenols</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Nutrition</subject><subj-group><subject>Diet</subject><subj-group><subject>Beverages</subject><subj-group><subject>Alcoholic beverages</subject><subj-group><subject>Wine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Nutrition</subject><subj-group><subject>Diet</subject><subj-group><subject>Beverages</subject><subj-group><subject>Alcoholic beverages</subject><subj-group><subject>Wine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Aspect ratio</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data visualization</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Ten quick tips for effective dimensionality reduction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3397-0380</contrib-id>
<name name-style="western">
<surname>Nguyen</surname>
<given-names>Lan Huong</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2208-8168</contrib-id>
<name name-style="western">
<surname>Holmes</surname>
<given-names>Susan</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Institute for Mathematical and Computational Engineering, Stanford University, Stanford, California, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Statistics, Stanford University, Stanford, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Ouellette</surname>
<given-names>Francis</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Toronto, CANADA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">susan@stat.stanford.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>20</day>
<month>6</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>6</issue>
<elocation-id>e1006907</elocation-id>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Nguyen, Holmes</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006907"/>
<funding-group>
<funding-statement>LHN and SH acknowledge support from the National Institute of Health (NIH) Grant No. R01 AI112401 (URL: <ext-link ext-link-type="uri" xlink:href="https://projectreporter.nih.gov/project_description.cfm?projectnumber=1R01AI112401-01" xlink:type="simple">https://projectreporter.nih.gov/project_description.cfm?projectnumber=1R01AI112401-01</ext-link>). SH also acknowledges support from the National Science Foundation (NSF) Grant No. DMS 1501767 (URL: <ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1501767" xlink:type="simple">https://www.nsf.gov/awardsearch/showAward?AWD_ID=1501767</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="2"/>
<page-count count="19"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Dimensionality reduction (DR) is frequently applied during the analysis of high-dimensional data. Both a means of denoising and simplification, it can be beneficial for the majority of modern biological datasets, in which it’s not uncommon to have hundreds or even millions of simultaneous measurements collected for a single sample. Because of “the curse of dimensionality,” many statistical methods lack power when applied to high-dimensional data. Even if the number of collected data points is large, they remain sparsely submerged in a voluminous high-dimensional space that is practically impossible to explore exhaustively (see chapter 12 [<xref ref-type="bibr" rid="pcbi.1006907.ref001">1</xref>]). By reducing the dimensionality of the data, you can often alleviate this challenging and troublesome phenomenon. Low-dimensional data representations that remove noise but retain the signal of interest can be instrumental in understanding hidden structures and patterns. Original high-dimensional data often contain measurements on uninformative or redundant variables. DR can be viewed as a method for latent feature extraction. It is also frequently used for data compression, exploration, and visualization. Although many DR techniques have been developed and implemented in standard data analytic pipelines, they are easy to misuse, and their results are often misinterpreted in practice. This article presents a set of useful guidelines for practitioners specifying how to correctly perform DR, interpret its output, and communicate results. Note that this is not a review article, and we recommend some important reviews in the references.</p>
</sec>
<sec id="sec002">
<title>Tip 1: Choose an appropriate method</title>
<p>The abundance of available DR methods can seem intimidating when you want to pick one out of the existing bounty for your analysis. The truth is, you don't really need to commit to only one tool; however, you must recognize which methods are appropriate for your application.</p>
<p>The choice of a DR method depends on the nature of your input data. For example, different methods apply to continuous, categorical, count, or distance data. You should also consider your intuition and domain knowledge about the collected measurements. It is often the case that observations can adequately capture only the small-scale relationships between nearby (or similar) data points but not the long-range interactions between distant observations. Considering the nature and the resolution of your data is important, as DR methods can be focused on recovering either global or local structures in the data. In general, linear methods such as principal component analysis (PCA) [<xref ref-type="bibr" rid="pcbi.1006907.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref003">3</xref>], correspondence analysis (CA) [<xref ref-type="bibr" rid="pcbi.1006907.ref004">4</xref>], multiple CA (MCA) [<xref ref-type="bibr" rid="pcbi.1006907.ref005">5</xref>], or classical multidimensional scaling (cMDS), also referred to as principal CA (PCoA) [<xref ref-type="bibr" rid="pcbi.1006907.ref006">6</xref>], are more adept at preserving global structure, whereas nonlinear methods such as kernel PCA [<xref ref-type="bibr" rid="pcbi.1006907.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref008">8</xref>], nonmetric multidimensional scaling (NMDS) [<xref ref-type="bibr" rid="pcbi.1006907.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref010">10</xref>], Isomap [<xref ref-type="bibr" rid="pcbi.1006907.ref011">11</xref>], diffusion maps [<xref ref-type="bibr" rid="pcbi.1006907.ref012">12</xref>], and varieties of neighbor embedding (NE) techniques [<xref ref-type="bibr" rid="pcbi.1006907.ref013">13</xref>] such as t-Stochastic NE (t-SNE) [<xref ref-type="bibr" rid="pcbi.1006907.ref014">14</xref>] are better at representing local interactions. NE approaches do not preserve long-range interactions between data points and generate visualizations in which the arrangement of nonneighboring groups of observations is not informative. As a consequence, inferences should not be made based on large-scale structures observed in NE plots. Reviews of linear and nonlinear DR methods are provided in [<xref ref-type="bibr" rid="pcbi.1006907.ref015">15</xref>] and [<xref ref-type="bibr" rid="pcbi.1006907.ref016">16</xref>], respectively.</p>
<p>If observations in your data have assigned class labels, and your goal is to obtain a representation that best separates them into known categories, you might consider using supervised DR techniques. Examples of supervised DR methods include partial least squares (PLS) [<xref ref-type="bibr" rid="pcbi.1006907.ref017">17</xref>], linear discriminant analysis (LDA) [<xref ref-type="bibr" rid="pcbi.1006907.ref018">18</xref>], neighborhood component analysis (NCA) [<xref ref-type="bibr" rid="pcbi.1006907.ref019">19</xref>], and the bottleneck neural network classifier [<xref ref-type="bibr" rid="pcbi.1006907.ref020">20</xref>]. Unlike the previously listed unsupervised methods, blind to observations' group memberships, these supervised DR techniques directly use the class assignment information to cluster together data points with the same labels.</p>
<p>For situations in which multidomain data are gathered, e.g., gene expression together with proteomics and methylation data, you might apply DR to each data table separately and then align them using a Procrustes transformation [<xref ref-type="bibr" rid="pcbi.1006907.ref021">21</xref>] or, instead, consider methods that allow integration of multiple datasets such as the conjoint analysis method for multiple tables known as STATIS [<xref ref-type="bibr" rid="pcbi.1006907.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref023">23</xref>] and the equivalent method for the conjoint analysis of multiple distance matrices called DiSTATIS [<xref ref-type="bibr" rid="pcbi.1006907.ref024">24</xref>] (see Tip 9 for more details). <xref ref-type="table" rid="pcbi.1006907.t001">Table 1</xref> gives a classification and a summary of the basic properties of the DR techniques. To assist practitioners, we also include in <xref ref-type="table" rid="pcbi.1006907.t002">Table 2</xref> a list of stable implementations of methods discussed in this article.</p>
<table-wrap id="pcbi.1006907.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.t001</object-id>
<label>Table 1</label> <caption><title>Dimensionality reduction methods.</title></caption>
<alternatives>
<graphic id="pcbi.1006907.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">Input Data</th>
<th align="left">Method Class</th>
<th align="left">Nonlinear</th>
<th align="left">Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">PCA</td>
<td align="left">continuous data</td>
<td align="left">unsupervised</td>
<td align="center"/>
<td align="justify"><inline-formula id="pcbi.1006907.e001"><alternatives><graphic id="pcbi.1006907.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">CA</td>
<td align="left">categorical data</td>
<td align="left">unsupervised</td>
<td align="left"/>
<td align="justify"><inline-formula id="pcbi.1006907.e002"><alternatives><graphic id="pcbi.1006907.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">MCA</td>
<td align="left">categorical data</td>
<td align="left">unsupervised</td>
<td align="left"/>
<td align="justify"><inline-formula id="pcbi.1006907.e003"><alternatives><graphic id="pcbi.1006907.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">PCoA (cMDS)</td>
<td align="left">distance matrix</td>
<td align="left">unsupervised</td>
<td align="center"/>
<td align="justify"><inline-formula id="pcbi.1006907.e004"><alternatives><graphic id="pcbi.1006907.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">NMDS</td>
<td align="left">distance matrix</td>
<td align="left">unsupervised</td>
<td align="center"/>
<td align="justify"><inline-formula id="pcbi.1006907.e005"><alternatives><graphic id="pcbi.1006907.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">Isomap</td>
<td align="left">continuous<xref ref-type="table-fn" rid="t001fn002">*</xref></td>
<td align="left">unsupervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e006"><alternatives><graphic id="pcbi.1006907.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">Diffusion Map</td>
<td align="left">continuous<xref ref-type="table-fn" rid="t001fn002">*</xref></td>
<td align="left">unsupervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e007"><alternatives><graphic id="pcbi.1006907.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">Kernel PCA</td>
<td align="left">continuous<xref ref-type="table-fn" rid="t001fn002">*</xref></td>
<td align="left">unsupervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e008"><alternatives><graphic id="pcbi.1006907.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">t-SNE</td>
<td align="left">continuous/distance</td>
<td align="left">unsupervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e009"><alternatives><graphic id="pcbi.1006907.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">Barnes–Hut t-SNE</td>
<td align="left">continuous/distance</td>
<td align="left">unsupervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e010"><alternatives><graphic id="pcbi.1006907.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">LDA</td>
<td align="left">continuous (X and Y)</td>
<td align="left">supervised</td>
<td align="center"/>
<td align="justify"><inline-formula id="pcbi.1006907.e011"><alternatives><graphic id="pcbi.1006907.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">PLS (NIPALS)</td>
<td align="left">continuous (X and Y)</td>
<td align="left">supervised</td>
<td align="center"/>
<td align="justify"><inline-formula id="pcbi.1006907.e012"><alternatives><graphic id="pcbi.1006907.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">NCA</td>
<td align="left">distance matrix</td>
<td align="left">supervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e013"><alternatives><graphic id="pcbi.1006907.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">Bottleneck NN</td>
<td align="left">continuous/categorical</td>
<td align="left">supervised</td>
<td align="center">✔</td>
<td align="justify"><inline-formula id="pcbi.1006907.e014"><alternatives><graphic id="pcbi.1006907.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">STATIS</td>
<td align="left">continuous</td>
<td align="left">multidomain</td>
<td align="left"/>
<td align="justify"><inline-formula id="pcbi.1006907.e015"><alternatives><graphic id="pcbi.1006907.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left">DiSTATIS</td>
<td align="left">distance matrix</td>
<td align="left">multidomain</td>
<td align="left"/>
<td align="justify"><inline-formula id="pcbi.1006907.e016"><alternatives><graphic id="pcbi.1006907.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006907.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi mathvariant="double-struck">O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Basic properties: input data required, method class, linear or nonlinear, and runtime complexity in terms of: <italic>n</italic>—the number of observations, <italic>p</italic>—the number of features in the original data, <italic>k</italic>—the selected number of nearest neighbors, <italic>h</italic>—the number of iterations, and <italic>P</italic>—the total number of variables in all available datasets collected on <italic>n</italic> samples in the case of multidomain data.</p></fn>
<fn id="t001fn002"><p>*Commonly, Isomap estimates geodesic distances between data points from Euclidean distances, and Diffusion Map and Kernel PCA compute Gaussian kernels and thus require continuous data input. However, it is possible to use categorical data if other dissimilarities or kernels are used.</p></fn>
<fn id="t001fn003"><p>Abbreviations: CA, correspondence analysis; cMDS, classical multidimensional scaling; LDA, linear discriminant analysis; MCA, multiple CA; NCA, neighborhood component analysis; NIPALS, nonlinear iterative partial least squares; NMDS, nonmetiric multidimensional scaling; NN, neural network; PCA, principal component analysis; PCoA, principal CA; t-SNE, t-Stochastic Neighbor Embedding; PLS, partial least squares</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pcbi.1006907.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.t002</object-id>
<label>Table 2</label> <caption><title>Example implementations.</title></caption>
<alternatives>
<graphic id="pcbi.1006907.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">R function</th>
<th align="left">Python function</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">PCA</td>
<td align="left"><monospace>stats::prcomp</monospace></td>
<td align="left"><monospace>sklearn.decomposition.PCA</monospace></td>
</tr>
<tr>
<td align="left">CATPCA</td>
<td align="left"><monospace>gifi::princals</monospace></td>
<td align="left"/>
</tr>
<tr>
<td align="left">CA</td>
<td align="left"><monospace>FactoMineR::CA</monospace></td>
<td align="left"/>
</tr>
<tr>
<td align="left">MCA</td>
<td align="left"><monospace>FactoMineR::MCA</monospace></td>
<td align="left"/>
</tr>
<tr>
<td align="left">PCoA (cMDS)</td>
<td align="left"><monospace>stats::cmdscale</monospace></td>
<td align="left"><monospace>sklearn.manifold.MDS</monospace></td>
</tr>
<tr>
<td align="left">NMDS</td>
<td align="left"><monospace>ecodist::nmds</monospace></td>
<td align="left"><monospace>sklearn.manifold.MDS</monospace></td>
</tr>
<tr>
<td align="left">Isomap</td>
<td align="left"><monospace>vegan::isomap</monospace></td>
<td align="left"><monospace>sklearn.manifold.Isomap</monospace></td>
</tr>
<tr>
<td align="left">Diffusion Map</td>
<td align="left"><monospace>diffusionMap::diffuse</monospace></td>
<td align="left"/>
</tr>
<tr>
<td align="left">(Barnes–Hut) t-SNE</td>
<td align="left"><monospace>Rtsne::Rtsne</monospace></td>
<td align="left"><monospace>sklearn.manifold.TSNE</monospace></td>
</tr>
<tr>
<td align="left">LDA</td>
<td align="left"><monospace>MASS::lda</monospace></td>
<td align="left"><monospace>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</monospace></td>
</tr>
<tr>
<td align="left">PLS (NIPALS)</td>
<td align="left"><monospace>mixOmics::pls</monospace></td>
<td align="left"><monospace>sklearn.cross_decomposition.PLSRegression</monospace></td>
</tr>
<tr>
<td align="left">DiSTATIS</td>
<td align="left"><monospace>DistatisR::distatis</monospace></td>
<td align="left"/>
</tr>
<tr>
<td align="left">Procrustes</td>
<td align="left"><monospace>vegan::procrustes</monospace></td>
<td align="left"><monospace>scipy.spatial.procrustes</monospace></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001"><p>Software packages and function performing specified DR techniques available in R and python. R implementations are given as <monospace>package_name::function_name</monospace>; listed python functions come from <monospace>sklearn</monospace> and <monospace>scipy</monospace> libraries. The outputs of most linear DR methods can be visualized in R with <monospace>factoextra</monospace> package [<xref ref-type="bibr" rid="pcbi.1006907.ref025">25</xref>], used to generate a number of the plots in this article. Abbreviations: CA, correspondence analysis; CATPCA, categorical PCA; cMDS, classical multidimensional scaling; DR, dimensionality reduction; LDA, linear discriminant analysis; MCA, multiple CA; NIPALS, nonlinear iterative partial least squares; NMDS, nonmetiric multidimensional scaling; PCA, principal component analysis; PCoA, principal CA; t-SNE, t-Stochastic Neighbor Embedding; PLS, partial least squares</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec003">
<title>Tip 2: Preprocess continuous and count input data</title>
<p>Before applying DR, suitable data preprocessing is often necessary. For example, data centering—subtracting variable means from each observation—is a required step for PCA on continuous variables and is applied by default in most standard implementations. Another commonly employed data transformation is scaling—multiplying each measurement of a variable by a scalar factor so that the resulting feature has a variance of one. The scaling step ensures equal contribution from each variable, which is especially important for datasets containing heterogeneous features with highly variable ranges or distinct units, e.g., patient clinical data or environmental factors data.</p>
<p>When the units of all variables are the same, e.g., in high-throughput assays, normalizing feature variances is not advised, because it results in shrinkage of features containing strong signals and inflation of features with no signal. Other data transformations may be required, depending on the application, the type of input data, and the DR method used. For example, if changes in your data are multiplicative, e.g., your variables measure percent increase/decrease, you should consider using a log-transform before applying PCA. When working with genomic sequencing data, two issues need to be addressed before you can apply DR. First, each sequencing sample has a different library size (sequencing depth)—a nuisance parameter that artificially differentiates observations. In order to make observations comparable to each other, samples need to be normalized by dividing each measurement by a corresponding sample size factor, estimated using specialized methods (e.g., <monospace>DESeq2</monospace> [<xref ref-type="bibr" rid="pcbi.1006907.ref026">26</xref>], <monospace>edgeR</monospace> [<xref ref-type="bibr" rid="pcbi.1006907.ref027">27</xref>]). Secondly, the assay data exhibit a mean-variance trend in which features with higher means have higher variances. A variance stabilization transformation (VST) is needed to adjust for this effect and to avoid bias toward the highly abundant features. For counts with a negative-binomial distribution, such as the sequencing read counts, an inverse hyperbolic sine transformation or similar techniques are recommended [<xref ref-type="bibr" rid="pcbi.1006907.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1006907.ref030">30</xref>]. Sample normalization and variance stabilization together are effective and sufficient preprocessing steps for high-throughput data.</p>
</sec>
<sec id="sec004">
<title>Tip 3: Handle categorical input data appropriately</title>
<p>In many cases, available measurements are not numerical but qualitative or categorical. The corresponding data variables represent categories—nonnumeric quantities, e.g., phenotypes, cohort memberships, sample sequencing runs, survey respondent ratings. When the relationship between the levels (distinct values) of two categorical variables is of interest, CA is applied to a contingency table (constructed from the data) whose entries are the categories' co-occurrence frequencies. If more than two categorical variables are available, MCA enables the study of both the relationship between the observations and the associations between variable categories. MCA is a generalization of CA and is simply CA applied to an indicator matrix formed by a dummy (one-hot) encoding of the categorical variables [<xref ref-type="bibr" rid="pcbi.1006907.ref005">5</xref>]. When the input data contain both numerical and categorical variables, two strategies are available. If only a few categorical variables are present, PCA is used on numerical variables, and the group means for the levels of the categorical variables can be projected as supplementary (unweighted) points (see chapter 9 of [<xref ref-type="bibr" rid="pcbi.1006907.ref001">1</xref>] for details). On the other hand, if the mixed dataset contains a large number of categorical variables, multiple factor analysis (MFA) [<xref ref-type="bibr" rid="pcbi.1006907.ref031">31</xref>] can be used. The method applies PCA on numerical and MCA on categorical variables and combines the results by weighing variable groups.</p>
<p>Another approach to working with categorical or mixed data is to perform PCA on variables transformed using an “optimal quantification.” Traditional PCA cannot be applied to categorical variables, because its objective is to maximize the variance accounted for, a concept that exists only for numerical variables. For “nominal” (unordered) or “ordinal” (ordered) categorical variables, variance can be replaced by a chi-squared distance on category frequencies (as in CA), or an appropriate variable transformation can be applied before doing a PCA. Converting categorical variables to dummy binary features is one method; another approach is to use optimal scaling categorical PCA (CATPCA) [<xref ref-type="bibr" rid="pcbi.1006907.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006907.ref034">34</xref>]. Optimal scaling replaces original levels of categorical variables with category quantifications such that the variance in the new variables is maximized [<xref ref-type="bibr" rid="pcbi.1006907.ref035">35</xref>]. CATPCA is then formulated as an optimization problem, in which the squared difference between the quantified data and the principal component is minimized iteratively, alternating between the component scores, the component loadings, and the variable quantification.</p>
<p>An advantage of optimal scaling is that it does not assume a linear relationship between variables. In fact, the ability of CATPCA to handle nonlinear relations between variables is important even when the input data are all numeric. Thus, when nonlinearities are present and the standard PCA explains only a low proportion of the variance, optimal scaling provides a possible remedy.</p>
</sec>
<sec id="sec005">
<title>Tip 4: Use embedding methods for reducing similarity and dissimilarity input data</title>
<p>When neither quantitative nor qualitative features are available, the relationships between data points, measured as dissimilarities (or similarities), can be the basis of DR performed as a low-dimensional embedding. Even when variable measurements are available, computing dissimilarities and using distance-based methods might be an effective approach. Make sure that you choose a dissimilarity metric that provides the best summary of your data, e.g., if the original data are binary, the Euclidean distance is not appropriate, and the Manhattan distance is better. If the features are sparse, however, then the Jaccard distance is preferred.</p>
<p>cMDS/PCoA and NMDS use pairwise dissimilarities between data points to find an embedding in Euclidean space that provides the best approximation to the supplied distances. Whereas cMDS is a matrix decomposition method akin to PCA, NMDS is an optimization technique that strives to retain only the ordering of the dissimilarities [<xref ref-type="bibr" rid="pcbi.1006907.ref036">36</xref>]. The latter approach is more applicable when you have low confidence in the values of the input distances. When the dissimilarity data are only available in nonstandard, qualitative formats, more specialized ordinal embedding methods are available, discussed in detail by Kleindessner and von Luxburg in [<xref ref-type="bibr" rid="pcbi.1006907.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref038">38</xref>]. When using optimization-based multidimensional scaling (MDS), you can choose to preserve only the local interactions by restricting the minimization problem to only the distances from data points to their neighbors, e.g., the k-nearest neighbors. This approach can be referred to as “local” MDS.</p>
<p>Dissimilarities can also be used as input to t-SNE. Similar to local MDS, t-SNE is only focused on representing the short-range interactions. However, the method achieves locality in a different way, by converting the supplied distances into proximity measures using a small-tail Gaussian kernel. A collection of neural network–based approaches, called <monospace>word2vec</monospace> [<xref ref-type="bibr" rid="pcbi.1006907.ref039">39</xref>], have been developed that also use similarity data (the co-occurrence data) to generate vector embeddings of objects in a continuous Euclidean space. These techniques have proven highly effective at generating word embeddings from text corpus–derived data and have since been adapted for gene coexpression data in the <monospace>gene2vec</monospace> program by Du and colleagues [<xref ref-type="bibr" rid="pcbi.1006907.ref040">40</xref>]. The robustness of these highly computational methods has not been yet extensively tested on many biological datasets.</p>
</sec>
<sec id="sec006">
<title>Tip 5: Consciously decide on the number of dimensions to retain</title>
<p>When performing DR, choosing a suitable number of new dimensions to compute is crucial. This step determines whether the signal of interest is captured in the reduced data, especially important when DR is applied as a preprocessing step preceding statistical analyses or machine learning tasks (e.g., clustering). Even when your primary goal is data visualization, in which only two or three axes can be displayed at a time, you still need to select a sufficient number of new features to generate. For example, the first two or three PCs might explain an insufficient fraction of the variance, in which case the higher-order components should be retained, and multiple combinations of the components should be used for visualizations (e.g., PC1 versus PC2, PC2 versus PC4, and PC3 versus PC5 etc.) In some cases, the strongest signal is a confounding factor, and the variation of interest is captured by higher-order PCs. If this is the case, you must use higher-order components to expose the desired patterns.</p>
<p>The optimal choice for the number of dimensions to keep depends largely on the data itself. You cannot decide on the right dimension for the output before consulting the data. Remember that the number of dimensions can be at most the minimum of the number of observations (rows) and the number of variables (columns) in your dataset. For example, if your dataset contains expression of 10,000 genes but for only 10 samples, there could not be more than 10 (or even 9 if the input data have been centered) axes in your reduced data representation. For DR methods based on spectral decompositions, such as PCA or PCoA, you could use the distribution of the eigenvalues to guide your choice of dimensions. In practice, people usually rely on “scree plots” (example in <xref ref-type="fig" rid="pcbi.1006907.g001">Fig 1</xref>) and “the elbow rule” when making decisions. A scree plot simply shows the eigenvalues corresponding to each of the axes in the output representation or, equivalently, the proportion of the variance each axis (e.g., a PC) explains. When viewing the plot, you should look for a cutoff point, in which an eigenvalue drops significantly below the level of the one immediately preceding it—the "elbow" point. Alternatively, you can inspect a histogram of the eigenvalues and search for the large values that "stand out" from the bulk. Formally, the Marchenko–Pastur distribution asymptotically models the distribution of the singular values of large random matrices. Therefore, for datasets large in both the number of observations and features, you use a rule of retaining only eigenvalues outside the support of the fitted Marchenko–Pastur distribution; however, remember that this applies only when your data have at least thousands of samples and thousands of features.</p>
<fig id="pcbi.1006907.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Scree plot.</title>
<p>For spectral methods, the eigenvalues can be used to decide how many dimensions are sufficient. The number of dimensions to keep can be selected based on an "elbow rule." In the example shown, you should keep the first five principal components.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g001" xlink:type="simple"/>
</fig>
<p>For nonspectral, optimization-based methods, the number of components is usually prespecified before DR computations. When using these approaches, the number of components can be chosen by repeating the DR process using an increasing number of dimensions and evaluating whether incorporating more components achieves a significantly lower value of the loss function that the method minimizes, e.g., the Kullback–Leibler (KL) divergence between transition probabilities defined for the input and the output data in the case of t-SNE. Ideally, you would like your findings (e.g., patterns seen in visualizations) to be robust to the number of dimensions you choose.</p>
</sec>
<sec id="sec007">
<title>Tip 6: Apply the correct aspect ratio for your visualizations</title>
<p>Visualization is an important part of the data exploration process. Therefore, it is crucial that the DR plots you generate accurately reflect the output of the DR methods you use. An important but frequently overlooked attribute of a visualization is its aspect ratio. The proportional relationship between the height and the width (and also the depth) of a 2D (and 3D) plot can strongly influence your perception of the data; therefore, the DR plots should obey the aspect ratio consistent with the relative amount of information explained by the output axes displayed.</p>
<p>In the case of PCA or PCoA, each output dimension has a corresponding eigenvalue proportional to the amount of variance it explains. If the relationship between the height and the width of a plot is arbitrary, an adequate picture of the data cannot be attained. Two-dimensional PCA plots with equal height and width are misleading but frequently encountered because popular software programs for analyzing biological data often produce square (2D) or cubical (3D) graphics by default. Instead, the height-to-width ratio of a PCA plot should be consistent with the ratio between the corresponding eigenvalues. Because eigenvalues reflect the variance in coordinates of the associated PCs, you only need to ensure that in the plots, one "unit" in direction of one PC has the same length as one "unit" in direction of another PC. (If you use <monospace>ggplot2 R</monospace> package for generating plots, adding <monospace>+ coords_fixed(1)</monospace> will ensure a correct aspect ratio.)</p>
<p>The aspect ratio issue is illustrated with a simulated example, depicted in <xref ref-type="fig" rid="pcbi.1006907.g002">Fig 2</xref>. In the rectangular (<xref ref-type="fig" rid="pcbi.1006907.g002">Fig 2A</xref>) and the square (<xref ref-type="fig" rid="pcbi.1006907.g002">Fig 2B</xref>) plots, the aspect ratio is inconsistent with the variance of the PC1 and PC2 coordinates; the result is an (incorrect) apparent grouping of the data points into a top and a bottom cluster. In contrast, <xref ref-type="fig" rid="pcbi.1006907.g002">Fig 2C</xref>, with lengths of the two axes set to respect the ratio between the corresponding eigenvalues, shows correct clustering, consistent with the true class assignment. For more examples of how the aspect ratio can affect the plot interpretation, see chapters 7 and 9 of [<xref ref-type="bibr" rid="pcbi.1006907.ref001">1</xref>].</p>
<fig id="pcbi.1006907.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Aspect ratio for PCA plots.</title>
<p>Two simulated Gaussian clusters projected on the first and the second PCs. Incorrect aspect ratio in a rectangular (a) and square (b) plot. Correct aspect ratio in (c, d) where the plot's height and width are adjusted to match the variances in PC1 and PC2 coordinates. Colors shown in (d) indicate the true Gaussian group membership. Dim1, dimension 1; Dim2, dimension 2; PC, principal component; PCA, PC analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g002" xlink:type="simple"/>
</fig>
<p>The ordering of the dimensions is not meaningful in many optimization-based DR methods. For example, in the case of t-SNE, you can choose the number of output dimensions (usually two or three) before computing the new representation. Unlike the PCs, the t-SNE dimensions are unordered and equally important because they have the same weight in the loss function minimized by the optimization algorithm. Thus, for t-SNE, the convention is to make the projection plots square or cubical.</p>
</sec>
<sec id="sec008">
<title>Tip 7: Understand the meaning of the new dimensions</title>
<p>Many linear DR methods, including PCA and CA, provide a reduced representation both for the observations and for the variables. Feature maps or correlation circles can be used to determine which original variables are associated with each other or with the newly generated output dimensions. The angles between the feature vectors or with the PC axes are informative: vectors at approximately 0° (180°) with each other indicate that the corresponding variables are closely, positively (negatively) related, whereas vectors with a 90° angle indicate rough independence.</p>
<p><xref ref-type="fig" rid="pcbi.1006907.g003">Fig 3A</xref> shows a correlation circle with scaled coordinates of the variables' projection. The plot indicates that high values of PC1 indicate low values in "Flav" (flavanoids) and "Phenols" (total phenols) and high values in "Malic Acid" and "AlcAsh"(alcalinity of ash). Additionally, "AlcAsh" (alcalinity of ash) levels seem to be closely negatively correlated with "NonFlav Phenols" (nonflavanoid phenols) and independent of "Alcohol" levels.</p>
<fig id="pcbi.1006907.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Variables' projection.</title>
<p>PCA on wine dataset shows how variables' representation can be used to understand the meaning of the new dimensions. Correlation circle (a) and PC1 contribution plot (b). AlcAsh, alcalinity of ash; Dim1, dimension 1; Dim2, dimension 2; Flav, flavanoids; NonFlav Phenols, nonflavanoid phenols; OD, OD280/OD315 of diluted wine; PC, principal component; PCA, PC analysis; Phenols, total phenols; Proa, proanthocyanins.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g003" xlink:type="simple"/>
</fig>
<p>Original variables' importances to the new dimensions can be visualized using contribution bar plots. A variable's contribution to a given new axis is computed as the ratio between its squared coordinate (in this axis) and the corresponding sum over all variables; the ratio is often converted to percentages. Many programs provide the variables' contributions as standard output; these contributions can be defined for not only a single but also multiple DR axes by summing the values corresponding to selected components. <xref ref-type="fig" rid="pcbi.1006907.g003">Fig 3B</xref> shows variables' percent contribution to PC1; note that the percent contribution does not carry information on the direction of the correlation. When working with high-dimensional datasets such as high-throughput assays, a contribution bar plot for thousands or more variables is not practical; instead, you can limit the plot, showing only the top few (e.g., 20) features with highest contribution.</p>
<p>Variables and observations can be included in the same graphic—referred to as a “biplot.” The term was coined by Kuno Ruben Gabriel [<xref ref-type="bibr" rid="pcbi.1006907.ref041">41</xref>] in 1971, but similar ideas were proposed by Jolicoeur and Mosimann already in 1960 [<xref ref-type="bibr" rid="pcbi.1006907.ref042">42</xref>]. Biplots such as the one in <xref ref-type="fig" rid="pcbi.1006907.g004">Fig 4</xref> allow you to explore the trends in the data samples and features simultaneously; looking at both at the same time, you might discover groups of similar (close by) observations that have high or low values for certain measured variables (see Tip 8 for further details).</p>
<fig id="pcbi.1006907.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g004</object-id>
<label>Fig 4</label>
<caption>
<title>PCA biplot.</title>
<p>A single plot for the wine dataset combines both the samples' and the variables' projection to the first two principal components. AlcAsh, alcalinity of ash; Dim1, dimension 1; Dim2, dimension 2; Flav, flavanoids; NonFlav Phenols, nonflavanoid phenols; OD, OD280/OD315 of diluted wine; PCA, principal component analysis; Phenols, total phenols; Proa, proanthocyanins.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Tip 8: Find the hidden signal</title>
<p>The primary objective of DR is to compress data while preserving most of the meaningful information. Compression facilitates the process of understanding the data because the reduced data representation is expected to capture the dominant sources of variation more efficiently. The aim is to uncover the "hidden variables" that can successfully expose the underlying structure of the data. The most frequently encountered latent patterns are discrete clusters or continuous gradients.</p>
<p>In the former case, similar observations bundle together away from other groups. An example of a simulated clustered dataset is shown in <xref ref-type="fig" rid="pcbi.1006907.g005">Fig 5A</xref>. When performing the cluster analysis, in which the goal is to estimate samples' group memberships, it is common practice to first apply PCA. More specifically, practitioners often use a set of the top (e.g., 50) PCs as input to a clustering algorithm. PCA reduction is intended as a noise-reduction step because the top eigenvectors are expected to contain all signals of interest [<xref ref-type="bibr" rid="pcbi.1006907.ref043">43</xref>]. Regrettably, this property does not extend to all DR methods. The output generated by neighborhood embedding techniques, such as t-SNE, should not be used for clustering, as they preserve neither distances nor densities—both quantities highly important in the interpretation of clustering output.</p>
<fig id="pcbi.1006907.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Latent structure.</title>
<p>Observations in PCA plots may cluster into groups (a) or follow a continuous gradient (b). Dim1, dimension 1; Dim2, dimension 2; PCA, principal component analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g005" xlink:type="simple"/>
</fig>
<p>Unlike discrete clusters, continuous changes in the data are less frequently identified. It is important to know how to identify and accurately interpret latent gradients, as they often appear in biological data associated with unknown continuous processes. Gradients are present when data points do not separate into distinct tightly packed clusters but instead exhibit a gradual shift from one extreme to another; they often emerge as smooth curves in DR visualizations. It is worth noting that data points are often arranged in horseshoes or arch-shaped configurations when PCA and cMDS (PCoA) is applied to data involving a linear gradient. A “horseshoe effect” can appear in PCA and cMDS plots when the associated eigenvectors take on a specific form [<xref ref-type="bibr" rid="pcbi.1006907.ref044">44</xref>] because of the properties of the data covariance or distance matrices used for computations, in particular, when these matrices can be expressed as centrosymmetric Kac–Murdock–Szego matrices [<xref ref-type="bibr" rid="pcbi.1006907.ref045">45</xref>].</p>
<p>You can see an example of this pattern for simulated data with a latent gradient in <xref ref-type="fig" rid="pcbi.1006907.g005">Fig 5B</xref>. Continuous transitions are frequently encountered when measurements are taken over time; for example, the cell development literature is rich with publications introducing methods for analyzing pseudotime, a gradient observed during cell differentiation or development [<xref ref-type="bibr" rid="pcbi.1006907.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref047">47</xref>]. There can be multiple gradients affecting the data, and a steady change can be recorded in different directions [<xref ref-type="bibr" rid="pcbi.1006907.ref048">48</xref>]. However, the variable behind the observed continuous gradient could be unknown. In this case, you should focus on finding the discrepancies between the observations at the endpoints (extremes) of the gradients by inspecting the differences between their values for any available external covariates [<xref ref-type="bibr" rid="pcbi.1006907.ref049">49</xref>], if collected (see Tip 7). Otherwise, you might need to gather additional information on the samples in your dataset to investigate the explanation of these differences.</p>
<p>Additional contiguous measurements—those not used for DR computations—are frequently collected on observations included in the dataset. The extra information can be used to improve the understanding of the data. The simplest and most common way to use the external covariates is to include them in DR visualizations—with their values encoded as color, shape, size, or even transparency of corresponding points on the plot. An example of this is shown in <xref ref-type="fig" rid="pcbi.1006907.g006">Fig 6A</xref>: the PCA embedding for a dataset on wine properties [<xref ref-type="bibr" rid="pcbi.1006907.ref050">50</xref>], in which the data points are colored by wine class, a variable that the DR was blind to. The observed grouping of the wines suggests that 13 wine properties used for DR can characterize the wine categories well. The "Wine Data Set" is accessible from the University of California Irvine (UCI) Machine Learning Repository [<xref ref-type="bibr" rid="pcbi.1006907.ref051">51</xref>].</p>
<fig id="pcbi.1006907.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Using external information.</title>
<p>(a) A PCA sample projection on the wine dataset shows that, based on their properties, wines tend to cluster in agreement with the grape variety classification: Nebbiolo, Grignolino, and Barbera. (b) A PCA biplot can be used to find which groups of wines tend to have higher levels of which property. Dim1, dimension 1; Dim2, dimension 2; PCA, principal component analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g006" xlink:type="simple"/>
</fig>
<p>Sometimes, directly plotting the external variable against the newly computed features is an effective way of exposing trends present in the data. For example, a scatter plot of a continuous variable, e.g., a patient's age or weight, versus coordinates of a selected output dimension shows correlation between the selected covariate and the new feature. If the external information is categorical instead of continuous, a boxplot of the PC coordinates (e.g., PC1, PC2, or others) can be generated for each level of the variable.</p>
<p>External information can also be incorporated in biplots. <xref ref-type="fig" rid="pcbi.1006907.g006">Fig 6B</xref> shows how combining the external information on the observations with the interpretation of the new axes in terms of the original variables (as described in Tip 7) allows you to discover that “Barbera” wines tend to have higher values of "Malic Acid" and lower "Flavanoids," and “Grignolinos” tend to have low "Ash" and "Alcohol" content.</p>
<p>Additionally, external information can be used to discover batch effects. Batch effects are technical or systemic sources of variation that obscure the main signal of interest. They are frequently encountered in sequencing data, in which samples from the same sequencing run (lane) cluster close together. Because batch effects can confound the signal of interest, it is a good practice to check for their presence and, if found, to remove them before proceeding with further downstream analysis. You can detect technical or systemic variations by generating a DR embedding map with the data points colored by their batch membership, e.g., by the sequencing run, the cage number, the study cohort. If a batch effect is discovered, you can remove it by shifting all observations in such a way that each batch has its centroid (the group's barycenter) located at the center of the plot (usually the origin of the coordinate system).</p>
</sec>
<sec id="sec010">
<title>Tip 9: Take advantage of multidomain data</title>
<p>Sometimes, more than one set of measurements is collected for the same set of samples; for example, high-throughput genomic studies involving data from multiple domains are often encountered. For the same biological sample microarray gene expression, miRNA expression, proteomics, and DNA methylation data might be gathered [<xref ref-type="bibr" rid="pcbi.1006907.ref052">52</xref>]. Integrating multiple datasets allows you to both obtain a more accurate representation of higher-order interactions and evaluate the associated variability. Samples often exhibit varying levels of uncertainty, as different regions of the data can be subject to different rates of changes or fluctuations.</p>
<p>One way of dealing with “multidomain,” also referred to as “multimodal,” “multiway,” “multiview,” or “multiomics” data, is to perform DR for each dataset separately and then align them together using a Procrustes transformation—a combination of translation, scaling, and rotation to align one configuration with another as closely as possible (see [<xref ref-type="bibr" rid="pcbi.1006907.ref021">21</xref>] and [<xref ref-type="bibr" rid="pcbi.1006907.ref036">36</xref>]). A number of more advanced methods have also been developed, for instance, STATIS [<xref ref-type="bibr" rid="pcbi.1006907.ref022">22</xref>] and DiSTATIS [<xref ref-type="bibr" rid="pcbi.1006907.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006907.ref053">53</xref>] are generalizations of PCA and classical MDS, respectively. Both methods are used to analyze several sets of data tables collected on the same set of observations, and both are based on an idea of combining datasets into a common consensus structure called the “compromise” [<xref ref-type="bibr" rid="pcbi.1006907.ref054">54</xref>].</p>
<p>The datasets can all be projected onto this consensus space. The projections of individual datasets can be helpful for observing different patterns in observations characterized by data from different domains. <xref ref-type="fig" rid="pcbi.1006907.g007">Fig 7</xref> shows an example of the use of DiSTATIS on five simulated distance tables for 20 synthetic data points. Different colors correspond to different data points, and different shapes correspond to different distance tables. The compromise points between the tables are denoted with larger diamond shape markers. For a detailed survey on the analysis of multitable data, with a focus on biological multiomics datasets, see Meng and colleagues [<xref ref-type="bibr" rid="pcbi.1006907.ref055">55</xref>].</p>
<fig id="pcbi.1006907.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Multidomain data.</title>
<p>DiSTATIS on multiple distance tables defined for the same observations. Multiple distances can be computed from different data modalities, e.g., gene expression, methylation, clinical data, or from data resampled from a known data-generating distribution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Tip 10: Check the robustness of your results and quantify uncertainties</title>
<p>For some datasets, the PCA PCs are ill defined, i.e., two or more successive PCs may have very similar variances, and the corresponding eigenvalues are almost exactly the same, as in <xref ref-type="fig" rid="pcbi.1006907.g008">Fig 8</xref>. Although a subspace spanned by these components together is meaningful, the eigenvectors (PCs) are not informative individually, and their loadings cannot be interpreted separately, because a very slight change in even one observation can lead to a completely different set of eigenvectors [<xref ref-type="bibr" rid="pcbi.1006907.ref001">1</xref>]. In these cases, we say that these PCs are unstable. The dimensions corresponding to similar eigenvalues need to be kept together and not individually interpreted.</p>
<fig id="pcbi.1006907.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Unstable eigenvalues.</title>
<p>When subsequent eigenvalues have close-to-equal values, PCA representation is unstable. PCA, principal component analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g008" xlink:type="simple"/>
</fig>
<p>When working with techniques that require parameter specification, you should also check the stability of your results against different parameter settings. For example, when running t-SNE, you need to pick a value for perplexity, and different settings can alter the results obtained even qualitatively. It has been frequently observed that when the perplexity is set to a very small value, “artificial clusters” start forming in t-SNE plots [<xref ref-type="bibr" rid="pcbi.1006907.ref056">56</xref>]. You should not use the values of the t-SNE objective function, the KL divergence, as a criterion to choose an "optimal perplexity," because the KL divergence always decreases (monotonically) as perplexity values increase. For t-SNE, a Bayesian information criterion (BIC)-type rule for selecting perplexities was proposed by Cao and Wang in [<xref ref-type="bibr" rid="pcbi.1006907.ref057">57</xref>]. However, in practice, you should repeat DR computations for a range of input parameters and visually evaluate whether the patterns discovered are consistent across varying specifications, as stability theory for t-SNE has not yet been developed. In particular, if the clustering pattern disappears with only a slight increase of the perplexity value, the grouping you observed might be only an artifact due to an unsuitably small choice of the parameter.</p>
<p>A separate concern is a method's stability against outliers. In general, it is known that observations far from the origin have more influence on the PCs than the ones close to the center; sometimes it is possible that only a small fraction of the samples in the data almost fully determines the PCs. You should be mindful of these situations and verify that the structure captured by DR represents the bulk of the data and not just a few outliers. In DR maps, the outliers are the remote points, distant from the majority of the observations. In the case of PCA and other linear methods, if all of the points in a sample projection plot are located close to the origin (the center of the plot), with only one or a few points lying very far away, the DR solution is said to be dominated by the outliers. You should inspect suitable data-specific quality control metrics for these points and consider their removal. If samples are removed, the DR needs to be recomputed, and the changes in the output representation should be noted. Observe how observations shift by comparing the DR visualizations before and after the removal of the outliers. You should consider removing not only the technical outliers but also the "outgroups," the aberrant groups known to be extensively different from the majority of the data. Eliminating the outgroups and recomputing the DR allows for patterns in the bulk of the data to emerge. On the other hand, if a dataset contains many aberrant observations, stable methods such as robust kernel PCA [<xref ref-type="bibr" rid="pcbi.1006907.ref058">58</xref>] should be used.</p>
<p>Additionally, you can estimate the uncertainties associated with observations by constructing a collection of "bootstrap" datasets, i.e., random subsets of the data generated by resampling observations with replacement. The bootstrap set can be treated as multiway data, and the STATIS or Procrustes aligning method described in Tip 8 can be applied to "match" the random subsets together. When a realistic noise model for the data is available, instead of using bootstrap subsamples, you can generate copies of all data points by perturbing the measurement values for each sample and then applying the STATIS or DiSTATIS methods as described in the previous tip to generate the coordinates for the “compromise” and for each individual perturbed copy of the data. Obtaining multiple coordinates estimates per data point allows you to estimate the corresponding uncertainty. You can visualize each sample's uncertainty on a DR embedding map using density contours or by plotting all data points from each bootstrap's projection onto the compromise. <xref ref-type="fig" rid="pcbi.1006907.g009">Fig 9</xref> shows the Procrustes alignments of PCA projections for two simulated datasets. The colored lines indicate density contours for the output coordinates of the bootstrap subsets, and the diamond markers correspond to the coordinates of the projection of the full data. Plots were produced for 20 synthetic data points from a true 2D and 5D Gaussian, both orthogonally projected to 10 dimensions. We can observe that uncertainties for points in the lower-rank data are much smaller, i.e., the first 2 PCs represent the first dataset better than the second one.</p>
<fig id="pcbi.1006907.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006907.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Data point uncertainties.</title>
<p>Stability in the DR output coordinates for each data point. Projections of bootstrap samples for two 10D simulated datasets with rank 2 (a) and rank 5 (b) onto the first two PCs aligned using a Procrustes transformation. Smaller, circular markers correspond to each bootstrap trial, and larger, diamond markers are coordinates of the full dataset. DR, dimensionality reduction; PC, principal component.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Conclusion</title>
<p>DR is very useful and sometimes essential when analyzing high-dimensional data. Despite their widespread adoption, DR methods are often misused or misinterpreted. Researchers performing DR might find the sheer number of available methods already intimidating, not to mention the wide variety of different dissimilarity metrics or parameter settings required by some of these methods. This set of ten tips serves as a checklist or informal guideline for practitioners. We described a general step-by-step procedure for performing effective DR and gave pointers for correctly interpreting and adequately communicating the output of DR algorithms. Most of the recommendations discussed here apply to any DR method, but some were instructions directed toward specific reduction approaches.</p>
<p>In addition to everything discussed earlier, we would like to offer one extra piece of advice: keep track of all the decisions you make, including the method you select, the distances or kernels you choose, and the values of parameters you use. The most convenient way to save all steps of your work together with the results obtained is through an R, an IPython, or a Jupyter notebook; these applications allow you to generate a full analysis report containing narrative text, code, and its output. Recording your choices is a crucial part of reproducible research [<xref ref-type="bibr" rid="pcbi.1006907.ref059">59</xref>]; it allows others to replicate the same results you obtained and speeds up your analysis process the next time you work with similar data. We provide an example of a reproducible report generated with R-markdown in the <xref ref-type="supplementary-material" rid="pcbi.1006907.s002">S1 Text</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006907.s001">S1 Code</xref> files.</p>
</sec>
<sec id="sec013">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006907.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.s001" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>An R-markdown file containing a reproducible record of all plots included in this article.</title>
<p>(RMD)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006907.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006907.s002" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>A pdf report rendered from an R-markdown file in <xref ref-type="supplementary-material" rid="pcbi.1006907.s001">S1 Code</xref> containing text, figures, and code.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="other" id="fn001">
<p>This is a <italic>PLOS Computational Biology</italic> Education paper.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="pcbi.1006907.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Holmes</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>W</given-names></name>. <source>Modern Statistics for Modern Biology</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2019</year> [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.huber.embl.de/msmb/" xlink:type="simple">https://www.huber.embl.de/msmb/</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pearson</surname> <given-names>K.</given-names></name> <article-title>On lines and planes of closest fit to systems of points in space</article-title>. <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source>. <year>1901</year>;<volume>2</volume>(<issue>11</issue>):<fpage>559</fpage>–<lpage>572</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/14786440109462720" xlink:type="simple">10.1080/14786440109462720</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hotelling</surname> <given-names>H.</given-names></name> <article-title>Analysis of a Complex of Statistical Variables with Principal Components</article-title>. <source>Journal of Educational Psychology</source>. <year>1933</year>;<volume>24</volume>:<fpage>417</fpage>–<lpage>441</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hirschfeld</surname> <given-names>HO</given-names></name>. <article-title>A Connection between Correlation and Contingency</article-title>. <source>Mathematical Proceedings of the Cambridge Philosophical Society</source>. <year>1935</year>;<volume>31</volume>(<issue>4</issue>):<fpage>520</fpage>–<lpage>524</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0305004100013517" xlink:type="simple">10.1017/S0305004100013517</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Valentin</surname> <given-names>D</given-names></name>. <article-title>Multiple Correspondence Analysis</article-title>. <source>Encyclopedia of Measurement and Statistics</source>. <year>2007</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4135/9781412952644" xlink:type="simple">10.4135/9781412952644</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref006"><label>6</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Torgerson</surname> <given-names>WS</given-names></name>. <source>Theory and methods of scaling</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1958</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Smola</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>KR</given-names></name>. <article-title>Nonlinear Component Analysis as a Kernel Eigenvalue Problem</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>5</issue>):<fpage>1299</fpage>–<lpage>1319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976698300017467" xlink:type="simple">10.1162/089976698300017467</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Smola</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>KR</given-names></name>. <source>Advances in Kernel Methods</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1999</year>. p. <fpage>327</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepard</surname> <given-names>RN</given-names></name>. <article-title>The analysis of proximities: Multidimensional scaling with an unknown distance function. II</article-title>. <source>Psychometrika</source>. <year>1962</year>;<volume>27</volume>(<issue>3</issue>):<fpage>219</fpage>–<lpage>246</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02289621" xlink:type="simple">10.1007/BF02289621</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kruskal</surname> <given-names>JB</given-names></name>. <article-title>Nonmetric multidimensional scaling: A numerical method</article-title>. <source>Psychometrika</source>. <year>1964</year>;<volume>29</volume>(<issue>2</issue>):<fpage>115</fpage>–<lpage>129</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02289694" xlink:type="simple">10.1007/BF02289694</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Silva</surname> <given-names>Vd</given-names></name>, <name name-style="western"><surname>Langford</surname> <given-names>JC</given-names></name>. <article-title>A Global Geometric Framework for Nonlinear Dimensionality Reduction</article-title>. <source>Science</source>. <year>2000</year>;<volume>290</volume>(<issue>5500</issue>):<fpage>2319</fpage>–<lpage>2323</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.290.5500.2319" xlink:type="simple">10.1126/science.290.5500.2319</ext-link></comment> <object-id pub-id-type="pmid">11125149</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coifman</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Lafon</surname> <given-names>S</given-names></name>. <article-title>Diffusion maps</article-title>. <source>Applied and Computational Harmonic Analysis</source>. <year>2006</year>;<volume>21</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.acha.2006.04.006" xlink:type="simple">10.1016/j.acha.2006.04.006</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Roweis ST. Stochastic Neighbor Embedding. In: Becker S, Thrun S, Obermayer K, editors. Advances in Neural Information Processing Systems 15. Proceedings of the 2002 Neural Information processing Systems Conference. Cambridge, MA: MIT Press; 2003. p. 857–864.</mixed-citation></ref>
<ref id="pcbi.1006907.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Maaten</surname> <given-names>LJP</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Visualizing Data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source>. <year>2008</year>;<volume>9</volume>:<fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>. <article-title>Linear Dimensionality Reduction: Survey, Insights, and Generalizations</article-title>. <source>Journal of Machine Learning Research</source>. <year>2015</year>;<volume>16</volume>:<fpage>2859</fpage>–<lpage>2900</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref016"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Ting D, Jordan MI. On Nonlinear Dimensionality Reduction, Linear Smoothing and Autoencoding. arXiv:1803.02432 [Preprint]. 2018 [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.02432" xlink:type="simple">https://arxiv.org/abs/1803.02432</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref017"><label>17</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wold</surname> <given-names>H.</given-names></name> <chapter-title>Estimation of Principal Components and Related Models by Iterative Least squares</chapter-title>. In: <source>Multivariate Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>1966</year>. p. <fpage>391</fpage>–<lpage>420</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fisher</surname> <given-names>RA</given-names></name>. <article-title>The Use of Multiple Measurements in Taxonomic Problems</article-title>. <source>Annals of Eugenics</source>. <year>1936</year>;<volume>7</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1469-1809.1936.tb02137.x" xlink:type="simple">10.1111/j.1469-1809.1936.tb02137.x</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref019"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Goldberger J, Roweis S, Hinton G, Salakhutdinov R. Neighbourhood Components Analysis. In: Proceedings of the 17th International Conference on Neural Information Processing Systems. Cambridge, MA: MIT Press; 2004. p. 513–520.</mixed-citation></ref>
<ref id="pcbi.1006907.ref020"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Parviainen E. Deep Bottleneck Classifiers in Supervised Dimension Reduction. In: Proceedings of the 20th International Conference on Artificial Neural Networks: Part III. ICANN'10. Berlin, Heidelberg: Springer-Verlag; 2010. p. 1–10.</mixed-citation></ref>
<ref id="pcbi.1006907.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hurley</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Cattell</surname> <given-names>RB</given-names></name>. <article-title>The procrustes program: Producing direct rotation to test a hypothesized factor structure</article-title>. <source>Behavioral Science</source>. <year>1962</year>;<volume>7</volume>(<issue>2</issue>):<fpage>258</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/bs.3830070216" xlink:type="simple">10.1002/bs.3830070216</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Escoufier</surname> <given-names>Y.</given-names></name> <article-title>L'analyse conjointe de plusieurs matrices de données</article-title>. <source>Biométrie et temps</source>. <year>1980</year>. p. <fpage>59</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lavit</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Escoufier</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sabatier</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Traissac</surname> <given-names>P</given-names></name>. <article-title>The ACT (STATIS method)</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>1994</year>;<volume>18</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0167-9473(94)90134-1" xlink:type="simple">10.1016/0167-9473(94)90134-1</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Abdi H, O'Toole AJ, Valentin D, Edelman B. DISTATIS: The Analysis of Multiple Distance Matrices. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)—Workshops; 2005. San Diego, CA. IEEE. p. 42–42.</mixed-citation></ref>
<ref id="pcbi.1006907.ref025"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Kassambara A, Mundt F. factoextra: Extract and Visualize the Results of Multivariate Data Analyses. Version 1.0.5 [software]. 2017 [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=factoextra" xlink:type="simple">https://CRAN.R-project.org/package=factoextra</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Love</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Anders</surname> <given-names>S</given-names></name>. <article-title>Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2</article-title>. <source>Genome Biology</source>. <year>2014</year>;<volume>15</volume>(<issue>12</issue>):<fpage>550</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13059-014-0550-8" xlink:type="simple">10.1186/s13059-014-0550-8</ext-link></comment> <object-id pub-id-type="pmid">25516281</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Smyth</surname> <given-names>GK</given-names></name>. <article-title>edgeR: a Bioconductor package for differential expression analysis of digital gene expression data</article-title>. <source>Bioinformatics</source>. <year>2010</year>;<volume>26</volume>(<issue>1</issue>):<fpage>139</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btp616" xlink:type="simple">10.1093/bioinformatics/btp616</ext-link></comment> <object-id pub-id-type="pmid">19910308</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laubscher</surname> <given-names>NF</given-names></name>. <article-title>On Stabilizing the Binomial and Negative Binomial Variances</article-title>. <source>Journal of the American Statistical Association</source>. <year>1961</year>;<volume>56</volume>(<issue>293</issue>):<fpage>143</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1961.10482100" xlink:type="simple">10.1080/01621459.1961.10482100</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burbidge</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Robb</surname> <given-names>AL</given-names></name>. <article-title>Alternative Transformations to Handle Extreme Values of the Dependent Variable</article-title>. <source>Journal of the American Statistical Association</source>. <year>1988</year>;<volume>83</volume>(<issue>401</issue>):<fpage>123</fpage>–<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1988.10478575" xlink:type="simple">10.1080/01621459.1988.10478575</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huber</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>von Heydebreck</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sültmann</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Poustka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vingron</surname> <given-names>M</given-names></name>. <article-title>Variance stabilization applied to microarray data calibration and to the quantification of differential expression</article-title>. <source>Bioinformatics</source>. <year>2002</year>;<volume>18</volume>(<issue>Suppl 1</issue>):<fpage>S96</fpage>–<lpage>S104</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Escofier</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pagès</surname> <given-names>J</given-names></name>. <article-title>Multiple factor analysis (AFMULT package)</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>1994</year>;<volume>18</volume>(<issue>1</issue>):<fpage>121</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0167-9473(94)90135-X" xlink:type="simple">10.1016/0167-9473(94)90135-X</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guttman</surname> <given-names>L.</given-names></name> <article-title>The quantification of a class of attributes: A theory and method of scale construction</article-title>. <source>The Prediction of Personal Adjustment</source>. <year>1941</year>; p. <fpage>319</fpage>–<lpage>348</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gifi</surname> <given-names>A.</given-names></name> <source>Nonlinear multivariate analysis</source>. <publisher-loc>Chichester, New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1990</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref034"><label>34</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Meulman</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Heiser</surname> <given-names>WJ</given-names></name>, <etal>et al</etal>. <source>SPSS Categories 10.0</source>. <publisher-loc>Chicago</publisher-loc>: <publisher-name>SPSS Incorporated</publisher-name>; <year>1999</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linting</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Meulman</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Groenen</surname> <given-names>PJF</given-names></name>, <name name-style="western"><surname>van der Koojj</surname> <given-names>AJ</given-names></name>. <article-title>Nonlinear principal components analysis: Introduction and application</article-title>. <source>Psychological Methods</source>. <year>2007</year>;<volume>12</volume>(<issue>3</issue>):<fpage>336</fpage>–<lpage>358</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1082-989X.12.3.336" xlink:type="simple">10.1037/1082-989X.12.3.336</ext-link></comment> <object-id pub-id-type="pmid">17784798</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Borg</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Groenen</surname> <given-names>PJF</given-names></name>. <source>Modern Multidimensional Scaling: Theory and Applications</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref037"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Kleindessner M, Luxburg U. Uniqueness of Ordinal Embedding. In: Balcan MF, Feldman V, Szepesvári C, editors. Proceedings of The 27th Conference on Learning Theory. vol. 35 of Proceedings of Machine Learning Research. Barcelona, Spain: PMLR; 2014. p. 40–67.</mixed-citation></ref>
<ref id="pcbi.1006907.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleindessner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>von Luxburg</surname> <given-names>U</given-names></name>. <article-title>Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis</article-title>. <source>Journal of Machine Learning Research</source>. <year>2017</year>;<volume>18</volume>(<issue>58</issue>):<fpage>1</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Mikolov T, Chen K, Corrado G, Dean J. Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781 [Preprint]. 2013 [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3781" xlink:type="simple">https://arxiv.org/abs/1301.3781</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Du</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jia</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tao</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhi</surname> <given-names>D</given-names></name>. <article-title>Gene2Vec: Distributed Representation of Genes Based on Co-Expression</article-title>. <source>BioRxiv</source> [Preprint]. <year>2018</year> [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/286096v2" xlink:type="simple">https://www.biorxiv.org/content/10.1101/286096v2</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gabriel</surname> <given-names>KR</given-names></name>. <article-title>The Biplot Graphic Display of Matrices with Application to Principal Component Analysis</article-title>. <source>Biometrika</source>. <year>1971</year>;<volume>58</volume>(<issue>3</issue>):<fpage>453</fpage>–<lpage>467</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolicoeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mosimann</surname> <given-names>JE</given-names></name>. <article-title>Size and shape variation in the painted turtle. A principal component analysis</article-title>. <source>Growth</source>. <year>1960</year>;<volume>24</volume>:<fpage>339</fpage>–<lpage>54</lpage>. <object-id pub-id-type="pmid">13790416</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Husson</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Josse</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pagès</surname> <given-names>J</given-names></name>. <source>Principal component methods-hierarchical clustering-partitional clustering: why would we need to choose for visualizing data? Rennes</source>, <publisher-loc>France</publisher-loc>: <publisher-name>Agrocampus Ouest</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diaconis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Goel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>S</given-names></name>. <article-title>Horseshoes in Multidimensional Scaling and Local Kernel Methods</article-title>. <source>The Annals of Applied Statistics</source>. <year>2008</year>;<volume>2</volume>(<issue>3</issue>):<fpage>777</fpage>–<lpage>807</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trench</surname> <given-names>WF</given-names></name>. <article-title>Spectral distribution of generalized Kac–Murdock–Szego matrices</article-title>. <source>Linear Algebra and its Applications</source>. <year>2002</year>;<volume>347</volume>(<issue>1</issue>):<fpage>251</fpage>–<lpage>273</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0024-3795(01)00561-4" xlink:type="simple">10.1016/S0024-3795(01)00561-4</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reid</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Wernisch</surname> <given-names>L</given-names></name>. <article-title>Pseudotime estimation: deconfounding single cell time series</article-title>. <source>Bioinformatics</source>. <year>2016</year>;<volume>32</volume>(<issue>19</issue>):<fpage>2973</fpage>–<lpage>2980</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btw372" xlink:type="simple">10.1093/bioinformatics/btw372</ext-link></comment> <object-id pub-id-type="pmid">27318198</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Campbell</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Yau</surname> <given-names>C</given-names></name>. <article-title>Uncovering pseudotemporal trajectories with covariates from single cell and bulk expression data</article-title>. <source>Nature Communications</source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>2442</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-018-04696-6" xlink:type="simple">10.1038/s41467-018-04696-6</ext-link></comment> <object-id pub-id-type="pmid">29934517</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Campbell</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Yau</surname> <given-names>C</given-names></name>. <article-title>Probabilistic modeling of bifurcations in single-cell gene expression data using a Bayesian mixture of factor analyzers</article-title>. <source>Wellcome Open Research</source>. <year>2017</year>;<volume>2</volume>(<issue>19</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.12688/wellcomeopenres.11087.1" xlink:type="simple">10.12688/wellcomeopenres.11087.1</ext-link></comment> <object-id pub-id-type="pmid">28503665</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nguyen</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>S</given-names></name>. <article-title>Bayesian Unidimensional Scaling for visualizing uncertainty in high dimensional datasets with latent ordering of observations</article-title>. <source>BMC Bioinformatics</source>. <year>2017</year>;<volume>18</volume>(<issue>10</issue>):<fpage>394</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s12859-017-1790-x" xlink:type="simple">10.1186/s12859-017-1790-x</ext-link></comment> <object-id pub-id-type="pmid">28929970</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Forina</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Leardi</surname> <given-names>R</given-names></name>, C A, <name name-style="western"><surname>Lanteri</surname> <given-names>S</given-names></name>. <source>PARVUS: An Extendable Package of Programs for Data Exploration</source>. <publisher-loc>Amsterdam, the Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>; <year>1988</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dheeru</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Karra Taniskidou</surname> <given-names>E</given-names></name>. <source>UCI Machine Learning Repository</source> [Internet]. <publisher-loc>Irvine, CA</publisher-loc>: <publisher-name>University of California, School of Information and Computer Science</publisher-name>; <year>2017</year> [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml" xlink:type="simple">http://archive.ics.uci.edu/ml</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ray</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Henaff</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Efstathiadis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Peskin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Picone</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Information content and analysis methods for multi-modal high-throughput biomedical data</article-title>. <source>Scientific Reports</source>. <year>2014</year>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep04411" xlink:type="simple">10.1038/srep04411</ext-link></comment> <object-id pub-id-type="pmid">24651673</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Valentin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bennani-Dosse</surname> <given-names>M</given-names></name>. <article-title>STATIS and DISTATIS: optimum multitable principal component analysis and three way metric multidimensional scaling</article-title>. <source>Wiley Interdisciplinary Reviews: Computational Statistics</source>. <year>2012</year>;<volume>4</volume>(<issue>2</issue>):<fpage>124</fpage>–<lpage>167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/wics.198" xlink:type="simple">10.1002/wics.198</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref054"><label>54</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>L'Hermier des Plantes</surname> <given-names>H.</given-names></name> <source>Structuration des tableaux à trois indices de la statistique: théorie et application d'une méthode d'analyse conjointe</source>. <publisher-loc>Montpellier, France</publisher-loc>: <publisher-name>Université des sciences et techniques du Languedoc</publisher-name>; <year>1976</year>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meng</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zeleznik</surname> <given-names>OA</given-names></name>, <name name-style="western"><surname>Thallinger</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Kuster</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gholami</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Culhane</surname> <given-names>AC</given-names></name>. <article-title>Dimension reduction techniques for the integrative analysis of multi-omics data</article-title>. <source>Brief Bioinform</source>. <year>2016</year>;<volume>17</volume>(<issue>4</issue>):<fpage>628</fpage>–<lpage>641</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bib/bbv108" xlink:type="simple">10.1093/bib/bbv108</ext-link></comment> <object-id pub-id-type="pmid">26969681</object-id></mixed-citation></ref>
<ref id="pcbi.1006907.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wattenberg</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Viégas</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>I</given-names></name>. <article-title>How to Use t-SNE Effectively</article-title>. <source>Distill</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.23915/distill.00002" xlink:type="simple">10.23915/distill.00002</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref057"><label>57</label><mixed-citation publication-type="other" xlink:type="simple">Cao Y, Wang L. Automatic Selection of t-SNE Perplexity. arXiv:1708.03229 [Preprint]. 2017 [cited 2019 May 30]. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.03229" xlink:type="simple">https://arxiv.org/abs/1708.03229</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1006907.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debruyne</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hubert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Horebeek</surname> <given-names>JV</given-names></name>. <article-title>Detecting influential observations in Kernel PCA</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2010</year>;<volume>54</volume>(<issue>12</issue>):<fpage>3007</fpage>–<lpage>3019</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csda.2009.08.018" xlink:type="simple">10.1016/j.csda.2009.08.018</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006907.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sandve</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Nekrutenko</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hovig</surname> <given-names>E</given-names></name>. <article-title>Ten Simple Rules for Reproducible Computational Research</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003285" xlink:type="simple">10.1371/journal.pcbi.1003285</ext-link></comment> <object-id pub-id-type="pmid">24204232</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>