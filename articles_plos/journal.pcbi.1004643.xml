<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004643</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01252</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Cultured Cortical Neurons Can Perform Blind Source Separation According to the Free-Energy Principle</article-title>
<alt-title alt-title-type="running-head">Blind Source Separation by Neural Nets In Vitro</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Isomura</surname>
<given-names>Takuya</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kotani</surname>
<given-names>Kiyoshi</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Jimbo</surname>
<given-names>Yasuhiko</given-names>
</name>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Human and Engineered Environmental Studies, Graduate School of Frontier Sciences, The University of Tokyo, Hongo, Bunkyo-ku, Tokyo, Japan</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Research Fellow of Japan Society for the Promotion of Science (JSPS), Kojimachi, Chiyoda-ku, Tokyo, Japan</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Research Center for Advanced Science and Technology, The University of Tokyo, Komaba, Meguro-ku, Tokyo, Japan</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>PRESTO, Japan Science and Technology Agency, Honcho, Kawaguchi-shi, Saitama, Japan</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Department of Precision Engineering, School of Engineering, The University of Tokyo, Hongo, Bunkyo-ku, Tokyo, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Beck</surname>
<given-names>Jeff</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Duke University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: TI. Performed the experiments: TI. Analyzed the data: TI. Wrote the paper: TI KK YJ.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">isomura@neuron.t.u-tokyo.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>21</day>
<month>12</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>12</issue>
<elocation-id>e1004643</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>7</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>11</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Isomura et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004643"/>
<abstract>
<p>Blind source separation is the computation underlying the cocktail party effect––a partygoer can distinguish a particular talker’s voice from the ambient noise. Early studies indicated that the brain might use blind source separation as a signal processing strategy for sensory perception and numerous mathematical models have been proposed; however, it remains unclear how the neural networks extract particular sources from a complex mixture of inputs. We discovered that neurons in cultures of dissociated rat cortical cells could learn to represent particular sources while filtering out other signals. Specifically, the distinct classes of neurons in the culture learned to respond to the distinct sources after repeating training stimulation. Moreover, the neural network structures changed to reduce free energy, as predicted by the free-energy principle, a candidate unified theory of learning and memory, and by Jaynes’ principle of maximum entropy. This implicit learning can only be explained by some form of Hebbian plasticity. These results are the first <italic>in vitro</italic> (as opposed to <italic>in silico</italic>) demonstration of neural networks performing blind source separation, and the first formal demonstration of neuronal self-organization under the free energy principle.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The ‘cocktail party’ effect is a phenomenon by which one is able to pick out and listen to a single person’s speech in a noisy room. In information engineering, this is termed blind source separation. Numerous computational studies demonstrate that simulated neural networks can perform blind source separation. However, if or how a living neural network learns to perform blind source separation remains unknown. Using a microelectrode array (MEA) system that allowed us to apply composite inputs and record responses from neurons throughout a cultured neural network, we discovered that even neurons in cultures of dissociated rat cortical cells can separate individual sources from a complex mixture of inputs in the absence of teacher signals. Given these findings, we then determined that the neural networks adapted to reduce free energy, as predicted by the free energy principle and Jaynes’ principle of maximum entropy. These results provide evidence that cultured neural networks can perform blind source separation and that they are governed by the free-energy principle, providing a compelling framework for understanding how the brain identifies and processes signals hidden in complex multivariate information.</p>
</abstract>
<funding-group>
<funding-statement>This work was partially supported by the Japan Society for the Promotion of Science (<ext-link ext-link-type="uri" xlink:href="https://www.jsps.go.jp/english/" xlink:type="simple">https://www.jsps.go.jp/english/</ext-link>) through Grants-in-Aid for Scientific Research (KAKENHI), Grants 23240065 and 26560202 (YJ), and Grant-in-Aid for JSPS Fellows, Grant 26-8435 (TI). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="0"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files (<xref ref-type="supplementary-material" rid="pcbi.1004643.s004">S1 Dataset</xref>). All spike number train files (detailed data more than 50 MB) are available from our web site (<ext-link ext-link-type="uri" xlink:href="http://neuron.t.u-tokyo.ac.jp/" xlink:type="simple">http://neuron.t.u-tokyo.ac.jp/</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Blind source separation is a problem of separating independent sources from a complex mixture of inputs without knowledge about sources [<xref ref-type="bibr" rid="pcbi.1004643.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref004">4</xref>] and is the computation underlying the cocktail party effect––a phenomenon by which one is able to listen to a single person’s speech in a noisy room [<xref ref-type="bibr" rid="pcbi.1004643.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref008">8</xref>]. Understanding the basis of blind source separation, as well as other learning and memory processes, requires characterization of the underlying functional network architecture. Presumably, this can be directly accomplished by measuring the activity of individual neurons during blind source separation processing to establish the role of each neuron in the network. In practice, this is enormously challenging, given both the large number of neurons that may reside in a network and the technical limitations encountered in attempting to distinguish the activity of neurons that perform blind source separation from others throughout the network. As a result, most studies of blind source separation rely on simulations and on computational models, and the possible electrophysiological basis for any such information processing in real neurons remains poorly understood.</p>
<p>Theoretically, blind source separation is classed as unsupervised learning, a type of learning that does not require teacher signals [<xref ref-type="bibr" rid="pcbi.1004643.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref011">11</xref>]. Blind source separation is modeled as principal component analysis (PCA) [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>], as independent component analysis (ICA) [<xref ref-type="bibr" rid="pcbi.1004643.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>], or as sparse coding [<xref ref-type="bibr" rid="pcbi.1004643.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref016">16</xref>]. These are widely used for signal processing where separation of sources from a complex mixture of inputs is desired. Neural network models that include neurons with linear firing rates can perform PCA, a model that describes how neurons in artificial networks can strengthen or weaken their interconnections over time [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>]. In contrast, ICA, which can be represented using model neurons with nonlinear firing rates [<xref ref-type="bibr" rid="pcbi.1004643.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>], maximizes Shannon entropy among outputs in order to detect several independent sources, thus separating a multivariate signal into individual components. The sparse coding model detects independent sources [<xref ref-type="bibr" rid="pcbi.1004643.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref016">16</xref>] using a calculation similar to that proposed by the predictive coding hypothesis of the cerebral cortex [<xref ref-type="bibr" rid="pcbi.1004643.ref017">17</xref>]. What all these models of unsupervised learning have in common is that they can be implemented with a form of Hebbian or associative plasticity [<xref ref-type="bibr" rid="pcbi.1004643.ref018">18</xref>] and that they are instances of the free energy principle––a candidate unified theory of learning and memory [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. Moreover, blind source separation, whether by PCA, ICA, or sparse coding, is one of the simplest problems that the free-energy principle addresses. Additionally, numerous computational studies have demonstrated that simulated neural networks can perform blind source separation. PCA, ICA, and sparse coding have been demonstrated in both firing-rate models and spiking-neuron models [<xref ref-type="bibr" rid="pcbi.1004643.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref027">27</xref>]. However, although early studies indicated that cortical neurons might use an ICA-like signal processing strategy for sensory perception [<xref ref-type="bibr" rid="pcbi.1004643.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref008">8</xref>] and described the relationship of sparse- and predictive coding to biological properties [<xref ref-type="bibr" rid="pcbi.1004643.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref029">29</xref>], examinations of the neural basis of ICA-like learning are few.</p>
<p>Experimental studies on <italic>in vivo</italic> or <italic>in vitro</italic> networks have demonstrated that neural networks can perform learning and memory tasks, when learning is defined as the process of changing activity or behavior by experiencing something, as it is in this study. One of the simplest networks can be constructed from actual cultured neurons, and such real neural networks can exhibit stimulation-dependent synaptic plasticity [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref031">31</xref>], supervised learning [<xref ref-type="bibr" rid="pcbi.1004643.ref032">32</xref>], adaptation to inputs [<xref ref-type="bibr" rid="pcbi.1004643.ref033">33</xref>], associative memory [<xref ref-type="bibr" rid="pcbi.1004643.ref034">34</xref>], aspects of logical operation [<xref ref-type="bibr" rid="pcbi.1004643.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref036">36</xref>], short-term memory [<xref ref-type="bibr" rid="pcbi.1004643.ref037">37</xref>], and homeostatic plasticity [<xref ref-type="bibr" rid="pcbi.1004643.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref039">39</xref>]. However, it is uncertain whether these biological neural networks can perform blind source separation. Previously, we have used the microelectrode array (MEA) to simultaneously stimulate and record from multiple neurons over long periods [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref040">40</xref>]. The MEA enables random electrical stimulation from 64 electrodes in parallel and the recording of evoked spikes immediately after each stimulation. Thus, by varying probabilities during stimulation trains, the MEA makes it possible to apply spatiotemporal inputs synthesized from hidden sources while measuring the response evoked from the entire neural network. Through this capability, we demonstrate here that cultured rat cortical neurons receiving multiple inputs can perform blind source separation, thereby providing an <italic>in vitro</italic> model of neural adaptation.</p>
<p>In brief, our approach consisted of two parts. First, we tried to establish whether single neuron responses preferred mixtures of sources or the individual sources per se. To address this, we examined the Kullback-Leibler divergence [<xref ref-type="bibr" rid="pcbi.1004643.ref011">11</xref>] between the probabilities of neuronal responses conditioned upon one of two sources. We hoped to see that neurons were able to discriminate between sources rather than mixtures, because this would imply a blind source separation––or the inversion of a generative model of stimulation patterns (i.e., sources). We were able to show that neurons preferred hidden sources, as opposed to mixtures of sources. This then allowed us to quantify the probabilistic encoding of sources by assuming that the expected amplitude of each hidden source was encoded by the mean activity of neuronal populations preferring one source or the other. By assuming a rate coding model, where mean firing rates encode the mean of a mixture of Gaussians, we were able to compute the variational free energy of the neuronal encodings in terms of energy and entropy. Crucially, the free energy principle suggests that with learning, energy should decrease and entropy should increase (where the free energy is the difference) [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. In this instance, the energy can be thought of as level of prediction error. Conversely, the entropy refers to the average uncertainty of the encoding. According to Jaynes’ maximum entropy principle [<xref ref-type="bibr" rid="pcbi.1004643.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref042">42</xref>], entropy should increase to ensure a generalizable inference that is in accordance with Occam’s principle. In short, we hoped to see an increase in the entropy of the probabilistic encoding that was offset by a decrease in energy (an increase in accuracy)––producing an overall decrease in free energy.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Generation and definition of neural stimuli</title>
<p>Rat cortical cells were cultured on MEAs (<xref ref-type="fig" rid="pcbi.1004643.g001">Fig 1A and 1B</xref>) and electrical stimulation and recordings were conducted. Typical stimulus-evoked responses of cultured neural networks recorded at the stimulated electrode are shown in <xref ref-type="fig" rid="pcbi.1004643.g001">Fig 1C</xref>. In accordance with previous studies, we observed tri-phasic responses [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref040">40</xref>].</p>
<fig id="pcbi.1004643.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Description of the cultured neuron network paradigm.</title>
<p>Panels show culture dish (top left), microscopic view of rat cortical cells (top right), and evoked voltage trace from one electrode (bottom). <bold>(A)</bold> Image of a microelectrode array (MEA) dish. Cells were seeded in the center of the MEA dishes. The microelectrode sampling frequency was 25 kHz and a 500–2000 Hz band-pass filter was applied to the recordings. <bold>(B)</bold> Phase-contrast microscopic images of cultured rat cortical cells on MEA dishes after 52 days in culture. Note the high concentration of cells near the electrode terminals. Black squares are electrodes. <bold>(C)</bold> A typical waveform of the extracellular potential. After a biphasic-pulse electrical stimulation (<italic>τ</italic> = 0), several stimulation-evoked spikes were observed at the stimulated electrode. The dashed line indicates the detection threshold. Red circles indicate detected spikes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g001" xlink:type="simple"/>
</fig>
<p>To study ICA-like learning in networks created in these neuronal cultures, we designed a generative process constructed from two independent binary sources <bold>u</bold>(<italic>t</italic>) = (<italic>u</italic><sub>1</sub>(<italic>t</italic>), <italic>u</italic><sub>2</sub>(<italic>t</italic>))<sup><italic>T</italic></sup> ∈ {0,1}, 32 inputs produced by the MEA <bold>s</bold>(<italic>t</italic>) = (<italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>))<sup><italic>T</italic></sup>, and a 32×2 matrix <italic>A</italic>, where (<italic>A</italic><sub>i1</sub>, <italic>A</italic><sub>i2</sub>) = (<italic>a</italic>, 1–<italic>a</italic>) for <italic>i</italic> = 1, …, 16 and (<italic>A</italic><sub>i1</sub>, <italic>A</italic><sub>i2</sub>) = (1–<italic>a</italic>, <italic>a</italic>) for <italic>i</italic> = 17, …, 32 (<xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2A</xref>). Note that <italic>t</italic> [s] is discrete time (a natural number) between 1 and 256.</p>
<fig id="pcbi.1004643.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Schematic images of experimental protocol.</title>
<p>The signal distribution scheme (top), signal generation protocol (bottom left), and training timeline (bottom right) are shown. <bold>(A)</bold> A schematic image of stimulation signals and neurons on an MEA. <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> are hidden sources (left), <italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub>16</sub> and <italic>s</italic><sub>17</sub>, …, <italic>s</italic><sub>32</sub> are two groups of inputs synthesized by computing weighted combinations of the sources (middle), and <italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub>64</sub> are the measures at the electrodes (right). By manipulating the inputs to the MEA, we induced an ICA-like processing system in neuronal cultures. Specifically, we created four conditions of stimulation by independently varying the probabilities of two binary hidden signals (given by a 2 × 2 matrix) such that neurons received no signal (0,0), a signal in which the two hidden signals (0,1) or (1,0) were differentially weighted, or a fully merged signal (1,1). The schematic heads illustrate the analogy between the experimental setup and the cocktail party effect: <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> are analogous to two voices to identify (left), the mapping from <bold>u</bold> to <bold>s</bold> (a matrix <italic>A</italic>) is analogous to the addition of background noise at a cocktail party, so that <bold>s</bold> represents sounds (sensory inputs) heard from the left and right ears (middle), and <bold>x</bold> is analogous to the listener’s auditory system performing blind source separation (right). <bold>(B)</bold> A schematic image of the generation of <bold>s</bold>(<italic>t</italic>) from <italic>u</italic><sub>1</sub>(<italic>t</italic>) and <italic>u</italic><sub>2</sub>(<italic>t</italic>). Each random variable <italic>ω</italic> is independently generated from a uniform distribution (0 ≤ <italic>ω</italic> &lt; 1). <italic>u</italic><sub>1</sub>(<italic>t</italic>) and <italic>u</italic><sub>2</sub>(<italic>t</italic>) will be 1 if <italic>ω</italic> &lt; <italic>ρ</italic> or 0 otherwise. Then, <italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>16</sub>(<italic>t</italic>) will be <italic>u</italic><sub>1</sub>(<italic>t</italic>) if <italic>ω</italic> &lt; <italic>a</italic> or <italic>u</italic><sub>2</sub>(<italic>t</italic>) otherwise. In contrast, <italic>s</italic><sub>17</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>) will be <italic>u</italic><sub>1</sub>(<italic>t</italic>) if <italic>ω</italic> &lt; 1–<italic>a</italic> or <italic>u</italic><sub>2</sub>(<italic>t</italic>) otherwise. The discrete time <italic>t</italic> is over one and 256. <bold>(C)</bold> A training timeline. As electrodes on the MEA are distributed as an 8 × 8 matrix, we illustrate the stimulating sites corresponding to <italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>) on 8 × 8 matrices. Thus, half (32) were dual-use electrodes of stimulating and recording, while the remaining 32 were for recording only. Red or blue squares indicate the electrode stimulated in a given time period, which is provided from <italic>u</italic><sub>1</sub>(<italic>t</italic>) or <italic>u</italic><sub>2</sub>(<italic>t</italic>), respectively. A trial is composed of 256 stimulation patterns with 1-s intervals. Overall, the training period is composed of 100 trials, where the stimulation pattern is common for all trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g002" xlink:type="simple"/>
</fig>
<p>In brief, we had an array of (8×8) 64 recording electrode sites of which half (32) were stimulated. The remaining 32 were for recording neural activities at non-stimulated electrodes. The detailed neural response properties are discussed in the next section. The stimuli were formed by mixing two underlying patterns, or hidden sources, to create stochastic stimulus patterns. These were mixed separately for each of two groups of 16 stimulation electrodes, such that the stimulation pattern comprised of probabilistic mixtures of the underlying sources. The responses from the 64 electrodes and 23 cultures were pooled, yielding over 1000 electrode responses to various mixtures of hidden sources.</p>
<p>In other words, <bold>u</bold>(<italic>t</italic>) was generated from the stationary Poisson process, while <bold>s</bold>(<italic>t</italic>) obeyed the non-stationary Poisson process with the time varying intensity of <italic>A</italic> <bold>u</bold>(<italic>t</italic>). The generative model ensured that the two sources contributed to the stimuli with an equal probability <italic>ρ</italic>. We used mixtures of these sources to produce stimulus patterns that contained no signal, one of the two sources, and a fully mixed source. Unless specifically mentioned, we used <italic>ρ</italic> = 1/2 and <italic>a</italic> = 3/4. Electrical stimulations with 256-s pulse trains were applied at 1-s intervals for 100 trials. A schematic image of how inputs <bold>s</bold>(<italic>t</italic>) were obtained from sources <bold>u</bold>(<italic>t</italic>) is shown in <xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2B and 2C</xref>. A detailed description is provided in the <xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2</xref> legend and the Methods section.</p>
</sec>
<sec id="sec004">
<title>Evoked responses show preferences to individual stimuli</title>
<p>Neural responses evoked by the input trains were recorded using a 64-electrode MEA. We used 23 cultures for a training group and a total of 37 cultures as control groups. We performed 100 trials (500 s for 1 trial; about 14 h in total) for each culture. An overview of the experimental paradigm is shown in <xref ref-type="fig" rid="pcbi.1004643.g003">Fig 3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004643.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004643.s002">S2</xref> Movies. A raster plot and post stimulus time histogram (PSTH) detailing the spike timing of evoked response recorded at a representative electrode (<italic>x</italic><sub><italic>i</italic></sub>(<italic>τ</italic>); <italic>τ</italic>, continuous time) is shown in <xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4A and 4B</xref>. Evoked response increased immediately after each stimulation for both stimulated and non-stimulated neuron groups. The peak of evoked responses was observed 10-to-20 ms after each stimulation in all trials. Compared to the results of the first trial (<xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4A</xref>), the evoked response for the hidden source of <bold>u</bold> = (0,1) (blue curve) decreased after the training stimulation (<xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4B</xref>), indicating that neurons recorded at this electrode tuned their activity to only respond to the (1,0) and (1,1) states, i.e., only to <italic>u</italic><sub>1</sub>.</p>
<fig id="pcbi.1004643.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Overview of the experiment.</title>
<p>Snapshots of <xref ref-type="supplementary-material" rid="pcbi.1004643.s001">S1 Movie</xref> at the <bold>u</bold> = (0,0), (1,0), (0,1), and (1,1) state, respectively, are shown. Setup is with the same as that described in <xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2A</xref>: hidden sources <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub> (left), merged inputs <italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub>16</sub> (middle top), <italic>s</italic><sub>17</sub>, …, <italic>s</italic><sub>32</sub> (middle bottom), and unit activities of cultured neurons <italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub>64</sub> (right). Each of the 64 panels on the right shows a raster plot of neural activity (unit activity) recorded at the electrode, where horizontal and vertical axes are 400 ms time window and trials 1–10, respectively. <bold>(A)</bold> When <bold>u</bold> = (0,0), evoked responses were not observed since there was no input, although spontaneous activities were recorded. <bold>(B)</bold> When <bold>u</bold> = (1,0), a group of <italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub>16</sub>, became 1 (red circles in the middle) with a high probability (namely, <italic>a</italic> = 3/4 probability), while a group of <italic>s</italic><sub>17</sub>, …, <italic>s</italic><sub>32</sub> became 1 with a low probability (1–<italic>a</italic> = 1/4). If <italic>s</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, …, 32) was 1, an electrical pulse stimulation was induced into a fixed corresponding electrode. Consequently, evoked responses were observed immediately after each stimulation. <bold>(C)</bold> When <bold>u</bold> = (0,1), a situation exactly opposite to that described in (B) occurs. <bold>(D)</bold> When u = (1,1), all stimulated electrodes (<italic>s</italic><sub>1</sub>, …, <italic>s</italic><sub>32</sub>) were stimulated, providing the largest evoked response.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g003" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004643.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Examples of neural responses to different stimulus inputs.</title>
<p>Raster plots of successive evoked responses before and after learning (A and B) for four source states; spikes-per-event transition over learning, average and histogram, for four source states and two different electrodes (C and D). <bold>(A)</bold> Top panel: A raster plot showing a typical pattern of stimulation-evoked spikes in cultured neurons recorded with an electrode at trial 1 (before training). Red circles indicate the timing of spikes. The horizontal axis corresponds to time (ms), and the vertical axis is the stimulation number sorted by source state. White, red, blue, and green areas indicate responses when the state of the source was <bold>u</bold> = (0,0), (1,0), (0,1), and (1,1), respectively. Between <italic>τ</italic> = –3 and 3 ms (area surrounded by dashed lines), reliable data were not obtained because of switching noise (artifact). Bottom panel: Post stimulus time histogram (PSTH) at trial 1. Black, red, blue, and green curves are PSTH when the state of the source was <bold>u</bold> = (0,0), (1,0), (0,1), and (1,1), respectively. <bold>(B)</bold> Same as (A), but at trial 100 (after training). <bold>(C)</bold> Left panel: example of a typical transition over trials of the conditional expectation of an evoked response recorded with the same electrode as in (A) and (B). Black, red, blue, and green circles give the conditional expectation of evoked responses when the state of the source was <bold>u</bold> = (0,0), (1,0), (0,1), and (1,1), respectively. Center and right panels: The conditional probability distributions of evoked responses recorded with the same electrode during trials 1 to 10 (center panel) and trials 91 to 100 (right panel). The four curves correspond to the four states of <bold>u</bold>. <bold>(D)</bold> Same as (C), but recorded with a different electrode.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g004" xlink:type="simple"/>
</fig>
<p>According to previous studies, the directly evoked responses occur immediately after stimulation and their jitters are relatively small; thus, large numbers of spikes that appear more than 10 ms after stimulation are generated by synaptic inputs [<xref ref-type="bibr" rid="pcbi.1004643.ref043">43</xref>]. Therefore, the change in number of evoked spikes generated 10–30 ms after each stimulation, defined as evoked response, occurred gradually over training (<xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4C</xref> left). The center and right panels in <xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4C</xref> illustrates a typical transition of a conditional probability distribution of evoked responses, i.e., the number of evoked spikes recorded at the electrode before and after training. In this case, a typical shift of a peak of the (0,1) type (blue curve) is presented. <xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4D</xref> shows the transition of responses over training at another stimulated electrode. In contrast to <xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4C</xref>, a shift of a peak of the (1,0) type (red curve) is shown. The transition of response at each electrode can be found in <xref ref-type="supplementary-material" rid="pcbi.1004643.s004">S1 Dataset</xref>.</p>
<p>These results suggested that neurons near stimulated electrodes had preferences to one of the two hidden signals, but not the other. Specifically, most neurons from electrodes 1–16 (<italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub>16</sub>) preferred <italic>u</italic><sub>1</sub> signals (neurons were activated more largely when <bold>u</bold> = (1,0) than when <bold>u</bold> = (0,1)), most neurons from electrodes 17–32 (<italic>x</italic><sub>17</sub>, …, <italic>x</italic><sub>32</sub>) preferred <italic>u</italic><sub>2</sub> signals, and most neurons at electrodes 33–64 (non-stimulated; <italic>x</italic><sub>33</sub>, …, <italic>x</italic><sub>64</sub>) showed no preference (<xref ref-type="fig" rid="pcbi.1004643.g005">Fig 5A and 5B</xref>). Note that <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> indicates the conditional expectation with the source state <bold>u</bold> and <inline-formula id="pcbi.1004643.e001"><alternatives><graphic id="pcbi.1004643.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is its over-trial average. Neurons near stimulated electrodes exhibited larger responses compared to these near non-stimulated electrodes. In <italic>u</italic><sub>1</sub>-preferring neurons, the increase in response strength was larger when the state of the source was <bold>u</bold> = (1,0) than when it was <bold>u</bold> = (0,1) (<xref ref-type="fig" rid="pcbi.1004643.g005">Fig 5C and 5D</xref>), while the exact opposite alteration profile was observed in <italic>u</italic><sub>2</sub>-preferring neurons (<xref ref-type="fig" rid="pcbi.1004643.g005">Fig 5E and 5F</xref>). Moreover, at 50 electrodes out of 371 <italic>u</italic><sub>1</sub>-preferring electrodes, <inline-formula id="pcbi.1004643.e002"><alternatives><graphic id="pcbi.1004643.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> was 3 times larger than <inline-formula id="pcbi.1004643.e003"><alternatives><graphic id="pcbi.1004643.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, and at 44 electrodes out of 345 <italic>u</italic><sub>2</sub>-preferring electrodes, <inline-formula id="pcbi.1004643.e004"><alternatives><graphic id="pcbi.1004643.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> was 3 times larger than <inline-formula id="pcbi.1004643.e005"><alternatives><graphic id="pcbi.1004643.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> as all trial average (<xref ref-type="supplementary-material" rid="pcbi.1004643.s006">S1A Fig</xref>). Additionally, the number of such electrodes increased during training (<xref ref-type="supplementary-material" rid="pcbi.1004643.s006">S1B Fig</xref>). If a neuron responded to <italic>s</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, …, 16), <inline-formula id="pcbi.1004643.e006"><alternatives><graphic id="pcbi.1004643.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> should be 3 times as large as <inline-formula id="pcbi.1004643.e007"><alternatives><graphic id="pcbi.1004643.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> by the relationship between <italic>s</italic><sub><italic>i</italic></sub> and <bold>u</bold>, while if a neuron responded to <italic>s</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 17, …, 32), <inline-formula id="pcbi.1004643.e008"><alternatives><graphic id="pcbi.1004643.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> should be 3 times as large as <inline-formula id="pcbi.1004643.e009"><alternatives><graphic id="pcbi.1004643.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, this indicates that at approximately 13% of <italic>u</italic><sub>1</sub>- or <italic>u</italic><sub>2</sub>-preferring electrodes, neural responses (<italic>x</italic><sub><italic>i</italic></sub>) were more likely to be determined by the state of hidden sources (<bold>u</bold>) rather than by induced stimulation itself (<italic>s</italic><sub><italic>i</italic></sub>) in the strict sense of the word. Taken together, these results suggest that neural responses were more likely determined by the state of hidden sources estimated based on inputs from multiple electrodes, termed source-coding, rather than the input from an electrode, e.g., the nearest electrode.</p>
<fig id="pcbi.1004643.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Response preference all-trial summary, transitions, and overall changes.</title>
<p><bold>(A)</bold> Response preference. Horizontal and vertical axes are the all-trial averages of the expectation of response when <bold>u</bold> = (1,0) and (0,1), respectively. Red circles are responses recorded with electrodes <italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub>16</sub>, blue circles, with electrodes <italic>x</italic><sub>17</sub>, …, <italic>x</italic><sub>32</sub>, and black circles, with electrodes <italic>x</italic><sub>33</sub>, …, <italic>x</italic><sub>64</sub>. Circles are superimposed data from all cultures (<italic>n</italic> = 1035 electrodes from 23 cultures; the total number of electrodes was 1472, but 437 electrodes were not available, see (B)). The solid line is the identity line. Dashed lines indicate ± 0.5 spike/event. <bold>(B)</bold> The expectation of the numbers of four types of electrode responses in a culture. Red, blue, and gray correspond to the number of electrodes that record <italic>u</italic><sub>1</sub>-preferring, <italic>u</italic><sub>2</sub>-preferring, and neither-preferring responses, respectively. White shows the number of electrodes that were not available or suitable for analysis because of insufficient spikes. Neural activities were recorded from the majority of the electrodes. <bold>(C)</bold> Transitions of the expectation of response averaged over just the <italic>u</italic><sub>1</sub>-preferring electrodes. As in <xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4C</xref>, the four curve colors correspond to the four states of <bold>u</bold>. <bold>(D)</bold> The change in responses between trials 1 and 100. Horizontal and vertical axes plot the difference in the conditional expectations between trials 1 and 100 when <bold>u</bold> = (1,0) and (0,1), respectively. At <italic>u</italic><sub>1</sub>-preferring electrodes, the increase of <italic>x</italic><sub><italic>i</italic></sub><sup>1,0</sup> was significantly greater than that of <italic>x</italic><sub><italic>i</italic></sub><sup>0,1</sup> (****, <italic>p</italic> &lt; 10<sup>−15</sup>; <italic>n</italic> = 371 electrodes from 23 cultures). The dashed diagonal line is the identity line. <bold>(E)</bold> and <bold>(F)</bold> Same as (C) and (D), but averaged over just the <italic>u</italic><sub>2</sub>-preferring electrodes. At <italic>u</italic><sub>2</sub>-preferring electrodes, the increase of <italic>x</italic><sub><italic>i</italic></sub><sup>0,1</sup> was significantly larger than that of <italic>x</italic><sub><italic>i</italic></sub><sup>1,0</sup> (****, <italic>p</italic> &lt; 10<sup>−15</sup>; <italic>n</italic> = 345 electrodes from 23 cultures). In (C) and (E), shadowed areas are S.E.M.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Increased response specificity to discrete stimuli in cultured neuron networks</title>
<p>The difference between the probability distribution at <bold>u</bold> = (1,0) and (0,1) is a well-established criterion to evaluate response preference, which in information theory is often defined by the Kullback-Leibler divergence (KLD) [<xref ref-type="bibr" rid="pcbi.1004643.ref011">11</xref>]. We calculated KLD of the evoked response at each electrode under the assumption that these conditional probabilities conformed to a Poisson distribution. We observed a significant change in KLD (represented as <italic>D</italic><sub><italic>KLi</italic></sub>, where <italic>i</italic> = 1, ……, 64 is the index of electrodes) between distributions given the (1,0) state and (0,1) state (<italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold> = (1,0)) and <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold> = (0,1), respectively). The values of <italic>D</italic><sub><italic>KLi</italic></sub> were increased in some electrodes after the training period (red circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6A</xref>), where trained neuron cultures are labeled as TRN. Moreover, the mean values for <italic>D</italic><sub><italic>KLi</italic></sub> averaged across all recording electrodes increased after training (<xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6B and 6C</xref>). The increase in the value of <italic>D</italic><sub><italic>KLi</italic></sub> in trained neuron cultures in the presence of 20 μM 2-Amino-5-phosphonopentanoic acid (APV), an N-methyl-D-aspartic acid (NMDA)-receptor inhibitor, was significantly smaller than in nontreated TRN cultures (black circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6B</xref>; ****, <italic>p</italic> &lt; 10<sup>−5</sup>). We confirmed that the alterations in KLD were maintained for a long time by comparing continuously stimulated trained neurons to partially trained (PRT) neurons. PRT neurons were trained for only 10 trials, then went unstimulated for 18–24 h (i.e. the resting period), and then went through 10 additional training trials. In PRT cultures, the values of <italic>D</italic><sub><italic>KLi</italic></sub> at trial 91 (i.e., first trial after the resting period) were significantly larger than that at trial 1 (<xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6D</xref>); however, the difference was significantly smaller than the difference in <italic>D</italic><sub><italic>KLi</italic></sub> observed between trial 1 and 91 in TRNs (white circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6B</xref>; ****, <italic>p</italic> &lt; 10<sup>−4</sup>). Interestingly, the values of <italic>D</italic><sub><italic>KLi</italic></sub> at trial 100 in PRTs were almost same level as that at trial 100 in TRNs (<italic>p</italic> = 0.268). The transition of KLD at each electrode can be found in <xref ref-type="supplementary-material" rid="pcbi.1004643.s004">S1 Dataset</xref>.</p>
<fig id="pcbi.1004643.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Alterations in Kullback-Leibler divergence (KLD) indicate distinct response sensitivity to different source stimuli.</title>
<p>Panels show typical (A) and mean (B) KLD transition, overall change in full (C) and partial (D) training, and transitions with alternative parameter settings (E and F). <bold>(A)</bold> Typical transition of the KLD recorded with an electrode. Red circles are KLDs recorded in a trained culture (TRN). Black circles are KLDs recorded in a culture trained in the presence of APV. White circles are KLDs recorded in a partially trained culture (PRT), where the culture was trained for 10 trials, then not stimulated for 18–24 h, then trained for a further 10 trials. <bold>(B)</bold> The mean transition of the KLD. Red circles are the mean KLDs averaged over electrodes in the TRN group (<italic>n</italic> = 1035 electrodes from 23 cultures). Black circles are the mean KLDs in the presence of APV (<italic>n</italic> = 435 electrodes from 9 cultures). White circles are the mean KLDs in the PRT group (<italic>n</italic> = 473 electrodes from 10 cultures). At trial 100, KLDs in TRN were significantly larger than those in APV (****, <italic>p</italic> &lt; 10<sup>−5</sup>). <bold>(C)</bold> The change of KLDs in TRN (trial 1 vs. trial 100). Circles are KLDs for each electrode. The dashed line is the identity line. KLDs significantly increased after training (****, <italic>p</italic> &lt; 10<sup>−15</sup>). <bold>(D)</bold> The change in the KLDs in PRT (trial 1 vs. trial 91). KLDs at trial 91 were significantly larger than those at trial 1 (****, <italic>p</italic> &lt; 10<sup>−4</sup>), indicating that the increase in KLD was maintained over the resting time. <bold>(E)</bold> The mean transition of the KLD with alternative parameter settings. Red, white, and black circles are KLDs with (merged balance (<italic>a</italic>), source firing probability (<italic>ρ</italic>)) = (3/4, 1/4), (3/4, 3/4), and (1/2, 1/2), respectively. The black curve is the mean KLD in the TRN group. <bold>(F)</bold> The mean transition of the KLD. Circles are KLDs with (<italic>a</italic>, <italic>ρ</italic>) = (1, 1/2). In (B), (E), and (F), bars are S.E.M.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g006" xlink:type="simple"/>
</fig>
<p>KLD was affected by the merged balance of inputs (<italic>a</italic>) and the frequency of inputs (<italic>ρ</italic>). Specifically, we varied input balance by comparing the change of the <italic>a</italic>:1–<italic>a</italic> = 3/4:1/4 balance condition with that of the 0:1 and 1/2:1/2 balance conditions and the source condition with a <italic>ρ</italic> = 1/2 probability with a 1/4 and 3/4 probability (<xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6E and 6F</xref>). Compared to the initial values (trial 1 vs. trial 100), KLD was not altered by inputs with 1/2:1/2 ratio of merged balance ((<italic>a</italic>, <italic>ρ</italic>) = (1/2, 1/2)) (black circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6E</xref>; <italic>p</italic> = 0.515; <italic>n</italic> = 147 from 4 cultures), suggesting that input variance was necessary to elicit these changes. As both PCA and ICA rely on variations of input, these results are consistent with the hypothesis that cultured neural networks use ICA-like signal processing. When sources summed to one with a probability of 1/4, i.e., (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 1/4) (red circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6E</xref>), KLD increased after training (***, <italic>p</italic> &lt; 10<sup>−3</sup>; <italic>n</italic> = 139 from 4 cultures; trial 1 vs. trial 100). Similarly, when (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 3/4) (white circles in <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6E</xref>), KLD increased after training (****, <italic>p</italic> &lt; 10<sup>−7</sup>; <italic>n</italic> = 234 from 6 cultures; trial 1 vs. trial 100). The change in KLD with (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 1/4) was slightly smaller than when (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 1/2) (<italic>p</italic> = 0.469, at trial 100), while the change in KLD with (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 3/4) was slightly larger than when (<italic>a</italic>, <italic>ρ</italic>) = (3/4, 1/2) (<italic>p</italic> = 0.166, at trial 100). When the input balance was 1:0 (not merged; (<italic>a</italic>, <italic>ρ</italic>) = (1, 1/2)), a large increase of KLD was observed (<xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6F</xref>; ****, <italic>p</italic> &lt; 10<sup>−4</sup>; <italic>n</italic> = 161 from 4 cultures; trial 1 vs. trial 100), which is an analog of conventional pattern separation [<xref ref-type="bibr" rid="pcbi.1004643.ref034">34</xref>]. Note that to calculate <xref ref-type="fig" rid="pcbi.1004643.g006">Fig 6F</xref>, when the change in KLD form trial 1 was larger than 10 or smaller than –10, it was shifted to 10 or –10, respectively.</p>
</sec>
<sec id="sec006">
<title>A recognition model used by cultured neural networks</title>
<p>We then set out to build a population-based model of neural network assembly based on our experimental paradigm. We defined the population model as <inline-formula id="pcbi.1004643.e010"><alternatives><graphic id="pcbi.1004643.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004643.e011"><alternatives><graphic id="pcbi.1004643.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004643.e012"><alternatives><graphic id="pcbi.1004643.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represent mean evoked responses of neurons in <italic>u</italic><sub>1</sub>- and <italic>u</italic><sub>2</sub>-preferring neuron groups in each culture preparation. Distribution of <inline-formula id="pcbi.1004643.e013"><alternatives><graphic id="pcbi.1004643.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> at trial 1 and 100 are shown in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7A and 7B</xref>, which represents the recognition density [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>] of <inline-formula id="pcbi.1004643.e014"><alternatives><graphic id="pcbi.1004643.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e015"><alternatives><graphic id="pcbi.1004643.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold" stretchy="true">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula>. Alterations observed in <inline-formula id="pcbi.1004643.e016"><alternatives><graphic id="pcbi.1004643.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold" stretchy="true">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> over the trial periods are show in <xref ref-type="supplementary-material" rid="pcbi.1004643.s003">S3 Movie</xref>. Notably, the total evoked response from all available electrodes (<inline-formula id="pcbi.1004643.e017"><alternatives><graphic id="pcbi.1004643.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>) was almost proportional to the total input (i.e., the number of stimulated electrodes) (<xref ref-type="supplementary-material" rid="pcbi.1004643.s007">S2A Fig</xref>).</p>
<fig id="pcbi.1004643.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Population activity of cultured neural networks.</title>
<p>Absence of stimulus classification before learning (A) and its presence after (B); summary of classification statistics (C); summary of the inverse recognition model (D) and dynamics of the inverse recognition model (E). <bold>(A)</bold> Evoked responses of populations of cultured neurons before training. Horizontal and vertical axes are the averaged responses of <italic>u</italic><sub>1</sub>- and <italic>u</italic><sub>2</sub>-preferring electrodes (<inline-formula id="pcbi.1004643.e018"><alternatives><graphic id="pcbi.1004643.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004643.e019"><alternatives><graphic id="pcbi.1004643.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>). Red, blue, and green indicate evoked responses when the state of <bold>u</bold> is (1,0), (0,1), and (1,1), respectively. The Fig corresponds to a superimposition of responses at <italic>t</italic> = 1, …, 256 from 23 cultures. Plus-marks and ellipses are the means and standard deviations of <inline-formula id="pcbi.1004643.e020"><alternatives><graphic id="pcbi.1004643.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> given <bold>u</bold> averaged over 23 cultures. The scale indicates the averaged spike number per stimulation. <bold>(B)</bold> Evoked responses after training. The evoked response transient throughout training is shown in <xref ref-type="supplementary-material" rid="pcbi.1004643.s003">S3 Movie</xref>. <bold>(C)</bold> The distribution of correlation. Red circles plot the correlation of <italic>x</italic><sub><italic>i</italic></sub> with <italic>u</italic><sub>1</sub> (horizontal) and <italic>u</italic><sub>2</sub> (vertical) (<italic>n</italic> = 1035 electrodes from 23 cultures). Black circles plot the correlation of <italic>x</italic><sub><italic>i</italic></sub> with <italic>e</italic><sub>1</sub> (horizontal) and <italic>e</italic><sub>2</sub> (vertical), where <italic>e</italic><sub>1</sub> and <italic>e</italic><sub>2</sub> are the error of <italic>x</italic><sub><italic>i</italic></sub> from <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub>, respectively. <bold>(D)</bold> Schematic image of a population model under the assumptions of the inverse recognition model. Our model assumes that <inline-formula id="pcbi.1004643.e021"><alternatives><graphic id="pcbi.1004643.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> is a column vector of inputs, <inline-formula id="pcbi.1004643.e022"><alternatives><graphic id="pcbi.1004643.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula><bold>’</bold> and <inline-formula id="pcbi.1004643.e023"><alternatives><graphic id="pcbi.1004643.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> are column vectors of the population activity of neuron groups, and <italic>W</italic> is a 2 × 2 matrix of connection strengths. Also, we assume that <inline-formula id="pcbi.1004643.e024"><alternatives><graphic id="pcbi.1004643.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> can be represented as a multiplication of the connection strength matrix <italic>W</italic> by <inline-formula id="pcbi.1004643.e025"><alternatives><graphic id="pcbi.1004643.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula><bold>’</bold> (linear, firing-rate neuron model). Based on the recognition model, <italic>W</italic> was calculated from the relationship between the amplitude of the stimulation and the evoked response using the maximum likelihood estimation. <bold>(E)</bold> Dynamics of the inverse recognition model. Upper, middle, and lower time courses represent input, direct response, and synaptic response, respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g007" xlink:type="simple"/>
</fig>
<p>Early computational studies proposed several learning models (recognition models) employing blind source separation. These models can be roughly separated into two types: the inverse recognition model [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref044">44</xref>] and the feed-forward recognition model [<xref ref-type="bibr" rid="pcbi.1004643.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. Considering the fact that inputs <bold>s</bold> were instantaneously induced in cultured neural networks and evoked responses recorded at stimulated electrodes decreased 20–30 ms after each stimulation (<xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4A and 4B</xref>), the feed-forward recognition model was not suitable in this situation, as it requires the dynamics of neural networks to converge towards an equilibrium state for learning. Moreover, large populations of neurons that we observed were state-coding and correlated with sources (<bold>u</bold>) (96.2% of electrodes were corr(<italic>x</italic><sub><italic>i</italic></sub>, <italic>u</italic><sub>1</sub>) &gt; 0.4 or corr(<italic>x</italic><sub><italic>i</italic></sub>, <italic>u</italic><sub>2</sub>) &gt; 0.4), while only a small population of neurons were correlated with estimation errors (<italic>e</italic><sub>1</sub> or <italic>e</italic><sub>2</sub>, where <italic>e</italic><sub>1</sub> and <italic>e</italic><sub>2</sub> are estimation errors of <italic>x</italic><sub><italic>i</italic></sub> from <italic>u</italic><sub>1</sub> and <italic>u</italic><sub>2</sub>; only 1.8% of neurons were |corr(<italic>x</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub>1</sub>)| &gt; 0.4 or |corr(<italic>x</italic><sub><italic>i</italic></sub>, <italic>e</italic><sub>2</sub>)| &gt; 0.4) (<xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7C</xref>). Therefore, our results indicated that the recognition model used by cultured neural networks is more consistent with the inverse model, as the inverse model does not require the equilibrium state of <inline-formula id="pcbi.1004643.e026"><alternatives><graphic id="pcbi.1004643.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> or the existence of error-coding neurons. Based on this evidence, we generated an inverse recognition model of cultured neural networks, as we show in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7D</xref>. Schematic images of the model’s dynamics are shown in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7E</xref>. Taken together our results indicated that cultured neural networks implement ICA-like learning and that their dynamics can be described by an inverse recognition model.</p>
</sec>
<sec id="sec007">
<title>Connection strengths are altered according to the principle of free energy minimization</title>
<p>Estimations of effective connectivity help in understanding neural dynamics [<xref ref-type="bibr" rid="pcbi.1004643.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref046">46</xref>]. To estimate parameters of the inverse model from observed evoked responses, we calculated the maximum likelihood estimator of connectivity <italic>W</italic> (a 2×2 matrix) to analyze the averaged synaptic connection strengths within and between assemblies. Changes in estimated connection strengths are shown in <xref ref-type="fig" rid="pcbi.1004643.g008">Fig 8A</xref>. After training (relative to trial 1), intrinsic connection strengths (<italic>W</italic><sub>11</sub>, <italic>W</italic><sub>22</sub>) increased significantly, while connectivity between different neuron groups (<italic>W</italic><sub>12</sub>, <italic>W</italic><sub>21</sub>) tended to decrease (<xref ref-type="fig" rid="pcbi.1004643.g008">Fig 8B</xref>). Notably, if we assumed a constraint on total synaptic strengths with a <italic>γ</italic>-norm (the 1/<italic>γ</italic> power of the <italic>γ</italic> power sum of synaptic strengths), and if <italic>γ</italic> was between 2 and 4, the <italic>γ</italic>-norm of the connection strengths maintained almost same value during the latter part of the training period (<xref ref-type="supplementary-material" rid="pcbi.1004643.s007">S2B Fig</xref>).</p>
<fig id="pcbi.1004643.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Free energy properties in cultured neural networks.</title>
<p>Transitions during learning (left) and overall before-and-after changes (right) in connection strengths (top) and in three descriptors of evoked responses modeled on information-theoretic functions (bottom). <bold>(A)</bold> Connection strengths of the neural population estimated from the number of evoked response per trial. Black circles and squares are <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub>. White circles and squares are <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub>. Bars are S.E.M. <bold>(B)</bold> The change in connection strengths (trial 1 vs. trial 100). <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub> significantly increased after training (****, <italic>p</italic> &lt; 10<sup>−6</sup>; <italic>n</italic> = 46 from 23 cultures), while <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> tended to decrease (<italic>p</italic> = 0.069; <italic>n</italic> = 46 from 23 cultures). <bold>(C)</bold> Transition of the expectation of internal energy (〈<italic>U</italic>〉; white circles), Shannon entropy (<italic>H</italic>; black circles), and free energy (<italic>F</italic>; red circles) in cultured neural networks estimated from evoked responses per trial. Bars are S.E.M. <bold>(D)</bold> The change in 〈<italic>U</italic>〉, <italic>H</italic>, and <italic>F</italic> (trial 1 vs. trial 100). After training, the expectation of internal energy decreased (*, <italic>p</italic> = 0.023; <italic>n</italic> = 23 cultures), Shannon entropy significantly increased (****, <italic>p</italic> &lt; 10<sup>−4</sup>; <italic>n</italic> = 23 cultures), and free energy significantly decreased (****, <italic>p</italic> &lt; 10<sup>−4</sup>; <italic>n</italic> = 23 cultures). These results suggest that learning in cultured neural networks was governed by the free-energy principle.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g008" xlink:type="simple"/>
</fig>
<p>As the model and connection parameters are well defined, we could calculate the internal energy and the Shannon entropy for these neural networks. To do this, we assumed that <inline-formula id="pcbi.1004643.e027"><alternatives><graphic id="pcbi.1004643.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold" stretchy="true">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> obeys a Gaussian mixture model with four peaks corresponding to the four states of <bold>u</bold>. Internal energy, <inline-formula id="pcbi.1004643.e028"><alternatives><graphic id="pcbi.1004643.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>U</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mo mathvariant="bold">s</mml:mo><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, is defined as the negative log likelihood function of prediction error at a moment, where <inline-formula id="pcbi.1004643.e029"><alternatives><graphic id="pcbi.1004643.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mover accent="true"><mml:mtext mathvariant="bold">s</mml:mtext><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004643.e030"><alternatives><graphic id="pcbi.1004643.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> are input and output, respectively. Shannon entropy, <italic>H</italic>, is defined by <inline-formula id="pcbi.1004643.e031"><alternatives><graphic id="pcbi.1004643.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mi>H</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:math></alternatives></inline-formula>. Friston’s free energy, <italic>F</italic>, is defined as the difference between 〈<italic>U</italic>〉 and <italic>H</italic> [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>], where 〈•〉 is an expectation under <inline-formula id="pcbi.1004643.e032"><alternatives><graphic id="pcbi.1004643.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold" stretchy="true">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula>. Therefore, <italic>F</italic> is represented as <inline-formula id="pcbi.1004643.e033"><alternatives><graphic id="pcbi.1004643.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:math></alternatives></inline-formula>. Generally, free energy gives an upper bound on ‘surprise’ of inputs, so the decrease of free energy implies that the system is changing to adapt to (or learn) its environment [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. The full details of these calculations are fully described in the Methods. These components of free energy changed dramatically over training trials (<xref ref-type="fig" rid="pcbi.1004643.g008">Fig 8C</xref>). We found that the expectation of internal energy 〈<italic>U</italic>〉 decreased, Shannon entropy <italic>H</italic> increased significantly, and free energy <italic>F</italic> decreased significantly after training (<xref ref-type="fig" rid="pcbi.1004643.g008">Fig 8D</xref>), which is consistent with the principle of free-energy minimization [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. These data thus indicate that connectivities in neural networks were established such that they minimize free energy (<italic>F</italic>).</p>
<p>As expected, as learning proceeds over trials, the implicit entropy of the probabilistic encoding increases in accord with Jaynes’ maximum entropy principle [<xref ref-type="bibr" rid="pcbi.1004643.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref042">42</xref>]. Crucially, this is accompanied by a profound decrease in energy (i.e., the amount of prediction error). Therefore, the decrease in the energy and the increase in the entropy both contributed to produce an overall reduction in free energy––that can only be attributed to learning or plasticity. This assertion was verified empirically by quantifying free energy changes in the presence of APV. Remarkably, free energy did not change at all during training under APV (<xref ref-type="supplementary-material" rid="pcbi.1004643.s008">S3 Fig</xref>).</p>
</sec>
<sec id="sec008">
<title>Learning rule of cultured neural networks</title>
<p>The changes in KLD and free energy we observed are indicative of synaptic plasticity and suggested that cultured neural networks are capable of performing blind source separation. These findings further suggested the existence of a transformation matrix (<italic>W</italic>) in cultured neural networks, which transforms merged inputs to independent outputs [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref044">44</xref>]. However, it is unclear whether the blind source separation is realized only by Hebbian learning [<xref ref-type="bibr" rid="pcbi.1004643.ref018">18</xref>]. To estimate the learning rule of cultured neural networks, we first considered a simple Hebbian plasticity model, where a learning efficacy <italic>α</italic><sub><bold>u</bold></sub> becomes 0 for <bold>u</bold> = (0,0) and <italic>α</italic> for other states (<italic>α</italic>-model; see also the <xref ref-type="sec" rid="sec010">Methods</xref>). We then estimated <italic>α</italic> for each culture sample. The estimated values of <italic>α</italic> are shown in <xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9A</xref> left and the Bayesian information criterion (BIC) [<xref ref-type="bibr" rid="pcbi.1004643.ref047">47</xref>] in <italic>α</italic>-model is shown in <xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9B</xref>. In this <italic>α</italic>-model, connections between different neuron groups (<italic>W</italic><sub>12</sub>, <italic>W</italic><sub>21</sub>) were expected to increase substantially, because Hebbian learning operates by simply increasing the correlation among neurons that fire together (<xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9C</xref>). However, we did not observe substantial increases between neuron groups, indicating that a simple Hebbian rule could not explain our experimental results.</p>
<fig id="pcbi.1004643.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004643.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The governing learning rule in cultured neural networks.</title>
<p>The rule was determined from (A) learning efficacies, (B) the Bayesian information criterion (BIC), and (C) model predictions of connectivity changes, and summarized in (D). <bold>(A)</bold> The expectation of learning efficacy estimated from connection strengths under the assumption that synaptic plasticity in cultured neural networks obeys either a Hebbian (<italic>μ</italic><sup><italic>α</italic></sup>; left) or state-dependent Hebbian (<italic>μ</italic><sup><italic>β</italic>1</sup>, <italic>μ</italic><sup><italic>β</italic>2</sup>; right) rule. <italic>μ</italic><sup><italic>β</italic>2</sup> was correlated with <italic>μ</italic><sup><italic>β</italic>1</sup> (*, <italic>p</italic> = 0.037; <italic>n</italic> = 23 cultures, Spearman test) and their ratio was 27.1%. <bold>(B)</bold> The BIC of the <italic>α</italic>- and <italic>β</italic>-models. BIC of the <italic>β</italic>-model was significantly smaller than that of the <italic>α</italic>-model (****, <italic>p</italic> &lt; 10<sup>−6</sup>; <italic>n</italic> = 23 cultures). <bold>(C)</bold> Measured changes in connection strengths and those estimated from Hebbian and state-dependent Hebbian plasticity rules. Black bars are the mean ± S.E.M of the change (trial 100 –trial 1) in <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub> (<italic>n</italic> = 46 from 23 cultures), which increased in all cases (****, <italic>p</italic> &lt; 10<sup>−5</sup>). White bars are the mean ± S.E.M of the change in <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> (<italic>n</italic> = 46 from 23 cultures). <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> tended to decrease after training when calculated from response data (left; <italic>p</italic> = 0.069). <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> estimated from the <italic>α</italic>-model significantly increased (center; ****, <italic>p</italic> &lt; 10<sup>−4</sup>). <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> estimated from the <italic>β</italic>-model significantly decreased (right; *, <italic>p</italic> = 0.023), in agreement with the experimentally determined results. <bold>(D)</bold> Suggested learning rule, which is based on state-dependent Hebbian plasticity, and leads to blind source separation. Synaptic plasticity in cultured neural networks almost followed a strict Hebbian rule. However, experimental data indicate that learning efficacy was not the same for each input state. Substantial Hebbian plasticity occurred when <bold>u</bold> = (1,0) or <bold>u</bold> = (0,1), while limited or anti-Hebbian plasticity occurred when <bold>u</bold> = (1,1). From the estimated efficacies <italic>μ</italic><sup><italic>β</italic>1</sup> and <italic>μ</italic><sup><italic>β</italic>2</sup>, the efficacy of <bold>u</bold> = (1,1) was apparently only 27% of that of <bold>u</bold> = (1,0) or (0,1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.g009" xlink:type="simple"/>
</fig>
<p>These results therefore suggested that blind source separation in our cultured neural networks required another mechanism. We thus considered a modified version of Hebbian plasticity (<italic>β</italic>-model), where a learning efficacy <italic>β</italic><sub><bold>u</bold></sub> depends on the state of <bold>u</bold>, 0 for <bold>u</bold> = (0,0), <italic>β</italic><sub>1</sub> for <bold>u</bold> = (1,0), (0,1), and <italic>β</italic><sub>2</sub> for <bold>u</bold> = (1,1). <italic>β</italic><sub>1</sub> and <italic>β</italic><sub>2</sub> were estimated for each culture. Interestingly, we found that estimated values of <italic>β</italic><sub>2</sub> were significantly smaller than the estimated values of <italic>β</italic><sub>1</sub> (approximately 27% of <italic>β</italic><sub>1</sub>; <xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9A</xref> right). Moreover, the BIC was significantly smaller than in the <italic>α</italic>-model (<xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9B</xref>). Accordingly, the <italic>β</italic>-model successfully explained the increase of intrinsic connections within neuron groups (<italic>W</italic><sub>11</sub>, <italic>W</italic><sub>22</sub>), and the absence of increases inter-connections between different groups (<italic>W</italic><sub>12</sub>, <italic>W</italic><sub>21</sub>) (<xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9C</xref>). Furthermore, as an additional Bayesian model comparison, we showed that Hebbian plasticity with state-dependent efficacy (the <italic>β</italic>-model) is better than Hebbian plasticity with <italic>γ</italic>-norm constraint on total synaptic strength (the <italic>α’</italic>-model) to explain our experimental results (see <xref ref-type="supplementary-material" rid="pcbi.1004643.s005">S1 Note</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004643.s009">S4 Fig</xref>).</p>
<p>These results suggest that cultured neural networks do not use the simplest form of the Hebbian plasticity rule (the <italic>α</italic>-model), but rather a state-dependent Hebbian plasticity rule (the <italic>β</italic>-model) in which learning efficacy is modified according to the state of sources. A conceptual conclusion is that the depression in inter-connections between different groups and the formation of cell assemblies are crucial to achieve blind source separation. Generally, the potentiation in connections makes the correlation between a neuronal group and a source stronger, while their depression makes the correlation between the neuronal group and the other source weaker. In our analysis, because the <italic>β</italic>-model encouraged stronger depression in connections from the other source and induced stronger competition between different neuronal groups, the <italic>β</italic>-model was better able to explain the results than the <italic>α</italic>-model. Moreover, this result supports the hypothesis that neurons render their activity independent of each other. This is consistent with early work on decorrelating or lateral interactions in PCA/ICA learning rules, which, importantly, can be formulated as variational free energy minimization [<xref ref-type="bibr" rid="pcbi.1004643.ref048">48</xref>].</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we discovered that cultured neural networks were able to identify and separate two hidden sources. We found that the distinct classes of neurons learned to respond to the distinct hidden sources and that this was reflected in differences in the Kullback-Leibler divergence (KLD). We then sought to determine how connection strength is determined between cultured neurons and found that connectivities are established such that they minimize free energy. Finally, we integrated these data to construct a model of learning in cultured neural networks and determined that learning is established by a modified Hebbian plasticity rule. Taken together these data indicate that cultured neural networks can infer multivariate hidden signals through blind source separation.</p>
<p>Although cultured neural networks are random and may not have functional structures for signal processing before training, our data indicated that the process of training enables them to self-organize and obtain functional structures to separate two hidden signals though activity-dependent synaptic plasticity, such as spike-timing dependent plasticity (STDP) [<xref ref-type="bibr" rid="pcbi.1004643.ref049">49</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref051">51</xref>]. This process was a clear example of unsupervised learning [<xref ref-type="bibr" rid="pcbi.1004643.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref011">11</xref>] in cultured neural networks.</p>
<p>Previous studies have reported that the response electrode almost agrees with the stimulating electrode [<xref ref-type="bibr" rid="pcbi.1004643.ref043">43</xref>] and that the increase in response strength at stimulated electrode is larger than in the non-stimulated electrodes [<xref ref-type="bibr" rid="pcbi.1004643.ref052">52</xref>]; our results are consistent with these findings. Synaptic plasticity and inputs with different merged points of balance are necessary for learning to occur. As spikes observed less than 10 ms after stimulation in our culture system corresponded to responses directly evoked by electrical stimulation and artifacts (switching noise), we only assessed spikes more than 10 ms after stimulation. This allowed the analysis of changes in neural activity related to mechanisms of synaptic plasticity. Indeed, we observed that changes in KLD were inhibited by APV, strongly suggesting that learning mechanism was mediated by long-term synaptic plasticity regulated by NMDA-receptor signaling. As a further indication of a role for long-term synaptic plasticity, assays of partially stimulated cultures indicated the changes brought about by neural activation were maintained after 18–24 h without stimulation. Additionally, our results indicated that differences in the size of inputs were necessary for blind source separation in cultured neurons. Neurons with larger initial states of KLD tended to exhibit greater changes, suggesting that learning is nuanced by the initial input strengths as would be consistent with most forms of Hebbian learning [<xref ref-type="bibr" rid="pcbi.1004643.ref018">18</xref>].</p>
<p>Although the specificity of the neuronal response to hidden sources increases significantly, there remains a possibility that the neurons merely responded to their neighbor input stimulation. In fact, responding to neighbor stimulation might be enough to increase the response specificity in the current stimulation design. Indeed, in a large portion of electrodes, neural responses were affected by the input from an electrode. However, we found that at least at 13% of <italic>u</italic><sub>1</sub>- or <italic>u</italic><sub>2</sub>-preferring electrodes, neural responses were more likely to be determined by the state of hidden sources rather than by the input from an electrode, typically the nearest one, in the strict sense of the word. Moreover, the number of such electrodes increased during training. In short, this means we might be observing the superimposition of the response to the input from an electrode and the response corresponding to the state of hidden sources. Hence, to reduce the effect of neighbor stimulation site and emphasize the response determined by the state of hidden sources, we should search the optimal stimulation design for investigating blind source separation as future work.</p>
<p>Even in the presence of APV, KLD increased slightly. One explanation is that this is the result of an NMDA-R-independent form of learning. For example, it is known that synaptic plasticity independent of NMDA-R activity occurs at GABAergic synapses [<xref ref-type="bibr" rid="pcbi.1004643.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref054">54</xref>], and could alter the neural network state to some degree. However, it could also be related to the drug’s imperfect blockade of NMDA-Rs.</p>
<p>In our experiments, evoked activities of cultured neurons were only synchronously generated immediately after each stimulation. This would be expected for both forward and inverse recognition models, given that the input was synchronous and instantaneous (discrete-time system), but the dynamics did not reach an equilibrium as is required for learning of a feed-forward model. Moreover, most neurons we observed were highly correlated with one of two sources (source-coding neurons). Taken together, these findings suggest that for our experimental protocol, the structure of cultured neural networks can be represented as a two-layer feed-forward network constructed from input and output layers and functioning as an inverse recognition model. However, it remains unclear which model applies to cultured neural networks with non-synchronous input.</p>
<p>Although some ICA models use information via non-local connections, several studies have proposed local rules that ICA can be constructed only using biologically plausible local connections [<xref ref-type="bibr" rid="pcbi.1004643.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref055">55</xref>]. Internal energy, or negative log likelihood, also decreased after training, indicating that our culture neural networks also performed a maximum likelihood estimation or a maximum a posteriori estimation. Consequently, the free energy of the population model decreased significantly after training as predicted by the free energy principle [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>], which can also be regarded as an increase in mutual information between input and output (infomax principle) [<xref ref-type="bibr" rid="pcbi.1004643.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref057">57</xref>]. Taken together these results suggest that in response to synchronous input, cultured neural networks perform ICA-like learning using an inverse recognition model constructed from local connections, and they adhere to the free-energy principle.</p>
<p>Experimental results suggest that the change in synaptic connection strengths in our model is better explained by a state-dependent Hebbian plasticity rule rather than the simplest Hebbian rule (<xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9D</xref>). A possible explanation is as follows: Initially, many neurons may respond strongly to the nearest electrode but may also respond weakly to distant electrodes. According to Hebbian plasticity, synapses that respond to stimulation of the nearest electrode (and thus, to the source that tends to activate the nearest electrode) are likely potentiated because of the large postsynaptic response to the nearest electrode. If depression is induced in the other synapses in accordance with a plasticity rule, the neural response to the source that effectively stimulates the nearest electrode will be facilitated, while that to the other source will be depressed. This scenario seems to qualitatively explain the experimental results; however, our analysis implies that the simplest Hebbian plasticity (the <italic>α</italic>-model) cannot change synaptic strengths in this manner because of larger LTP in the <bold>u</bold> = (1,1) state than LTD in the <bold>u</bold> = (1,0) and (0,1) states, and that state-dependent Hebbian plasticity (the <italic>β</italic>-model) better explains the results since it suppresses LTP in the <bold>u</bold> = (1,1) state, providing stronger competition between neurons. Nevertheless, a more biologically plausible Hebbian plasticity model, such as the STDP model [<xref ref-type="bibr" rid="pcbi.1004643.ref021">21</xref>], should be analyzed in a systematic future study. It is unclear whether such a model fully explains the experimental results, and we would like to investigate this in the future.</p>
<p>Indeed, cultured neurons could not directly know the state of <bold>u</bold>; however, they could distinguish the state of <bold>u</bold> = (1,1) from other states as the total of evoked activity was significantly larger for this state than the other states. It is likely cultured neurons use the total of evoked activity to determine learning efficacy. Although we considered a 2-state model of learning efficacy, since the <bold>u</bold> = (1,0) and (0,1) states are symmetrical in our experimental setup, a 3-state model could be considered if presented with an asymmetrical stimulation pattern. Some mathematical models of ICA [<xref ref-type="bibr" rid="pcbi.1004643.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref055">55</xref>] conform to a modified form of Hebbian plasticity as well. Moreover, modulation of synaptic plasticity by GABAergic input [<xref ref-type="bibr" rid="pcbi.1004643.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref059">59</xref>] may operate learning-efficacy modulation; nevertheless, additional experiments are necessary to determine the physiology of the modulation of Hebbian plasticity we observed.</p>
<p>It is known that animals have a high aptitude for pattern separation [<xref ref-type="bibr" rid="pcbi.1004643.ref060">60</xref>]. Spontaneous prior activity of a visual area learns the properties of natural pictures [<xref ref-type="bibr" rid="pcbi.1004643.ref028">28</xref>]. In the visual and olfactory systems, structures that decorrelate inputs and raise contrast are functional from birth [<xref ref-type="bibr" rid="pcbi.1004643.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref062">62</xref>]. Although many studies of the pattern separation have been conducted, there is little research investigating blind source separation in biological neural circuits, as the decomposition of merged inputs is a more complex process than simple pattern separation. Owing to the simple properties of cultured neural networks, we observed the process by which neural networks actually learn to perform blind source separation. Evoked responses likewise changed after training to correspond to sources of the generative model. Practically, sensory inputs are a mixture of several sources except in a few ideal cases. Without blind source separation, these signals cannot be processed appropriately because the brain would fail to adequately decorrelate inputs. Therefore, our findings may be very important in understanding sensory perception.</p>
<p>Alternatively, one might consider that blind source separation can occur online, and seems to be more a matter of attention rather than learning, e.g., one can separate voices with different timbers at a cocktail party without experiences those particular timbers before. However, to direct one’s attention to a specific voice, the brain needs to separate a mixture of signals in advance. Therefore, although additional studies are required to explain the difference in the time scale of blind source separation between that considered in a cocktail party problem and that we observed, it is likely that ICA-like learning is necessary for blind source separation and a cocktail party effect.</p>
<p>Recently, the free-energy principle was proposed [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>], which specifically includes PCA/ICA, and has been applied to explain recognition models with highly hierarchical structure [<xref ref-type="bibr" rid="pcbi.1004643.ref017">17</xref>]. Cultured neural networks are useful to examine these theories as they can easily build any network structure [<xref ref-type="bibr" rid="pcbi.1004643.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref036">36</xref>] and reproduce a variety of functions [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref039">39</xref>]. In addition, dynamic causal modeling for spike data [<xref ref-type="bibr" rid="pcbi.1004643.ref063">63</xref>] helps to investigate the detail structure of the recognition models of cultured neural networks. Moreover, there remains the possibility that cultured neural networks can perform even more complex types of unsupervised learning. These findings contribute not only to an increased understanding of learning and memory from a neuroscience perspective, but also in examining the free-energy principle at the cellular level.</p>
<p>In summary, we found that dissociated cultures of cortical neurons have the ability to carry out blind source separation in response to hidden signals. Learning in this paradigm used an inverse recognition model and was carried out according to a modified form Hebbian plasticity, which is likely regulated, at least in part, by NMDA signaling. These results are entirely consistent with the free-energy principle, suggesting that cultured neural networks perform blind source separation according to the free-energy principle. Most importantly, the free energy formulation allows us to quantify probabilistic encoding at the neuronal level in terms of information theory, and to test hypotheses about the changes in energy and entropy that are implicit in Bayes-optimal perception. We could have also assessed the accuracy and complexity of these representations with a slight change of variables. The free energy formalism prescribes Bayes-optimal update rules for the connection strengths that are associative in nature. Taken together these data provide a compelling framework for understanding the process by which the brain interprets hidden signals from complex multivariate information.</p>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec011">
<title>Cell cultures</title>
<p>All animal experiments were performed with the approval of the animal experiment ethics committee at the University of Tokyo (approval number, C-12-02, KA-14-2) and according to the University of Tokyo guidelines for the care and use of laboratory animals. The procedure for preparing dissociated cultured of cortical neurons was based on a modified version of the procedure described in a previous study [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>]. Pregnant females of Wistar rat (Charles River Laboratories, Japan) were anaesthetized with isoflurane and immediately sacrificed. 19-day-old embryos (E19) were extracted and sacrificed by decapitation under ice-cold anesthesia. Cortical cells were removed from embryos and dissociated into single cells with Trypsin (Life Technologies) at 37°C for 20 min. The density of cells was adjusted to 1 × 10<sup>7</sup> cells/mL. 5 × 10<sup>5</sup> of the dissociated cells in 50 μL were seeded on the center of MEA dishes (<xref ref-type="fig" rid="pcbi.1004643.g001">Fig 1 and 1B</xref>), where the surface of MEA was previously coated with polyethyleneimine (Sigma-Aldrich) overnight. Note that to prepare high-density cultures, cells were dropped on the region where electrode terminals were disposed. The culture medium consisted of Dulbecco’s modified Eagle’s medium (DMEM) (Life Technologies) containing 10% heat-inactivated fetal bovine serum (FBS) (Cosmo Bio), 5% heat-inactivated horse serum (HS) (Life Technologies), and 5–40 U/ml penicillin/streptomycin (Life Technologies). After sitting undisturbed in the MEA dishes for 30 min, the fresh culture medium and medium conditioned for 3 days in glial cell cultures, were added into MEA dishes at a ratio of 1:1. The cells were cultivated in a CO<sub>2</sub> incubator, an environment of 37°C and a 5% CO<sub>2</sub>/95% air concentration. Half of the culture medium was changed once every third day. These cultures were cultivated for 18 to 83 days before electrophysiological measurements. Although the electrophysiological properties of cultured cortical neurons change during development, it has been reported that at the stage of culture using our experiments, the spontaneous firing patterns of neurons have reached a developmentally stable period [<xref ref-type="bibr" rid="pcbi.1004643.ref064">64</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref066">66</xref>]. Note that same cultures were used more than once for experiments with other stimulation-pattern conditions since learning history with other stimulation-pattern did not affect our experiments and evaluations of results. We used 27 different cultures for 7 experiments, which were performed 40 ± 18 days after seeding.</p>
</sec>
<sec id="sec012">
<title>Recording</title>
<p>The MEA system (NF Corporation, Japan) was used for extracellular recording of cultured neural networks. Electrode terminals and circuits on MEA dishes were handmade using a photolithography technique. The 8×8 electrode terminals of MEA were disposed on a grid with 250-μm distance. Platinum black was coated on all 50-μm-each side electrode terminals. Neural signals were recorded with a 25 kHz sampling frequency and band-pass filtered between 500–2000 Hz, and were recorded over 14 h. All recordings and stimulation were conducted in a CO<sub>2</sub> incubator. From the spike sorting analysis [<xref ref-type="bibr" rid="pcbi.1004643.ref067">67</xref>], an electrode was expected to record the activities from up to four neurons. For more details of MEA recording, see previous studies [<xref ref-type="bibr" rid="pcbi.1004643.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref040">40</xref>].</p>
</sec>
<sec id="sec013">
<title>Electrical stimulation</title>
<p>Electrical stimulation was applied through 32 electrodes in pulse trains with 1 s intervals (<xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2</xref>). Pulses were biphasic with each phase having a duration of 0.2 ms, and were delivered with 1 V amplitudes. Stimuli were delivered for each stimulating electrode only once in 1 s (1 Hz).</p>
<p>Before making inputs, we created hidden sources <italic>u</italic><sub>1</sub>(<italic>t</italic>), <italic>u</italic><sub>2</sub>(<italic>t</italic>), which corresponded to two independent random binary sources, <italic>u</italic><sub>1</sub>(<italic>t</italic>), <italic>u</italic><sub>2</sub>(<italic>t</italic>) ∈ {0,1} (<italic>t</italic> = 1, 2, …, 256 [s]). In this equation, <italic>u</italic><sub>1</sub>(<italic>t</italic>) and <italic>u</italic><sub>2</sub>(<italic>t</italic>) are signal patterns such that <italic>u</italic><sub>1</sub>(<italic>t</italic>) = 0,1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,… and <italic>u</italic><sub>2</sub>(<italic>t</italic>) = 1,1,0,0,1,0,0,1,1,0,1,1,1,0,1,1,… as shown in <xref ref-type="fig" rid="pcbi.1004643.g002">Fig 2B and 2C</xref>. The value of <italic>u</italic><sub>1</sub>(<italic>t</italic>) and <italic>u</italic><sub>2</sub>(<italic>t</italic>) will be 1 with a probability of <italic>ρ</italic> = 1/2 at each time period. The terms <italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>) correspond to merged inputs (electrical pulses), which were what we actually applied to cultured neurons. Therefore, cultured neurons did not know directly what the exact state of (<italic>u</italic><sub>1</sub>(<italic>t</italic>), <italic>u</italic><sub>2</sub>(<italic>t</italic>)) was because we did not induce (<italic>u</italic><sub>1</sub>(<italic>t</italic>), <italic>u</italic><sub>2</sub>(<italic>t</italic>)) directly.</p>
<p>The electrical stimulations (<italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>)) were constructed from two independent binary sources, <italic>u</italic><sub>1</sub>(<italic>t</italic>) and <italic>u</italic><sub>2</sub>(<italic>t</italic>), in the following manner:</p>
<list list-type="order">
<list-item><p>Values for half of the input train (<italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>16</sub>(<italic>t</italic>)) were randomly selected as <italic>u</italic><sub>1</sub>(<italic>t</italic>) with a <italic>a</italic> = 3/4 probability, or <italic>u</italic><sub>2</sub>(<italic>t</italic>) with a 1–<italic>a</italic> = 1/4 probability for each time period. This indicates, for example, that when <italic>u</italic><sub>1</sub>(1) = 1 and <italic>u</italic><sub>2</sub>(1) = 0, <italic>s</italic><sub>1</sub>(1) = 1 would be expected to occur with 75% certainty and <italic>s</italic><sub>1</sub>(1) = 0 would be expected to occur with 25% certainty. These expectations are common among <italic>s</italic><sub>1</sub>(1), …, <italic>s</italic><sub>16</sub>(1). As each component of <italic>s</italic><sub>1</sub>(1), …, <italic>s</italic><sub>16</sub>(1) was independently randomly selected, <italic>s</italic><sub>1</sub>(1), …, <italic>s</italic><sub>16</sub>(1) would become something like 1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1, which means, as population, 75% would be 1 and 25% would be 0 (although it is one example and the percentage would move stochastically). Thus, stimuli were chosen at random at each time period.</p></list-item>
<list-item><p>The values for the rest of trains (<italic>s</italic><sub>17</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>)) were randomly selected by <italic>u</italic><sub>1</sub>(<italic>t</italic>), with a 1–<italic>a</italic> = 1/4 probability, or <italic>u</italic><sub>2</sub>(<italic>t</italic>) with that of <italic>a</italic> = 3/4. In other terms, the expectations of <italic>s</italic><sub>17</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>) were exactly opposite to that of <italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>16</sub>(<italic>t</italic>).</p></list-item>
<list-item><p>The location of 32 stimulated electrodes corresponding to <italic>s</italic><sub>1</sub>(<italic>t</italic>), …, <italic>s</italic><sub>32</sub>(<italic>t</italic>) were randomly selected and fixed over trials. Stimulus evoked responses were recorded with the 64 MEA electrodes.</p></list-item>
</list>
<p>In other words, the generative model was composed of two hidden sources <bold>u</bold>(<italic>t</italic>) generated from the stationary Poisson process with the <italic>ρ</italic> intensity, <bold>u</bold>(<italic>t</italic>) ~ <italic>Po</italic>((<italic>ρ</italic>, <italic>ρ</italic>)<sup><italic>T</italic></sup>), 32 merged inputs <bold>s</bold>(<italic>t</italic>) generated from the non-stationary Poisson process with the time varying intensity of <italic>A</italic> <bold>u</bold>(<italic>t</italic>), <bold>s</bold>(<italic>t</italic>) ~ <italic>Po</italic>(<italic>A</italic> <bold>u</bold>(<italic>t</italic>)), and a 32 × 2 transform matrix <italic>A</italic>, in which (<italic>A</italic><sub><italic>i</italic>1</sub>, <italic>A</italic><sub><italic>i</italic>2</sub>) = (<italic>a</italic>, 1–<italic>a</italic>) for <italic>i</italic> = 1, …, 16 and (<italic>A</italic><sub><italic>i</italic>1</sub>, <italic>A</italic><sub><italic>i</italic>2</sub>) = (1–<italic>a</italic>, <italic>a</italic>) for <italic>i</italic> = 17, …, 32. Unless specifically mentioned, we used <italic>ρ</italic> = 1/2 and <italic>a</italic> = 3/4.</p>
</sec>
<sec id="sec014">
<title>Pharmacology</title>
<p>In the control condition, 2-Amino-5-phosphonopentanoic acid (APV) (a glutaminergic NMDA-receptor antagonist; Sigma-Aldrich) was used. APV was adjusted to 20 mM using PBS, and induced 2 μL into culture medium in an MEA dish to make a final concentration of 20 μM. After the injection, cultured neurons were placed for 30 min in a CO<sub>2</sub> incubator, and stable activity of cultured neurons was confirmed before recording.</p>
</sec>
<sec id="sec015">
<title>Analysis</title>
<sec id="sec016">
<title>Spike detection</title>
<p>Before spike detection, artifacts were removed as follows: (i) values in saturated regions in raw data were detected and modified to 0, (ii) 500–2000 Hz band-pass filter were applied for the data, and (iii) values in regions that were modified in the first step were shifted to 0 again (see <xref ref-type="fig" rid="pcbi.1004643.g001">Fig 1C</xref>). Mean (<italic>μ</italic>) and standard deviation (<italic>σ</italic>) of extracellular potential (<italic>v</italic>) were calculated for each second. A spike was defined as the lowest point of a valley (<italic>dv</italic>/<italic>dτ</italic> = 0 and <italic>dv</italic><sup>2</sup>/<italic>dτ</italic><sup>2</sup> &gt; 0) that was lower than 5 times standard deviation (<italic>v</italic> − <italic>μ</italic> &lt; −5 <italic>σ</italic>). Similar to previous study [<xref ref-type="bibr" rid="pcbi.1004643.ref067">67</xref>], if more than two spikes were detected during 0.25 ms, only a spike with lowest valley was chosen.</p>
</sec>
<sec id="sec017">
<title>Conditional probability and expectation of a response</title>
<p>The Firing probability of neurons recorded at electrode <italic>i</italic> (<italic>i</italic> = 1, …, 64) is shown as <italic>x</italic><sub><italic>i</italic></sub>(<italic>τ</italic>) [spike/ms]. The strength of evoked response against the <italic>t</italic>th stimulus (<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) [spike/event]; <italic>t</italic> = 1, …, 256) is defined as the number of spikes generated until 10–30 ms after each stimulation,
<disp-formula id="pcbi.1004643.e034">
<alternatives>
<graphic id="pcbi.1004643.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e034" xlink:type="simple"/>
<mml:math display="block" id="M34">
<mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>1000</mml:mn><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>1000</mml:mn><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>Using histogram method, conditional probability distribution <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = <bold>u</bold>) is non-parametrically calculated (<xref ref-type="fig" rid="pcbi.1004643.g004">Fig 4C and 4D</xref>), where <bold>u</bold> = (<italic>u</italic><sub>1</sub>, <italic>u</italic><sub>2</sub>)<sup><italic>T</italic></sup> is a column vector of the source state. Moreover, as the parametric method, we assume that the probability distribution of <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) given <bold>u</bold>(<italic>t</italic>) obeys the Poisson distribution, which is given by
<disp-formula id="pcbi.1004643.e035">
<alternatives>
<graphic id="pcbi.1004643.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e035" xlink:type="simple"/>
<mml:math display="block" id="M35">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> (a parameter of Poisson distribution) was a conditional expectation of <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) when the state of <bold>u</bold> is given. The maximum likelihood estimator of <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> was defined as <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> = <italic>E</italic>[<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = <bold>u</bold>, <italic>t</italic> = 1,…,256], where <italic>E</italic>[•] indicates the expectation, i.e., <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> is a mean value of <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) when <bold>u</bold>(<italic>t</italic>) = <bold>u</bold>. <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> was calculated for each trial. All trial average of <italic>x</italic><sub><italic>i</italic></sub><sup><bold>u</bold></sup> is represented as <inline-formula id="pcbi.1004643.e036"><alternatives><graphic id="pcbi.1004643.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. We only evaluated electrode with <inline-formula id="pcbi.1004643.e037"><alternatives><graphic id="pcbi.1004643.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> spike/event as a recording electrode to be used for analysis. We assumed that a neuron group recorded at electrode <italic>i</italic> was <italic>u</italic><sub>1</sub>-preferring when <inline-formula id="pcbi.1004643.e038"><alternatives><graphic id="pcbi.1004643.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>≥</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.5</mml:mn></mml:math></alternatives></inline-formula> spike/event, <italic>u</italic><sub>2</sub>-preferring when <inline-formula id="pcbi.1004643.e039"><alternatives><graphic id="pcbi.1004643.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>≤</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:math></alternatives></inline-formula> spike/event, and no preference when otherwise. These neurons were categorized into <italic>G</italic><sub>1</sub> (<italic>u</italic><sub>1</sub>-preferring), <italic>G</italic><sub>2</sub> (<italic>u</italic><sub>2</sub>-preferring), and <italic>G</italic><sub>0</sub> (no preference), respectively.</p>
</sec>
<sec id="sec018">
<title>Kullback-Leibler divergence</title>
<p>The Kullback-Leibler divergence (KLD) is the distance of two probability distributions [<xref ref-type="bibr" rid="pcbi.1004643.ref011">11</xref>]. KLD between <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = (1,0)) and <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = (0,1)) was defined by
<disp-formula id="pcbi.1004643.e040">
<alternatives>
<graphic id="pcbi.1004643.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e040" xlink:type="simple"/>
<mml:math display="block" id="M40">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:mtext>log</mml:mtext><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> log</mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>P</mml:mi></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where 〈•〉<sub><italic>P</italic>(<italic>xi</italic>(t)| <bold>u</bold>(<italic>t</italic>) = (1,0))</sub> is an expectation around <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = (1,0)) (Malkov bracket). Since we assume that <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = (1,0)) and <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>)| <bold>u</bold>(<italic>t</italic>) = (0,1)) obey Poisson distribution, <xref ref-type="disp-formula" rid="pcbi.1004643.e040">Eq 3</xref> was calculated as
<disp-formula id="pcbi.1004643.e041">
<alternatives>
<graphic id="pcbi.1004643.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e041" xlink:type="simple"/>
<mml:math display="block" id="M41">
<mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>KLD is a non-negative value and becomes 0 if and only if two probability distributions are exactly equal. When the difference between two distributions is small, KLD becomes a small value; when the difference is large KLD becomes a large value.</p>
</sec>
<sec id="sec019">
<title>Statistical test</title>
<p>The Wilcoxon signed-rank test was used as a paired testing. The Mann-Whitney <italic>U</italic> test was used as an unpaired testing. The Spearman test was used as a test of no correlation.</p>
</sec>
<sec id="sec020" sec-type="materials|methods">
<title>Modeling</title>
<p>Neurons in a culture that respond to stimulation with the same property were assumed to be in the same cell assembly, such that we considered the population model constructed from groups of <italic>u</italic><sub>1</sub>- and <italic>u</italic><sub>2</sub>-preferring neurons. Thus, we defined <inline-formula id="pcbi.1004643.e042"><alternatives><graphic id="pcbi.1004643.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> [spike/event] by
<disp-formula id="pcbi.1004643.e043">
<alternatives>
<graphic id="pcbi.1004643.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e043" xlink:type="simple"/>
<mml:math display="block" id="M43">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>Furthermore, we assumed that the recognition model used by cultured neurons is the inverse model with linear firing function, which is represented as
<disp-formula id="pcbi.1004643.e044">
<alternatives>
<graphic id="pcbi.1004643.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e044" xlink:type="simple"/>
<mml:math display="block" id="M44">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>ξ</mml:mo></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <inline-formula id="pcbi.1004643.e045"><alternatives><graphic id="pcbi.1004643.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e046"><alternatives><graphic id="pcbi.1004643.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>’</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> [spike/ms] and <inline-formula id="pcbi.1004643.e047"><alternatives><graphic id="pcbi.1004643.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> [event/ms] are column vectors of synaptic response, direct response, and input with continuous time (<italic>τ</italic> [ms]). <bold>ξ</bold>(<italic>τ</italic>) [spike/ms] is a background noise and were assumed to obey a Gaussian distribution <bold>ξ</bold>(<italic>τ</italic>) ~ <italic>N</italic>(<bold>ξ</bold>; <bold>0</bold>, <italic>Σ</italic><sup><italic>ξ</italic></sup>). <italic>W</italic> [spike/event] is a 2×2 connection strength matrix representing identical connections and connections between two groups (<xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7D</xref>). Note that <italic>d</italic><sub><italic>d</italic></sub> [ms] and <italic>d</italic><sub><italic>s</italic></sub> [ms] were latencies of responses directly evoked by stimulations and indirectly evoked via synaptic connections. It is known that direct responses evoked by extracellular stimulation are highly reproducible with small time variance, while indirect responses via synaptic connection have larger time variance [<xref ref-type="bibr" rid="pcbi.1004643.ref043">43</xref>]. Therefore, although the direct response <inline-formula id="pcbi.1004643.e048"><alternatives><graphic id="pcbi.1004643.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>’</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> was difficult to observe, due to the artifact and saturation, evoked responses against pulse inputs could be regarded as a two-layer feed-forward model, which is the same form as a linear firing rate neuron model constructed from input and output layers [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref044">44</xref>]. As input was induced at a moment (assuming <italic>τ</italic> = 0), using the discrete time <italic>t</italic>, the response around <italic>τ</italic> = <italic>d</italic><sub><italic>d</italic></sub> + <italic>d</italic><sub><italic>s</italic></sub> could be represented as
<disp-formula id="pcbi.1004643.e049">
<alternatives>
<graphic id="pcbi.1004643.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e049" xlink:type="simple"/>
<mml:math display="block" id="M49">
<mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>ξ</mml:mo></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <inline-formula id="pcbi.1004643.e050"><alternatives><graphic id="pcbi.1004643.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula>, a column vector, is defined by <inline-formula id="pcbi.1004643.e051"><alternatives><graphic id="pcbi.1004643.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>16</mml:mn></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>17</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>32</mml:mn></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:math></alternatives></inline-formula>. A schematic image of the dynamics of the model is shown in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7E</xref>.</p>
<p>Generally, inverse recognition models [<xref ref-type="bibr" rid="pcbi.1004643.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref044">44</xref>] (e.g., <bold>x</bold> = <italic>W</italic><sub><italic>inv</italic></sub> <bold>s</bold>, where <bold>s</bold> and <bold>x</bold> are input and output vectors, and <italic>W</italic><sub><italic>inv</italic></sub> is a transform matrix corresponding to synaptic connection strengths) learn the inverse of a transformation matrix <italic>A</italic> (<italic>W</italic><sub><italic>inv</italic></sub> = <italic>A</italic><sup>–1</sup>), i.e., <italic>W</italic><sub><italic>inv</italic></sub> converges to <italic>A</italic><sup>–1</sup> after learning, where <italic>A</italic> is a transform matrix of sources (<bold>u</bold>) to inputs (<bold>s</bold>) in the generative model, <bold>s</bold> = <italic>A</italic> <bold>u</bold>. Whereas, feed-forward recognition models [<xref ref-type="bibr" rid="pcbi.1004643.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1004643.ref017">17</xref>] (e.g., an equilibrium state can be represented as <italic>W</italic><sub><italic>for</italic></sub> <bold>x</bold> = <bold>s</bold>) learn <italic>A</italic> itself, i.e., a connection strength matrix <italic>W</italic><sub><italic>for</italic></sub> converges to <italic>A</italic> after learning. Because the model we assumed was constructed from a two-layer feed-forward model and <italic>W</italic> was expected to converge to <italic>A</italic><sup>–1</sup>, our model is categorized into the inverse model.</p>
</sec>
<sec id="sec021">
<title>Cross-correlation between each electrode and population</title>
<p>Cross-correlation between <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <bold>u</bold>(<italic>t</italic>) is defined by <inline-formula id="pcbi.1004643.e052"><alternatives><graphic id="pcbi.1004643.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:mtext>corr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:mtext> cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where cov(<italic>x</italic><sub><italic>i</italic></sub>, <bold>u</bold>) is covariance between <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <bold>u</bold>(<italic>t</italic>), and Var(<italic>x</italic><sub><italic>i</italic></sub>) and Var(<bold>u</bold>) are variance of them. Then, error of <inline-formula id="pcbi.1004643.e053"><alternatives><graphic id="pcbi.1004643.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> from <bold>u</bold> is defined by <inline-formula id="pcbi.1004643.e054"><alternatives><graphic id="pcbi.1004643.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>i</italic> = 1, 2. We also defined cross-correlation between <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <bold>e</bold>(<italic>t</italic>) by <inline-formula id="pcbi.1004643.e055"><alternatives><graphic id="pcbi.1004643.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:mtext>corr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:mtext> cov</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. corr(<italic>x</italic><sub><italic>i</italic></sub>, <bold>u</bold>) and corr(<italic>x</italic><sub><italic>i</italic></sub>, <bold>e</bold>) were used for evaluating whether <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) was state-coding (representing <italic>u</italic><sub>1</sub>, <italic>u</italic><sub>2</sub>) or error-coding (representing <italic>e</italic><sub>1</sub>, <italic>e</italic><sub>2</sub>) (<xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7C</xref>).</p>
</sec>
<sec id="sec022">
<title>Estimation of connection strengths</title>
<p>Internal energy <inline-formula id="pcbi.1004643.e056"><alternatives><graphic id="pcbi.1004643.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is defined as a negative log likelihood function, <inline-formula id="pcbi.1004643.e057"><alternatives><graphic id="pcbi.1004643.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext>log </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ξ</mml:mi><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Note that <bold>ξ</bold> is regarded as the difference between an actual output, <inline-formula id="pcbi.1004643.e058"><alternatives><graphic id="pcbi.1004643.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula>, and an expected output <italic>W</italic> <inline-formula id="pcbi.1004643.e059"><alternatives><graphic id="pcbi.1004643.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula>. Thus, <bold>ξ</bold> is the error for a kind of optimal decoder, and <inline-formula id="pcbi.1004643.e060"><alternatives><graphic id="pcbi.1004643.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> indicates an amount of prediction error. Since we assumed <bold>ξ</bold> obeys Gaussian distribution, <bold>ξ</bold> ~ <italic>N</italic>(<bold>ξ</bold>; <bold>0</bold>, <italic>Σ</italic><sup><italic>ξ</italic></sup>), we get
<disp-formula id="pcbi.1004643.e061">
<alternatives>
<graphic id="pcbi.1004643.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e061" xlink:type="simple"/>
<mml:math display="block" id="M61">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>ξ</mml:mo></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup><mml:mo>)</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>ξ</mml:mo></mml:mstyle><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup><mml:mo>)</mml:mo><mml:msup><mml:mrow/><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>As there are no hidden states and hyper-parameters, the expectation of <italic>W</italic> can be estimated using the conventional maximum a posteriori estimation, which is analog of the conventional model-based connection strength estimation [<xref ref-type="bibr" rid="pcbi.1004643.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref068">68</xref>]. Since we assumed <italic>W</italic> obeys a Gaussian distribution <italic>W</italic> ~ <italic>q</italic>(<italic>W</italic>) = <italic>N</italic>(<italic>W</italic>; <italic>μ</italic><sup><italic>W</italic></sup>, <italic>∑</italic><sup><italic>W</italic></sup>) and the change in <italic>W</italic> during a trial is small, the mean value of <italic>W</italic>, <italic>μ</italic><sup><italic>W</italic></sup>, is given by <italic>W</italic> that minimizes the internal action <inline-formula id="pcbi.1004643.e062"><alternatives><graphic id="pcbi.1004643.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. By solving the extreme value of <inline-formula id="pcbi.1004643.e063"><alternatives><graphic id="pcbi.1004643.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e064"><alternatives><graphic id="pcbi.1004643.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mo>∂</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, we obtain
<disp-formula id="pcbi.1004643.e065">
<alternatives>
<graphic id="pcbi.1004643.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e065" xlink:type="simple"/>
<mml:math display="block" id="M65">
<mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mi>W</mml:mi></mml:msup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
<italic>μ</italic><sup><italic>W</italic></sup> was calculated for each trial. Thereby, we obtained the model, states, and parameters for both the generative and recognition models.</p>
</sec>
<sec id="sec023">
<title>Estimation of internal energy, Shannon entropy, and free energy for neurons</title>
<p>Next, we calculated the free energy for neurons according to the free-energy principle [<xref ref-type="bibr" rid="pcbi.1004643.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004643.ref020">20</xref>]. As above, the internal energy for neurons was defined by <inline-formula id="pcbi.1004643.e066"><alternatives><graphic id="pcbi.1004643.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext>log </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">ξ</mml:mi><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which describes amount of prediction error. In the recognition model of neurons <inline-formula id="pcbi.1004643.e067"><alternatives><graphic id="pcbi.1004643.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the posterior on the activity of state coding neurons <inline-formula id="pcbi.1004643.e068"><alternatives><graphic id="pcbi.1004643.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></alternatives></inline-formula> and the posterior on parameters <italic>W</italic> can be regarded as independent, <inline-formula id="pcbi.1004643.e069"><alternatives><graphic id="pcbi.1004643.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. We have already obtained <italic>q</italic>(<italic>W</italic>) as a Gaussian distribution. On the other hand, as shown in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7A and 7B</xref>, <inline-formula id="pcbi.1004643.e070"><alternatives><graphic id="pcbi.1004643.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> cannot be readily regarded as a Gaussian distribution. Thus, we assumed <inline-formula id="pcbi.1004643.e071"><alternatives><graphic id="pcbi.1004643.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> would be a Gaussian mixture model with 4 peaks corresponding to 4 stimulus source states. Specifically, <inline-formula id="pcbi.1004643.e072"><alternatives><graphic id="pcbi.1004643.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is represented as
<disp-formula id="pcbi.1004643.e073">
<alternatives>
<graphic id="pcbi.1004643.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e073" xlink:type="simple"/>
<mml:math display="block" id="M73">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <inline-formula id="pcbi.1004643.e074"><alternatives><graphic id="pcbi.1004643.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e075"><alternatives><graphic id="pcbi.1004643.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e076"><alternatives><graphic id="pcbi.1004643.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1004643.e077"><alternatives><graphic id="pcbi.1004643.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mtext> </mml:mtext><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are represented as <inline-formula id="pcbi.1004643.e078"><alternatives><graphic id="pcbi.1004643.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold">μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e079"><alternatives><graphic id="pcbi.1004643.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold">μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004643.e080"><alternatives><graphic id="pcbi.1004643.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold">μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1004643.e081"><alternatives><graphic id="pcbi.1004643.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold">μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, respectively. Estimators of <bold>μ</bold><sup><italic>xm</italic></sup>s, <italic>Σ</italic><sup><italic>xm</italic></sup>s and <italic>Σ</italic><sup><italic>ξ</italic></sup> are calculated as
<disp-formula id="pcbi.1004643.e082">
<alternatives>
<graphic id="pcbi.1004643.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e082" xlink:type="simple"/>
<mml:math display="block" id="M82"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>256</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup><mml:mo>|</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup><mml:mo>|</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>–</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>–</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mo>μ</mml:mo></mml:mstyle><mml:mrow><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>256</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>–</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mo mathvariant="italic">μ</mml:mo><mml:mi>W</mml:mi></mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>–</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mo mathvariant="italic">μ</mml:mo><mml:mi>W</mml:mi></mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>256</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
where <bold>u</bold> becomes (0,0), (1,0), (0,1), and (1,1) when <italic>m</italic> is 1, 2, 3, and 4, respectively. The expectation of <inline-formula id="pcbi.1004643.e083"><alternatives><graphic id="pcbi.1004643.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e083" xlink:type="simple"/><mml:math display="inline" id="M83"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is given by
<disp-formula id="pcbi.1004643.e084">
<alternatives>
<graphic id="pcbi.1004643.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e084" xlink:type="simple"/>
<mml:math display="block" id="M84">
<mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext>log </mml:mtext><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p>Shannon entropy of <inline-formula id="pcbi.1004643.e085"><alternatives><graphic id="pcbi.1004643.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is given by
<disp-formula id="pcbi.1004643.e086">
<alternatives>
<graphic id="pcbi.1004643.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e086" xlink:type="simple"/>
<mml:math display="block" id="M86">
<mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
which is approximated as <inline-formula id="pcbi.1004643.e087"><alternatives><graphic id="pcbi.1004643.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>256</mml:mn><mml:mtext> </mml:mtext><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:msup><mml:mtext> log </mml:mtext><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. As Shannon entropy of <italic>q</italic>(<italic>W</italic>), <italic>H</italic>[<italic>q</italic>(<italic>W</italic>)], only depends on <inline-formula id="pcbi.1004643.e088"><alternatives><graphic id="pcbi.1004643.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and is a constant over trials, we omit <italic>H</italic>[<italic>q</italic>(<italic>W</italic>)]. Accordingly, the free energy for neurons <inline-formula id="pcbi.1004643.e089"><alternatives><graphic id="pcbi.1004643.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e089" xlink:type="simple"/><mml:math display="inline" id="M89"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>μ</mml:mi><mml:mi>W</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is represented as
<disp-formula id="pcbi.1004643.e090">
<alternatives>
<graphic id="pcbi.1004643.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e090" xlink:type="simple"/>
<mml:math display="block" id="M90">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>W</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>H</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mi>ξ</mml:mi></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>256</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mtext>log</mml:mtext><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
<inline-formula id="pcbi.1004643.e091"><alternatives><graphic id="pcbi.1004643.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">s</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>μ</mml:mi><mml:mi>W</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is an upper bound of surprise of input and becomes minimum if and only if <inline-formula id="pcbi.1004643.e092"><alternatives><graphic id="pcbi.1004643.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext> </mml:mtext><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the same as the generative model (the true distribution of source).</p>
</sec>
<sec id="sec024">
<title>Estimation of learning efficacy</title>
<p>Learning of cultured neural networks is assumed to obey Hebbian plasticity [<xref ref-type="bibr" rid="pcbi.1004643.ref018">18</xref>], which is represented as
<disp-formula id="pcbi.1004643.e093">
<alternatives>
<graphic id="pcbi.1004643.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e093" xlink:type="simple"/>
<mml:math display="block" id="M93">
<mml:mrow><mml:mi>d</mml:mi><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
where 〈•〉 is an expectation around <inline-formula id="pcbi.1004643.e094"><alternatives><graphic id="pcbi.1004643.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic>α</italic><sub><bold>u</bold></sub> is a learning efficacy depending on state of <bold>u</bold> (<italic>α</italic><sub>0,0</sub>, <italic>α</italic><sub>1,0</sub>, <italic>α</italic><sub>0,1</sub>, and <italic>α</italic><sub>1,1</sub> are efficacies at the condition of <bold>u</bold> = (0,0), (1,0), (0,1), and (1,1), respectively). <italic>ε</italic><sup><italic>α</italic></sup> is a 2×2 matrix that represents the error and its elements are assumed to be independent of each other. We defined <xref ref-type="disp-formula" rid="pcbi.1004643.e093">Eq 15</xref> as an <italic>α</italic>-model. <xref ref-type="disp-formula" rid="pcbi.1004643.e093">Eq 15</xref> can be derived from the additive STDP model [<xref ref-type="bibr" rid="pcbi.1004643.ref021">21</xref>] when the source state changes rapidly. The aim is to estimate the value of <italic>α</italic><sub><bold>u</bold></sub>. Let us set <italic>z</italic><sub><italic>ij</italic></sub><sup><bold>u</bold></sup> as <inline-formula id="pcbi.1004643.e095"><alternatives><graphic id="pcbi.1004643.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>|</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which is an element of a 2×2 matrix <italic>z</italic><sup><bold>u</bold></sup>. We assume <italic>α</italic><sub>0,0</sub> = 0 since without activation, activity-dependent synaptic plasticity does not occur. As a simple Hebbian rule, we also assumed <italic>α</italic><sub>10</sub> = <italic>α</italic><sub>01</sub> = <italic>α</italic><sub>11</sub> = <italic>α</italic>, i.e., learning efficacies were common for all states of <bold>u</bold> except <bold>u</bold> = (0,0). As <xref ref-type="disp-formula" rid="pcbi.1004643.e093">Eq 15</xref> is rewritten as <italic>dW</italic><sub><italic>ij</italic></sub> = <italic>α</italic> (<italic>z</italic><sub><italic>ij</italic></sub><sup>1,0</sup> + <italic>z</italic><sub><italic>ij</italic></sub><sup>0,1</sup> + <italic>z</italic><sub><italic>ij</italic></sub><sup>1,1</sup>) + <italic>ε</italic><sup><italic>α</italic></sup>, under the assumption that <italic>p</italic>(<italic>ε</italic><sup><italic>α</italic></sup><sub><italic>ij</italic></sub>| <italic>α</italic>) is a Gaussian distribution <italic>N</italic>(<italic>ε</italic><sup><italic>α</italic></sup><sub><italic>ij</italic></sub>; 0, <italic>Σ</italic><sup><italic>εαij</italic></sup>), the negative log likelihood function for <italic>α</italic> is defined by
<disp-formula id="pcbi.1004643.e096">
<alternatives>
<graphic id="pcbi.1004643.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e096" xlink:type="simple"/>
<mml:math display="block" id="M96">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>log</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ε</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mn>50</mml:mn><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
where <italic>dW</italic><sub><italic>ij</italic></sub>(<italic>l</italic>) and <italic>z</italic><sub><italic>ij</italic></sub><sup><bold>u</bold></sup>(<italic>l</italic>) are the change of <italic>W</italic><sub><italic>ij</italic></sub> in <italic>l</italic>th trial and <italic>z</italic><sub><italic>ij</italic></sub><sup><bold>u</bold></sup> in <italic>l</italic>th trial. Since <italic>dW</italic><sub><italic>ij</italic></sub> is noisy and saturated in latter part, we assume <italic>dW</italic><sub><italic>ij</italic></sub>(<italic>l</italic>) = (<italic>W</italic><sub><italic>ij</italic></sub>(100) − <italic>W</italic><sub><italic>ij</italic></sub>(1))/100. Additionally, we assume <italic>Σ</italic><sup><italic>εαij</italic></sup>s are common among all <italic>i</italic> and <italic>j</italic>. From <xref ref-type="disp-formula" rid="pcbi.1004643.e096">Eq 16</xref>, under the assumption that <italic>α</italic> obeys a Gaussian distribution <italic>q</italic>(<italic>α</italic>) = <italic>N</italic>(<italic>α</italic>; <italic>μ</italic><sup><italic>α</italic></sup>, <italic>Σ</italic><sup><italic>α</italic></sup>), the expectation of <italic>α</italic> that gives the minimum of <italic>L</italic><sub><italic>α</italic></sub> is given by
<disp-formula id="pcbi.1004643.e097">
<alternatives>
<graphic id="pcbi.1004643.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e097" xlink:type="simple"/>
<mml:math display="block" id="M97">
<mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula></p>
<p>Next, we considered the situation where learning efficacies were different depending on the condition of <bold>u</bold> (<italic>β</italic>-model). We define a learning efficacy <italic>β</italic><sub><bold>u</bold></sub> (<italic>β</italic><sub>0,0</sub>, <italic>β</italic><sub>1,0</sub>, <italic>β</italic><sub>0,1</sub>, and <italic>β</italic><sub>1,1</sub>) as a function of <bold>u</bold>. We assumed <italic>β</italic><sub>0,0</sub> = 0, <italic>β</italic><sub>1,0</sub> = <italic>β</italic><sub>0,1</sub> = <italic>β</italic><sub>1</sub> since <bold>u</bold> = (1,0) and (0,1) are symmetric, and <italic>β</italic><sub>1,1</sub> = <italic>β</italic><sub>2</sub>. The learning rule of a <italic>β</italic>-model is given by
<disp-formula id="pcbi.1004643.e098">
<alternatives>
<graphic id="pcbi.1004643.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e098" xlink:type="simple"/>
<mml:math display="block" id="M98">
<mml:mrow><mml:mi>d</mml:mi><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle></mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>ε</mml:mi><mml:mi>β</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where <italic>ε</italic><sup><italic>β</italic></sup> is a matrix of error and its elements obey <italic>p</italic>(<italic>ε</italic><sup><italic>β</italic></sup><sub><italic>ij</italic></sub>| <italic>β</italic><sub>1</sub>,<italic>β</italic><sub>2</sub>) = <italic>N</italic>(<italic>ε</italic><sup><italic>β</italic></sup><sub><italic>ij</italic></sub>; 0, <italic>Σ</italic><sup><italic>εβij</italic></sup>). The negative log likelihood function for <italic>β</italic> is defined by
<disp-formula id="pcbi.1004643.e099">
<alternatives>
<graphic id="pcbi.1004643.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e099" xlink:type="simple"/>
<mml:math display="block" id="M99">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>log</mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ε</mml:mi><mml:mi>β</mml:mi></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>β</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>β</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mn>50</mml:mn><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>ε</mml:mi><mml:mi>β</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula></p>
<p>We also assume <italic>Σ</italic><sup><italic>εβij</italic></sup>s are common among all <italic>i</italic> and <italic>j</italic>. Under the assumption that (<italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub>)<sup><italic>T</italic></sup> obeys <italic>q</italic>((<italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub>)<sup><italic>T</italic></sup>) = <italic>N</italic>((<italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub>)<sup><italic>T</italic></sup>; (<italic>μ</italic><sup><italic>β</italic>1</sup>, <italic>μ</italic><sup><italic>β</italic>2</sup>)<sup><italic>T</italic></sup>, <italic>Σ</italic><sup><italic>β</italic></sup>), the expectation of (<italic>β</italic><sub>1</sub>, <italic>β</italic><sub>2</sub>)<sup><italic>T</italic></sup> that gives the minimum of <italic>L</italic><sub><italic>β</italic></sub> is given by
<disp-formula id="pcbi.1004643.e100">
<alternatives>
<graphic id="pcbi.1004643.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e100" xlink:type="simple"/>
<mml:math display="block" id="M100">
<mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula>
where <italic>dW</italic><sub><italic>ij</italic></sub>(<italic>l</italic>) and <italic>z</italic><sub><italic>ij</italic></sub><sup><bold>u</bold></sup>(<italic>l</italic>) are simplified as <italic>dW</italic><sub><italic>ij</italic></sub> and <italic>z</italic><sub><italic>ij</italic></sub><sup><bold>u</bold></sup>.</p>
</sec>
</sec>
</sec>
<sec id="sec025">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004643.s001" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s001" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>A schematic movie of experimental procedure at trial 1–10.</title>
<p>Setup is the same as that described in <xref ref-type="fig" rid="pcbi.1004643.g003">Fig 3</xref>.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s002" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s002" xlink:type="simple">
<label>S2 Movie</label>
<caption>
<title>A schematic movie of experimental procedure at trial 91–100.</title>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s003" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s003" xlink:type="simple">
<label>S3 Movie</label>
<caption>
<title>The evoked response transient of populations of cultured neurons throughout training.</title>
<p>Axes and colors are same as those in <xref ref-type="fig" rid="pcbi.1004643.g007">Fig 7A and 7B</xref>.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s004" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s004" xlink:type="simple">
<label>S1 Dataset</label>
<caption>
<title>Summarized dataset of responses of cultured neurons.</title>
<p>Data are composed of conditional expectation transients for each condition (x_u_trn.csv, …, x_u_alt4.csv), Kullback-Leibler divergence transients for each condition (kld_trn.csv, …, kld_alt4.csv), and trains of evoked spike number at trial 1, 11, …, 91 in each culture in the TRN group (x(t)_trn_1.csv, …, x(t)_trn_23.csv). In file names, alt1, alt2, alt3, and alt4 indicate data under the alternative conditions where (<italic>a</italic>, <italic>ρ</italic>) = (1/2, 1/2), (3/4, 1/4), (3/4, 3/4), and (1, 1/2), respectively.</p>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s005" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s005" xlink:type="simple">
<label>S1 Note</label>
<caption>
<title>Estimation of learning rule.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s006" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Response properties of cultured neurons to a mixture set of hidden sources.</title>
<p><bold>(A)</bold> Distribution. Red circles (open and filled) are <italic>u</italic><sub>1</sub>-preferring electrodes (<italic>n</italic> = 371 electrodes from 23 cultures). Blue circles (open and filled) are <italic>u</italic><sub>2</sub>-preferring electrodes (<italic>n</italic> = 345 electrodes from 23 cultures). As all trial average, the response of 13.5% of <italic>u</italic><sub>1</sub>-preferring electrodes to the <bold>u</bold> = (1,0) state was 3 times larger than that to the (0,1) state (filled red circles; <italic>n</italic> = 50 electrodes from 23 cultures). In addition, the response of 12.8% of <italic>u</italic><sub>2</sub>-preferring electrodes to the (0,1) state was 3 times larger than that to the (1,0) state (filled blue circles; <italic>n</italic> = 44 from 23 cultures). A black solid line, <inline-formula id="pcbi.1004643.e101"><alternatives><graphic id="pcbi.1004643.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. Black dashed lines, <inline-formula id="pcbi.1004643.e102"><alternatives><graphic id="pcbi.1004643.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>±</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. A red line, <inline-formula id="pcbi.1004643.e103"><alternatives><graphic id="pcbi.1004643.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. A blue line, <inline-formula id="pcbi.1004643.e104"><alternatives><graphic id="pcbi.1004643.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. <bold>(B)</bold> Transient. A red curve is the ratio of electrodes with 3· <italic>x</italic><sub><italic>i</italic></sub><sup>0,1</sup>(<italic>l</italic>) &lt; <italic>x</italic><sub><italic>i</italic></sub><sup>1,0</sup>(<italic>l</italic>) to <italic>u</italic><sub>1</sub>-preferring electrodes. A blue curve is the ratio of electrodes with <italic>x</italic><sub><italic>i</italic></sub><sup>0,1</sup>(<italic>l</italic>) &gt;3· <italic>x</italic><sub><italic>i</italic></sub><sup>1,0</sup>(<italic>l</italic>) to <italic>u</italic><sub>2</sub>-preferring electrodes. Both curves increased during training. Shadowed areas are S.E.M.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s007" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Properties of the neural population model.</title>
<p><bold>(A)</bold> I/O function of evoked response of the neural population model. Horizontal axis, total inputs (<inline-formula id="pcbi.1004643.e105"><alternatives><graphic id="pcbi.1004643.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>). Vertical axis, total outputs of neural population (<inline-formula id="pcbi.1004643.e106"><alternatives><graphic id="pcbi.1004643.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>). A black curve is the mean of total output for each total input. The shadowed area is the standard deviation. Total neural output is almost proportional to total input except when <inline-formula id="pcbi.1004643.e107"><alternatives><graphic id="pcbi.1004643.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> = 0, i.e., when <bold>u</bold> = (0,0) state. Since we assume that Hebbian plasticity does not occur when <bold>u</bold> = (0,0) state, effectively, we can regard the I/O function as linear for considering learning rule of neural networks. <bold>(B)</bold> <italic>γ</italic>-norm of connection strengths. Notably, we define <italic>γ</italic>-norm by <inline-formula id="pcbi.1004643.e108"><alternatives><graphic id="pcbi.1004643.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004643.e108" xlink:type="simple"/><mml:math display="inline" id="M108"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>γ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>γ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>γ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>γ</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Red, black, and gray curves are transients of norms with <italic>γ</italic> = 1, 2, and 4, respectively. The red curve gradually decreased between trial 20 and 100, while the black and gray curves maintained almost same value between trial 20 and 100. Therefore, if there is a constraint on total synaptic strength as predicted by theoretical studies [<xref ref-type="bibr" rid="pcbi.1004643.ref009">9</xref>], norm with <italic>γ</italic> = 2–4 is more consistent with experimental data than that with <italic>γ</italic> = 1.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s008" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Free energy properties in cultured neural networks in the presence of 20-μM APV.</title>
<p><bold>(A)</bold> Connection strengths of the neural population. Black circles and squares are <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub>. White circles and squares are <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub>. Bars are S.E.M. <bold>(B)</bold> The change in connection strengths (trial 1 vs. trial 100). In the presence of 20-μM APV, <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub> increased after training (**, <italic>p</italic> &lt; 10<sup>−2</sup>; <italic>n</italic> = 18 from 9 cultures), and <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub> also increased (*, <italic>p</italic> = 0.024; <italic>n</italic> = 18 from 9 cultures). <bold>(C)</bold> Transition of the expectation of internal energy (〈<italic>U</italic>〉; white circles), Shannon entropy (<italic>H</italic>; black circles), and free energy (<italic>F</italic>; red circles). Bars are S.E.M. <bold>(D)</bold> The change in 〈<italic>U</italic>〉, <italic>H</italic>, and <italic>F</italic> (trial 1 vs. trial 100) in the presence of 20-μM APV. After training, the expectation of internal energy did not change (<italic>p</italic> = 0.250; <italic>n</italic> = 9 cultures), Shannon entropy slightly increased (*, <italic>p</italic> = 0.027; <italic>n</italic> = 9 cultures), and free energy did not change (<italic>p</italic> = 1.000; <italic>n</italic> = 9 cultures).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004643.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004643.s009" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Hebbian plasticity with a <italic>γ</italic>-norm constraint on total connection strength.</title>
<p><bold>(A)</bold> The expectations of <italic>α’</italic> and <italic>λ</italic> when we change the degree of <italic>γ</italic>-norm constraint. Black and gray curves are the mean of <italic>μ</italic><sup><italic>α’</italic></sup>(<italic>γ</italic>) and <italic>μ</italic><sup><italic>λ</italic></sup>(<italic>γ</italic>), respectively. <bold>(B)</bold> BIC of the <italic>α’</italic>-model when we change the degree of <italic>γ</italic>-norm constraint. A black curve is the mean of BIC. A dashed line is BIC of the <italic>β</italic>-model. Red, black, and gray arrows correspond to red, black, and gray curves in <xref ref-type="supplementary-material" rid="pcbi.1004643.s007">S2B Fig</xref>, respectively. <bold>(C)</bold> Bayesian model comparison between <italic>α’</italic>- and <italic>β</italic>-models. For the wide range of <italic>γ</italic>, the <italic>β</italic>-model is more plausible than the <italic>α’</italic>-model to represent experimental data (*, <italic>p</italic> = 0.035 for <italic>γ</italic> = 1, red circles; ****, <italic>p</italic> &lt; 10<sup>−5</sup> for <italic>γ</italic> = 2, black circles; ****, <italic>p</italic> &lt; 10<sup>−5</sup> for <italic>γ</italic> = 4, gray circles). Circle colors correspond to the arrow colors in (B). <bold>(D)</bold> The change in connection strengths estimated from the <italic>α’</italic>-model. A black curve, the mean of <italic>W</italic><sub>11</sub> and <italic>W</italic><sub>22</sub>. A gray curve, the mean of <italic>W</italic><sub>12</sub> and <italic>W</italic><sub>21</sub>. Solid lines, the true change. Dashed lines, the change estimated from the <italic>β</italic>-model, same as <xref ref-type="fig" rid="pcbi.1004643.g009">Fig 9C</xref>. In (A), (B), (D), shadowed areas are S.E.M.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004643.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belouchrani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Abed-Meraim</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cardoso</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Moulines</surname> <given-names>E</given-names></name> (<year>1997</year>) <article-title>A blind source separation technique using second-order statistics</article-title>. <source>Signal Processing IEEE Trans on</source> <volume>45</volume>(<issue>2</issue>): <fpage>434</fpage>–<lpage>444</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Choi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cichocki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>SY</given-names></name> (<year>2005</year>) <article-title>Blind source separation and independent component analysis: A review</article-title>. <source>Neural Information Processing-Letters and Reviews</source> <volume>6</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>57</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cichocki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zdunek</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Phan</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name> (<year>2009</year>) <source>Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation</source>. (<publisher-name>John Wiley &amp; Sons</publisher-name>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Comon</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jutten</surname> <given-names>C</given-names></name> (Eds.) (<year>2010</year>) <source>Handbook of Blind Source Separation: Independent component analysis and applications</source>. (<publisher-name>Academic press</publisher-name>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bronkhorst</surname> <given-names>AW</given-names></name> (<year>2000</year>) <article-title>The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions</article-title>. <source>Acta Acustica united with Acustica</source> <volume>86</volume>(<issue>1</issue>): <fpage>117</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Narayan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Best</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ozmeral</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>McClaine</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dent</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sen</surname> <given-names>K</given-names></name> (<year>2007</year>) <article-title>Cortical interference effects in the cocktail party problem</article-title>. <source>Nat Neurosci</source> <volume>10</volume>(<issue>12</issue>): <fpage>1601</fpage>–<lpage>1607</lpage>. <object-id pub-id-type="pmid">17994016</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mesgarani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name> (<year>2012</year>) <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source> <volume>485</volume>(<issue>7397</issue>): <fpage>233</fpage>–<lpage>236</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11020" xlink:type="simple">10.1038/nature11020</ext-link></comment> <object-id pub-id-type="pmid">22522927</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Golumbic</surname> <given-names>EMZ</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bickel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schevon</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>McKhann</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name> (<year>2013</year>) <article-title>Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”</article-title>. <source>Neuron</source> <volume>77</volume>(<issue>5</issue>): <fpage>980</fpage>–<lpage>991</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.12.037" xlink:type="simple">10.1016/j.neuron.2012.12.037</ext-link></comment> <object-id pub-id-type="pmid">23473326</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref009"><label>9</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2001</year>) <source>Theoretical neuroscience: computational and mathematical modeling of neural systems</source>. (<publisher-name>MIT Press</publisher-name>, <publisher-loc>London</publisher-loc>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref010"><label>10</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kistler</surname> <given-names>WM</given-names></name> (<year>2002</year>) <source>Spiking neuron models</source>. (<publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Nasrabadi</surname> <given-names>NM</given-names></name> (<year>2006</year>) <source>Pattern recognition and machine learning</source>. (<publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oja</surname> <given-names>E</given-names></name> (<year>1982</year>) <article-title>A simplified neuron model as a principal component analyzer</article-title>. <source>J Math Biol</source> <volume>15</volume>(<issue>3</issue>): <fpage>267</fpage>–<lpage>273</lpage>. <object-id pub-id-type="pmid">7153672</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name> (<year>1995</year>) <article-title>An information-maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput</source> <volume>7</volume>(<issue>6</issue>): <fpage>1129</fpage>–<lpage>1159</lpage>. <object-id pub-id-type="pmid">7584893</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name> (<year>1997</year>) <article-title>The “independent components” of natural scenes are edge filters</article-title>. <source>Vision Res</source> <volume>37</volume>(<issue>23</issue>): <fpage>3327</fpage>–<lpage>3338</lpage>. <object-id pub-id-type="pmid">9425547</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name> (<year>1996</year>) <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>(<issue>6583</issue>): <fpage>607</fpage>–<lpage>609</lpage>. <object-id pub-id-type="pmid">8637596</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name> (<year>1997</year>) <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title>. <source>Vision Res</source> <volume>37</volume>(<issue>23</issue>): <fpage>3311</fpage>–<lpage>3325</lpage>. <object-id pub-id-type="pmid">9425546</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bastos</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Usrey</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Mangun</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name> (<year>2012</year>) <article-title>Canonical microcircuits for predictive coding</article-title>. <source>Neuron</source> <volume>76</volume>(<issue>4</issue>): <fpage>695</fpage>–<lpage>711</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.10.038" xlink:type="simple">10.1016/j.neuron.2012.10.038</ext-link></comment> <object-id pub-id-type="pmid">23177956</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hebb</surname> <given-names>DO</given-names></name> (<year>1949</year>) <source>Organization of Behavior: A Neurophysiological Theory</source> (<publisher-name>John Wiley &amp; Sons</publisher-name>, <publisher-loc>New York</publisher-loc>).</mixed-citation></ref>
<ref id="pcbi.1004643.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name> (<year>2008</year>) <article-title>Hierarchical model in the brain</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>(<issue>11</issue>): <fpage>e1000211</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment> <object-id pub-id-type="pmid">18989391</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name> (<year>2010</year>) <article-title>The free-energy principle: a unified brain theory?</article-title>. <source>Nat Rev Neurosci</source> <volume>11</volume>(<issue>2</issue>): <fpage>127</fpage>–<lpage>138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2787" xlink:type="simple">10.1038/nrn2787</ext-link></comment> <object-id pub-id-type="pmid">20068583</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2000</year>) <article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity</article-title>. <source>Nat Neurosci</source> <volume>3</volume>(<issue>9</issue>): <fpage>919</fpage>–<lpage>926</lpage>. <object-id pub-id-type="pmid">10966623</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toyoizumi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Aihara</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2005</year>) <article-title>Generalized Bienenstock-Cooper-Munro rule for spiking neurons that maximizes information transmission</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>102</volume>(<issue>14</issue>): <fpage>5239</fpage>–<lpage>5244</lpage>. <object-id pub-id-type="pmid">15795376</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Büsing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title>. <source>Nat Neurosci</source> <volume>13</volume>(<issue>3</issue>): <fpage>344</fpage>–<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2479" xlink:type="simple">10.1038/nn.2479</ext-link></comment> <object-id pub-id-type="pmid">20098420</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Savin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Joshi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Triesch</surname> <given-names>J</given-names></name> (<year>2010</year>) <article-title>Independent component analysis in spiking neurons</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>(<issue>4</issue>): <fpage>e1000757</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000757" xlink:type="simple">10.1371/journal.pcbi.1000757</ext-link></comment> <object-id pub-id-type="pmid">20421937</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fukai</surname> <given-names>T</given-names></name> (<year>2011</year>) <article-title>Stability versus neuronal specialization for STDP: Long-tail weight distributions solve the dilemma</article-title>. <source>PLoS ONE</source> <volume>6</volume>(<issue>10</issue>): <fpage>e25339</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0025339" xlink:type="simple">10.1371/journal.pone.0025339</ext-link></comment> <object-id pub-id-type="pmid">22003389</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fukai</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Burkitt</surname> <given-names>AN</given-names></name> (<year>2012</year>) <article-title>Spectral analysis of input spike trains by spike-timing-dependent plasticity</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>(<issue>7</issue>): <fpage>e1002584</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002584" xlink:type="simple">10.1371/journal.pcbi.1002584</ext-link></comment> <object-id pub-id-type="pmid">22792056</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Deneve</surname> <given-names>S</given-names></name> (<year>2013</year>) <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>(<issue>11</issue>): <fpage>e1003258</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003258" xlink:type="simple">10.1371/journal.pcbi.1003258</ext-link></comment> <object-id pub-id-type="pmid">24244113</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name> (<year>2011</year>) <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source> <volume>331</volume>(<issue>6013</issue>): <fpage>83</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1195870" xlink:type="simple">10.1126/science.1195870</ext-link></comment> <object-id pub-id-type="pmid">21212356</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barth</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poulet</surname> <given-names>JFA</given-names></name> (<year>2012</year>) <article-title>Experimental evidence for sparse firing in the neocotex</article-title>. <source>Trends Neurosci</source> <volume>35</volume>(<issue>6</issue>): <fpage>345</fpage>–<lpage>355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2012.03.008" xlink:type="simple">10.1016/j.tins.2012.03.008</ext-link></comment> <object-id pub-id-type="pmid">22579264</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jimbo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tateno</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>HPC</given-names></name> (<year>1999</year>) <article-title>Simultaneous induction of pathway-specific potentiation and depression in networks of cortical neurons</article-title>. <source>Biophys J</source> <volume>76</volume>(<issue>2</issue>): <fpage>670</fpage>–<lpage>678</lpage>. <object-id pub-id-type="pmid">9929472</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Goel</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name> (<year>2010</year>) <article-title>Neural dynamics of in vitro cortical networks reflects experienced temporal patterns</article-title>. <source>Nat Neurosci</source> <volume>13</volume>(<issue>8</issue>): <fpage>917</fpage>–<lpage>919</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2579" xlink:type="simple">10.1038/nn.2579</ext-link></comment> <object-id pub-id-type="pmid">20543842</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shahaf</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Marom</surname> <given-names>S</given-names></name> (<year>2001</year>) <article-title>Learning in networks of cortical neurons</article-title>. <source>J Neurosci</source> <volume>21</volume>(<issue>22</issue>): <fpage>8782</fpage>–<lpage>8788</lpage>. <object-id pub-id-type="pmid">11698590</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eytan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Marom</surname> <given-names>S</given-names></name> (<year>2003</year>) <article-title>Selective adaptation in networks of cortical neurons</article-title>. <source>J Neurosci</source> <volume>23</volume>(<issue>28</issue>): <fpage>9349</fpage>–<lpage>9356</lpage>. <object-id pub-id-type="pmid">14561862</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruaro</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Bonifazi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Torre</surname> <given-names>V</given-names></name> (<year>2005</year>) <article-title>Toward the neurocomputer: image processing and pattern recognition with neuronal cultures</article-title>. <source>IEEE Trans Biomed Eng</source> <volume>52</volume>(<issue>3</issue>): <fpage>371</fpage>–<lpage>383</lpage>. <object-id pub-id-type="pmid">15759567</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feinerman</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>E</given-names></name> (<year>2006</year>) <article-title>Transport of information along unidimensional layered networks of dissociated hippocampal neurons and implications for rate coding</article-title>. <source>J Neurosci</source> <volume>26</volume>(<issue>17</issue>): <fpage>4526</fpage>–<lpage>4534</lpage>. <object-id pub-id-type="pmid">16641232</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feinerman</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rotem</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moses</surname> <given-names>E</given-names></name> (<year>2008</year>) <article-title>Reliable neuronal logic devices from patterned hippocampal cultures</article-title>. <source>Nat Phys</source> <volume>4</volume>(<issue>12</issue>): <fpage>967</fpage>–<lpage>973</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dranias</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Ju</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rajaram</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>VanDongen</surname> <given-names>AMJ</given-names></name> (<year>2013</year>) <article-title>Short-term memory in networks of dissociated cortical neurons</article-title>. <source>J Neurosci</source> <volume>33</volume>(<issue>5</issue>): <fpage>1940</fpage>–<lpage>1953</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2718-12.2013" xlink:type="simple">10.1523/JNEUROSCI.2718-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23365233</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turrigiano</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SB</given-names></name> (<year>2004</year>) <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>(<issue>2</issue>): <fpage>97</fpage>–<lpage>107</lpage>. <object-id pub-id-type="pmid">14735113</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fong</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Newman</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Potter</surname> <given-names>SM</given-names></name> (<year>2015</year>) <article-title>Upward synaptic scaling is dependent on neurotransmission rather than spiking</article-title>. <source>Nat Comm</source> <volume>6</volume>: <fpage>6339</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jimbo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kasai</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Torimitsu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tateno</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>HPC</given-names></name> (<year>2003</year>) <article-title>A system for MEA-based multisite stimulation</article-title>. <source>IEEE Trans Biomed Eng</source> <volume>50</volume>(<issue>2</issue>): <fpage>241</fpage>–<lpage>248</lpage>. <object-id pub-id-type="pmid">12665038</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname> <given-names>E. T.</given-names></name> (<year>1957</year>). <article-title>Information theory and statistical mechanics</article-title>. <source>Physical Review</source>, <volume>106</volume>(<issue>4</issue>): <fpage>620</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname> <given-names>E. T.</given-names></name> (<year>1957</year>). <article-title>Information theory and statistical mechanics</article-title>. <source>II. Physical Review</source>, <volume>108</volume>(<issue>2</issue>): <fpage>171</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bakkum</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Chao</surname> <given-names>ZC</given-names></name>, <name name-style="western"><surname>Potter</surname> <given-names>SM</given-names></name> (<year>2008</year>) <article-title>Long-term activity-dependent plasticity of action potential propagation delay and amplitude in cortical networks</article-title>. <source>PLoS ONE</source> <volume>3</volume>(<issue>5</issue>): <fpage>e2088</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0002088" xlink:type="simple">10.1371/journal.pone.0002088</ext-link></comment> <object-id pub-id-type="pmid">18461127</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Cichocki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>HH</given-names></name> (<year>1996</year>) <article-title>A new learning algorithm for blind signal separation</article-title>. <source>Adv Neural Inf Proc Sys</source> <volume>8</volume>: <fpage>757</fpage>–<lpage>763</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldt</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bonifazi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cossart</surname> <given-names>R</given-names></name> (<year>2011</year>) <article-title>Dissecting functional connectivity of neuronal microcircuits: experimental and theoretical insights</article-title>. <source>Trends Neurosci</source> <volume>34</volume>(<issue>5</issue>): <fpage>225</fpage>–<lpage>236</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2011.02.007" xlink:type="simple">10.1016/j.tins.2011.02.007</ext-link></comment> <object-id pub-id-type="pmid">21459463</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stetter</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Battaglia</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Soriano</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisel</surname> <given-names>T</given-names></name> (<year>2012</year>) <article-title>Model-free reconstruction of excitatory neuronal connectivity from calcium imaging signals</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>(<issue>8</issue>): <fpage>e1002653</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002653" xlink:type="simple">10.1371/journal.pcbi.1002653</ext-link></comment> <object-id pub-id-type="pmid">22927808</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name> (<year>1978</year>) <article-title>Estimating the dimension of a model</article-title>. <source>Ann Stat</source> <volume>6</volume>(<issue>2</issue>): <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Földiak</surname> <given-names>P</given-names></name> (<year>1990</year>) <article-title>Forming sparse representations by local anti-Hebbian learning</article-title>. <source>Biol Cybern</source> <volume>64</volume>(<issue>2</issue>): <fpage>165</fpage>–<lpage>170</lpage>. <object-id pub-id-type="pmid">2291903</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lübke</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Frotscher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name> (<year>1997</year>) <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title>. <source>Science</source> <volume>275</volume>(<issue>5297</issue>): <fpage>213</fpage>–<lpage>215</lpage>. <object-id pub-id-type="pmid">8985014</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bi</surname> <given-names>GQ</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>MM</given-names></name> (<year>1998</year>) <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J Neurosci</source> <volume>18</volume>(<issue>24</issue>): <fpage>10464</fpage>–<lpage>10472</lpage>. <object-id pub-id-type="pmid">9852584</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kanold</surname> <given-names>PO</given-names></name>, <name name-style="western"><surname>Shatz</surname> <given-names>CJ</given-names></name> (<year>2007</year>) <article-title>A burst-based “Hebbian” learning rule at retinogeniculate synapses links retinal waves to activity-dependent refinement</article-title>. <source>PLoS Biol</source> <volume>5</volume>(<issue>3</issue>): <fpage>e61</fpage>. <object-id pub-id-type="pmid">17341130</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name> (<year>2013</year>) <article-title>Chronic electrical stimulation homeostatically decreases spontaneous activity, but paradoxically increases evoked network activity</article-title>. <source>J Neurophysiol</source> <volume>109</volume>(<issue>7</issue>): <fpage>1836</fpage>–<lpage>2013</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lourenço</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Pacioni</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Rebola</surname> <given-names>N.</given-names></name>, <name name-style="western"><surname>van Woerden</surname> <given-names>G. M.</given-names></name>, <name name-style="western"><surname>Marinelli</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>DiGregorio</surname> <given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Bacci</surname> <given-names>A.</given-names></name> (<year>2014</year>). <article-title>Non-associative Potentiation of Perisomatic Inhibition Alters the Temporal Coding of Neocortical Layer 5 Pyramidal Neurons</article-title>. <source>PLoS Biol</source> <volume>12</volume>(<issue>7</issue>): <fpage>e1001903</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1001903" xlink:type="simple">10.1371/journal.pbio.1001903</ext-link></comment> <object-id pub-id-type="pmid">25003184</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>D’amour</surname> <given-names>J. A.</given-names></name>, &amp; <name name-style="western"><surname>Froemke</surname> <given-names>R. C.</given-names></name> (<year>2015</year>). <article-title>Inhibitory and excitatory spike-timing-dependent plasticity in the auditory cortex</article-title>. <source>Neuron</source>, <volume>86</volume>(<issue>2</issue>), <fpage>514</fpage>–<lpage>528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.03.014" xlink:type="simple">10.1016/j.neuron.2015.03.014</ext-link></comment> <object-id pub-id-type="pmid">25843405</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linsker</surname> <given-names>R</given-names></name> (<year>1997</year>) <article-title>A local learning rule that enables information maximization for arbitrary input distributions</article-title>. <source>Neural Comput</source> <volume>9</volume>(<issue>8</issue>): <fpage>1661</fpage>–<lpage>1665</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linsker</surname> <given-names>R</given-names></name> (<year>1992</year>) <article-title>Local synaptic learning rules suffice to maximize mutual information in a linear network</article-title>. <source>Neural Comput</source> <volume>4</volume>(<issue>5</issue>): <fpage>691</fpage>–<lpage>702</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bell</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name> (<year>2000</year>) <article-title>A unifying information-theoretic framework for independent component analysis</article-title>. <source>Comput Math Appl</source> <volume>39</volume>(<issue>11</issue>): <fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hayama</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noguchi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Takahashi</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Matsuzaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kasai</surname> <given-names>H</given-names></name> (<year>2013</year>) <article-title>GABA promotes the competitive selection of dendritic spines by controlling local Ca<sup>2+</sup> signaling</article-title>. <source>Nat Neurosci</source> <volume>16</volume>(<issue>10</issue>): <fpage>1409</fpage>–<lpage>1416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3496" xlink:type="simple">10.1038/nn.3496</ext-link></comment> <object-id pub-id-type="pmid">23974706</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paille</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Fino</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Du</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Morera-Herreras</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Perez</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kotaleski</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Venance</surname> <given-names>L</given-names></name> (<year>2013</year>) <article-title>GABAergic Circuits Control Spike-Timing-Dependent Plasticity</article-title>. <source>J Neurosci</source> <volume>33</volume>(<issue>22</issue>): <fpage>9353</fpage>–<lpage>9363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5796-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5796-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23719804</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leutgeb</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Leutgeb</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name> (<year>2007</year>) <article-title>Pattern separation in the dentate gyrus and CA3 of the hippocampus</article-title>. <source>Science</source> <volume>315</volume>(<issue>5814</issue>): <fpage>961</fpage>–<lpage>966</lpage>. <object-id pub-id-type="pmid">17303747</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiechert</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Judkewitz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Riecke</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Friedrich</surname> <given-names>RW</given-names></name> (<year>2010</year>) <article-title>Mechanisms of pattern decorrelation by recurrent neuronal circuits</article-title>. <source>Nat Neurosci</source> <volume>13</volume>(<issue>8</issue>): <fpage>1003</fpage>–<lpage>1010</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2591" xlink:type="simple">10.1038/nn.2591</ext-link></comment> <object-id pub-id-type="pmid">20581841</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name> (<year>2012</year>) <article-title>Decorrelation and efficient coding by retinal ganglion cells</article-title>. <source>Nat Neurosci</source> <volume>15</volume>(<issue>4</issue>): <fpage>628</fpage>–<lpage>635</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3064" xlink:type="simple">10.1038/nn.3064</ext-link></comment> <object-id pub-id-type="pmid">22406548</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isomura</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ogawa</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kotani</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jimbo</surname> <given-names>Y</given-names></name> (<year>2015</year>) <article-title>Accurate connection strength estimation based on variational Bayes for detecting synaptic plasticity</article-title>. <source>Neural Comput</source> <volume>27</volume>(<issue>4</issue>): <fpage>819</fpage>–<lpage>844</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00721" xlink:type="simple">10.1162/NECO_a_00721</ext-link></comment> <object-id pub-id-type="pmid">25710089</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kamioka</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Maeda</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jimbo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>HPC</given-names></name>, <name name-style="western"><surname>Kawana</surname> <given-names>A</given-names></name> (<year>1996</year>) <article-title>Spontaneous periodic synchronized bursting during formation of mature patterns of connections in cortical cultures</article-title>. <source>Neurosci Lett</source> <volume>206</volume>(<issue>2</issue>): <fpage>109</fpage>–<lpage>112</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mukai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shiina</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Jimbo</surname> <given-names>Y</given-names></name> (<year>2003</year>) <article-title>Continuous monitoring of developmental activity changes in cultured cortical networks</article-title>. <source>Electr Eng Jpn</source> <volume>145</volume>(<issue>4</issue>): <fpage>28</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004643.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetzlaff</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Okujeni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Egert</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Butz</surname> <given-names>M</given-names></name> (<year>2010</year>) <article-title>Self-organized criticality in developing neuronal networks</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>(<issue>12</issue>): <fpage>e1001013</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001013" xlink:type="simple">10.1371/journal.pcbi.1001013</ext-link></comment> <object-id pub-id-type="pmid">21152008</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takekawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Isomura</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Fukai</surname> <given-names>T</given-names></name> (<year>2010</year>) <article-title>Accurate spike sorting for multi-unit recordings</article-title>. <source>Eur J Neurosci</source> <volume>31</volume>(<issue>2</issue>): <fpage>263</fpage>–<lpage>272</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2009.07068.x" xlink:type="simple">10.1111/j.1460-9568.2009.07068.x</ext-link></comment> <object-id pub-id-type="pmid">20074217</object-id></mixed-citation></ref>
<ref id="pcbi.1004643.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name> (<year>2004</year>) <article-title>Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model</article-title>. <source>Neural Comput</source> <volume>16</volume>(<issue>12</issue>): <fpage>2533</fpage>–<lpage>2561</lpage>. <object-id pub-id-type="pmid">15516273</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>