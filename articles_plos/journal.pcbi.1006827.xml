<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00571</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006827</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Management engineering</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Optimizing the depth and the direction of prospective planning using information values</article-title>
<alt-title alt-title-type="running-head">Optimizing the depth and the direction of prospective planning using information values</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9661-2519</contrib-id>
<name name-style="western">
<surname>Sezener</surname> <given-names>Can Eren</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Dezfouli</surname> <given-names>Amir</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Keramati</surname> <given-names>Mehdi</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Bernstein Center for Computational Neuroscience Berlin, Berlin, Germany</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Technische Universitaet Berlin, Berlin, Germany</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Data61, CSIRO, Australia</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>School of Psychology, UNSW, Sydney, Australia</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Gatsby Computational Neuroscience Unit, Sainsbury Wellcome Centre, University College London, London, UK</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Max Planck Centre for Computational Psychiatry and Ageing Research, University College London, London, UK</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Mattar</surname> <given-names>Marcelo Gomes</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Pennsylvania School of Arts and Sciences, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">erensezener@gmail.com</email> (CES); <email xlink:type="simple">m.keramati@ucl.ac.uk</email> (MK)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>12</day>
<month>3</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e1006827</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>28</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Sezener et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006827"/>
<abstract>
<p>Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are “in which directions the search tree should be expanded?”, and “when should the expansion stop?”. Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>When faced with several choices in complex environments like chess, thinking about all the potential consequences of each choice, infinitely deep into the future, is simply impossible due to time and cognitive limitations. An outstanding question is what is the best direction and depth of thinking about the future? Here we propose a mathematical algorithm that computes, along the course of planning, the benefit of thinking another step in a given direction into the future, and compares that with the cost of thinking in order to compute the net benefit. We show that this algorithm is consistent with several behavioral patterns observed in humans and animals, suggesting that they, too, make efficient use of their time and cognitive resources when deciding how deep to think.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000923</institution-id>
<institution>Australian Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>DP150104878</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Dezfouli</surname> <given-names>Amir</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000324</institution-id>
<institution>Gatsby Charitable Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Keramati</surname> <given-names>Mehdi</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>AD was supported by grant DP150104878 from the Australian Research Council, and MK by the Gatsby Charitable Foundation and the Max Planck Society. We acknowledge support by the German Research Foundation and the Open Access Publication Fund of TU Berlin. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="21"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-03-29</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<disp-quote>
<p>“<italic>There is proportional value in our attention to each action—so you will not lose heart if you devote no more time than they warrant to matters of less importance</italic>.”</p>
<p>– Marcus Aurelius, <italic>Meditations</italic> [<xref ref-type="bibr" rid="pcbi.1006827.ref001">1</xref>]</p>
</disp-quote>
<p>When confronted with several choices, we need to have an evaluation of how good each option is. Each choice has some immediate consequences, but also takes us into a new state where new choices emerge, and so on. Think of chess as an example. One intuitive way to solve a sequential decision-making problem like chess is to prospectively think into the future. This idea, known as model-based planning in the reinforcement learning literature [<xref ref-type="bibr" rid="pcbi.1006827.ref002">2</xref>], expands a mental decision-tree by simulating a number of future action sequences. Although this method is accurate (in terms of statistical efficiency), evaluating deep trees is computationally expensive (in terms of time, working memory, metabolic energy, etc.). In chess, for example, it is impossible even for the best supercomputers to expand the tree of all possible strategies up to the end of the game. Therefore, several solutions have been provided in the artificial intelligence literature for how to approximate the values of choices without expanding a search tree to its fullest extent [<xref ref-type="bibr" rid="pcbi.1006827.ref003">3</xref>] or how to make the best use of limited computational resources to plan better [<xref ref-type="bibr" rid="pcbi.1006827.ref004">4</xref>].</p>
<p>To avoid the costs of planning altogether, a drastic alternative is to rely on heuristic methods that evaluate choices without any tree expansion. For example, a chess player can evaluate a chess position, without investigating the possibility of that position leading to a win or lose, by simply counting up the values of their pieces—a common heuristic utilized by novice players. Another example of approximate evaluation techniques, widely used in both natural and artificial intelligence. is using habits. This method, known as model-free reinforcement learning [<xref ref-type="bibr" rid="pcbi.1006827.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref005">5</xref>], simply “caches” the average of previously realized rewards ensued by performing each action, and uses the cached values for evaluating those choices should they come up again in the future. Although using such heuristics frees cognitive resources from model-based planning, the downside is their inaccuracy. Habits, for example, take many trials to form, and they are always unreliable in changing environments.</p>
<p>Rather than clinging to one of these extreme solutions (i.e., full planning vs. heuristics/habits), an intelligent agent can instead combine the two in order to harvest the relative advantages (i.e., accuracy vs. affordability) of both techniques [<xref ref-type="bibr" rid="pcbi.1006827.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1006827.ref009">9</xref>]. This, in theory, is achievable by forward planning up to some depth and then exploiting heuristic values as proxies for consequences that may arise in the further future. That is, when the depth of planning is say <italic>d</italic>, the agent computes the value of a choice by adding the first <italic>d</italic> rewards predicted by explicit simulation, to the value of the remaining actions estimated by the heuristic/habitual values. For example, a chess player could think three steps ahead, and then estimate, heuristically, the strength of the position he could achieve after those three moves. This integrative approach has been used in artificial intelligence for example for obtaining super-human Go performance [<xref ref-type="bibr" rid="pcbi.1006827.ref010">10</xref>]). Furthermore, it was shown recently that humans also use this scheme, named plan-until-habit, for integrating planning and habitual processes in a normative way, and that their depth of planning depends on the time-pressure imposed on them [<xref ref-type="bibr" rid="pcbi.1006827.ref011">11</xref>].</p>
<p>The plan-until-habit (or plan-until-heuristic, in general) scheme aims at mitigating the computational costs of planning by appealing to the habitual system after the planning system has <italic>sufficiently</italic> expanded the decision-tree. Obviously, the first questions to be asked in this framework are “in which directions the decision-tree should be expanded?”, and “when should the expansion stop?”. In this paper, we present, for the first time, a principled algorithm for optimal tree-expansion in the plan-until-habit framework. The algorithm is based on a speed/accuracy tradeoff: deeper planning leads to more accurate evaluations, but at the cost of slower decision-making. As a proof of concept, we show through simulations how this algorithm expands the decision-tree effectively and efficiently in a simulated grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior during training. The algorithms also provide several predictions testable in animal/human experiments.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Theory sketch</title>
<p>From an <italic>external-observer</italic> viewpoint, the questions to be answered by an agent are of the type “what action should be taken?”. From a <italic>metacognitive</italic> perspective, however, the agent should first think about how to think (e.g., how deep she should plan). In fact, the question she could ask at each step of the planning process is “Should I expand the decision-tree one step further?”, and if yes, “In what direction?”.</p>
<p>To answer these, assume that the agent has already expanded a tree to a certain extent (<xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1A</xref>). This means that the agent knows, possibly with some uncertainties, a few next states to be visited upon taking each action, and the immediate rewards associated with each of those transitions. She can, therefore, sum up the predicted rewards along each trajectory (i.e., action-sequence) and have an estimate of the total rewards to be achieved. On the top of this “total immediate rewards”, each trajectory ends in a frontier state which represents the edge of the current planning horizon along that trajectory. The habitual (or any other heuristic) values on this frontier state supposedly reflect the total (discounted) rewards to be expected from that point on. Therefore, the sum of “total immediate rewards” and the habitual value of the frontier node provides an estimate of the total expected reward of each trajectory (<xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1B</xref>).</p>
<fig id="pcbi.1006827.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006827.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Overview of the pruning scheme, illustrated via an example.</title>
<p>(A) A snapshot of the search tree. Nodes of the tree represent states, and each state has a number of available actions, denoted with circles, that lead to next states. Blue graphs show value distributions for the leaves of the tree, estimated by the model-free (MF) or any other heuristic system. Green graphs show the immediate rewards for previously expanded state-actions, estimated via the model-based (MB) system. (B) Each path from the root to a leave forms a strategy, <italic>A</italic><sub><italic>i</italic></sub>, with a corresponding value distribution. These distributions are obtained by summing up the value distributions of the leaves with the immediate reward distributions accumulated along the way. (C) To compute the value of uncertainty resolution (<sc>vur</sc>), say for <italic>A</italic><sub>3</sub>, the agents assumes that one further expansion would result in a sharper value distribution (one of the black/grey distributions). The location (i.e., the mean) of the new distribution cannot be known in advance, but it can be treated as a random variable, whose distribution can be analytically obtained (<xref ref-type="disp-formula" rid="pcbi.1006827.e066">Eq 14</xref>). The <sc>vur</sc> for <italic>A</italic><sub>3</sub> is therefore the expected value, over all possible sharper distributions (grey curves), of the additional rewards that can be obtained by a policy improvement in the light of that potential new information (i.e., the sharper distribution). (D) After computing <sc>vur</sc> for all strategies <italic>A</italic><sub><italic>i</italic></sub>, the highest <sc>vur</sc> (in this case, for <italic>A</italic><sub>3</sub>) is compared to the cost of expansion. If it is bigger than the cost, the tree expands along the direction of that strategy. This corresponds to loading a new node, which is the successor state of the leaf of <italic>A</italic><sub>3</sub>, from the MB system and adding it to the tree.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.g001" xlink:type="simple"/>
</fig>
<p>Habitual values, however, can be highly unreliable due to the inflexible nature of habit formation. For each given trajectory, therefore, the dependence of its estimated total rewards on uncertain habitual values renders the whole estimation uncertain. If expanding the tree along that trajectory would make value estimation less dependent on habitual values and thus reduce uncertainty, that expansion is worth considering. In this sense, the critical value to be computed for each trajectory is the “value of uncertainty reduction” (<sc>vur</sc>). <sc>vur</sc> computation for a trajectory should examine whether a new piece of information, possibly providable by a further expansion of the tree along that trajectory, could change agent’s decision about what action to be taken, and how much extra value is expected to be gained by that policy improvement. <sc>vur</sc> is, in fact, the expected value of policy improvement-induced rewards, computed over all possible new pieces of information that could be provided by expanding the trajectory one step further (<xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1C</xref>). Although the agent readily possesses those new pieces of information in her memory (because she has a model of the environment), loading them into working memory and taking them into the value-estimation account is worth doing only if the value of uncertainty reduction is more than its cost.</p>
<p>Here is the general scheme of our algorithm: at each stage of planning, <sc>vur</sc> is computed for each trajectory on the search tree (we discuss later that previously-computed <sc>vur</sc>-values can be reused later under certain conditions). The trajectory with the highest <sc>vur</sc> is expanded if its <sc>vur</sc> is bigger than the cost of expansion. Otherwise, the expansion process is terminated and the agent chooses an action (e.g., using soft-max rule) according to the estimated values derived from the tree.</p>
<p>In this paper, we assume that the cost of expansion simply reflects the opportunity cost of time. That is, assuming that each expansion takes <italic>ϵ</italic> time units, the total cost of one expansion is <inline-formula id="pcbi.1006827.e001"><alternatives><graphic id="pcbi.1006827.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006827.e002"><alternatives><graphic id="pcbi.1006827.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mover accent="true"><mml:mi>R</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the average reward the agent receives in the given environment.</p>
<p>As explained before, the main motivation for expanding the tree is reducing value-estimation uncertainties. There could be several reasons for why expansion reduces uncertainty. In many cases, like chess, heuristic estimations become more precise as the game advances. In general, proximity to goal sometimes makes it easier to evaluate the states. Another way that expansion reduces uncertainty, which is the focus of our formal model, is through temporal discounting. By each level of expanding a trajectory, the dependence of its estimated value on the less-reliable habitual system is shifted one step further into the future.</p>
<p>As a simplified example, imagine you are in a maze and you have already thought two steps ahead along a certain trajectory, <italic>T</italic><sub>1</sub>, of actions, and those two steps will take you to the state <italic>s</italic>′. You can use the MF value, <italic>V</italic><sub>MF</sub>(<italic>s</italic>′) of that state to compute the total value of the trajectory: <italic>V</italic>(<italic>T</italic><sub>1</sub>) = <italic>r</italic><sub>1</sub> + <italic>γ</italic>.<italic>r</italic><sub>2</sub> + <italic>γ</italic><sup>2</sup>.<italic>V</italic><sub>MF</sub>(<italic>s</italic>′), where <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> are the immediate rewards expected to be received by performing the first and the second actions on the trajectory <italic>T</italic><sub>1</sub>. Assuming that the estimates of the immediate rewards have zero uncertainty, and that the MF estimates always have variance <italic>σ</italic><sup>2</sup> (i.e., uncertainty)), the total uncertainty of <italic>V</italic>(<italic>T</italic><sub>1</sub>) will be (<italic>γ</italic><sup>2</sup>.<italic>σ</italic>)<sup>2</sup> = <italic>γ</italic><sup>4</sup>.<italic>σ</italic><sup>2</sup>. Now, if you think one step deeper and expect to land in state <italic>s</italic><sup>′′</sup> after taking the first three steps of trajectory <italic>T</italic><sub>2</sub>, then <italic>V</italic>(<italic>T</italic><sub>2</sub>) = <italic>r</italic><sub>1</sub> + <italic>γ</italic>.<italic>r</italic><sub>2</sub> + <italic>γ</italic><sup>2</sup>.<italic>r</italic><sub>3</sub> + <italic>γ</italic><sup>3</sup>.<italic>V</italic><sub>MF</sub>(<italic>s</italic><sup>′′</sup>). Therefore, its variance will be (<italic>γ</italic><sup>3</sup>.<italic>σ</italic>)<sup>2</sup> = <italic>γ</italic><sup>6</sup>.<italic>σ</italic><sup>2</sup>. This toy example shows that as a natural consequence of temporal discounting, by increasing the depth of planning, the total uncertainty of trajectories decreases, due to the reduced reliance on uncertain MF values. Therefore, the discount factor is the critical variable that determines the extent of uncertainty reduction by each expansion.</p>
<p>In this paper, we only consider environments where the transition between states via actions are deterministic (i.e., deterministic transition function for the Markov decision process; See <xref ref-type="sec" rid="sec011">Methods</xref> for how this assumption can be relaxed). Therefore, the expanded tree, at each point, is a deterministic tree. In order to compute <sc>vur</sc>, let’s define a <italic>strategy</italic> in a tree as a combination of actions that an agent can take to reach a leaf in the tree (see <xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1</xref>), and define a <italic>frontier search</italic> as the set of all strategies that agent can take in a given tree (e.g., the search frontier in <xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1</xref> is {<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>, <italic>A</italic><sub>3</sub>, <italic>A</italic><sub>4</sub>, <italic>A</italic><sub>5</sub>}). Based on this definitions, as shows in the Methods section, the value of uncertainty reduction for strategy <italic>A</italic><sub><italic>i</italic></sub>, given the search frontier <italic>F</italic>, can be written as:
<disp-formula id="pcbi.1006827.e003"><alternatives><graphic id="pcbi.1006827.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext mathsize="small">VUR</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:msub> <mml:mo>[</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mtext>with</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>expansion</mml:mtext></mml:mrow></mml:munder> <mml:mo>-</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mtext>without</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>expansion</mml:mtext></mml:mrow></mml:munder> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>F</italic> − <italic>A</italic><sub><italic>i</italic></sub> is the set <italic>F</italic> excluding <italic>A</italic><sub><italic>i</italic></sub>. According to this equation, computing <sc>vur</sc>(<italic>A<sub>i</sub></italic>|<italic>F</italic>) requires <inline-formula id="pcbi.1006827.e004"><alternatives><graphic id="pcbi.1006827.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, which is the expected mean of strategy <italic>A</italic><sub><italic>i</italic></sub> <italic>after</italic> the potential expansion. However, this variable can be computed before expansion, by <inline-formula id="pcbi.1006827.e005"><alternatives><graphic id="pcbi.1006827.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (see <xref ref-type="sec" rid="sec011">Methods</xref> section), in which <italic>γ</italic> is the discount factor, and <italic>μ</italic><sub><italic>i</italic></sub> and <inline-formula id="pcbi.1006827.e006"><alternatives><graphic id="pcbi.1006827.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> are respectively the mean and the variance of the MF-value distribution for the last action on <italic>A</italic><sub><italic>i</italic></sub>. In other words, <sc>vur</sc> is computable based on <inline-formula id="pcbi.1006827.e007"><alternatives><graphic id="pcbi.1006827.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, the expectation with respect to the predicted value of <italic>A</italic><sub><italic>i</italic></sub> after expansion, instead of its realized value which is not available before the expansion (a more general form of the above equation without reliance on the discount factor is presented in the Methods section).</p>
<p>The right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006827.e003">Eq 1</xref> is composed of two parts: the amount of future rewards that are expected to be gained with the expansion of strategy <italic>A</italic><sub><italic>i</italic></sub>, and the amount expected to be gained without the expansion of <italic>A</italic><sub><italic>i</italic></sub>. <sc>vur</sc> is the difference between these two quantities. The without-expansion term is simply the value of the best strategy that is currently available to the agent. In the with-expansion term, the outer ‘max’ operator implies that if after expanding, <italic>A</italic><sub><italic>i</italic></sub> turns out to be worse than the other available strategies (<italic>F</italic> − <italic>A</italic><sub><italic>i</italic></sub>), then the best strategy among the other ones will be taken. Otherwise, <italic>A</italic><sub><italic>i</italic></sub> will be taken.</p>
<p>The agent, however, needs to calculate this term before the expansion of <italic>A</italic><sub><italic>i</italic></sub> and therefore the term is calculated based on the expectation with respect to the predicted value of <italic>A</italic><sub><italic>i</italic></sub> after expansion (denoted by <inline-formula id="pcbi.1006827.e008"><alternatives><graphic id="pcbi.1006827.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>) instead of its realized value which is not available before the expansion.</p>
<p>It can be shown that in the case of normally distributed MF value functions, <xref ref-type="disp-formula" rid="pcbi.1006827.e003">Eq 1</xref> has a closed-form solution (see <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref> for details):
<disp-formula id="pcbi.1006827.e009"><alternatives><graphic id="pcbi.1006827.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext mathsize="small">VUR</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>β</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>β</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>β</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>β</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="4pt"/><mml:mtext>is</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>the</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>best</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>strategy</mml:mtext></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>α</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>α</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>α</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/> <mml:mspace width="0.222222em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>μ</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub> are, respectively, the mean and the standard deviation of strategy <italic>A</italic><sub><italic>i</italic></sub>. Furthermore, <italic>μ</italic><sub><italic>α</italic></sub> and <italic>μ</italic><sub><italic>β</italic></sub> are the means of the, respectively, first-best and second-best strategies in the currently-expanded tree. First-best and second-best strategies are the strategies that have the highest and the second-highest mean values. Finally, <italic>ϕ</italic> and <italic>Φ</italic> are, respectively, the probability density and cumulative distribution functions of a standard normal distribution.</p>
<p>A central principle for any meta-control algorithm is that the cost of meta-reasoning (here, the cost of computing arg max<sub><italic>A</italic></sub> <italic>VUR</italic>(<italic>A</italic>|<italic>F</italic>)) should be lower than the cost of expensive reasoning (here, one-step expansion of the decision-tree). In terms of memory cost, tree-expansion would require loading information about the expanding nodes from the long-term to the working memory. Furthermore, it would require engaging an additional working memory slot to store such information. Meta-reasoning, however, has minimal memory cost, since all the variables for computing arg max<sub><italic>A</italic></sub> <italic>VUR</italic>(<italic>A</italic>|<italic>F</italic>) already exist in the working memory (i.e., are in the already-expanded tree).</p>
<p>In terms of computational-time cost, we should stress that even though we want to find the strategy with the maximum <sc>vur</sc> value, this does not necessarily require computing <sc>vur</sc>’s of all strategies at each time step. <sc>vur</sc>(<italic>A<sub>i</sub></italic>|<italic>F</italic>) only depends on <italic>μ</italic><sub><italic>i</italic></sub>, <italic>σ</italic><sub><italic>i</italic></sub> and <italic>μ</italic><sub><italic>α</italic></sub> (or <italic>μ</italic><sub><italic>β</italic></sub>). Therefore, <sc>vur</sc> values can be cached, and reused as long as the aforementioned parameters have not changed (i.e., the newly-added strategies are not first- nor second-best strategies). From an algorithmic point of view, computing <sc>vur</sc> of a given <italic>A</italic><sub><italic>i</italic></sub> can be viewed as a constant time operation. Therefore computing arg max<sub><italic>A</italic></sub> <sc>vur</sc>(<italic>A</italic>|<italic>F</italic>) is in the order of <inline-formula id="pcbi.1006827.e010"><alternatives><graphic id="pcbi.1006827.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in the worst case, where |<italic>F</italic>| is the cardinality of <italic>F</italic> (i.e., number of items in the search frontier). However, as shown in the appendix, as the tree expands, the expected cost becomes constant (i.e., <inline-formula id="pcbi.1006827.e011"><alternatives><graphic id="pcbi.1006827.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) asymptotically, given that the agent caches previously computed <sc>vur</sc> values. This is intuitively becuase as the depth of the tree grows, the uncertainty around the value of the to-be-expanded strategy shrinks (becuase of the discounting factor), which makes it less likely that the strategy (which is not currently the best strategy) becomes the best one after expnasion (or second best strategy). As such, the chances that a new expansion affects previusly computed <sc>vur</sc> values becomes smaller and smaller as the tree gets deeper. This rate of decrement is faster than the rate at which new potential strategies are added to the tree as it gets deeper, and therefore overall the number of <sc>vur</sc> values that need re-computation remains constant as in the limit.</p>
</sec>
<sec id="sec004">
<title>Pruning in a grid world environment</title>
<p>Just as a proof of concept, we would like to see whether our method can be beneficial in a setting in which an agent is combining both MF and MB information for efficient planning. For this, we first trained an agent in an episodic grid-world environment where she obtains <italic>imperfect</italic> estimates of state-values by the model-free system. After training, she utilizes both the MF and the MB systems to use the plan-until-habit scheme, where the MB system is used to construct the tree, and the MF systems is used for estimating the values of state-actions that lie on the frontier of the tree. We predict that the increased accuracy in model-free estimates, as a result of training, would bias the direction of expanding the tree towards better states.</p>
<p>The agent starts each episode in the center of a 7 × 7 grid and can choose to go up, down, left, or right at each state. All the transitions are deterministic and are associated with a unit cost. The bottom right cell is the goal state that concludes the episode. This state is not associated with any reward, but is implicitly rewarding since it terminates the costly walk in the grid world. Evidently, the optimal policies are combinations of three right moves and three down moves. Given the structure of the task, for easier geometric interpretation and without loss of generality, the MF system learns state values, rather than state-action values.</p>
<p>To apply our plan-until-habit pruning algorithm, we require an MF system that learns not just the mean, but also the variance (i.e., uncertainty) over the state values. In our implementation, the agent estimates the value of a state by generating a number of trajectory samples from the state, similar to the first-visit Monte Carlo method described in [<xref ref-type="bibr" rid="pcbi.1006827.ref002">2</xref>], and utilizing the trajectories’ return statistics. However, instead of estimating the <italic>Q</italic>-values with Monte Carlo averages, we use independent conjugate normal priors and obtain posterior estimates of <italic>Q</italic>’s, which are conditioned on the trajectory returns (see <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref>). We obtain <italic>N</italic> trajectory samples starting from each state, such that each sample consists of a trajectory resulting from a fixed uniform random policy that assigns <inline-formula id="pcbi.1006827.e012"><alternatives><graphic id="pcbi.1006827.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>4</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula> probability to each direction {UP, DOWN, LEFT, RIGHT}.</p>
<p>We test our planning model in two different settings. First, we assume the agent has no experience interacting with the environment (i.e., <italic>N</italic> = 0). This condition results in the posterior <italic>Q</italic>-values having large and equal variances. We compare this with the case where the agent has collected some samples (i.e., <italic>N</italic> = 10), resulting in more accurate estimates of state values. In both cases, we employ the same pruning mechanism, with a variable number of possible tree expansions (capturing working-memory limitations; see <xref ref-type="sec" rid="sec010">Discussion</xref> section) selected uniformly from [5, 25] and <italic>γ</italic> = 0.95.</p>
<p>As displayed in <xref ref-type="fig" rid="pcbi.1006827.g002">Fig 2A</xref>, in the no-experience condition, the search tree is explored in all directions almost uniformly. In the second condition, however, the search is directed more towards the goal state as illustrated in <xref ref-type="fig" rid="pcbi.1006827.g002">Fig 2B</xref>. These results are in line with our intuition that the agent prunes more aggressively as she gathers more experience and thus, is better able to judge what the promising states or actions are.</p>
<fig id="pcbi.1006827.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006827.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Grid-world pruning simulation results.</title>
<p>Reaching the bottom-right corner of the map with minimum moves is rewarding. The heatmaps show the frequencies of state-visits during the tree expansion when the agent starts from the middle of the map, and (A) the agent has had no prior exposure to the environment, or (B) after some exposure (i.e., 10 trajectory samples from each state) resulting in more accurate estimates of model-free values.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Human-like pruning</title>
<p>Behavioral evidence suggests that humans, when planning, curtail any further evaluation of a sequence of actions as soon as they encounter a large punishment on the sequence [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>]. In a behavioral task [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>], subjects were required to plan ahead in order to maximize their income gain. The environment in the task is composed of six states. Each state affords two actions, each of which transitions the subject to another state deterministically. Subjects see their current state on a display and press the ‘U’ or ‘I’ buttons on the keyboard to transition to a different state.</p>
<p>In the first phase of the experiment, subjects learn the deterministic transition structure of the environment. In the second phase, transitions are associated with specific gains or losses, which are visually cued to make it easier to remember. At each trial in this stage, subjects are told to take a certain number of actions, varying between 2 and 8, and collect all the rewards and punishments along their chosen trajectory. This forces them to think ahead and plan in order to find a relatively profitable trajectory among 2<sup>2</sup> = 4 to 2<sup>8</sup> = 256 options. For example, in the setting described in <xref ref-type="fig" rid="pcbi.1006827.g003">Fig 3A</xref>, 8 possible trajectories resulting from 3 consecutive actions are displayed.</p>
<fig id="pcbi.1006827.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006827.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Example search trees from [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>].</title>
<p><bold>A</bold>: Starting at state 3, subjects make three consecutive decisions (pressing ‘U’ or ‘I’), each of which are associated with a gain or loss. Two trajectories maximize the cumulative rewards in this example and achieve −20. <bold>B</bold> and <bold>C</bold>: State transition frequencies of subjects. Higher frequencies are illustrated with thicker lines. If a transition is not taken by any of the subjects, then it is illustrated with a dashed line. Yellow backgrounds show the optimal trajectories. Colors red, black, green, and blue denote the transition rewards of <italic>P</italic>, −20, + 20 and + 140 respectively. <bold>B</bold>: <italic>P</italic> = −140 condition. It can be seen that the subjects avoid the action associated with the large punishment. <bold>C</bold>: <italic>P</italic> = −70 condition. Subjects are eager to take transitions with large losses when such transitions lead to large gains (i.e., + 140), which in fact is the optimal strategy. Reprinted with permission from [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.g003" xlink:type="simple"/>
</fig>
<p>Out of all 12 transitions, 3 of them are associated with a large loss. The magnitude of this loss is manipulated across trials (from {−140, −100, −70}) such that for certain losses (i.e., −100 and −70), Pavlovian pruning results in suboptimal strategies. In other words, pruning a strategy that starts with a −100 or −70 loss would result in discarding the most profitable course of actions, since such actions will eventually lead to highly rewarding states. The results of this experiment show that humans prune infrequently if pruning results in prematurely discarding optimal trajectories. Conversely, they tend to prune liberally when pruning does not eliminate the optimal trajectories. That is, they prune more when the loss on a trajectory is so large (i.e., −140) that cannot be compensated for by future rewards.</p>
<p>We aimed to replicate this task in our simulations. Because in the first part of the experiments subjects learn the transition and the immediate rewards through repetitive exposure, we assume that the agent (i.e., our simulation of a subject) knows the transition and reward structures. Since the immediate state-action rewards are visually cued, subjects, after observing their starting state <italic>s</italic> and their available actions <italic>a</italic><sub>1</sub> and <italic>a</italic><sub>2</sub>, presumably incorporate the immediate rewards of those actions into their planning at no cost. Therefore, we assume that the agent starts the decision tree with two already-expanded actions, with values <italic>Q</italic>(<italic>a</italic><sub><italic>i</italic></sub>) = <italic>R</italic>(<italic>s</italic>, <italic>a</italic><sub><italic>i</italic></sub>) + <italic>γV</italic>(<italic>T</italic>(<italic>s</italic>, <italic>a</italic><sub><italic>i</italic></sub>)), where <italic>i</italic> ∈ 1, 2, and <italic>R</italic>(<italic>s</italic>, <italic>a</italic>) and <italic>T</italic>(<italic>s</italic>, <italic>a</italic>) are the immediate reward and successor states resulting from taking action <italic>a</italic> at state <italic>s</italic>.</p>
<p>As in the previous experiment, we obtain the posterior <italic>Q</italic>-value distributions of the agent through a training stage. Similar to the training phase of the original study, we have the simulated agent interact with the environment for 100 episodes, during which she observes transitions and collects reinforcements. At each trial, the agent is located in a random state and is allowed to make a certain number of moves, which is sampled uniformly from {2, 3, 4}. She selects actions following uniform random policy, and stores the mean cumulative reinforcements collected after taking action <italic>a</italic> at state <italic>s</italic>, similar to the first-visit Monte Carlo algorithm [<xref ref-type="bibr" rid="pcbi.1006827.ref002">2</xref>]. Those mean values are then used for obtaining the posterior <italic>Q</italic>-distributions assuming a conjugate normal distribution as in the previous experiment (see <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref>). The prior is a normal distribution with mean and standard deviation of 0 and 1000, respectively. After the training stage, the agent moves on to the pruning state, where she starts at state <italic>s</italic> and is asked to mentally expand the planning tree for <italic>n</italic> ∈ {2, 4, 6, 8, 10, 12<italic>s</italic>} steps. We record the frequency with which the agent expands the early branch with the large punishment, which we very between −40 and −140. Finally, we set <italic>γ</italic> to 0.95 as before.</p>
<p>One critical observation in [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>] is that subjects prune more frequently as the magnitude of the punishment increases. As shown in <xref ref-type="fig" rid="pcbi.1006827.g004">Fig 4</xref>, our simulation results account for this pattern. Intuitively, observing a punishment on a trajectory reduces the expected value of the trajectory and thus, reduces the overlap between the value-distribution of that trajectory and that of the best trajectory. When the punishment is large enough, the overlap becomes very small even if the trajectories have highly uncertain value estimates. Small overlap is equivalent to low “value of uncertainty resolution” expected from expanding the unpromising trajectory, because there is a very small chance that the new pieces of information will render the unpromising trajectory better than the currently best strategy.</p>
<fig id="pcbi.1006827.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006827.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The frequency of pruning the branch with the large punishment.</title>
<p>The black area on the right is the region where the agent does not prune (i.e., expands) the punishment branch. Each condition is averaged over 300 simulations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.g004" xlink:type="simple"/>
</fig>
<p>In the simulations, we also vary the maximum number of branches allowed to be expanded, reflecting constraints on the working memory capacity (see <xref ref-type="sec" rid="sec010">Discussion</xref> section). Not surprisingly, as the memory capacity is increased, pruning frequency decreases (<xref ref-type="fig" rid="pcbi.1006827.g004">Fig 4</xref>).</p>
<p>Another important aspect of the study is that the likelihood of selecting the optimal sequence of actions by the subjects was affected by three factors: (i) subjects were less likely to choose the “Optimal Lookahead” sequence when it contained a large loss, (ii) this effect became larger as the size of the loss increased, and (iii) the optimal sequence was more likely to be chosen when the tree was shallow (i.e., when the subjects were supposed to choose a smaller number of actions). These three effects are shown in the top panel of <xref ref-type="fig" rid="pcbi.1006827.g005">Fig 5</xref> for the data reported in Huys et. al. [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>]. The bottom panel displays the prediction of our method based on the simulations in the same task. It can be seen that similar to the actual data, we predict that the subjects will be more successful in picking the optimal sequence when it does not contain a large loss, the tree is shallow and the loss is small (i.e., the effect is strongest in the −140 group and the weakest in the −70 group). One notable qualitative mismatch between the top and bottom panels is that, our model assigns a higher probability of choosing optimal sequences for smaller depths than what is shown for the actual data on the top panel. This is because, in our setting, the agent is very likely to make enough expansions to find the optimal sequence for a tree of depth 2, as there are only 2<sup>2</sup> = 4 possible sequences—which can be spanned with a small number of expansions. The number of expansions are sampled from <code>round</code>(Gamma(4, 2)) + 1, where + 1 ensures positivity. Given this distribution, it is often the case that the agent performs enough expansions to find the optimal. However, if we look at the top left plot in <xref ref-type="fig" rid="pcbi.1006827.g005">Fig 5</xref>, we see that the probability of choosing the optimal sequence is low if it contains a large loss—even for depth of 2. This might suggest that the subjects do not fully use their “expansion budgets”, if performing expansions do not seem advantageous. The same could be done in our scheme by stopping expansions altogether if the maximum <sc>vur</sc> is below a threshold. However, we refrained from doing so, and instead used a random number of expansions for simplicity, and for limiting the flexibility of the model to prevent overfitting. Other than this, all other parameters are kept the same as the ones used for generating <xref ref-type="fig" rid="pcbi.1006827.g003">Fig 3</xref>.</p>
<fig id="pcbi.1006827.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006827.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The top panels show the effect of different factors on choosing the optimal sequence of action.</title>
<p>The panels are adapted from [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>]. The x-axis denotes the number of actions the subjects were supposed to take, which determines the maximum depth of the search tree. The y-axis denotes the probability of choosing the Optimal Lookahead sequence. The blue lines represent the condition that the optimal sequences of actions included a big loss, and the green lines represent the condition that the optimal sequence of actions did not include a big loss. The amount of big loss is varied among the panels, and is mentioned by Group X on top of the panels, in which X denotes the amount of big loss (X = -140, -100, -70). The bottom panels are similar to the top panels but using the data obtained from the simulations of the model in the same settings.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.g005" xlink:type="simple"/>
</fig>
<p>Previously, the punishment-induced pruning discussed here was explained assuming that a Pavlovian system, reflexively evoked by large losses, curtails further evaluation of the corresponding sub-tree [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref013">13</xref>]. In our computational framework, however, this pruning pattern emerges naturally, rather than devising new mechanisms, from a speed-accuracy tradeoff. Furthermore, the normative nature of our explanation depicts punishment-induced pruning as an adaptive mechanism in the face of cognitive limitations, rather than depicting it an a “maladaptive” Pavlovian response [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>].</p>
</sec>
<sec id="sec006">
<title>The effects of training and decision-making times on depth of planning</title>
<p>Several lines of research have shown a transfer of control over behavior from goal-directed to habitual decision-making during the course of learning [<xref ref-type="bibr" rid="pcbi.1006827.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006827.ref017">17</xref>]. Previous accounts of interaction between MB and MF algorithms [<xref ref-type="bibr" rid="pcbi.1006827.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref019">19</xref>] explained this behavior by showing that the MF value estimates become more and more accurate along the course of experiencing a task. As a result, they eventually become more accurate than MB estimates [<xref ref-type="bibr" rid="pcbi.1006827.ref018">18</xref>], or become accurate enough that the extra information that MB planning can provide is not worth the cost of planning [<xref ref-type="bibr" rid="pcbi.1006827.ref019">19</xref>]. Therefore, a binary transition from goal-directed to habitual responding occurs in behavior.</p>
<p>Our model also explains the transition, but also suggests that it is gradual, rather than binary. As MF estimates become more accurate, the variance in strategy values decrease and thus, <sc>vur</sc> values also decrease monotonically (see <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref> for an analytical proof of this effect). This implies that an experienced agent would construct a shallower search tree and hence, spends less time planning compared to an inexperienced agent. Furthermore, in contrast to the previous accounts that propose ad-hoc [<xref ref-type="bibr" rid="pcbi.1006827.ref018">18</xref>] or optimal, but with very strong assumptions (i.e., MB tree-expansion has an infinite depth), [<xref ref-type="bibr" rid="pcbi.1006827.ref019">19</xref>] models for MB-MF arbitration mechanisms, our proposed model’s optimality is based on more reasonable assumptions.</p>
<p>Our algorithm further predicts that in a plan-until-habit scheme, time-limitation would reduce the depth of planning. That is, time pressure would monotonically limit the total number of branches to be expanded, pressing the agent to switch to habitual/heuristic values at a shallower depth. This is due to the fact that every tree-expansion step is assumed to take a certain amount of time, <italic>ϵ</italic>. Therefore, our model, for the first time, accounts for recent evidence showing that humans use a plan-until-habit scheme and that time pressure reduces their depth of MB planning [<xref ref-type="bibr" rid="pcbi.1006827.ref011">11</xref>], resulting to a relying on habitual responses at a shallower level.</p>
<p>In this experimental study [<xref ref-type="bibr" rid="pcbi.1006827.ref011">11</xref>], participants first learned the stationary transition structure of the environment in a three-step task. They then navigated through the decision tree, in each trial, to reach their desired terminal state. The rewarding value of the terminal states was non-stationary and changed along the trials, allowing to measure, from participants’ choices, whether or not they use a plan-to-habit scheme; and if they do, what depth of planning they adopt. The experiment imposed a decision time-limit of either 2000 or 700 milliseconds to two different groups of participants. While both groups showed a significant behavioral signature of plan-to-habit responding, participants that experienced a shorter time-limitation showed pruning the tree and switching to MF values at shallower levels.</p>
</sec>
<sec id="sec007">
<title>Plan-to-habit pruning in comparison</title>
<p>In this section, we qualitatively compare our plan-to-habit pruning algorithm to other methods, such as Monte Carlo tree search.</p>
<sec id="sec008">
<title>Mean-based pruning, variance-based pruning</title>
<p>Let us consider a simple pruning algorithm that expands the tree only according to the mean value of the strategies, and ignores their variances (e.g., the algorithm always—or stochastically- expands the strategy with the highest mean value, <inline-formula id="pcbi.1006827.e013"><alternatives><graphic id="pcbi.1006827.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mi>A</mml:mi></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>). The critical drawback of such algorithm is that it does not expand uncertain trajectories that have relatively smaller mean values. The true value of a strategy with a low estimated mean but high estimated uncertainty might be even higher than the strategy known to have the highest estimated mean. Therefore, uncertain strategies should be given the chance to prove their worth. In this sense, our algorithm proposes an optimal weighting of mean and variance in order to prioritize expansions.</p>
<p>Furthermore, note that an algorithm that only takes into account the mean values cannot explain the canonical experimental evidence of the gradual transition from goal-directed to habitual behavior over time [<xref ref-type="bibr" rid="pcbi.1006827.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006827.ref017">17</xref>]. Explaining such a transition, at least in all the existing accounts, requires keeping track of the MB and MF uncertainties, and taking them into account when arbitrating between the two systems [<xref ref-type="bibr" rid="pcbi.1006827.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref019">19</xref>].</p>
<p>Similarly, an algorithm that expands the tree only on the basis of the uncertainty of trajectories’ values, would only favor mental exploration of uncertain trajectories, even when their low mean value renders them totally unpromising.</p>
</sec>
<sec id="sec009">
<title>Monte Carlo tree search</title>
<p>Monte Carlo tree search (MCTS) is a family of algorithms that incrementally and stochastically builds a search tree to approximate state-action values. This incremental growth, as in our algorithm, prioritizes the promising regions of the search space by directing the growth of the tree towards high-value states.</p>
<p>A so-called tree policy is used to traverse the search tree and select a node which is not fully expanded, i.e., it has immediate successors that are not included in the tree. The node is then expanded by adding one of its unexplored children to the tree, from which a trajectory will be simulated for a fixed number of steps or until a terminal state is reached. Such trajectories are generated using a rollout policy which is typically fast to compute—for instance at each step of the trajectory actions are selected randomly and uniformly. The outcome of this trajectory (i.e., cumulative discounted rewards along the trajectory) is used to update the value estimates of the nodes in the tree that lie along the path from the root to the expanded node.</p>
<p>MCTS algorithms diverges from our approach mainly in how the value of states and actions are computed. The former relies on simulated experiences, called rollouts, whereas the latter relies on summaries of past experiences in terms of “cached” values (or model-free values). As such, the latter is much cheaper to compute, but is dependent on the policy with which those experiences are collected. In MCTS, however, values depend mostly on the tree policy, which is adaptive. Consequently, relying on past experiences, as in <sc>vur</sc> model, is cheaper but less flexible.</p>
<p>Our plan-to-habit pruning algorithm can be compared to MCTS methods on another level by focusing on tree policies. The most popular MCTS tree policy is “UCT” (Upper Confidence Bound 1 applied to trees) [<xref ref-type="bibr" rid="pcbi.1006827.ref020">20</xref>], which is based on a successful multi-armed bandit algorithm called “UCB1” (Upper Confidence Bound 1). UCB1 assigns scores to actions as a combination of their (empirical) mean returns and their exploration coefficients, which reflects how many times an action is sampled in comparison to other actions. UCT adapts this UCB1 rule to MCTS by recursively applying this rule to select actions down the tree starting from the root node.</p>
<p>UCT is simple and has successfully been utilized for many applications. However, it has also been noted [<xref ref-type="bibr" rid="pcbi.1006827.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref022">22</xref>] that UCT’s goal is different from that of approximate planning. UCT attempts to ensure a high net <italic>simulated</italic> worth for the actions that are taken during the Monte Carlo simulations that comprise planning. However, all that actually matters is the <italic>real</italic> worth of the single action that is ultimately taken in the world after all the simulations have terminated. To put it in another way, in planning, simulations and expansions are valuable, only because they help select the best action. However, UCT actually aims to maximize the sum of rewards obtained in simulations, rather than paying direct attention to the quality of actual (i.e., not simulated) actions. Consequently, it tries to avoid simulations with potentially low rewards, even though they might help select better actions. In other words, even though UCT explicitly computes an “exploration bonus” that favors infrequently visited nodes, it still underestimates how valuable exploration is. In fact, it has been shown that modifying UCT to explore (asymptotically) more when selecting root actions increases its performance [<xref ref-type="bibr" rid="pcbi.1006827.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref022">22</xref>]. Our model does not suffer from this problem of underexploration as it explictly quantifies the expected gain of expanding a node.</p>
</sec>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>Finding optimal or near optimal actions requires comparing the expected value of all possible plans that can be taken in the future. This can be achieved by explicitly expanding a model that represents the underlying structure of the environment, followed by calculating the expected value of each plan. However, the computational complexity of this process grows exponentially with the depth of search for optimal plans, which makes it infeasible to implement in all but the smallest environments. Indeed, evidence shows that humans and other animals use alternative ways that have lower computational complexities than explicit search. Examples are using ‘cached’ values of actions instead of recalculating them at each decision point [<xref ref-type="bibr" rid="pcbi.1006827.ref018">18</xref>], or using ‘action chunking’, in which actions span over multiple future states [<xref ref-type="bibr" rid="pcbi.1006827.ref023">23</xref>]. Here, we suggest that such decision-making strategies are not operating independent of the planning processes, but they interact in order to provide a planning process that adapts its extent according to time and cognitive resource and therefore, scales to complex environments. In particular, the model that we suggest is built upon two bases: (i) the planning process is directed toward the parts of the environment’s model that are most likely to benefit from further deliberation, and (ii) the planning process uses ‘cached’ action values for the unexpanded (i.e., pruned) parts of the tree. Simulation results showed that the model prunes effectively in a synthetic grid world, and that it explains several patterns reported in humans/animals.</p>
<p>Namely, a sequential decision-making task has demonstrated that humans use strategies such as ‘fragmentation’ and ‘hoarding’, in addition to pruning, for efficient planning. The pruning process, however, was shown to play a significant role on the top of those strategies [<xref ref-type="bibr" rid="pcbi.1006827.ref013">13</xref>]. Indeed, the data shows that humans stop expanding a branch of the model once they encounter a large punishment. This effect was previously accounted for, in the model-based planning framework, by adding a new parameter that encodes the probability of stopping the search after encountering a large punishment. The model here does not explicitly contain such a parameter, but the pruning effect emerges naturally based on the fact that the value of uncertainty resolution is lower for the branches of the model that start with large punishments and therefore, they are more likely to be pruned.</p>
<p>Another component of the model here is using the cached values for unexpanded parts of the model, which is in line with previous works [<xref ref-type="bibr" rid="pcbi.1006827.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>]. The psychological nature of such cached values can be related to either Pavlovian (as used in [<xref ref-type="bibr" rid="pcbi.1006827.ref012">12</xref>]) or instrumental (as used in [<xref ref-type="bibr" rid="pcbi.1006827.ref011">11</xref>]) processes in the brain, depending on whether cached values are coded for state or for state-action pairs, respectively. In the former case, our algorithm represents a collaborative interaction between instrumental model-based and Pavlovian processes [<xref ref-type="bibr" rid="pcbi.1006827.ref024">24</xref>]. In the latter case, it represents interaction between instrumental model-based and instrumental model-free processes. The theoretical framework we presented here is readily compatible with either case.</p>
<p>As discussed in the previous sections, temporal discounting of future rewards (and punishments) is a necessary component in the current framework. Reduction of uncertainty is a variable that changes monotonically with the discount factor: the smaller the <italic>γ</italic>, the less dependence of the value of each strategy on uncertain cached values on the leaves and therefore, the more reduction of uncertainty by deepening the tree. However, when a new piece of information on a leaf at depth <italic>d</italic> is achieved, its policy-improvement impact on the root-level actions is measured at the root of the tree, thus discounted by a factor <italic>γ</italic><sup><italic>d</italic></sup>. Therefore, the smaller the <italic>γ</italic> is, the less valuable a given uncertainty reduction is. This effect counteracts the above-mentioned effect of <italic>γ</italic> on the degree of uncertainty reduction. As a result, discount factor has a non-monotonic effect on <sc>vur</sc> and thus, on the depth of planning. <sc>vur</sc> is equal to zero for <italic>γ</italic>-values of zero and one, and reaches a maximum for an intermediate value of <italic>γ</italic> (its exact value depends on other parameters).</p>
<p>In sum, we proposed a principled algorithm for pruning in a plan-until-heuristic scheme. While we showed the ability of the model in accounting for several behavioral patterns in humans/animals, whether or not people use such algorithm requires further direct experiments. Such experiments could test the effect of variables like the mean and the variance of cached values on the probability of expanding a node. On the theoretical front, our algorithm can benefit from several improvements, most notably, from relaxing the assumption that the environment has a deterministic transition structure. In that case, the algorithm could increase the efficiency of the state-of-the-art algorithms that use a plan-until-heuristic scheme in complex games [<xref ref-type="bibr" rid="pcbi.1006827.ref010">10</xref>]. Furthermore, whereas we simply assume here that planning and action execution cannot be performed in parallel, it is reasonable to assume that agents deliberate over upcoming choices while performing previously chosen actions.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Methods</title>
<p>We focus on deterministic Markov decision processes (MDPs). The environment is composed of a finite set of states <inline-formula id="pcbi.1006827.e014"><alternatives><graphic id="pcbi.1006827.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>; a finite set of actions <inline-formula id="pcbi.1006827.e015"><alternatives><graphic id="pcbi.1006827.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi mathvariant="script">A</mml:mi></mml:math></alternatives></inline-formula>; a (potentially partial) transition function <inline-formula id="pcbi.1006827.e016"><alternatives><graphic id="pcbi.1006827.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>:</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:mo>×</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mrow><mml:mspace width="2.77771pt"/><mml:mo>⇸</mml:mo></mml:mrow> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>; and a reward function <inline-formula id="pcbi.1006827.e017"><alternatives><graphic id="pcbi.1006827.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>:</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:mo>×</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>×</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:mo>→</mml:mo> <mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The agent interacts with the environment via a (potentially stochastic) policy <inline-formula id="pcbi.1006827.e018"><alternatives><graphic id="pcbi.1006827.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mi>π</mml:mi> <mml:mo>:</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:mo>×</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mrow><mml:mspace width="2.77771pt"/><mml:mo>⇸</mml:mo></mml:mrow> <mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> s.t. ∑<sub><italic>a</italic></sub> <italic>π</italic>(<italic>s</italic>, <italic>a</italic>) = 1 for all <italic>s</italic>, with the goal of maximizing the expected value of the cumulative discounted rewards <inline-formula id="pcbi.1006827.e019"><alternatives><graphic id="pcbi.1006827.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006827.e020"><alternatives><graphic id="pcbi.1006827.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mi>R</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow> <mml:mi>i</mml:mi></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <italic>s</italic> is the start state, and <italic>γ</italic> is the discount factor. The state-action values of a policy <italic>π</italic> are defined as <inline-formula id="pcbi.1006827.e021"><alternatives><graphic id="pcbi.1006827.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mi>π</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>π</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Finally, the optimal state-action values are defined as <italic>Q</italic>*(<italic>s</italic>, <italic>a</italic>) = max<sub><italic>π</italic></sub> <italic>Q</italic><sup><italic>π</italic></sup>(<italic>s</italic>, <italic>a</italic>).</p>
<p>We assume for now that the model-based (MB) system has perfect knowledge of the environment (i.e., the reward and transition functions) (we will relax this assumption later). The agent uses some of this information to build a search tree representation, which relates the current state <italic>s</italic><sub><italic>t</italic></sub> to other states that can potentially be occupied in the future. The root of the tree is <italic>s</italic><sub><italic>t</italic></sub>, and its immediate children include the one-step-reachable states.</p>
<p>Let us illustrate the formation of a search tree. The agent creates a tree node, containing information about her current state <italic>s</italic><sub><italic>t</italic></sub>, which becomes the root of the tree, meaning all other nodes will stem directly or indirectly from it. The agent picks an action <italic>a</italic> available at <italic>s</italic><sub><italic>t</italic></sub> to expand, which in turn adds <italic>s</italic>′ ≔ <italic>T</italic>(<italic>s</italic><sub><italic>t</italic></sub>, <italic>a</italic>) to the tree as a child node of <italic>s</italic><sub><italic>t</italic></sub>. Now, if the agent continues planning, she can either expand an action from <italic>s</italic><sub><italic>t</italic></sub>, assuming there are more than one action available at <italic>s</italic><sub><italic>t</italic></sub>, or she can choose to expand from <italic>s</italic>′. The planning process is composed of iteratively selecting an action to expand from the set of unexplored node-action pairs and adding the resulting new state to the tree as a new node.</p>
<p>Let us consider the state of a tree at a given time, containing a total number of <italic>n</italic> unexpanded node-action pairs. This means, there are <italic>n</italic> trajectories that start from <italic>s</italic><sub><italic>t</italic></sub> and terminate at one of the unexpanded state-action pairs. We call each trajectory a “strategy”, denoted by <italic>A</italic><sub><italic>i</italic></sub>, which is a tuple of state-action pairs, and introduce the search frontier <italic>F</italic> = {<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>, …, <italic>A</italic><sub><italic>n</italic></sub>} as the set of all strategies for a given tree. We define <italic>expanding</italic> a strategy <italic>A</italic> by adding <italic>s</italic>′, the immediate successor state of the unexplored state-action pair at the end of <italic>A</italic>, to the tree and adding the resulting new strategies to the frontier. These new strategies have the form <italic>A</italic> + 〈<italic>s</italic>′, <italic>a</italic>′〉, where <italic>a</italic>′ denotes any action available at <italic>s</italic>′, and + is a tuple-concatenation operator. Note that after the expansion, if <italic>A</italic> is no longer unexplored—that is, has no unexpanded actions—then <italic>A</italic> is removed from <italic>F</italic>. This process of tree expansion goes on until an action is taken or the frontier is empty. The latter condition means the tree captures all possible trajectories in the MDP, which can only happen in an episodic MDP where no matter what actions the agent takes, she ends up in a terminal state (i.e., the state that ends the episode) after a finite number of actions.</p>
<p>We also assume that the agent has an estimation of the expected cumulative discounted rewards of each state-action pair 〈<italic>s</italic>, <italic>a</italic>〉, encoded by a random variable <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>). A model-free (MF) system, for example, can represent such <italic>Q</italic>-values as random normal variables by tracking the first order statistics (i.e., mean) and second order statistics (i.e., variance) of the values [<xref ref-type="bibr" rid="pcbi.1006827.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref026">26</xref>]. Given that state-action values are the <italic>expected</italic> longterm discounted rewards, any stochastic estimation of it will be normally distributed given the Central Limit Theorem assuming a fixed sampling policy and a reasonable (<italic>f</italic><sub><italic>R</italic></sub> has finite variance for all 〈<italic>s</italic>, <italic>a</italic>, <italic>s</italic>′〉) reward structure. Thus, it is reasonable to represent <italic>Q</italic>’s as random normal variables. With these settings, and in keeping with the plan-until-habit scheme, the value of a strategy <italic>A</italic><sub><italic>i</italic></sub> that ends with an 〈<italic>s</italic><sub><italic>M</italic></sub>, <italic>a</italic><sub><italic>M</italic></sub>〉 at depth <italic>M</italic> with <inline-formula id="pcbi.1006827.e022"><alternatives><graphic id="pcbi.1006827.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> can be estimated by
<disp-formula id="pcbi.1006827.e023"><alternatives><graphic id="pcbi.1006827.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>M</mml:mi></mml:msup> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where each <italic>r</italic><sub><italic>i</italic></sub> corresponds to the MB estimation of reward after taking the <italic>i</italic><sup><italic>th</italic></sup> action in the strategy. Assuming that there is no uncertainty in estimating the immediate rewards (As discussed later, it is straightforward to relax the assumption of zero uncertainty for immediate rewards), <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, .., <italic>r</italic><sub><italic>M</italic></sub>, the total variance of <inline-formula id="pcbi.1006827.e024"><alternatives><graphic id="pcbi.1006827.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is <inline-formula id="pcbi.1006827.e025"><alternatives><graphic id="pcbi.1006827.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. It can be seen that as a strategy gets deeper, MF value distributions (i.e., <italic>Q</italic>’s) get discounted more, which will form the basis of our method.</p>
<p>We seek to compute the value of expanding the tree along <italic>A</italic><sub><italic>i</italic></sub>. The agent knows that expanding <italic>A</italic><sub><italic>i</italic></sub> will lead to a new, yet unknown state, <italic>s</italic><sub><italic>M</italic>+1</sub>, where an action <italic>a</italic><sub><italic>M</italic>+1</sub> with the highest <italic>Q</italic>-value, <italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>), among other actions of that state exists. This potential expansion will lead to a new strategy, <inline-formula id="pcbi.1006827.e026"><alternatives><graphic id="pcbi.1006827.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, with its value estimated by:
<disp-formula id="pcbi.1006827.e027"><alternatives><graphic id="pcbi.1006827.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>M</mml:mi></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.222222em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>Note that <italic>r</italic><sub><italic>M</italic>+1</sub>, <italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>, and <italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>) are unknown prior to expansion. To reflect this, we use the notation <inline-formula id="pcbi.1006827.e028"><alternatives><graphic id="pcbi.1006827.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to denote an unknown value estimation:
<disp-formula id="pcbi.1006827.e029"><alternatives><graphic id="pcbi.1006827.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>M</mml:mi></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <inline-formula id="pcbi.1006827.e030"><alternatives><graphic id="pcbi.1006827.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e031"><alternatives><graphic id="pcbi.1006827.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote, respectively, the immediate reward and the value distribution of the successor state-action pair, both unknown prior to expansion and thus, denoted with a bar (¯). Intuitively, <inline-formula id="pcbi.1006827.e032"><alternatives><graphic id="pcbi.1006827.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> should be equal to <inline-formula id="pcbi.1006827.e033"><alternatives><graphic id="pcbi.1006827.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, because they result from the same information prior to an expansion. Only with the extra information obtained from an expansion, namely after observing <inline-formula id="pcbi.1006827.e034"><alternatives><graphic id="pcbi.1006827.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e035"><alternatives><graphic id="pcbi.1006827.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the agent hopes to gain precision. In fact, we assume the agent’s probability estimates are coherent in the sense that her expectations of <inline-formula id="pcbi.1006827.e036"><alternatives><graphic id="pcbi.1006827.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e037"><alternatives><graphic id="pcbi.1006827.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are in line with <inline-formula id="pcbi.1006827.e038"><alternatives><graphic id="pcbi.1006827.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, we have:
<disp-formula id="pcbi.1006827.e039"><alternatives><graphic id="pcbi.1006827.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo> <mml:mspace width="4pt"/><mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where we drop the subscript <italic>M</italic> + 1 of <italic>r</italic> and arguments <inline-formula id="pcbi.1006827.e040"><alternatives><graphic id="pcbi.1006827.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> of <inline-formula id="pcbi.1006827.e041"><alternatives><graphic id="pcbi.1006827.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for brevity. This equality is also known as the law of total expectation, and here it suggests that an expansion may change the expected value of <inline-formula id="pcbi.1006827.e042"><alternatives><graphic id="pcbi.1006827.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> but not <italic>in expectation</italic>. We should emphasize that an agent does not necessarily need to obey this, but not doing so might result in inefficiencies. Particularly, if <xref ref-type="disp-formula" rid="pcbi.1006827.e039">Eq 6</xref> is not obeyed, then a Dutch book may be formed such that the agent would expect to lose value by performing tree expansions.</p>
<p>Also, note that,
<disp-formula id="pcbi.1006827.e043"><alternatives><graphic id="pcbi.1006827.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo> <mml:mo>≥</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mo>[</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
which means that while the agent knows the exact mean of <italic>A</italic><sub><italic>i</italic></sub>’s value (<inline-formula id="pcbi.1006827.e044"><alternatives><graphic id="pcbi.1006827.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mtext>Var</mml:mtext> <mml:mo>[</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), the mean of the new strategy’s value is unknown prior to expansion. This variability in the expected value of the new strategy creates the possibility that the true (i.e., after expansion) expected value of <inline-formula id="pcbi.1006827.e045"><alternatives><graphic id="pcbi.1006827.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is even higher than the mean value of the best currently-expanded strategy. In fact, prior to expansion, the agent believes that acting on the basis of its currently-expanded tree will pay her <inline-formula id="pcbi.1006827.e046"><alternatives><graphic id="pcbi.1006827.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which is the mean value of the best strategy. However, if the true expected value of <inline-formula id="pcbi.1006827.e047"><alternatives><graphic id="pcbi.1006827.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is even higher than <inline-formula id="pcbi.1006827.e048"><alternatives><graphic id="pcbi.1006827.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, then the agent can change her policy and “gain” extra reward. The expectation of this “gain”, given the distribution over the expected value of <inline-formula id="pcbi.1006827.e049"><alternatives><graphic id="pcbi.1006827.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, computes the value of expanding a strategy. In other words, expanding a strategy will yield a net expected increase (assuming the expanded strategy has variance in its value) in the expected value of the best strategy, which we refer to as the <italic>value of uncertainty resolution</italic> (<sc>vur</sc>). The <sc>vur</sc> along the strategy <italic>A</italic><sub><italic>i</italic></sub> is equal to the expected value of policy improvement-induced reward resulting from observing <inline-formula id="pcbi.1006827.e050"><alternatives><graphic id="pcbi.1006827.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e051"><alternatives><graphic id="pcbi.1006827.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> Formally, given the current state of the search frontier <italic>F</italic>, <sc>vur</sc>(<italic>A<sub>i</sub></italic>|<italic>F</italic>) is simply the difference between the expected value of best strategy <italic>after</italic> expanding <italic>A</italic><sub><italic>i</italic></sub> (i.e., observing <inline-formula id="pcbi.1006827.e052"><alternatives><graphic id="pcbi.1006827.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e053"><alternatives><graphic id="pcbi.1006827.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>) and <italic>before</italic> expanding <italic>A</italic><sub><italic>i</italic></sub>:
<disp-formula id="pcbi.1006827.e054"><alternatives><graphic id="pcbi.1006827.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mrow><mml:mtext mathsize="small">VUR</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mo>(</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula> <disp-formula id="pcbi.1006827.e055"><alternatives><graphic id="pcbi.1006827.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="0.222222em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>F</italic> − <italic>A</italic><sub><italic>i</italic></sub> is the set <italic>F</italic> excluding <italic>A</italic><sub><italic>i</italic></sub> assuming <italic>A</italic><sub><italic>i</italic></sub> will be fully explored after expansion, and thus be removed from <italic>F</italic>. Otherwise, the max should run over <italic>F</italic>. The second (with minus) term in <xref ref-type="disp-formula" rid="pcbi.1006827.e054">Eq 8</xref> is the expected value of the best strategy in the frontier. The first term is the expected value of the best strategy after expansion. The <sc>vur</sc> is always non-negative because of Jensen’s inequality: max is convex and thus, the expectation of the max of random variables has to be larger than or equal to the maximum of expectations.</p>
<p>In order to progress further analytically, we make an assumption and assert that Var[<italic>Q</italic>(<italic>s</italic><sub><italic>M</italic></sub>, <italic>a</italic><sub><italic>M</italic></sub>)] = Var[<italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>)]. That is, we assume that MF value distributions for 〈<italic>s</italic>, <italic>a</italic>〉 and its immediate successor state-action pairs have the same uncertainty, possibly because the habitual system has had a similar number of experiences (i.e., samples) of neighboring actions and they are possibly of similar values. We can see in <xref ref-type="disp-formula" rid="pcbi.1006827.e027">Eq 4</xref> that only <italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>) contributes to the uncertainty in <inline-formula id="pcbi.1006827.e056"><alternatives><graphic id="pcbi.1006827.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Therefore we have,
<disp-formula id="pcbi.1006827.e057"><alternatives><graphic id="pcbi.1006827.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula> <disp-formula id="pcbi.1006827.e058"><alternatives><graphic id="pcbi.1006827.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula> <disp-formula id="pcbi.1006827.e059"><alternatives><graphic id="pcbi.1006827.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1006827.e060"><alternatives><graphic id="pcbi.1006827.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∈</mml:mo> <mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the mean, which we will obtain shortly, and <inline-formula id="pcbi.1006827.e061"><alternatives><graphic id="pcbi.1006827.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the variance of <italic>V</italic>(<italic>A</italic><sub><italic>i</italic></sub>). However, both <italic>V</italic>(<italic>A</italic><sub><italic>i</italic></sub>) (magenta curve in <xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1C</xref>) and <inline-formula id="pcbi.1006827.e062"><alternatives><graphic id="pcbi.1006827.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (black/grey curves in <xref ref-type="fig" rid="pcbi.1006827.g001">Fig 1C</xref>) are estimating the value for the same action at the root state, <italic>s</italic><sub><italic>t</italic></sub>. Therefore, the value distributions <inline-formula id="pcbi.1006827.e063"><alternatives><graphic id="pcbi.1006827.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006827.e064"><alternatives><graphic id="pcbi.1006827.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> should be consistent as in <xref ref-type="disp-formula" rid="pcbi.1006827.e039">Eq 6</xref>, implying
<disp-formula id="pcbi.1006827.e065"><alternatives><graphic id="pcbi.1006827.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
which can only be satisfied if
<disp-formula id="pcbi.1006827.e066"><alternatives><graphic id="pcbi.1006827.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.222222em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>The distribution over <inline-formula id="pcbi.1006827.e067"><alternatives><graphic id="pcbi.1006827.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> represents the probability distribution of the expected value of a strategy after expansion. This variability comes from the fact that we will have additional pieces of information, namely <italic>r</italic><sub><italic>M</italic>+1</sub> and <italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>).</p>
<p>Note that in equation <xref ref-type="disp-formula" rid="pcbi.1006827.e057">Eq 10</xref>, the only source of variance in <inline-formula id="pcbi.1006827.e068"><alternatives><graphic id="pcbi.1006827.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is assumed to be the variance in <italic>Q</italic>(<italic>s</italic><sub><italic>M</italic>+1</sub>, <italic>a</italic><sub><italic>M</italic>+1</sub>). In other words, the agent is assumed to have no uncertainty in estimating <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, .., and <italic>r</italic><sub><italic>M</italic></sub>. It is straightforward to relax this assumption by keeping track of the variance of <italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>, .., and <italic>r</italic><sub><italic>M</italic></sub>, denoted by <inline-formula id="pcbi.1006827.e069"><alternatives><graphic id="pcbi.1006827.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. In that case, <xref ref-type="disp-formula" rid="pcbi.1006827.e057">Eq 10</xref> will be replaced by
<disp-formula id="pcbi.1006827.e070"><alternatives><graphic id="pcbi.1006827.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e070" xlink:type="simple"/><mml:math display="block" id="M70"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula> <disp-formula id="pcbi.1006827.e071"><alternatives><graphic id="pcbi.1006827.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
which gives
<disp-formula id="pcbi.1006827.e072"><alternatives><graphic id="pcbi.1006827.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e072" xlink:type="simple"/><mml:math display="block" id="M72"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <inline-formula id="pcbi.1006827.e073"><alternatives><graphic id="pcbi.1006827.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>M</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>M</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> again.</p>
<p>This will take MB imperfection information about the reward function into account. <xref ref-type="disp-formula" rid="pcbi.1006827.e057">Eq 10</xref> also assumes that the agent has perfect information regarding the transition function. Given that our algorithm is only developed for MDPs with deterministic transition function, this assumption is feasible. Relaxing these assumptions (i.e., deterministic, and perfect knowledge of, transition function) are left for future work.</p>
<p>Relaxing the assumption on deterministic transition function would result the estimated value, <inline-formula id="pcbi.1006827.e074"><alternatives><graphic id="pcbi.1006827.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, of the strategy <inline-formula id="pcbi.1006827.e075"><alternatives><graphic id="pcbi.1006827.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> to become a mixture of Gaussians, rather than a simple Gaussian distribution. Computing <inline-formula id="pcbi.1006827.e076"><alternatives><graphic id="pcbi.1006827.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and <sc>vur</sc> for such cases would significantly increase the computational cost of meta-cognition and hence, developing approximation methods would be required. For example, one could resort to Monte Carlo methods, where a set of transitions are sampled from the stochastic transition function, over which the <sc>vur</sc> is averaged.</p>
<p>Given we now know the distribution of <inline-formula id="pcbi.1006827.e077"><alternatives><graphic id="pcbi.1006827.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, we can rewrite the <sc>vur</sc> definition given in <xref ref-type="disp-formula" rid="pcbi.1006827.e054">Eq 8</xref>:
<disp-formula id="pcbi.1006827.e078"><alternatives><graphic id="pcbi.1006827.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e078" xlink:type="simple"/><mml:math display="block" id="M78"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext mathsize="small">VUR</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:msub> <mml:mo>[</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:munder> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mspace width="0.222222em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where <inline-formula id="pcbi.1006827.e079"><alternatives><graphic id="pcbi.1006827.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is distributed according to <xref ref-type="disp-formula" rid="pcbi.1006827.e066">Eq 14</xref> and <italic>F</italic> − <italic>A</italic><sub><italic>i</italic></sub> is the set <italic>F</italic> excluding <italic>A</italic><sub><italic>i</italic></sub>. We show in <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref> that there is a closed-form solution for <sc>vur</sc>(<italic>A<sub>i</sub></italic>|<italic>F</italic>) defined above.</p>
<p>Utilizing this uncertainty resolution mechanism, the agent can simply find the most promising strategy to expand, via <inline-formula id="pcbi.1006827.e080"><alternatives><graphic id="pcbi.1006827.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi></mml:mrow></mml:msub> <mml:mtext mathsize="small">VUR</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The agent can continue expanding the search tree by reducing the uncertainties of the most promising branches until the value gained by expansion is less than the opportunity cost of expanding (as in [<xref ref-type="bibr" rid="pcbi.1006827.ref019">19</xref>]), or the search can continue until the working memory is full. The latter termination condition could be implemented based on the assumption that the working memory has a limited number of slots [<xref ref-type="bibr" rid="pcbi.1006827.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1006827.ref028">28</xref>] (e.g., for storing states of the expanded tree). Alternatively, one could assume that the working memory is inherently corrupted by noise, and that the level of this noise increases with the number of items in memory [<xref ref-type="bibr" rid="pcbi.1006827.ref029">29</xref>]. It is straightforward to incorporate this mechanism into our algorithm: expansion results in the variance of <inline-formula id="pcbi.1006827.e081"><alternatives><graphic id="pcbi.1006827.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to decrease by a factor <italic>γ</italic><sup>2</sup>, but also increases by an additive factor that is proportional to the number of items (e.g. states) currently stored in the working memory. Thus, one can compute when the noise overwhelms the resolved uncertainty.</p>
<p>It is noteworthy that in this paper, computing <sc>vur</sc> is based on the assumption that when the value of expansion is bigger than its cost and thus an expansion should occur, an action will be executed immediately after that expansion. In fact, our model does not compute the value of further expansions following the next potential expansion. Relaxing this assumption would require computing the value of expanding all <italic>subsets</italic> of available and potentially-emerging strategies. In this case, for a certain subset like <italic>T</italic><sub>1</sub>, <italic>T</italic><sub>2</sub>, one needs to compute <sc>vur</sc>(<italic>T</italic><sub>1</sub>, <italic>T</italic><sub>2</sub>|<italic>F</italic>) and compare it with <italic>B</italic>.<italic>C</italic>, where <italic>B</italic> = 2 is the number of expansions being considered, and <italic>C</italic> is the cost of one single expansion. We show in <xref ref-type="supplementary-material" rid="pcbi.1006827.s001">S1 Text</xref> (section “on considering <sc>vur</sc> values independently”) that the value of expanding several strategies before performing an action is not necessarily equal to the sum of the value of expanding each of those strategies independently. In general, computing the optimal sequence of expansions for a budget of B would be NP-complete in B, as it reduces to stochastic knapsack problem [<xref ref-type="bibr" rid="pcbi.1006827.ref030">30</xref>].</p>
<p>Another interesting outcome of this model is that the relationship between <sc>vur</sc> and <italic>γ</italic> roughly follows an inverse U-shaped curve. If <italic>γ</italic> = 0, then <italic>V</italic>(<italic>A</italic>) as given in <xref ref-type="disp-formula" rid="pcbi.1006827.e023">Eq 3</xref> will be a scalar; as such, <sc>vur</sc> will be 0. If <italic>γ</italic> = 1, then the variance of <inline-formula id="pcbi.1006827.e082"><alternatives><graphic id="pcbi.1006827.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006827.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>A</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as given <xref ref-type="disp-formula" rid="pcbi.1006827.e066">Eq 14</xref> will be zero, which too will result in <sc>vur</sc> being 0. The interpretation of these conditions is easy: if you do not care about the future, then no need to plan; and in the latter condition, the agent cannot gain precision by discounting the model-free estimates.</p>
</sec>
<sec id="sec012">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006827.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006827.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Proofs and derivations.</title>
<p>We provide <sc>vur</sc>-related proofs and derivations.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Peter Dayan for helpful discussions and comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006827.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Aurelius</surname> <given-names>M</given-names></name>. <source>Meditations</source>. <publisher-loc>Great Britain</publisher-loc>: <publisher-name>Penguin Books</publisher-name>; <year>2014</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Introduction to Reinforcement Learning</source>. <edition>1st ed</edition>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Russell</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Norvig</surname> <given-names>P</given-names></name>. <source>Artificial Intelligence: A Modern Approach</source> (<edition>2nd Edition</edition>). <publisher-name>Prentice Hall</publisher-name>; <year>2002</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&amp;amp;path=ASIN/0137903952" xlink:type="simple">http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&amp;amp;path=ASIN/0137903952</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref004">
<label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Russell</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wefald</surname> <given-names>E</given-names></name>. <chapter-title>Do the right thing</chapter-title>. <source>Studies in limited rationality</source>. <publisher-name>MIT Press</publisher-name>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>RP</given-names></name>. <article-title>A Neural Substrate of Prediction and Reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>:<fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>6</issue>):<fpage>1204</fpage>–<lpage>1215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>. <article-title>Flexibility to contingency changes distinguishes habitual and goal-directed strategies in humans</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>9</issue>):<fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005753" xlink:type="simple">10.1371/journal.pcbi.1005753</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Human and Rodent Homologies in Action Control: Corticostriatal Determinants of Goal-Directed and Habitual Action</article-title>. <source>Neuropsychopharmacology</source>. <year>2010</year>;<volume>35</volume>(<issue>1</issue>):<fpage>48</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/npp.2009.131" xlink:type="simple">10.1038/npp.2009.131</ext-link></comment> <object-id pub-id-type="pmid">19776734</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref009">
<label>9</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <chapter-title>The role of learning in motivation</chapter-title>. In: <name name-style="western"><surname>Pashler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gallistel</surname> <given-names>R</given-names></name>, editors. <source>Steven’s Handbook of Experimental Psychology: Learning, Motivation, and Emotion</source> (<volume>Vol.3</volume>). <publisher-name>Wiley</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Maddison</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Guez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sifre</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>van den Driessche</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Mastering the game of Go with deep neural networks and tree search</article-title>. <source>Nature</source>. <year>2016</year>;<volume>529</volume>(<issue>7587</issue>):<fpage>484</fpage>–<lpage>489</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature16961" xlink:type="simple">10.1038/nature16961</ext-link></comment> <object-id pub-id-type="pmid">26819042</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Smittenaar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Adaptive integration of habits into depth-limited planning defines a habitual-goal?directed spectrum</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>(<issue>45</issue>):<fpage>12868</fpage>–<lpage>12873</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1609094113" xlink:type="simple">10.1073/pnas.1609094113</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>O’Nions</surname> <given-names>EJP</given-names></name>, <name name-style="western"><surname>Sheridan</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Roiser</surname> <given-names>JP</given-names></name>. <article-title>Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices by Pruning Decision Trees</article-title>. <source>PLoS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002410" xlink:type="simple">10.1371/journal.pcbi.1002410</ext-link></comment> <object-id pub-id-type="pmid">22412360</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Lally</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Faulkner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Seifritz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Interplay of approximate planning strategies</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>10</issue>):<fpage>3098</fpage>–<lpage>3103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1414219112" xlink:type="simple">10.1073/pnas.1414219112</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Watt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gonzalez</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Boakes</surname> <given-names>RA</given-names></name>. <article-title>Motivational control after extended instrumental training</article-title>. <source>Animal Learning &amp; Behavior</source>. <year>1995</year>;<volume>23</volume>(<issue>2</issue>):<fpage>197</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03199935" xlink:type="simple">10.3758/BF03199935</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Holland</surname> <given-names>PC</given-names></name>. <article-title>Relations Between Pavlovian-Instrumental Transfer and Reinforcer Devaluation</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source>. <year>2004</year>;<volume>30</volume>(<issue>2</issue>):<fpage>104</fpage>–<lpage>117</lpage>. <object-id pub-id-type="pmid">15078120</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Killcross</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Coutureau</surname> <given-names>E</given-names></name>. <article-title>Coordination of Actions and Habits in the Medial Prefrontal Cortex of Rats</article-title>. <source>Cerebral Cortex</source>. <year>2003</year>;<volume>13</volume>(<issue>4</issue>):<fpage>400</fpage>–<lpage>408</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/13.4.400" xlink:type="simple">10.1093/cercor/13.4.400</ext-link></comment> <object-id pub-id-type="pmid">12631569</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title>. <source>European Journal of Neuroscience</source>. <year>2004</year>;<volume>19</volume>(<issue>1</issue>):<fpage>181</fpage>–<lpage>189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2004.03095.x" xlink:type="simple">10.1111/j.1460-9568.2004.03095.x</ext-link></comment> <object-id pub-id-type="pmid">14750976</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nature Neuroscience</source>. <year>2005</year>;<volume>8</volume>:<fpage>1704</fpage> EP –. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1560" xlink:type="simple">10.1038/nn1560</ext-link></comment> <object-id pub-id-type="pmid">16286932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keramati</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>. <article-title>Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes</article-title>. <source>PLoS Computational Biology</source>. <year>2011</year>;<volume>7</volume>(<issue>5</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002055" xlink:type="simple">10.1371/journal.pcbi.1002055</ext-link></comment> <object-id pub-id-type="pmid">21637741</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Kocsis L, Szepesvári C. Bandit Based Monte-carlo Planning. In: Proceedings of the 17th European Conference on Machine Learning. ECML’06. Berlin, Heidelberg: Springer-Verlag; 2006. p. 282–293. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/11871842_29" xlink:type="simple">http://dx.doi.org/10.1007/11871842_29</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Tolpin D, Shimony SE. MCTS Based on Simple Regret. In: Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26, 2012, Toronto, Ontario, Canada.; 2012. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4798" xlink:type="simple">http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4798</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Hay N, Russell S, Tolpin D, Shimony SE. Selecting Computations: Theory and Applications. In: Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence. UAI’12. Arlington, Virginia, United States: AUAI Press; 2012. p. 346–355. Available from: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3020652.3020691" xlink:type="simple">http://dl.acm.org/citation.cfm?id=3020652.3020691</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Habits, action sequences and reinforcement learning</article-title>. <source>European Journal of Neuroscience</source>. <year>2012</year>;<volume>35</volume>(<issue>7</issue>):<fpage>1036</fpage>–<lpage>1051</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2012.08050.x" xlink:type="simple">10.1111/j.1460-9568.2012.08050.x</ext-link></comment> <object-id pub-id-type="pmid">22487034</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>. <article-title>Serotonin, Inhibition, and Negative Mood</article-title>. <source>PLOS Computational Biology</source>. <year>2008</year>;<volume>4</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.0040004" xlink:type="simple">10.1371/journal.pcbi.0040004</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Geist</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pietquin</surname> <given-names>O</given-names></name>. <article-title>Kalman Temporal Differences</article-title>. <source>J Artif Int Res</source>. <year>2010</year>;<volume>39</volume>(<issue>1</issue>):<fpage>483</fpage>–<lpage>532</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Dearden R, Friedman N, Russell S. Bayesian Q-learning. In: Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence. AAAI’98/IAAI’98. Menlo Park, CA, USA: American Association for Artificial Intelligence; 1998. p. 761–768.</mixed-citation>
</ref>
<ref id="pcbi.1006827.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>GA</given-names></name>. <article-title>The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information</article-title>. <source>The Psychological Review</source>. <year>1956</year>;<volume>63</volume>(<issue>2</issue>):<fpage>81</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0043158" xlink:type="simple">10.1037/h0043158</ext-link></comment> <object-id pub-id-type="pmid">13310704</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cowan</surname> <given-names>N</given-names></name>. <article-title>The Magical Number 4 in Short-term Memory: A Reconsideration of Mental Storage Capacity</article-title>. <source>Behavioral and Brain Sciences</source>. <year>2001</year>;<volume>24</volume>(<issue>1</issue>):<fpage>87</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X01003922" xlink:type="simple">10.1017/S0140525X01003922</ext-link></comment> <object-id pub-id-type="pmid">11515286</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Husain</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bays</surname> <given-names>PM</given-names></name>. <article-title>Changing concepts of working memory</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>):<fpage>347</fpage>–<lpage>356</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3655" xlink:type="simple">10.1038/nn.3655</ext-link></comment> <object-id pub-id-type="pmid">24569831</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006827.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Madani</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Lizotte</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Greiner</surname> <given-names>R</given-names></name>. <source>Budgeted Learning, Part I: The Multi-Armed Bandit Case</source>; <year>2003</year>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>