<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01408</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004353</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Modeling Inhibitory Interneurons in Efficient Sensory Coding Models</article-title>
<alt-title alt-title-type="running-head">Modeling Inhibitory Interneurons in Efficient Sensory Coding Models</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zhu</surname> <given-names>Mengchen</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Rozell</surname> <given-names>Christopher J.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MZ CJR. Performed the experiments: MZ. Analyzed the data: MZ. Wrote the paper: MZ CJR.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">crozell@gatech.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>7</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>14</day>
<month>7</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>7</issue>
<elocation-id>e1004353</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>7</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>5</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Zhu, Rozell</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004353" xlink:type="simple"/>
<abstract>
<p>There is still much unknown regarding the computational role of inhibitory cells in the sensory cortex. While modeling studies could potentially shed light on the critical role played by inhibition in cortical computation, there is a gap between the simplicity of many models of sensory coding and the biological complexity of the inhibitory subpopulation. In particular, many models do not respect that inhibition must be implemented in a separate subpopulation, with those inhibitory interneurons having a diversity of tuning properties and characteristic E/I cell ratios. In this study we demonstrate a computational framework for implementing inhibition in dynamical systems models that better respects these biophysical observations about inhibitory interneurons. The main approach leverages recent work related to decomposing matrices into low-rank and sparse components via convex optimization, and explicitly exploits the fact that models and input statistics often have low-dimensional structure that can be exploited for efficient implementations. While this approach is applicable to a wide range of sensory coding models (including a family of models based on Bayesian inference in a linear generative model), for concreteness we demonstrate the approach on a network implementing sparse coding. We show that the resulting implementation stays faithful to the original coding goals while using inhibitory interneurons that are much more biophysically plausible.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Cortical function is a result of coordinated interactions between excitatory and inhibitory neural populations. In previous theoretical models of sensory systems, inhibitory neurons are often ignored or modeled too simplistically to contribute to understanding their role in cortical computation. In biophysical reality, inhibition is implemented with interneurons that have different characteristics from the population of excitatory cells. In this study, we propose a computational approach for including inhibition in theoretical models of neural coding in a way that respects several of these important characteristics, such as the relative number of inhibitory cells and the diversity of their response properties. The main idea is that the significant structure of the sensory world is reflected in very structured models of sensory coding, which can then be exploited in the implementation of the model using modern computational techniques. We demonstrate this approach on one specific model of sensory coding (called “sparse coding”) that has been successful at modeling other aspects of sensory cortex.</p>
</abstract>
<funding-group>
<funding-statement>This work was partially supported by NIH grant R01EY019965 awarded under the CRCNS program. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Computer code and modeling outputs are available at <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://siplab.gatech.edu">http://siplab.gatech.edu</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The diverse inhibitory interneuron population in cortex has been increasingly recognized as an important component in shaping cortical activity [<xref ref-type="bibr" rid="pcbi.1004353.ref001">1</xref>]. However, it remains unclear in many settings how the inhibitory circuit specifically contributes to the neural code. While theoretical and simulation investigations of proposed neural coding models could be extremely valuable for providing insight into the role of inhibition, many current high-level functional and mechanistic models do not include inhibitory cell populations that approach the biophysical complexity seen in nature.</p>
<p>Though the main ideas likely extend to other areas, for concreteness we will focus the present discussion on the primary visual cortex (V1). In V1, visual information is encoded using a rich interconnected network of excitatory principal cells and inhibitory cells, and different coding functions appear to be implemented by distinct inhibitory populations [<xref ref-type="bibr" rid="pcbi.1004353.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref003">3</xref>]. Though V1 has been extensively studied through experiment and modeling, there are often significant discrepancies between what is known about biophysical sources of inhibition and how inhibitory influences are instantiated in a model. For example, in previous high-level functional coding models (e.g. in [<xref ref-type="bibr" rid="pcbi.1004353.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref006">6</xref>], with the exception of [<xref ref-type="bibr" rid="pcbi.1004353.ref007">7</xref>] as discussed later), neural activity is often treated as a signed quantity without explicitly distinguishing between excitatory and inhibitory cell types. On the other hand, while state-of-the-art large scale mechanistic models (e.g. [<xref ref-type="bibr" rid="pcbi.1004353.ref008">8</xref>]) typically include a distinct inhibitory population, these types of models often use a single recurrent connectivity pattern (e.g., weights that decrease with spatial separation). This approach results in interneurons with uniform physiological properties and without the complex tuning diversity observed in inhibitory interneurons.</p>
<p>For theoretical and simulation studies to illuminate the role of inhibition in neural coding, it is imperative that coding models begin to incorporate experimental observations regarding the distinct properties of excitatory cells and inhibitory interneurons. Specifically, to realistically investigate the role of inhibition in neural coding, models should incorporate at least three major properties while staying faithful to the coding rule and other desirable properties (e.g., robustness):
<list list-type="order"><list-item><p>Inhibitory and excitatory interactions arise from distinct cell types, and synapses from an inhibitory cell cannot have excitatory influences on postsynaptic cells and vice versa (Dale’s law [<xref ref-type="bibr" rid="pcbi.1004353.ref009">9</xref>]);</p></list-item> <list-item><p>Excitatory neurons generally outnumber inhibitory interneurons, with E/I ratios recently estimated to be in the range 7:1 to 6:1 (apparently preserved across animals [<xref ref-type="bibr" rid="pcbi.1004353.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref011">11</xref>]); and</p></list-item> <list-item><p>The interneuron population has diverse tuning properties [<xref ref-type="bibr" rid="pcbi.1004353.ref012">12</xref>], including to varying degrees both orientation tuned and untuned interneurons in cat [<xref ref-type="bibr" rid="pcbi.1004353.ref013">13</xref>] and rodent V1 (reviewed in [<xref ref-type="bibr" rid="pcbi.1004353.ref014">14</xref>]).</p></list-item></list></p>
<p>The main contribution of this paper is to demonstrate a systematic computational method for effectively incorporating these biophysical interneuron properties into dynamical systems implementing neural coding models. In our proposed approach we exploit the fact that in many cases of interest, the total required inhibition is highly structured due to the relationship between the coding model and the statistics of the inputs being encoded. Similar to efficient coding hypotheses that postulate compact representations of sensory stimuli, the structure of the sensory statistics and the coding model can also be used to implement the required inhibition with a parsimonious computational structure. Specifically, we propose to reformulate the connectivity matrix to respect Dale’s law and exploit the inhibition structure in a matrix factorization to minimize the number of inhibitory interneurons. Furthermore, we leverage recent results from the applied mathematics community on advanced matrix factorizations to develop an approach that demonstrates the observed diversity of orientation tuning properties in inhibitory interneurons.</p>
<p>The end result of this approach is a network implementation that is functionally equivalent to the original model, but which has an interneuron population that better respects the three major biophysical properties ignored by many current coding models. In addition to this primary goal of providing a recipe for including inhibitory interneurons into coding models, this approach also suggests possible functional interpretations of some biophysical properties of the interneuron population. In particular, we propose that while Dale’s law may reflect a physical constraint of individual cells, in contrast the E/I ratio can be viewed as an emergent characteristic of a population implementation that maximizes efficiency by minimizing the number of interneurons and thus maintenance costs. In addition, we demonstrate that the orientation tuning diversity in the inhibitory population can arise from differential connectivity patterns between the excitatory and inhibitory cells.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Network implementation of neural coding models</title>
<p>In a recurrent network implementing a neural coding model, each node in the network is generally driven by both exogenous inputs (i.e., bottom-up inputs due to the stimulus or top-down feedback) and lateral connections from other cells in the same network. These lateral connections are often described in terms of a connectivity matrix <italic>G</italic>, where the element [<italic>G</italic>]<sub><italic>m</italic>, <italic>n</italic></sub> describes synaptic strength from the <italic>n</italic><sup>th</sup> neuron to the <italic>m</italic><sup>th</sup> neuron. While <italic>G</italic> can take many forms, the structure is governed by the coding model and the statistics of the stimuli being encoded.</p>
<p>To illustrate how <italic>G</italic> arises for a family of commonly-used coding models, we consider the Bayesian inference paradigm that has found increasing support as a framework for studying neural coding [<xref ref-type="bibr" rid="pcbi.1004353.ref015">15</xref>]. While there are many ways to develop a neural coding model based on the ideas of optimal inference, one of the most common approaches is to assume a generative model where the sensory scene is composed of a linear combination of basic features (i.e., causes) that must be inferred. Specifically, a linear generative model for vision proposes that an image patch <bold>s</bold> ∈ ℝ<sup><italic>N</italic></sup> (i.e., an <italic>N</italic>-pixel image patch) can be approximately written as a linear superposition of <italic>M</italic> dictionary elements {<italic>ϕ</italic><sub><italic>i</italic></sub>} representing basic visual features (i.e., there are <italic>M</italic> principal cells):
<disp-formula id="pcbi.1004353.e001"><alternatives><graphic id="pcbi.1004353.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>=</mml:mo> <mml:mo>Φ</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where the coefficients for each feature are {<italic>a</italic><sub><italic>i</italic></sub>}, <bold>n</bold> represents a noise source, and the <italic>N</italic> × <italic>M</italic> matrix Φ consists of one dictionary element on each column. These dictionary elements are often interpreted as the receptive fields (RFs) of a principal cell, such as spiny stellate cells or pyramidal cells.</p>
<p>Given the dictionary Φ and the stimulus <bold>s</bold>, the coefficients <bold>a</bold> in the linear generative model (taken to be principal cell activities, such as instantaneous firing rates) can be found by maximum a posteriori (MAP) estimation. Assuming Gaussian noise and a prior distribution <italic>P</italic>(<bold>a</bold>), the MAP estimate is found by minimizing the negative log of the posterior:
<disp-formula id="pcbi.1004353.e002"><alternatives><graphic id="pcbi.1004353.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msubsup><mml:mrow><mml:mo>‖</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Φ</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>λ</italic> is a scalar capturing the model SNR. When the prior distribution is log-concave (as are many common distributions including the exponential family [<xref ref-type="bibr" rid="pcbi.1004353.ref016">16</xref>]), the inference can be achieved by simple descent methods. The simplest dynamical system for this coding strategy would be a network implementing gradient descent with population dynamics given by
<disp-formula id="pcbi.1004353.e003"><alternatives><graphic id="pcbi.1004353.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi>τ</mml:mi> <mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>-</mml:mo> <mml:mi>G</mml:mi> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mi>∇</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>τ</italic> is the system time constant and the <italic>M</italic> × <italic>M</italic> recurrent weight (connectivity) matrix is given by <italic>G</italic> = Φ<sup><italic>T</italic></sup>Φ. <italic>G</italic> can be interpreted as a recurrent matrix because its off-diagonal terms capture the influence between cell activities. In particular when we assume that the prior is independent, i.e. log <italic>P</italic>(<bold>a</bold>) = ∑<sub><italic>i</italic></sub> log <italic>P</italic>(<italic>a</italic><sub><italic>i</italic></sub>), as is common in efficient coding models, <italic>G</italic> captures <italic>all</italic> the recurrent influence. Note that any dynamical system involving a derivative of an energy function such as <xref ref-type="disp-formula" rid="pcbi.1004353.e002">Eq (2)</xref> will contain a recurrent matrix <italic>G</italic> of this form.</p>
<p>While the most obvious implementation of the network would use a single interneuron for each entry of <italic>G</italic> (connecting two cells), there are many implementations that would result in a functionally equivalent coding rule. For example, one of the approaches we will utilize is to model the connectivity between the interneurons and principal cells using a matrix factorization:
<disp-formula id="pcbi.1004353.e004"><alternatives><graphic id="pcbi.1004353.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mi>U</mml:mi> <mml:mo>Σ</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></disp-formula>
where the <italic>V</italic><sup><italic>T</italic></sup> matrix captures the synaptic connections onto a set of interneurons from the principal cells, the <italic>U</italic> matrix captures the synaptic connections from these interneurons back onto the network of principal cells, and Σ is a diagonal matrix representing the independent gains/sensitivity of each interneuron.</p>
</sec>
<sec id="sec004">
<title>Example: Sparse coding</title>
<p>As a concrete relevant example, we will demonstrate the proposed approach in the context of a dynamical system implementing a sparse coding model of V1, where a population of cells encodes a stimulus at a given time using as few active units as possible. The sparse coding model (combined with unsupervised learning using the statistics of natural images) has been shown to be sufficient to explain the emergence of V1 classical and nonclassical response properties [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref019">19</xref>], potentially has many benefits for sensory systems [<xref ref-type="bibr" rid="pcbi.1004353.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref023">23</xref>], and is consistent with many recent electrophysiology experiments [<xref ref-type="bibr" rid="pcbi.1004353.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref026">26</xref>]. The sparse coding model has been implemented in networks that have varying degrees of biophysical plausibility (e.g., [<xref ref-type="bibr" rid="pcbi.1004353.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref030">30</xref>]), though this model has rarely been implemented with distinct inhibitory neural populations (excepting [<xref ref-type="bibr" rid="pcbi.1004353.ref007">7</xref>], discussed later).</p>
<p>The sparse coding model can be viewed as a special case of inference in the linear generative model described above with
<disp-formula id="pcbi.1004353.e005"><alternatives><graphic id="pcbi.1004353.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msubsup><mml:mrow><mml:mo>‖</mml:mo> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Φ</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <inline-formula id="pcbi.1004353.e006"><alternatives><graphic id="pcbi.1004353.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo> <mml:mtext mathvariant="bold">a</mml:mtext> <mml:msub><mml:mo stretchy="false">‖</mml:mo> <mml:mn>1</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">∣</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, corresponding to a Laplacian prior with zero-mean. We base our discussion on a dynamical system proposed in [<xref ref-type="bibr" rid="pcbi.1004353.ref027">27</xref>] that uses neurally plausible computational primitives to implement sparse coding. This system has strong convergence guarantees [<xref ref-type="bibr" rid="pcbi.1004353.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref032">32</xref>], can implement many variations of the sparse coding hypothesis [<xref ref-type="bibr" rid="pcbi.1004353.ref033">33</xref>], and is implementable in neuromorphic architectures [<xref ref-type="bibr" rid="pcbi.1004353.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref035">35</xref>]. Specifically, the system dynamics for this sparse coding model are:
<disp-formula id="pcbi.1004353.e007"><alternatives><graphic id="pcbi.1004353.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>-</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>T</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>I</italic> is the identity matrix, <bold>u</bold> are internal state variables for each node (e.g., membrane potentials), <italic>G</italic> = Φ<sup><italic>T</italic></sup>Φ governs the connectivity between nodes, and <italic>T</italic><sub><italic>λ</italic></sub>(⋅) is the soft thresholding function. Note that despite not using steepest descent on <xref ref-type="disp-formula" rid="pcbi.1004353.e005">Eq (3)</xref>, this network model still has recurrent connections described by the connectivity matrix <italic>G</italic> = Φ<sup><italic>T</italic></sup>Φ. In the simulations in this study, the dictionary Φ is pre-adapted to the statistics of the natural scene with a standard unsupervised learning method, resulting in Gabor wavelet-like kernels that resemble V1 classical receptive fields [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>].</p>
<p>This dynamical system model requires influences between cells that are described by the matrix <italic>G</italic>, but it is agnostic about the network mechanism that implements these interactions. Specifically, the model as described in [<xref ref-type="bibr" rid="pcbi.1004353.ref027">27</xref>] does not incorporate a separate population of inhibitory interneurons with any non-trivial interneuron structure, and this naïve description would only imply a point-to-point connection between all pairs of cells in the network as illustrated in <xref ref-type="fig" rid="pcbi.1004353.g001">Fig 1</xref>. This model is therefore unhelpful in its current form for understanding the coding properties of the inhibitory population.</p>
<fig id="pcbi.1004353.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Achieving Dale’s law.</title>
<p>(A) An example generic neural network of visual encoding with feedforward and bi-directional recurrent connections (arrows) showing the implementation details of a single cell E<sub>3</sub> (other cells would be similar but are not pictured for simplicity). The sparse coding dynamics in <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> is a special case. The internal state <italic>u</italic><sub>3</sub> (e.g., membrane potential) of this neuron is determined by the filtered input ⟨<italic>ϕ</italic><sub>3</sub>,<bold>s</bold>⟩, with the dictionary elements <italic>ϕ</italic>’s depending on the natural scene statistics (e.g., [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>]), the inhibitory recurrent input (green input <italic>G</italic><sub>13</sub> <italic>a</italic><sub>1</sub> and <italic>G</italic><sub>23</sub> <italic>a</italic><sub>2</sub> from E<sub>1</sub> and E<sub>2</sub>), and the excitatory recurrent input (blue input <italic>G</italic><sub>43</sub> <italic>a</italic><sub>4</sub> from E<sub>4</sub>). The membrane potential is thresholded by function <italic>T</italic><sub><italic>λ</italic></sub>(⋅) to generate the response <italic>a</italic><sub>3</sub> (e.g., the instantaneous spike rate) that drives other neurons. Note that both the excitatory and inhibitory influences are generated by the same generic cell type, violating Dale’s law. (B) In this study, we incorporate distinct inhibitory interneuron populations (e.g. I<sub>1</sub>) that are connected to the principal cells (the E-population) in specific patterns. The computational property of this type of E-I network can be shown to be equivalent to the one in (A).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g001"/>
</fig>
<p>This sparse coding network will serve as a concrete demonstration of the proposed strategy to incorporate more biophysically realistic inhibitory interneurons. The example network we use has 2048 excitatory neurons and has the same parameters as in a previous work [<xref ref-type="bibr" rid="pcbi.1004353.ref019">19</xref>] (see <xref ref-type="sec" rid="sec012">Materials and Methods</xref>).</p>
</sec>
<sec id="sec005">
<title>Achieving Dale’s law through factorization</title>
<p>As a first step towards a biologically realistic interneuron population encoding model, we show that Dale’s law can be respected in the model by decomposing the recurrent connectivity matrix <italic>G</italic> into matrices representing excitatory and inhibitory interactions. Specifically, the recurrent connectivity matrix <italic>G</italic> can be decomposed into inhibitory and excitatory effects:
<disp-formula id="pcbi.1004353.e008"><alternatives><graphic id="pcbi.1004353.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Inhib</mml:mtext></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>G</italic><sub>+</sub> are the positive elements of the matrix (representing the inhibitory recurrent connections) and <italic>G</italic><sub>−</sub> are the negative elements (representing excitatory recurrent connections).</p>
<p>While <italic>G</italic><sup>Excite</sup> can be implemented by direct synapses between excitatory principal cells, the inhibitory component <italic>G</italic><sup>Inhib</sup> requires inhibitory interneurons between the relevant principal cells. To capture these disynaptic connections, we factor the inhibitory matrix into two matrices: <italic>G</italic><sup>Inhib</sup> = <italic>UV</italic><sup><italic>T</italic></sup>. For a simple stylized illustration, the network in <xref ref-type="fig" rid="pcbi.1004353.g001">Fig 1</xref> shows an example implementation with
<disp-formula id="pcbi.1004353.e009"><alternatives><graphic id="pcbi.1004353.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mtext>Inhib</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mtext>I</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mi>U</mml:mi></mml:munder><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mspace width="1pt"/><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>I</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>I</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
and
<disp-formula id="pcbi.1004353.e010"><alternatives><graphic id="pcbi.1004353.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mn>4</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Using the approach above, we can derive a network implementation that is equivalent to the dynamical system instantiating the desired neural coding rule but that also has inhibitory cell properties that can be varied by the choice of factorization for <italic>G</italic><sup>Inhib</sup>. For a simple concrete example, we can achieve the same encoding as <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> while incorporating an inhibitory population by using the decomposition:
<disp-formula id="pcbi.1004353.e011"><alternatives><graphic id="pcbi.1004353.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>G</mml:mi> <mml:mtext>Inhib</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>I</italic> is the identity and plays the role of <italic>U</italic>; <italic>G</italic><sub>+</sub> as defined in <xref ref-type="disp-formula" rid="pcbi.1004353.e008">Eq (5)</xref> plays the role of <italic>V</italic><sup><italic>T</italic></sup>. The resulting network is shown in <xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2A</xref>. While this approach does utilize distinct excitatory and inhibitory sub-populations, it still requires <italic>M</italic> inhibitory neurons (i.e., one for each principal cell) and all inhibitory cells in this implementation have the same orientation tuning properties as the excitatory cells (see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1004353.s001">S1 Text</xref> “RFs of inhibitory cells in the direct implementation”). While this may introduce orientation tuning diversity due to the orientation tunings of the excitatory population, the diversity is distributed uniformly [<xref ref-type="bibr" rid="pcbi.1004353.ref036">36</xref>] instead of a bimodal dichotomy observed in the inhibitory population [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>].</p>
<fig id="pcbi.1004353.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Achieving E/I cell ratio.</title>
<p>(A) A subnetwork showing the connectivity and RFs in the network implementation of <xref ref-type="disp-formula" rid="pcbi.1004353.e011">Eq (8)</xref>. The excitatory connection weight from E<sub><italic>i</italic></sub> to the inhibitory interneurons I<sub><italic>j</italic></sub> is −⟨<italic>ϕ</italic><sub><italic>i</italic></sub>, <italic>ϕ</italic><sub><italic>j</italic></sub>⟩ (forming the (<italic>i</italic>, <italic>j</italic>)<sup>th</sup> entry of <italic>G</italic><sub>+</sub> in <xref ref-type="disp-formula" rid="pcbi.1004353.e011">Eq (8)</xref>). The recurrent connections from the inhibitory neurons back to the excitatory ones (in green) are one-to-one (rows of the identity matrix). This implementation results in an inhibitory population with similar size and orientation tuning properties as the presynaptic excitatory cells. (B) A stylized sub-network showing the network implementing <xref ref-type="disp-formula" rid="pcbi.1004353.e013">Eq (9)</xref>. The RFs (mapped out by sparse dots [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>]) of the interneurons are dot-like, with extreme localization and no orientation tuning. (C) A stylized sub-network implementing <xref ref-type="disp-formula" rid="pcbi.1004353.e016">Eq (11)</xref>. The interneurons receive excitatory inputs weighted by the corresponding row in <inline-formula id="pcbi.1004353.e012"><alternatives><graphic id="pcbi.1004353.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e012"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, adjust the gain by the corresponding diagonal entry in Σ, and projects back to the excitatory population with connectivity weights determined by the corresponding row in <italic>U</italic><sub>+</sub>. These interneurons receive dense input from many principal cells and have unstructured receptive fields, again with no discernible orientation tuning.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g002"/>
</fig>
</sec>
<sec id="sec006">
<title>Achieving E/I ratio through low-rank decomposition</title>
<p>In areas such as V1, the principal excitatory cells are presumed to form the explicit representation of the stimulus that is communicated to higher cortical areas while inhibitory neurons are presumed to play a more localized computational role within a circuit. Using limited physical resources, there are many desirable properties for the stimulus representation: informational efficiency matched to scene statistics [<xref ref-type="bibr" rid="pcbi.1004353.ref038">38</xref>], stability to small stimulus changes [<xref ref-type="bibr" rid="pcbi.1004353.ref004">4</xref>], and simple downstream decoding [<xref ref-type="bibr" rid="pcbi.1004353.ref039">39</xref>]. The principal cell population in V1 appears to be substantially <italic>overcomplete</italic> (i.e., in both cats and primates, the estimated ratio between the output fibers and the input fibers ranges from 25:1 to 50:1 [<xref ref-type="bibr" rid="pcbi.1004353.ref040">40</xref>]), which is a feature adopted in some coding models because it can help achieve these desirable properties [<xref ref-type="bibr" rid="pcbi.1004353.ref040">40</xref>].</p>
<p>In contrast, if inhibitory neurons only need to achieve a computational goal for the circuit without requiring these same stimulus coding properties, there is no need for an overcomplete inhibitory population. In fact, the system could exploit this structure to use the fewest number of inhibitory cells possible to avoid incurring unnecessary cell maintenance costs [<xref ref-type="bibr" rid="pcbi.1004353.ref041">41</xref>]. In contrast to the direct model of <xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2A</xref>, this approach would require interneurons that communicate simultaneously with a <italic>population</italic> of excitatory neurons rather than a single excitatory neuron. As an aside, we note that the reasoning above suggests that the inhibitory population should be overcomplete in systems where these neurons <italic>do</italic> form the explicit stimulus representation. Indeed, this is proposed in a theory of olfactory bulb encoding where granule cell interneurons form the olfactory representation and are an overcomplete population [<xref ref-type="bibr" rid="pcbi.1004353.ref042">42</xref>].</p>
<p>A natural question to ask is, what is the minimum number of inhibitory cells required to implement the influences specified by the matrix <italic>G</italic>? Said mathematically, what choice of factorization results in the fewest number of inhibitory cells, corresponding to the number of columns of <italic>U</italic> and <italic>V</italic>? In many cases of interest, the connectivity matrix <italic>G</italic> is likely to be low-rank (i.e. <italic>M</italic> &gt; rank(<italic>G</italic>)), providing an opportunity to achieve an efficient implementation of the interneuron population by “compressing” the recurrent connectivity to its most essential components. There are two different causes of low-rank structure in <italic>G</italic> for the types of models considered in this study. First, an overcomplete representation of the principal cells implies directly that <italic>G</italic> is low-rank (i.e., <italic>M</italic> &gt; <italic>N</italic> ≥ rank(Φ) = rank(Φ<sup><italic>T</italic></sup>Φ) = rank(<italic>G</italic>)). Second, natural images are highly structured, meaning that image patches have fewer “degrees of freedom” than the number of photoreceptors <italic>N</italic> being used to transduce the image (i.e. <italic>N</italic> &gt; rank(Φ) = rank(<italic>G</italic>)) [<xref ref-type="bibr" rid="pcbi.1004353.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref044">44</xref>]. This high level of input redundancy means that the connectivity structure implementing this coding rule also has structure that can lead to a simplified implementation. Taking both of these aspects together, models that encode stimuli with low-dimensional structure using an overcomplete code could expect to efficiently implement the encoding rule with highly-structured, low-rank connectivity matrix <italic>G</italic>.</p>
<p>In detail, these two sources of low-rank structure can be exploited to achieve the same coding function of <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> with fewer interneurons than a direct implementation of <xref ref-type="disp-formula" rid="pcbi.1004353.e011">Eq (8)</xref>. The original description in <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> of <italic>G</italic> as a Gramian matrix gives rise to the following decomposition of the recurrent matrix:
<disp-formula id="pcbi.1004353.e013"><alternatives><graphic id="pcbi.1004353.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo></mml:msub></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Inhib</mml:mtext></mml:msup></mml:munder> <mml:mo>+</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>Φ</mml:mo> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mo>Φ</mml:mo> <mml:mo>+</mml:mo></mml:msub></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup></mml:munder> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
shown in <xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2B</xref>. Assuming first that we only take advantage of an overcomplete representation (i.e. the Φ matrix has more columns than rows because <italic>M</italic> &gt; <italic>N</italic>), the resulting E/I ratio is <italic>M</italic>:<italic>N</italic> and requires (potentially many) fewer inhibitory cells than excitatory cells. However, this implementation does not produce the diversity of tuning properties observed in V1 interneurons, which can be either orientation tuned or non-orientation tuned (with no apparent structure) [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>]. In fact, when using sparse dot stimuli to map out the RFs [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>] of these interneurons, the resulting RFs have a dot-shaped structure (<xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2B</xref>) inconsistent with cortical observations (see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1004353.s001">S1 Text</xref> “RFs of inhibitory cells in the Gramian decomposition” for discussion relating this RF shape to the network structure).</p>
<p>Further assuming that we exploit the fact that <italic>G</italic> encodes redundant structure in natural scenes, the recurrent connectivity can be represented by an even lower dimensional decomposition than <xref ref-type="disp-formula" rid="pcbi.1004353.e013">Eq (9)</xref>. This can be achieved by seeking a lowest-rank (i.e., fewest number of interneurons) recurrent matrix that is also a good approximation to <italic>G</italic> (noting that up to this point we have only examined strategies that <italic>exactly</italic> solve the original encoding problem). Written mathematically, this approximation is:
<disp-formula id="pcbi.1004353.e014"><alternatives><graphic id="pcbi.1004353.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix">arg min</mml:mo> <mml:mi>L</mml:mi></mml:munder> <mml:mtext>rank</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mtext>s.t.</mml:mtext> <mml:mspace width="0.277778em"/><mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:mi>G</mml:mi> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mi>F</mml:mi></mml:msub> <mml:mo>≤</mml:mo> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where ‖⋅‖<sub><italic>F</italic></sub> is the Frobenius norm. This is equivalent to solving:
<disp-formula id="pcbi.1004353.e015"><alternatives><graphic id="pcbi.1004353.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e015"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix">arg min</mml:mo> <mml:mi>L</mml:mi></mml:munder> <mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:mi>G</mml:mi> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mi>F</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mtext>s.t.</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>rank</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
with a suitable choice of <italic>r</italic> and <italic>ϵ</italic>. The solution to this problem can be found by the truncated singular value decomposition (SVD), known commonly as Principal Component Analysis (PCA). Note that in our case the truncated singular values are equivalent to the truncated eigenvalues because <italic>G</italic> is symmetric semi-positive definite. Specifically, we can decompose
<disp-formula id="pcbi.1004353.e016"><alternatives><graphic id="pcbi.1004353.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>G</mml:mi></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>≈</mml:mo> <mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:mi>U</mml:mi> <mml:mo>Σ</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Inhib</mml:mtext></mml:msup></mml:munder> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup></mml:munder> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>U</italic> and <italic>V</italic> are truncated singular vectors with orthogonal columns and implement the recurrent synaptic weights (see the <xref ref-type="sec" rid="sec008">Discussion</xref> section for the biological plausibility of assuming orthogonal connectivity); Σ is a positive diagonal matrix truncated from the full SVD and implements the interneuron gain (see <xref ref-type="sec" rid="sec012">Materials and Methods</xref>).</p>
<p>The resulting inhibitory population receives dense, low-rank connections from the principal cells with synaptic weights defined by <inline-formula id="pcbi.1004353.e017"><alternatives><graphic id="pcbi.1004353.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (i.e., each row representing synapses convergent onto a single interneuron) as illustrated in <xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2C</xref>. Note that another group of low-rank inhibitory cells with different detailed connectivity is defined by <inline-formula id="pcbi.1004353.e018"><alternatives><graphic id="pcbi.1004353.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, but the qualitative characteristics of these cells are similar to those defined by <inline-formula id="pcbi.1004353.e019"><alternatives><graphic id="pcbi.1004353.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Both groups in this population have a gain modulation defined by the diagonals of Σ, followed by projection back to the principal cells with synaptic weights defined by <italic>U</italic><sub>+</sub> and −<italic>U</italic><sub>−</sub> (i.e., each row represents synapses convergent onto a single principal cell).</p>
<p>In our example sparse coding network, this implementation only requires 220 interneurons to capture about 99% of the variance in <italic>G</italic>, representing a significant savings compared to 2048 and 256 interneurons required in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004353.e011">8</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004353.e013">9</xref>) respectively. However, the resulting interneurons are again not orientation tuned, lacking the diversity observed in V1 interneurons (<xref ref-type="fig" rid="pcbi.1004353.g002">Fig 2C</xref>). In the Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1004353.s001">S1 Text</xref> “RF of inhibitory cells in low-rank decomposition”, we show that the receptive fields of this population approximate the principal components of Φ in a generative linear model and are thus untuned.</p>
</sec>
<sec id="sec007">
<title>Achieving tuning diversity via convex optimization</title>
<p>Inhibitory neurons are diverse. There are at least two populations with either tuned or untuned orientation selectivity [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>]. At the same time, different inhibitory neurons connect to the excitatory population with different frequencies [<xref ref-type="bibr" rid="pcbi.1004353.ref045">45</xref>]. Could the diverse connectivity contribute to the differences in tuning? It is indeed conceivable that inhibitory neurons densely connected to the excitatory population combine inputs from different sources, and as a result have a broader selectivity. Conversely, inhibitory neurons connecting more sparsely and locally with the excitatory population might be more selective to the stimulus.</p>
<p>To test the hypothesis that tuning diversity could arise from differential connectivity, we decompose the recurrent connectivity matrix into two distinct matrices <italic>L</italic> and <italic>S</italic> <disp-formula id="pcbi.1004353.e020"><alternatives><graphic id="pcbi.1004353.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where <italic>L</italic> is a dense matrix and <italic>S</italic> is a sparse matrix capturing relatively few inhibitory influences in <italic>G</italic>. To also respect the E/I cell ratio constraint, we would like <italic>L</italic> to be low-rank in particular so that a condensed representation could be achieved using SVD as demonstrated in the previous section.</p>
<p>Recently the applied mathematics community has developed a principled algorithmic approach known as Robust PCA (RPCA) [<xref ref-type="bibr" rid="pcbi.1004353.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1004353.ref048">48</xref>] that effectively solves this decomposition problem. In this approach, a sparse matrix <italic>S</italic> that models “outliers” (having a disproportionate effect on the rank of <italic>G</italic>) is included so that the remainder <italic>L</italic> has a lower rank than <italic>G</italic>.</p>
<p>In the context of our study, an unstructured sparse connectivity matrix can result in a relatively large number of interneurons because there can be a large number of columns containing at least one non-zero value. To maintain the small number of interneurons, we also want the sparse matrix to be row or column-sparse (see for example the connectivity represented in <xref ref-type="disp-formula" rid="pcbi.1004353.e009">Eq (6)</xref>). To achieve this, we used an adaptive version of RPCA (ARPCA) [<xref ref-type="bibr" rid="pcbi.1004353.ref049">49</xref>] to decompose the recurrent connectivity matrix <italic>G</italic> = Φ<sup><italic>T</italic></sup>Φ into a low-rank matrix <italic>L</italic> and a column-sparse matrix <italic>S</italic> by solving the following convex optimization program iteratively:
<disp-formula id="pcbi.1004353.e021"><alternatives><graphic id="pcbi.1004353.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi> <mml:mo>,</mml:mo> <mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mo form="prefix">arg</mml:mo> <mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>,</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:mi>L</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:mi>Λ</mml:mi> <mml:mi>S</mml:mi> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="0.277778em"/><mml:mtext>subject</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="0.277778em"/><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
where ‖⋅‖<sub>*</sub> is the nuclear norm (i.e., the sum of absolute values of eigenvalues) to encourage <italic>L</italic> to have low rank, ‖⋅‖<sub>1</sub> is the ℓ<sub>1</sub>-norm (i.e., the sum of absolute values of the vectorized matrix) to encourage sparsity, and Λ is a diagonal weighting matrix updated at each iteration to encourage column sparsity in <italic>S</italic>. The update rule for Λ is given by
<disp-formula id="pcbi.1004353.e022"><alternatives><graphic id="pcbi.1004353.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Λ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>β</mml:mi> <mml:mrow><mml:mrow><mml:mo>‖</mml:mo></mml:mrow> <mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where <italic>S</italic><sup>(<italic>i</italic>)</sup> is the <italic>i</italic><sup>th</sup> column of <italic>S</italic>, and <italic>β</italic> and <italic>γ</italic> control the speed of adaptation. At each iteration, the columns of <italic>S</italic> with smaller entries are assigned larger values of <italic>λ</italic>, thus encouraging the values in that column to become even smaller and eventually approach zero. The end effect is that the algorithm converges to a decomposition where only a few columns in <italic>S</italic> are non-zero (see <xref ref-type="sec" rid="sec012">Materials and Methods</xref> for details). We note that the RPCA formulation in <xref ref-type="disp-formula" rid="pcbi.1004353.e021">Eq (13)</xref> is a natural extension to SVD in <xref ref-type="disp-formula" rid="pcbi.1004353.e014">Eq (10)</xref>: instead of constraining the power in <italic>G</italic>−<italic>L</italic> (via the Frobenius norm), we now model this difference using a structured matrix <italic>S</italic>.</p>
<p>After convergence, as before we perform a singular value decomposition (SVD) on the low-rank matrix <italic>L</italic> = <italic>U</italic>Σ<italic>V</italic><sup><italic>T</italic></sup>. To respect Dale’s law we separate out the excitatory and inhibitory influence similar to <xref ref-type="disp-formula" rid="pcbi.1004353.e013">Eq (9)</xref> in each matrix:
<disp-formula id="pcbi.1004353.e023"><alternatives><graphic id="pcbi.1004353.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e023"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>G</mml:mi></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mi>U</mml:mi> <mml:mo>Σ</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mo>+</mml:mo></mml:msub></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Inhib</mml:mtext></mml:msup></mml:munder> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mo>-</mml:mo></mml:msub></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup></mml:munder> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
With this decomposition, the recurrent matrix can be rewritten with separate excitatory and inhibitory recurrent interactions. In the sparse coding model example described earlier (<xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref>), the equivalent network dynamics are:
<disp-formula id="pcbi.1004353.e024"><alternatives><graphic id="pcbi.1004353.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msup><mml:mo>Φ</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>feed-forward</mml:mtext></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>Σ</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>Σ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>low-rank</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mi>D</mml:mi></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>sparse</mml:mtext></mml:mrow></mml:munder></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>recurrent inhibitory</mml:mtext></mml:mrow></mml:munder><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mtext>Excite</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>︸</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>recurrent excitatory</mml:mtext></mml:mrow></mml:munder><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>u</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>D</italic> is a diagonal matrix with 0s and 1s on the diagonal and represents the synaptic weights on the sparsely-connected interneurons made by the principal cells (<xref ref-type="fig" rid="pcbi.1004353.g003">Fig 3</xref>).</p>
<fig id="pcbi.1004353.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Low-rank plus sparse decomposition of the recurrent connectivity matrix.</title>
<p>On the left we show a stylized network structure of the model with low-rank plus sparse decomposition of the recurrent connectivity matrix. The first inhibitory neuron I<sub>1</sub> belongs to the low-rank subpopulation. The second inhibitory neuron I<sub>2</sub> belongs to the sparse subpopulation. It receives inputs from a single excitatory neuron (E<sub>2</sub> in this illustration) with the connectivity matrix implemented by the diagonal matrix <italic>D</italic>, and sends projections back to the excitatory population with weights determined by a non-zero column of the connectivity matrix <italic>S</italic><sub>+</sub>. This inhibitory cell has the same receptive field as E<sub>2</sub>. The matrices on the right show the decomposition of the recurrent inhibitory connections exemplified in the network on the left. The low rank and sparse inhibitory populations together implement the recurrent inhibition −<italic>G</italic><sup>Inhib</sup>. The excitatory recurrent influences are implemented by direct connections <italic>I</italic>−<italic>G</italic><sup>Excite</sup> between the principal cells.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g003"/>
</fig>
<p>With a parameter choice that strikes a balance between sparseness and low rank (see <xref ref-type="sec" rid="sec012">Materials and Methods</xref>), the E/I cell ratio in the model network is also close to the observed ratio. Specifically, with 2048 principal cells and 320 inhibitory interneurons (220 in the low rank population and 100 in the sparse population), the model network has an E/I cell ratio of 6.4:1.</p>
<p>Decomposing the connectivity matrix in this manner results in two distinct populations of inhibitory interneurons with a relative size controlled by the magnitude of the average weights in the matrix Λ. The first subpopulation (exemplified by the inhibitory cell I<sub>1</sub> in <xref ref-type="fig" rid="pcbi.1004353.g003">Fig 3</xref>) originates from the low-rank connectivity matrix <italic>L</italic>, and has properties described in the previous section. The second subpopulation (exemplified by I<sub>2</sub> in <xref ref-type="fig" rid="pcbi.1004353.g003">Fig 3</xref>) originates from the sparse connectivity matrix <italic>S</italic>. This population receives one-to-one (i.e. sparse) connections with unit weights defined by the diagonal matrix <italic>D</italic> from the principal cells, and projects back to the principal cells with weights defined by <italic>S</italic>. Because <italic>S</italic> is column-sparse, the rows in <italic>D</italic> that correspond to the zero columns in <italic>S</italic> can be set to 0 without altering the recurrent influence. Said another way, we can eliminate the zero rows of the <italic>D</italic> matrix and the zero columns of <italic>S</italic>, meaning that only a few interneurons are required in this subpopulation (<xref ref-type="fig" rid="pcbi.1004353.g003">Fig 3</xref>).</p>
<p>This model network accurately solves the sparse coding inference problem (<xref ref-type="disp-formula" rid="pcbi.1004353.e005">Eq (3)</xref>), despite using only the top principal components of <italic>L</italic> in the approximation to the recurrent matrix. This is shown in <xref ref-type="fig" rid="pcbi.1004353.g004">Fig 4</xref>, where we compare the original network (i.e., the idealized implementation of <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> that is not biophysically plausible) with the approximation described above in the metrics of interest. Specifically, for a number of grating test patches we plot the final value of the energy function (i.e., the quantity to be minimized in <xref ref-type="disp-formula" rid="pcbi.1004353.e005">Eq (3)</xref>), along with the individual quantities relevant to the objective: the sparsity of the final answer (measured by the number of active coefficients ‖<bold>a</bold>‖<sub>0</sub>) and the relatve ℓ<sup>2</sup> error for the input image (‖<bold>s</bold>−Φ<bold>a</bold>‖<sub>2</sub>/‖<bold>s</bold>‖<sub>2</sub>). As demonstrated in <xref ref-type="fig" rid="pcbi.1004353.g004">Fig 4</xref>, the approximation achieves performance very similar to the original (mean relative error of energy approximation 0.008±0.001). We note specifically that in both the approximated and the original network, the activity is very sparse—only up to 5% of all 2048 neurons are active.</p>
<fig id="pcbi.1004353.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The network implements efficient coding.</title>
<p>Comparison of original sparse coding network model to approximation with idealized interneurons. Different markers represent results using different stimuli. (A) The energy function representing the total objective being optimized. (B) The sparsity of the response <bold>a</bold>. (C) The relative ℓ<sup>2</sup> error of the image reconstruction.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g004"/>
</fig>
<p>Interestingly, the sparse and low-rank interneuron populations in RPCA show the same kind of diverse orientation tuning as V1 inhibitory cells in vivo. The low-rank inhibitory population has RFs that are mostly untuned (<xref ref-type="fig" rid="pcbi.1004353.g005">Fig 5A and 5C</xref>; orientation tuning mapped using a grating stimulus centered in the middle of the visual field), comparable to the untuned inhibitory neurons observed in cats [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>] (<xref ref-type="fig" rid="pcbi.1004353.g005">Fig 5B</xref>). The sparse inhibitory population has RFs that resemble the primary cell RFs in Φ and are orientation tuned (<xref ref-type="fig" rid="pcbi.1004353.g005">Fig 5D and 5F</xref>; orientation tuning mapped using a grating stimulus centered on the RF of the interneuron), comparable to the tuned inhibitory neurons observed in cats [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>] (<xref ref-type="fig" rid="pcbi.1004353.g005">Fig 5E</xref>). This tuning dichotomy is expected from the difference in connectivity: the orientation-tuned inhibitory RFs arise from orientation-selective inputs from single principal cells (i.e., sparse synaptic connections), whereas untuned RFs arise from dense synaptic inputs from many principal cells of different tunings.</p>
<fig id="pcbi.1004353.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Achieving tuning diversity.</title>
<p>(A) Example RFs of the low-rank subnetwork of inhibitory interneurons in the simulation. (B) An example RF and orientation tuning curve from physiological recordings (Reprinted by permission from Macmillan Publishers Ltd: Nature Neuroscience, Fig 7c from [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>]); (C) An example orientation tuning curve from the simulation. (D) Example RFs of the sparse subnetwork of inhibitory interneurons. (E) An example RF and orientation tuning curve from physiological recordings (Reprinted by permission from Macmillan Publishers Ltd: Nature Neuroscience, Fig 4d from [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>]); (F) An example orientation tuning curve from the simulation.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g005"/>
</fig>
<p>For simplicity in the above model we treat the interneurons as instantaneous linear units (<xref ref-type="disp-formula" rid="pcbi.1004353.e024">Eq (16)</xref>). To make the model more biologically realistic, we can incorporate the same first-order dynamics (i.e., leaky integration) used by the principal cells into the interneurons. Specifically, the full population dynamics can be written as:
<disp-formula id="pcbi.1004353.e025"><alternatives><graphic id="pcbi.1004353.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004353.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>[</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">s</mml:mi> <mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L1</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>-</mml:mo></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L2</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mo>+</mml:mo></mml:msub> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>S</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mtext>Excite</mml:mtext></mml:msup> <mml:mo>)</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>T</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L1</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Σ</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L1</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L2</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>V</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>L2</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>S</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mtext>I,</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>S</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <bold>a</bold><sub>I, L1</sub>(<italic>t</italic>) and <bold>a</bold><sub>I, L2</sub>(<italic>t</italic>) are the dynamic responses of the two low rank interneuron populations and <bold>a</bold><sub>I, S</sub>(<italic>t</italic>) is the response of the sparse population. Here we assume that inhibitory neurons have the same time constant as the principal cells. As shown in <xref ref-type="fig" rid="pcbi.1004353.g006">Fig 6</xref>, the model defined in <xref ref-type="disp-formula" rid="pcbi.1004353.e025">Eq (17)</xref> still accurately solves the original sparse coding problem (mean relative error of energy approximation 0.029±0.004). Note that due to the added dynamics, the new dynamical system needs more numerical integration steps to converge (all systems run for 100 steps in <xref ref-type="fig" rid="pcbi.1004353.g006">Fig 6</xref> vs. 25 steps in <xref ref-type="fig" rid="pcbi.1004353.g004">Fig 4</xref>, resulting in some differences in the sparsity-rMSE tradeoff).</p>
<fig id="pcbi.1004353.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g006</object-id>
<label>Fig 6</label>
<caption>
<title>A network with dynamic interneurons implements sparse coding.</title>
<p>Comparison of original sparse coding network model to approximation with plausible interneurons with a dynamical model. Different markers represent results using different stimuli. (A) The energy function representing the total objective being optimized. (B) The sparsity of the response <bold>a</bold>. (C) The relative ℓ<sup>2</sup> error of the image reconstruction.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g006"/>
</fig>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>The main contribution of this study is a biologically plausible computational framework for including inhibitory interneurons in efficient dynamical system models of neural coding based on ideas from matrix factorization and convex optimization. From the demonstrated results, we conclude that techniques such as low-rank plus sparse decomposition can be used to find implementations of a recurrent connectivity matrix that produce equivalent population dynamics while using an inhibitory structure that matches many biophysical properties, including respecting Dale’s law, known E/I cell ratios, and diversity of orientation tuning properties. In our example of a network implementing sparse coding, the resulting representation is nearly as accurate as the idealized coding model while being much more faithful to the biophysics of the inhibitory population. Because the proposed approach only depends on the structure of the recurrent matrix (which may be common among many energy based models, including many other derivatives of sparse coding [<xref ref-type="bibr" rid="pcbi.1004353.ref033">33</xref>]), we expect that the results will be applicable to many dynamical systems implementing neural coding models.</p>
<p>Our approach suggests that the excitatory to inhibitory cell ratio in V1 is an emergent property of interneurons implementing efficient visual coding in a resource-conserving way. Specifically, in our model a comparatively small number of interneurons efficiently route the inhibitory influence by taking advantage of the overcomplete and low-rank (redundant) structure in the recurrent connectivity pattern. We have further demonstrated that the tuning diversity of interneurons could arise from differential connectivity with the excitatory population—a prediction that could be tested experimentally.</p>
<sec id="sec009">
<title>Related studies</title>
<p>Recently a few studies explicitly introduced inhibitory interneuron populations into high-level functional encoding models. Lochmann et al. [<xref ref-type="bibr" rid="pcbi.1004353.ref050">50</xref>] developed a generative model that demonstrates contextual effects in sensory coding and includes a population of inhibitory neurons. These inhibitory cells contribute to efficient perceptual inference through input targeted divisive inhibition. However, this model only works with binary one dimensional inputs and the inhibitory connectivity pattern predicted by this model presently lacks anatomical support at the cortical level. Therefore, its connection with realistic visual encoding remains unclear.</p>
<p>In a more recent work, Boerlin et al. [<xref ref-type="bibr" rid="pcbi.1004353.ref051">51</xref>] illustrated a way to include a separate inhibitory population in an efficient coding spiking network that estimates the state of an arbitrary linear dynamical system. While providing a spiking model for the inhibitory cells, their approach did not investigate the issues of excitatory-inhibitory cell ratio and tuning diversity. It should also be noted that the Gram recurrent matrix in our model also occurs in their model (their <xref ref-type="disp-formula" rid="pcbi.1004353.e014">Eq 10</xref>). It is therefore possible that our approach could be applied in their scenario.</p>
<p>Another recent study [<xref ref-type="bibr" rid="pcbi.1004353.ref007">7</xref>] has developed a spiking sparse coding network based on [<xref ref-type="bibr" rid="pcbi.1004353.ref028">28</xref>] that incorporates a population of inhibitory cells with connectivity weights adapted to natural scenes. Similar to the results of our study, the work in [<xref ref-type="bibr" rid="pcbi.1004353.ref007">7</xref>] has found that a relatively small number of inhibitory cells are sufficient to provide recurrent competition required for sparse coding. In contrast, the present study formulates a framework for including biologically plausible inhibitory interneurons in a wide range of models in a way that can potentially be proven equivalent computationally to the original model objective (e.g., <xref ref-type="disp-formula" rid="pcbi.1004353.e002">Eq (2)</xref>). Furthermore, the present work captures the observed tuning diversity of inhibitory interneurons in V1. We note that the work in [<xref ref-type="bibr" rid="pcbi.1004353.ref007">7</xref>] does use a more biophysically realistic learning rule, whereas the present paper uses a global convex optimization approach on a fixed connectivity matrix that may have been established through a learning process.</p>
</sec>
<sec id="sec010">
<title>Model predictions on the interneuron properties</title>
<p>Our model gives several experimentally verifiable predictions of interneuron properties that we detail in this section. We also note that while biologically plausible, there are limitations with the current model (see the Caveats section later).</p>
<p>First of all, our model predicts the existence of two distinct connectivity patterns between inhibitory interneurons and principal cells: the recurrent connections between principal cells and the low-rank interneurons are dense while the recurrent connections between the principal cells and the sparse interneurons are selective. According to these patterns, a likely biological correlate for the low-rank interneurons in mice is the fast-spiking parvalbumin-expressing (PV) interneurons, which receive dense synaptic inputs from nearby pyramidal cells of diverse selectivities [<xref ref-type="bibr" rid="pcbi.1004353.ref052">52</xref>], and project densely back to neighboring pyramidal cells [<xref ref-type="bibr" rid="pcbi.1004353.ref053">53</xref>]. Interestingly, as predicted by our model, the PV neurons indeed have broader selectivity than principal cells [<xref ref-type="bibr" rid="pcbi.1004353.ref054">54</xref>]. Similarly in cats, a subgroup of fast-spiking interneurons were found to have broader tunings than other interneurons [<xref ref-type="bibr" rid="pcbi.1004353.ref055">55</xref>]. Note that this broad selectivity means that the interneuron population derived from the low-rank component will use a dense code (i.e., most cells participating for most stimuli) even in coding rules such as the sparse coding example used in this work.</p>
<p>It is less clear what biological correspondence is most appropriate for the sparse interneuron population arising in the model. One candidate is the irregular firing cannabinoid receptor-expressing (CB1+) neurons, which have been shown to be more sparsely connected to the principal cells than the PV neurons [<xref ref-type="bibr" rid="pcbi.1004353.ref045">45</xref>]. However it is unclear what selectivity properties these neurons have in the visual cortex. Another candidate is the somatostatin expressing (SOM) neurons, which are orientation selective and have weaker response [<xref ref-type="bibr" rid="pcbi.1004353.ref054">54</xref>], similar to the sparse population in our model. If they indeed correspond to the sparse population in our model, we predict that these neurons receive sparser connections from the principal cells compared to the PV neurons (this however might differ from layer to layer, as evidenced by a recent study in L2/3 [<xref ref-type="bibr" rid="pcbi.1004353.ref056">56</xref>]).</p>
<p>In addition to general connectivity patterns, our model also provides predictions on the distribution of inhibitory synaptic weights in V1. As shown in <xref ref-type="fig" rid="pcbi.1004353.g007">Fig 7A</xref>, we observe a near log-normal distribution of the inhibitory synaptic weights when using a dictionary adapted to the statistics of natural scenes. Compared to a standard log-normal distribution however, the model distribution has a longer tail towards the smaller values as visible from the Q-Q plot (<xref ref-type="fig" rid="pcbi.1004353.g007">Fig 7B</xref>). Note that while the heavy tail is significant, in fact only a small part of the distribution deviates from log-normal (below the -2.33 quantile—corresponding to 1% of the cumulative density). Compounded with the difficulty of measuring from weak synapses, we anticipate that this tail would be hard to capture from experimental measurements. We note that there was a previous study [<xref ref-type="bibr" rid="pcbi.1004353.ref057">57</xref>] demonstrating a log-normal distribution between excitatory neurons, but we are unaware of similar findings for inhibitory cells. It should be noted that this model distribution is in agreement with the prediction of a previous model of spiking sparse coding [<xref ref-type="bibr" rid="pcbi.1004353.ref028">28</xref>]. Whether this is true in physiology requires further experimental validation.</p>
<fig id="pcbi.1004353.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004353.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Distribution of synaptic weights.</title>
<p>(A) The non-zero inhibitory synaptic weights in the RPCA model have a near log-normal distribution. (B) The Quantile-Quantile (QQ) plot of the starndardized log of the model distribution vs. a standard normal distribution. A line is drawn through the 25% and 75% quantile to illustrate the goodness of fit. The model distribution has a visible tail towards the smaller weights.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.g007"/>
</fig>
<p>In discussing the recurrent connections in the network of <xref ref-type="fig" rid="pcbi.1004353.g003">Fig 3</xref>, we concentrate mostly on the inhibitory connections represented by the <italic>G</italic><sup>Inhib</sup> term. The excitatory influences are assumed to be implemented by direct excitatory-excitatory connections represented by the connectivity matrix <italic>I</italic>−<italic>G</italic><sup>Excite</sup>. The identity matrix <italic>I</italic> is assumed to be implemented by an independent mechanism that results in self-excitation. Biologically, there are at least three ways this self-excitation could be achieved: through “autapses” [<xref ref-type="bibr" rid="pcbi.1004353.ref058">58</xref>] (although most of these self-connections were observed in inhibitory cells); through excitatory interneurons that connect back to the principal cells; or through dendritic back-propagation [<xref ref-type="bibr" rid="pcbi.1004353.ref059">59</xref>].</p>
</sec>
<sec id="sec011">
<title>Caveats</title>
<p>We note that some of the biological features of inhibitory circuits modeled in this work are still controversial among physiology studies. For example, although Dale’s law is a generally accepted operating principle, it was recently suggested that neurons can segregate neural transmitters to different synapses [<xref ref-type="bibr" rid="pcbi.1004353.ref060">60</xref>]. As another example, the diversity of tuning properties and the functional roles of inhibitory interneurons are still controversial. Most studies on this topic were conducted in rodents (the study we compared our simulation to [<xref ref-type="bibr" rid="pcbi.1004353.ref037">37</xref>] in the Results being a notable exception), with few implications for primates and leaving substantial uncertainty even in mouse neocortex [<xref ref-type="bibr" rid="pcbi.1004353.ref014">14</xref>]. For example, it is still unclear whether PV interneurons have a diversity of tuning properties [<xref ref-type="bibr" rid="pcbi.1004353.ref061">61</xref>] or are mostly broadly tuned [<xref ref-type="bibr" rid="pcbi.1004353.ref062">62</xref>]. In addition, in our simulation the recurrent inhibition sharpens the orientation tuning of the principal cells [<xref ref-type="bibr" rid="pcbi.1004353.ref019">19</xref>]; in physiology, there are conflicting accounts of whether this is the case [<xref ref-type="bibr" rid="pcbi.1004353.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref003">3</xref>]. In summary, the modeling results here should be considered as a demonstration of the capability of a theoretical model to reproduce a variety of detailed biological phenomenon, not as support for any specific anatomical inhibitory circuit structures and functions.</p>
<p>There are several biological details of the inhibitory population that the current model does not capture. First, the non orientation-tuned inhibitory interneurons in cat primary visual cortex have complex cell characteristics such as overlapping ON/OFF receptive fields (<xref ref-type="fig" rid="pcbi.1004353.g005">Fig 5B</xref>). To capture such features, a coding model involving complex cells may be necessary. Second, the current model does not attempt to capture the prevalent electrical and chemical interconnections between inhibitory interneurons in the cortex [<xref ref-type="bibr" rid="pcbi.1004353.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004353.ref063">63</xref>]. These recurrent connections can potentially be incorporated by allowing off-diagonal entries in the gain matrix Σ. Third, we have treated inhibitory interneurons as continuous-time units with instantaneous dynamics (<xref ref-type="disp-formula" rid="pcbi.1004353.e024">Eq (16)</xref>) or with first-order dynamics (<xref ref-type="disp-formula" rid="pcbi.1004353.e025">Eq (17)</xref>). In reality, interneurons emit spikes and have diverse temporal dynamics involving short-term plasticity [<xref ref-type="bibr" rid="pcbi.1004353.ref064">64</xref>]. A previous work from our group [<xref ref-type="bibr" rid="pcbi.1004353.ref035">35</xref>] showed that the non-spiking sparse coding network (without a separate inhibitory population) can be equivalently implemented by a network of integrate and fire cells. While we would expect a spiking network with a similar connectivity pattern as we have demonstrated would exhibit similar kind of interneuron properties, it is unclear without further analysis whether using more biologically realistic spiking neurons would affect the overall dynamics. Finally, though it is well-known that thalamic inputs innervate inhibitory interneurons constituting feedforward inhibition [<xref ref-type="bibr" rid="pcbi.1004353.ref001">1</xref>], the model discussed in the main text does not include a detailed model of this feedforward component. However, we argue in the Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1004353.s001">S1 Text</xref> “Feedforward inhibition” section that the cell ratio and orientation tuning properties could be modeled in a similar manner as the recurrent network.</p>
<p>It is known that neural network models with different parameters may share the same input-output functionality [<xref ref-type="bibr" rid="pcbi.1004353.ref065">65</xref>]. Similarly, there are other model configurations (i.e. inhibitory connection patterns) not considered in this work that could implement the same coding functionality. For one example, in the Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1004353.s001">S1 Text</xref> “Global inhibition” section we consider the example of global inhibition structures. In this case, while very few inhibitory cells are needed, only non orientation-tuned inhibitory cells can be modeled.</p>
<p>A remaining question is whether the proposed decomposition can be learned in a biologically plausible way. While it is out of the scope of the current study, we do expect the orthonormal low-rank connectivity matrices to be learnable in a biologically plausible fashion. Indeed, with Sanger’s learning rule—a classical unsupervised learning method for feedforward neural networks that can be implemented locally—the network weights converge to orthonormal eigenvectors of the input [<xref ref-type="bibr" rid="pcbi.1004353.ref066">66</xref>]. Note that while the orthonormality emerges automatically from the learning rule, we are not suggesting that the singular vectors are the only plausible weights in the interneuron network. For example, performing a linear transform (e.g. a rotation) in the low-rank principal subspace gives rise to an alternative decomposition that maintains the cell ratio and tuning properties we have modeled. This alternative implementation may in fact have additional computationally benefits. For example, a linear transform equalizes the gain distribution in the SVD and potentially improves the robustness of the network against noise.</p>
</sec>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec013">
<title>Adaptive Robust PCA</title>
<p>
<xref ref-type="disp-formula" rid="pcbi.1004353.e021">Eq (13)</xref> is a convex optimization problem that can be solved efficiently through numerical optimization techniques. In this study we solve this optimization problem through an adaptive version of Alternating Direction Method of Multipliers (ADMM), a robust dual ascent method [<xref ref-type="bibr" rid="pcbi.1004353.ref067">67</xref>]. Specifically, the inner loop of the algorithm finds the optimal <italic>L</italic> and <italic>S</italic> given a choice of Λ by alternating between a primal update that achieves (augmented) Lagrangian minimization and a dual update. The outer loop updates Λ according to <xref ref-type="disp-formula" rid="pcbi.1004353.e022">Eq (14)</xref>. See [<xref ref-type="bibr" rid="pcbi.1004353.ref049">49</xref>] for details of the algorithm.</p>
</sec>
<sec id="sec014">
<title>Implementation details</title>
<p>We start with a model network of 2048 principal neurons with receptive fields adapted to 16 × 16 natural image patches using sparse coding [<xref ref-type="bibr" rid="pcbi.1004353.ref017">17</xref>]. The principal cell activities are interpreted as the sparse coefficients of a dynamical system implementing sparse coding (<bold>a</bold> in <xref ref-type="disp-formula" rid="pcbi.1004353.e007">Eq (4)</xref> constrained to be positive) [<xref ref-type="bibr" rid="pcbi.1004353.ref027">27</xref>] with a threshold value <italic>λ</italic> = 0.1, as was done previously in [<xref ref-type="bibr" rid="pcbi.1004353.ref019">19</xref>].</p>
<p>In the proposed implementation, the required number of inhibitory interneurons is governed by the rank of <italic>L</italic> and the number of non-zero columns in <italic>S</italic>. To achieve a biophysically accurate small E/I cell ratio, we would like both the rank of <italic>L</italic> and the number of non-zero columns of <italic>S</italic> to be small. However, these are two competing requirements whose tradeoff depends on the parameters in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004353.e021">13</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004353.e022">14</xref>). Indeed, making <italic>L</italic> lower rank necessarily makes <italic>S</italic> less column-sparse. To find a compromise solution, we chose the following set of parameters: the initial diagonal of Λ is 0.038; <italic>α</italic> = 2.5; <italic>β</italic> = 0.01. After convergence, we chose to keep 110 cells (implementing top 110 eigenvalues in <italic>L</italic>) in each of the two low-rank inhibitory populations with a total of 220 cells so that 99% of the variance in <italic>L</italic> was retained. We also used 220 interneurons in the SVD implementation to facilitate comparison between the models.</p>
</sec>
</sec>
<sec id="sec015">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004353.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Mathematical derivations of model receptive fields; models of feedforward inhibition and global inhibition.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004353.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.s002" mimetype="image/tiff" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Feed-forward inhibition.</title>
<p>Feedforward push-pull could also be implemented with fewer inhibitory neurons than excitatory neurons.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004353.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004353.s003" mimetype="image/tiff" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Global inhibition.</title>
<p>(<bold>A</bold>) The recurrent network that implements the global inhibition (Eq. (S8)). I<sub>1</sub> pools all activities from the excitatory population, weighs them by <italic>c</italic>, and projects back to the excitatory population. (<bold>B</bold>) The orientation tuning curve of the inhibitory neuron I<sub>1</sub>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank B. Olshausen for helpful discussions about the present work and A. Charles for providing ARPCA code.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004353.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Isaacson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>How Inhibition Shapes Cortical Activity</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>2</issue>):<fpage>231</fpage>–<lpage>243</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0896627311008798">http://www.sciencedirect.com/science/article/pii/S0896627311008798</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.09.027" xlink:type="simple">10.1016/j.neuron.2011.09.027</ext-link></comment> <object-id pub-id-type="pmid">22017986</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lee</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Kwan</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Phoumthipphavong</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Flannery</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Masmanidis</surname> <given-names>SC</given-names></name>, <etal>et al</etal>. <article-title>Activation of specific interneurons improves V1 feature selectivity and visual perception</article-title>. <source>Nature</source>. <year>2012</year> <month>Aug</month>;<volume>488</volume>(<issue>7411</issue>):<fpage>379</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11312" xlink:type="simple">10.1038/nature11312</ext-link></comment> <object-id pub-id-type="pmid">22878719</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilson</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Runyan</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>FL</given-names></name>, <name name-style="western"><surname>Sur</surname> <given-names>M</given-names></name>. <article-title>Division and subtraction by distinct cortical inhibitory networks in vivo</article-title>. <source>Nature</source>. <year>2012</year> <month>Aug</month>;<volume>488</volume>(<issue>7411</issue>):<fpage>343</fpage>–<lpage>348</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11347" xlink:type="simple">10.1038/nature11347</ext-link></comment> <object-id pub-id-type="pmid">22878717</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title> <source>Vision research</source>. <year>1997</year>;<volume>37</volume>(<issue>23</issue>):<fpage>3311</fpage>–<lpage>3325</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(97)00169-7" xlink:type="simple">10.1016/S0042-6989(97)00169-7</ext-link></comment> <object-id pub-id-type="pmid">9425546</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>. <year>1999</year>;<volume>2</volume>:<fpage>79</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/4580" xlink:type="simple">10.1038/4580</ext-link></comment> <object-id pub-id-type="pmid">10195184</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nature neuroscience</source>. <year>2001</year>;<volume>4</volume>(<issue>8</issue>):<fpage>819</fpage>–<lpage>825</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/90526" xlink:type="simple">10.1038/90526</ext-link></comment> <object-id pub-id-type="pmid">11477428</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>King</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Inhibitory Interneurons Decorrelate Excitatory Cells to Drive Sparse Code Formation in a Spiking Model of V1</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>13</issue>):<fpage>5475</fpage>–<lpage>5485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4188-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4188-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23536063</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rasch</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Schuch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Statistical Comparison of Spike Responses to Natural Stimuli in Monkey Area V1 With Simulated Responses of a Detailed Laminar Network Model for a Patch of V1</article-title>. <source>Journal of Neurophysiology</source>. <year>2011</year>;<volume>105</volume>(<issue>2</issue>):<fpage>757</fpage>–<lpage>778</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://jn.physiology.org/content/105/2/757.abstract">http://jn.physiology.org/content/105/2/757.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00845.2009" xlink:type="simple">10.1152/jn.00845.2009</ext-link></comment> <object-id pub-id-type="pmid">21106898</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Strata</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Harvey</surname> <given-names>R</given-names></name>. <article-title>Dale’s principle</article-title>. <source>Brain Research Bulletin</source>. <year>1999</year>;<volume>50</volume>(<issue>5–6</issue>):<fpage>349</fpage>–<lpage>350</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0361923099001008">http://www.sciencedirect.com/science/article/pii/S0361923099001008</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0361-9230(99)00100-8" xlink:type="simple">10.1016/S0361-9230(99)00100-8</ext-link></comment> <object-id pub-id-type="pmid">10643431</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Meyer</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Schwarz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wimmer</surname> <given-names>VC</given-names></name>, <name name-style="western"><surname>Schmitt</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Kerr</surname> <given-names>JND</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name>, <etal>et al</etal>. <article-title>Inhibitory interneurons in a cortical column form hot zones of inhibition in layers 2 and 5A</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>40</issue>):<fpage>16807</fpage>–<lpage>16812</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/108/40/16807.abstract">http://www.pnas.org/content/108/40/16807.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1113648108" xlink:type="simple">10.1073/pnas.1113648108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Meyer</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Egger</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Guest</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Foerster</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Reissl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Oberlaender</surname> <given-names>M</given-names></name>. <article-title>Cellular organization of cortical barrel columns is whisker-specific</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year>;<volume>110</volume>(<issue>47</issue>):<fpage>19113</fpage>–<lpage>19118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1312691110" xlink:type="simple">10.1073/pnas.1312691110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Toledo-Rodriguez</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Silberberg</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>C</given-names></name>. <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nat Rev Neurosci</source>. <year>2004</year> <month>Oct</month>;<volume>5</volume>(<issue>10</issue>):<fpage>793</fpage>–<lpage>807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1519" xlink:type="simple">10.1038/nrn1519</ext-link></comment> <object-id pub-id-type="pmid">15378039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hirsch</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>LM</given-names></name>. <article-title>Circuits that build visual cortical receptive fields</article-title>. <source>Trends in neurosciences</source>. <year>2006</year>;<volume>29</volume>(<issue>1</issue>):<fpage>30</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2005.11.001" xlink:type="simple">10.1016/j.tins.2005.11.001</ext-link></comment> <object-id pub-id-type="pmid">16309753</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lee</surname> <given-names>WCA</given-names></name>, <name name-style="western"><surname>Reid</surname> <given-names>RC</given-names></name>. <article-title>Specificity and randomness: structure-function relationships in neural circuits</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2011</year>;<volume>21</volume>(<issue>5</issue>):<fpage>801</fpage>–<lpage>807</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0959438811001279">http://www.sciencedirect.com/science/article/pii/S0959438811001279</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2011.07.004" xlink:type="simple">10.1016/j.conb.2011.07.004</ext-link></comment> <object-id pub-id-type="pmid">21855320</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <source>Probabilistic models of the brain: Perception and neural function</source>. <publisher-name>The MIT Press</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Boyd</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Vandenberghe</surname> <given-names>L</given-names></name>. <source>Convex optimization</source>. <publisher-loc>Cambridge</publisher-loc> <publisher-name>university press</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rehn</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2007</year>;<volume>22</volume>(<issue>2</issue>):<fpage>135</fpage>–<lpage>146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-006-0003-9" xlink:type="simple">10.1007/s10827-006-0003-9</ext-link></comment> <object-id pub-id-type="pmid">17053994</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zhu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>. <article-title>Visual Nonclassical Receptive Field Effects Emerge from Sparse Coding in a Dynamical System</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year> <month>08</month>;<volume>9</volume>(<issue>8</issue>):<fpage>e1003191</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003191" xlink:type="simple">10.1371/journal.pcbi.1003191</ext-link></comment> <object-id pub-id-type="pmid">24009491</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding of sensory inputs</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2004</year>;<volume>14</volume>(<issue>4</issue>):<fpage>481</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2004.07.007" xlink:type="simple">10.1016/j.conb.2004.07.007</ext-link></comment> <object-id pub-id-type="pmid">15321069</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Niven</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>. <article-title>Energy limitation as a selective pressure on the evolution of sensory systems</article-title>. <source>Journal of Experimental Biology</source>. <year>2008</year>;<volume>211</volume>(<issue>11</issue>):<fpage>1792</fpage>–<lpage>1804</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://jeb.biologists.org/content/211/11/1792.abstract">http://jeb.biologists.org/content/211/11/1792.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1242/jeb.017574" xlink:type="simple">10.1242/jeb.017574</ext-link></comment> <object-id pub-id-type="pmid">18490395</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Baum</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Moody</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wilczek</surname> <given-names>F</given-names></name>. <article-title>Internal representations for associative memory</article-title>. <source>Biological Cybernetics</source>. <year>1988</year>;<volume>59</volume>(<issue>4</issue>):<fpage>217</fpage>–<lpage>228</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00332910" xlink:type="simple">10.1007/BF00332910</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Charles</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Yap</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>. <article-title>Short term memory capacity in networks via the restricted isometry property</article-title>. <source>Neural Computation</source>. <year>2014</year>;<volume>26</volume>:<fpage>1198</fpage>–<lpage>1235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00590" xlink:type="simple">10.1162/NECO_a_00590</ext-link></comment> <object-id pub-id-type="pmid">24684446</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Haider</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Krause</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Duque</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Touryan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mazer</surname> <given-names>JA</given-names></name>, <etal>et al</etal>. <article-title>Synaptic and Network Mechanisms of Sparse and Reliable Visual Cortical Activity during Nonclassical Receptive Field Stimulation</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>121</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.12.005" xlink:type="simple">10.1016/j.neuron.2009.12.005</ext-link></comment> <object-id pub-id-type="pmid">20152117</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vinje</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source>Science</source>. <year>2000</year>;<volume>287</volume>(<issue>5456</issue>):<fpage>1273</fpage>–<lpage>1276</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.287.5456.1273" xlink:type="simple">10.1126/science.287.5456.1273</ext-link></comment> <object-id pub-id-type="pmid">10678835</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wolfe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Houweling</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Brecht</surname> <given-names>M</given-names></name>. <article-title>Sparse and powerful cortical spikes</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2010</year>;p. <fpage>306</fpage>–<lpage>312</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2010.03.006" xlink:type="simple">10.1016/j.conb.2010.03.006</ext-link></comment> <object-id pub-id-type="pmid">20400290</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Baraniuk</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Sparse coding via thresholding and local competition in neural circuits</article-title>. <source>Neural Computation</source>. <year>2008</year>;<volume>20</volume>(<issue>10</issue>):<fpage>2526</fpage>–<lpage>2563</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.03-07-486" xlink:type="simple">10.1162/neco.2008.03-07-486</ext-link></comment> <object-id pub-id-type="pmid">18439138</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>A Sparse Coding Model with Synaptically Local Plasticity and Spiking Neurons Can Account for the Diverse Shapes of V1 Simple Cell Receptive Fields</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year> <month>10</month>;<volume>7</volume>(<issue>10</issue>):<fpage>e1002250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002250" xlink:type="simple">10.1371/journal.pcbi.1002250</ext-link></comment> <object-id pub-id-type="pmid">22046123</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shapero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Hasler</surname> <given-names>P</given-names></name>. <article-title>Configurable hardware integrate and fire neurons for sparse approximation</article-title>. <source>Neural Networks</source>. <year>2013</year>;<volume>45</volume>(<issue>0</issue>):<fpage>134</fpage>–<lpage>143</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S089360801300097X">http://www.sciencedirect.com/science/article/pii/S089360801300097X</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2013.03.012" xlink:type="simple">10.1016/j.neunet.2013.03.012</ext-link></comment> <object-id pub-id-type="pmid">23582485</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Genkin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>A Network of Spiking Neurons for Computing Sparse Representations in an Energy-Efficient Way</article-title>. <source>Neural Computation</source>. <year>2012</year> <month>Aug</month>;<volume>24</volume>(<issue>11</issue>):<fpage>2852</fpage>–<lpage>2872</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00353" xlink:type="simple">10.1162/NECO_a_00353</ext-link></comment> <object-id pub-id-type="pmid">22920853</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Balavoine</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Romberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>. <article-title>Convergence and Rate Analysis of Neural Networks for Sparse Approximation</article-title>. <source>IEEE Transactions on Neural Networks and Learning Systems</source>. <year>2012</year> <month>sept</month>;<volume>23</volume>(<issue>9</issue>):<fpage>1377</fpage>–<lpage>1389</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNNLS.2012.2202400" xlink:type="simple">10.1109/TNNLS.2012.2202400</ext-link></comment> <object-id pub-id-type="pmid">24199030</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="other">Balavoine, A, Rozell, CJ, Romberg, J. Convergence of a Neural Network for Sparse Approximation using the Nonsmooth Łojasiewicz Inequality. In: Int. Joint Conf. Neural Netw. (IJCNN); 2013..</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Charles</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Garrigues</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>. <article-title>A Common Network Architecture Efficiently Implements a Variety of Sparsity-Based Inference Problems</article-title>. <source>Neural Computation</source>. <year>2012</year> <month>Sep</month>;<volume>24</volume>(<issue>12</issue>):<fpage>3317</fpage>–<lpage>3339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00372" xlink:type="simple">10.1162/NECO_a_00372</ext-link></comment> <object-id pub-id-type="pmid">22970876</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shapero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Charles</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Hasler</surname> <given-names>P</given-names></name>. <article-title>Low power sparse approximation on reconfigurable analog hardware</article-title>. <source>Emerging and Selected Topics in Circuits and Systems, IEEE Journal on</source>. <year>2012</year> <month>sept</month>;<volume>2</volume>(<issue>3</issue>):<fpage>530</fpage>–<lpage>541</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/JETCAS.2012.2214615" xlink:type="simple">10.1109/JETCAS.2012.2214615</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shapero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hasler</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rozell</surname> <given-names>C</given-names></name>. <article-title>Optimal sparse approximation with integrate and fire neurons</article-title>. <source>International Journal of Neural Systems</source>. <year>2014</year>;<volume>24</volume>(<issue>05</issue>):<fpage>1440001</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0129065714400012" xlink:type="simple">10.1142/S0129065714400012</ext-link></comment> <object-id pub-id-type="pmid">24875786</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>. <article-title>Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2002</year>;<volume>88</volume>(<issue>1</issue>):<fpage>455</fpage>. <object-id pub-id-type="pmid">12091567</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hirsch</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Pillai</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Alonso</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Functionally distinct inhibitory neurons at the first stage of visual cortical processing</article-title>. <source>Nat Neurosci</source>. <year>2003</year> <month>Dec</month>;<volume>6</volume>(<issue>12</issue>):<fpage>1300</fpage>–<lpage>1308</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1152" xlink:type="simple">10.1038/nn1152</ext-link></comment> <object-id pub-id-type="pmid">14625553</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Vision and the statistics of the visual environment</article-title>. <source>Current opinion in Neurobiology</source>. <year>2003</year>;<volume>13</volume>(<issue>2</issue>):<fpage>144</fpage>–<lpage>149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(03)00047-3" xlink:type="simple">10.1016/S0959-4388(03)00047-3</ext-link></comment> <object-id pub-id-type="pmid">12744966</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year> <month>Feb</month>;<volume>10</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>125</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2558" xlink:type="simple">10.1038/nrn2558</ext-link></comment> <object-id pub-id-type="pmid">19145235</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>How close are we to understanding V1?</article-title> <source>Neural Computation</source>. <year>2005</year>;<volume>17</volume>(<issue>8</issue>):<fpage>1665</fpage>–<lpage>1699</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766054026639" xlink:type="simple">10.1162/0899766054026639</ext-link></comment> <object-id pub-id-type="pmid">15969914</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Communication in neuronal networks</article-title>. <source>Science</source>. <year>2003</year>;<volume>301</volume>(<issue>5641</issue>):<fpage>1870</fpage>–<lpage>1874</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1089662" xlink:type="simple">10.1126/science.1089662</ext-link></comment> <object-id pub-id-type="pmid">14512617</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Koulakov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rinberg</surname> <given-names>D</given-names></name>. <article-title>Sparse Incomplete Representations: A Potential Role of Olfactory Granule Cells</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>1</issue>):<fpage>124</fpage>–<lpage>136</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0896627311006891">http://www.sciencedirect.com/science/article/pii/S0896627311006891</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.07.031" xlink:type="simple">10.1016/j.neuron.2011.07.031</ext-link></comment> <object-id pub-id-type="pmid">21982374</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lee</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Pedersen</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Mumford</surname> <given-names>D</given-names></name>. <article-title>The nonlinear statistics of high-contrast patches in natural images</article-title>. <source>International Journal of Computer Vision</source>. <year>2003</year>;<volume>54</volume>(<issue>1–3</issue>):<fpage>83</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1023705401078" xlink:type="simple">10.1023/A:1023705401078</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Srivastava</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>SC</given-names></name>. <article-title>On Advances in Statistical Modeling of Natural Images</article-title>. <source>Journal of Mathematical Imaging and Vision</source>. <year>2003</year>;<volume>18</volume>:<fpage>17</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1021889010444" xlink:type="simple">10.1023/A:1021889010444</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Galarreta</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Erdélyi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Szabó</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hestrin</surname> <given-names>S</given-names></name>. <article-title>Cannabinoid Sensitivity and Synaptic Properties of 2 GABAergic Networks in the Neocortex</article-title>. <source>Cerebral Cortex</source>. <year>2008</year>;<volume>18</volume>(<issue>10</issue>):<fpage>2296</fpage>–<lpage>2305</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cercor.oxfordjournals.org/content/18/10/2296.abstract">http://cercor.oxfordjournals.org/content/18/10/2296.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhm253" xlink:type="simple">10.1093/cercor/bhm253</ext-link></comment> <object-id pub-id-type="pmid">18203691</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Candès</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>J</given-names></name>. <article-title>Robust Principal Component Analysis?</article-title> <source>Journal of the ACM—Association for Computing Machinery</source>. <year>2011</year>;<volume>58</volume>(<issue>3</issue>).</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="other">Lin Z, Chen M, Ma Y. The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices. ArXiv e-prints. 2010 Sep;p.</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chandrasekaran</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sanghavi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Parrilo</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Willsky</surname> <given-names>AS</given-names></name>. <article-title>Rank-sparsity incoherence for matrix decomposition</article-title>. <source>SIAM Journal on Optimization</source>. <year>2011</year>;<volume>21</volume>(<issue>2</issue>):<fpage>572</fpage>–<lpage>596</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1137/090761793" xlink:type="simple">10.1137/090761793</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="other">Charles A, Ahmed A, Joshi A, Conover S, Turnes C, Davenport M. Cleaning up toxic waste: removing nefarious contributions to recommendation systems. In: ICASSP 2013; 2013..</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lochmann</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>UA</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name>. <article-title>Perceptual Inference Predicts Contextual Modulations of Sensory Responses</article-title>. <source>The Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>12</issue>):<fpage>4179</fpage>–<lpage>4195</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.jneurosci.org/content/32/12/4179.abstract">http://www.jneurosci.org/content/32/12/4179.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0817-11.2012" xlink:type="simple">10.1523/JNEUROSCI.0817-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22442081</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name>. <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS computational biology</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>):<fpage>e1003258</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003258" xlink:type="simple">10.1371/journal.pcbi.1003258</ext-link></comment> <object-id pub-id-type="pmid">24244113</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hofer</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pichler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vogelstein</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ros</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zeng</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Differential connectivity and response dynamics of excitatory and inhibitory neurons in visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Aug</month>;<volume>14</volume>(<issue>8</issue>):<fpage>1045</fpage>–<lpage>1052</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2876" xlink:type="simple">10.1038/nn.2876</ext-link></comment> <object-id pub-id-type="pmid">21765421</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Packer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>Dense, Unspecific Connectivity of Neocortical Parvalbumin-Positive Interneurons: A Canonical Microcircuit for Inhibition?</article-title> <source>The Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>37</issue>):<fpage>13260</fpage>–<lpage>13271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3131-11.2011" xlink:type="simple">10.1523/JNEUROSCI.3131-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21917809</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ma</surname> <given-names>Wp</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Bh</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Yt</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>LI</given-names></name>, <name name-style="western"><surname>Tao</surname> <given-names>HW</given-names></name>. <article-title>Visual representations by cortical somatostatin inhibitory neurons—selective but with weak and delayed responses</article-title>. <source>The Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>43</issue>):<fpage>14371</fpage>–<lpage>14379</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3248-10.2010" xlink:type="simple">10.1523/JNEUROSCI.3248-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20980594</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nowak</surname> <given-names>LG</given-names></name>, <name name-style="western"><surname>Sanchez-Vives</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>McCormick</surname> <given-names>DA</given-names></name>. <article-title>Lack of Orientation and Direction Selectivity in a Subgroup of Fast-Spiking Inhibitory Interneurons: Cellular and Synaptic Mechanisms and Comparison with Other Electrophysiological Cell Types</article-title>. <source>Cerebral Cortex</source>. <year>2008</year>;<volume>18</volume>(<issue>5</issue>):<fpage>1058</fpage>–<lpage>1078</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cercor.oxfordjournals.org/content/18/5/1058.abstract">http://cercor.oxfordjournals.org/content/18/5/1058.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhm137" xlink:type="simple">10.1093/cercor/bhm137</ext-link></comment> <object-id pub-id-type="pmid">17720684</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Adesnik</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bruns</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Taniguchi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>A neural circuit for spatial summation in visual cortex</article-title>. <source>Nature</source>. <year>2012</year> <month>Oct</month>;<volume>490</volume>(<issue>7419</issue>):<fpage>226</fpage>–<lpage>231</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11526" xlink:type="simple">10.1038/nature11526</ext-link></comment> <object-id pub-id-type="pmid">23060193</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly Nonrandom Features of Synaptic Connectivity in Local Cortical Circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year> <month>03</month>;<volume>3</volume>(<issue>3</issue>):<fpage>e68</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment> <object-id pub-id-type="pmid">15737062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ikeda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bekkers</surname> <given-names>JM</given-names></name>. <article-title>Autapses</article-title>. <source>Current Biology</source>. <year>2006</year>;<volume>16</volume>(<issue>9</issue>):<fpage>R308</fpage>–<lpage>R308</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0960982206014187">http://www.sciencedirect.com/science/article/pii/S0960982206014187</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2006.03.085" xlink:type="simple">10.1016/j.cub.2006.03.085</ext-link></comment> <object-id pub-id-type="pmid">16682332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Larkum</surname> <given-names>M</given-names></name>. <article-title>A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</article-title>. <source>Trends in Neurosciences</source>. <year>2013</year>;<volume>36</volume>(<issue>3</issue>):<fpage>141</fpage>–<lpage>151</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0166223612002032">http://www.sciencedirect.com/science/article/pii/S0166223612002032</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2012.11.006" xlink:type="simple">10.1016/j.tins.2012.11.006</ext-link></comment> <object-id pub-id-type="pmid">23273272</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sámano</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Cifuentes</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Morales</surname> <given-names>MA</given-names></name>. <article-title>Neurotransmitter segregation: Functional and plastic implications</article-title>. <source>Progress in Neurobiology</source>. <year>2012</year>;<volume>97</volume>(<issue>3</issue>):<fpage>277</fpage>–<lpage>287</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0301008212000524">http://www.sciencedirect.com/science/article/pii/S0301008212000524</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2012.04.004" xlink:type="simple">10.1016/j.pneurobio.2012.04.004</ext-link></comment> <object-id pub-id-type="pmid">22531669</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Runyan</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Schummers</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wart</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Kuhlman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <etal>et al</etal>. <article-title>Response Features of Parvalbumin-Expressing Interneurons Suggest Precise Roles for Subtypes of Inhibition in Visual Cortex</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>67</volume>(<issue>5</issue>):<fpage>847</fpage>–<lpage>857</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/B6WSS-50YV1JT-M/2/785cac652dc78dc532f215b1b43a4f2d">http://www.sciencedirect.com/science/article/B6WSS-50YV1JT-M/2/785cac652dc78dc532f215b1b43a4f2d</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.08.006" xlink:type="simple">10.1016/j.neuron.2010.08.006</ext-link></comment> <object-id pub-id-type="pmid">20826315</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Huberman</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Niell</surname> <given-names>CM</given-names></name>. <article-title>What can mice tell us about how vision works?</article-title> <source>Trends in Neurosciences</source>. <year>2011</year>;<volume>34</volume>(<issue>9</issue>):<fpage>464</fpage>–<lpage>473</lpage>. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0166223611001068">http://www.sciencedirect.com/science/article/pii/S0166223611001068</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2011.07.002" xlink:type="simple">10.1016/j.tins.2011.07.002</ext-link></comment> <object-id pub-id-type="pmid">21840069</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pfeffer</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Xue</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title>. <source>Nat Neurosci</source>. <year>2013</year> <month>Aug</month>;<volume>16</volume>(<issue>8</issue>):<fpage>1068</fpage>–<lpage>1076</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3446" xlink:type="simple">10.1038/nn.3446</ext-link></comment> <object-id pub-id-type="pmid">23817549</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Silberberg</surname> <given-names>G</given-names></name>. <article-title>Polysynaptic subcircuits in the neocortex: spatial and temporal diversity</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2008</year>;<volume>18</volume>(<issue>3</issue>):<fpage>332</fpage>–<lpage>337</lpage>. Signalling mechanisms. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0959438808000822">http://www.sciencedirect.com/science/article/pii/S0959438808000822</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2008.08.009" xlink:type="simple">10.1016/j.conb.2008.08.009</ext-link></comment> <object-id pub-id-type="pmid">18801433</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>DiMattina</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>K</given-names></name>. <article-title>How to Modify a Neural Network Gradually Without Changing Its Input-Output Functionality</article-title>. <source>Neural Computation</source>. <year>2009</year> <month>Oct</month>;<volume>22</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.05-08-781" xlink:type="simple">10.1162/neco.2009.05-08-781</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004353.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>RG</given-names></name>. <source>Introduction to the theory of neural computation</source>. <volume>vol. 1</volume>. <publisher-name>Westview press</publisher-name>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004353.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Boyd</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Parikh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chu</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Peleato</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>J</given-names></name>. <article-title>Distributed optimization and statistical learning via the alternating direction method of multipliers</article-title>. <source>Foundations and Trends in Machine Learning</source>. <year>2011</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1561/2200000016" xlink:type="simple">10.1561/2200000016</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>