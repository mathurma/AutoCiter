<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00955</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003364</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized</article-title>
<alt-title alt-title-type="running-head">Hierarchical Control of Decision-Making</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dezfouli</surname><given-names>Amir</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Balleine</surname><given-names>Bernard W.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Brain &amp; Mind Research Institute, University of Sydney, Sydney, New South Wales, Australia</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Oxford, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">bernard.balleine@sydney.edu.au</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: AD BWB. Performed the experiments: AD. Analyzed the data: AD. Wrote the paper: AD BWB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>12</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>5</day><month>12</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>12</issue>
<elocation-id>e1003364</elocation-id>
<history>
<date date-type="received"><day>29</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>11</day><month>10</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Dezfouli, Balleine</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Behavioral evidence suggests that instrumental conditioning is governed by two forms of action control: a goal-directed and a habit learning process. Model-based reinforcement learning (RL) has been argued to underlie the goal-directed process; however, the way in which it interacts with habits and the structure of the habitual process has remained unclear. According to a flat architecture, the habitual process corresponds to model-free RL, and its interaction with the goal-directed process is coordinated by an external arbitration mechanism. Alternatively, the interaction between these systems has recently been argued to be hierarchical, such that the formation of action sequences underlies habit learning and a goal-directed process selects between goal-directed actions and habitual sequences of actions to reach the goal. Here we used a two-stage decision-making task to test predictions from these accounts. The hierarchical account predicts that, because they are tied to each other as an action sequence, selecting a habitual action in the first stage will be followed by a habitual action in the second stage, whereas the flat account predicts that the statuses of the first and second stage actions are independent of each other. We found, based on subjects' choices and reaction times, that human subjects combined single actions to build action sequences and that the formation of such action sequences was sufficient to explain habitual actions. Furthermore, based on Bayesian model comparison, a family of hierarchical RL models, assuming a hierarchical interaction between habit and goal-directed processes, provided a better fit of the subjects' behavior than a family of flat models. Although these findings do not rule out all possible model-free accounts of instrumental conditioning, they do show such accounts are not necessary to explain habitual actions and provide a new basis for understanding how goal-directed and habitual action control interact.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>In order to make choices that lead to desirable outcomes, individuals tend to deliberate over the consequences of various alternatives. This goal-directed deliberation is, however, slow and cognitively demanding. As a consequence, under appropriate conditions decision-making can become habitual and automatic. The nature of these habitual actions, how they are learned, expressed, and interact with the goal-directed process is not clearly understood. Here we report that (1) habits interact with the goal-directed process in a hierarchical manner (i.e., the goal-directed system selects a goal, and then determines which habit should be executed to reach that goal), and (2) habits are learned sequences of actions that, once triggered by the goal-directed process, can be expressed quickly and in an efficient manner. The findings provide critical new experimental and computational information on the nature of habits and how they interact with the goal-directed decision-making.</p>
</abstract>
<funding-group><funding-statement>The research reported in this paper was supported by grants from the National Institute of Child Health and Human Development #HD059257 (<ext-link ext-link-type="uri" xlink:href="http://www.nichd.nih.gov" xlink:type="simple">http://www.nichd.nih.gov</ext-link>), from the National Health and Medical Research Council of Australia #633267 (<ext-link ext-link-type="uri" xlink:href="http://www.nhmrc.gov.au" xlink:type="simple">http://www.nhmrc.gov.au</ext-link>), and a Laureate Fellowship from the Australian Research Council, #FL0992409, to BWB (<ext-link ext-link-type="uri" xlink:href="http://www.arc.gov.au" xlink:type="simple">http://www.arc.gov.au</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>There is now considerable evidence from studies of instrumental conditioning in rats and humans that the performance of reward-related actions reflects the involvement of two learning processes, one controlling the acquisition of goal-directed actions and the other of habits <xref ref-type="bibr" rid="pcbi.1003364-Adams1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Dickinson2">[4]</xref>. This evidence suggests that goal-directed decision-making involves deliberating over the consequences of alternative actions in order to predict their outcomes after which action selection is guided by the value of the predicted outcome of each action. In this respect, action evaluation relies on the representation of contingencies between actions and outcomes as well as the value of the outcomes, which in sum constitute a <italic>model of the environment</italic>. In contrast, habitual actions reflect the tendency of individuals to repeat behaviors that have led to desirable outcomes in the past and respect neither their causal relationship to, nor the value of their consequences. As such, they are not guided by a model of the environment, and are relatively inflexible in the face of environmental changes <xref ref-type="bibr" rid="pcbi.1003364-Daw1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Doya1">[7]</xref>.</p>
<p>Although these features of goal-directed and habitual action are reasonably well accepted, the structure of habitual control, and the way in which it interacts with the goal-directed process in exerting that control, is not well understood. Two types of architecture have been proposed: a hierarchical architecture and a flat architecture. We have recently described a version of the hierarchical structure in the context of advancing a new theory of habits <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>. Although habits are usually described as single step actions, their tendency to combine or chunk with other actions <xref ref-type="bibr" rid="pcbi.1003364-Graybiel1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Newell1">[15]</xref> and their insensitivity to changes in the value of, and the causal relationship to, their consequences <xref ref-type="bibr" rid="pcbi.1003364-Balleine1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Dickinson3">[16]</xref> suggests that they may best be viewed as action sequences <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>. On this view habit sequences are represented independently of the individual actions and outcomes embedded in them such that the decision-maker treats the whole sequence of actions as a single response unit. As a consequence, the evaluation of action sequences is divorced from offline environmental changes in individual action-outcome contingencies or the value of outcomes inside the sequence boundaries and, as they are no longer guided by the model of the environment <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>, are executed irrespective of the outcome of each individual action <xref ref-type="bibr" rid="pcbi.1003364-Pew1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Keele1">[17]</xref>; i.e., the actions run off in an order predetermined by the sequence, without requiring immediate feedback.</p>
<p>On this hierarchical view, such action sequences are utilized by a global goal-directed system in order to efficiently reach its goals. This is achieved by learning the contingencies between action sequences and goals and assessing at each decision point whether there is a habit that can achieve that goal. If there is, it executes that habit after which control returns to the goal-directed system. In essence, the goal-directed system functions at a higher level and selects which habit should be executed whereas the role of habits is limited to the efficient implementation of the decisions made by the goal-directed process <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Ostlund1">[18]</xref> (see <xref ref-type="bibr" rid="pcbi.1003364-Botvinick1">[19]</xref> for a review of other schemas).</p>
<p>Assume, for example, you are deciding whether to go to a restaurant on this side of the road or on the other side of the road ( <xref ref-type="fig" rid="pcbi-1003364-g001">Figure 1A</xref>). The goal-directed system evaluates both options, and decides to go to the restaurant across the road. It thus triggers a ‘crossing the road’ habit, and transfers the control to the habitual system. The habit is an action sequence composed of several individual actions: (1) head to the crossing point, (2) look left, and (3) cross the road. Individual actions are executed one after another, and after they finish, the control transfers back to the goal-directed system to make the next decision such as, for example, choosing from the menu in the restaurant.</p>
<fig id="pcbi-1003364-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g001</object-id><label>Figure 1</label><caption>
<title>An example illustrating the difference between the hierarchical and flat organizations.</title>
<p>(A) Hierarchical interaction. The goal-directed system (GD) selects goals and decides whether to go to a restaurant on this side of the road (Rr1) or on the other side of the road (Rr2). If it chooses to go to the restaurant on the other side of the road, then it triggers the habit of crossing the road and control transfers to the habitual process. After execution of the habit finishes, control returns to the goal-directed system. (B) Flat interaction. At each decision point, the arbitration mechanism (Ar) decides whether the next action should be controlled by the goal-directed system or the habitual system (H).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g001" position="float" xlink:type="simple"/></fig>
<p>In contrast to the hierarchical architecture, the flat architecture treats habits as single step actions rather than action sequences (e.g. <xref ref-type="bibr" rid="pcbi.1003364-Daw1">[5]</xref>). At each step, an arbitration mechanism decides whether the next action should be controlled by the goal-directed system or the habitual system. In the context of the above example, at the beginning the arbitration mechanism selects one of the systems to decide whether to go to the restaurant on this side of the road or to the crossing point. Again, at the crossing point, the arbitration mechanism selects one of the systems to decide whether to look left, or right, and similarly at each future step the arbitration mechanism selects one of the systems to control behavior (<xref ref-type="fig" rid="pcbi-1003364-g001">Figure 1B</xref>). It should be clear, therefore, that, in the flat approach, both systems are at the same level and action evaluation happens in both processes; both systems evaluate available alternatives, and the arbitration mechanism determines how these two evaluations combine to make the final decision.</p>
<p>From the flat perspective, another difference between goal-directed and habitual processes lies in how they evaluate actions. The goal-directed process obeys the same principles sketched earlier: learning the model of the environment, and making predictions based on that model (<italic>model-based</italic> evaluation). In contrast, the habitual system is <italic>model-free</italic> and evaluates actions based on their ‘cached’ reward history without searching through the action-outcome contingencies <xref ref-type="bibr" rid="pcbi.1003364-Daw1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Doya1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Fermin1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Redish1">[22]</xref>.</p>
<p>More recently, Daw et al <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref> have exploited the difference between model-free and model-based evaluation to investigate the interaction of goal-directed and habit processes in a flat structure reasoning that, because model-free evaluation is retrospective, chaining predictions backward across previous trials, and model-based evaluation is prospective, directly assessing available future possibilities, it is possible to distinguish the two using a sequential, multistage choice task. In this task subjects first make a binary choice (the first stage) then transition to the second stage in which they make a second choice to earn a reward. The best choice at the second stage varies depending on the first choice and, to maintain a constant trade-off between habitual and goal-directed systems, the reward probabilities in the second stage are continually varied. By examining first stage choices, Daw et al <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref> were able to find evidence of mixed goal-directed and habitual predictions.</p>
<p>Here we show that first stage habitual actions, explained by the model-free evaluation in previous work, can also be explained by assuming that first stage actions chunk with second stage actions, reducing the source of habitual actions to the formation of action sequences. Based on this finding we next examined specific predictions of each account. With regard to the two-stage task, the flat account predicts that feedback received after the execution of an action will affect subsequent decisions and, therefore, that arbitration between goal-directed and habit controllers will recur anew at each stage. As a consequence, action-control at each stage of the task should be independently established; in particular it should be noted that action control in stage two should not depend on stage one. In contrast, because our hierarchical account treats habits as action sequences, and because the execution of habits is open-loop, it predicts that, during the execution of a habit, actions will be executed one after another without considering feedback from the environment during the sequence and, therefore, that, when habitual, the action taken at stage 2 is already determined when starting the habit sequence at stage 1. We made two further predictions from the hierarchical account: first, because of their relative freedom from feedback, action sequences should be elicited more quickly than single actions <xref ref-type="bibr" rid="pcbi.1003364-Nissen1">[24]</xref> predicting that, when habitual, reaction times between stage 1 and stage 2 actions will be faster than when non-habitual. Second, and based on these predictions, we anticipated that the hierarchical model would better fit the performance of subjects working on this two-stage task than the flat model.</p>
</sec><sec id="s2">
<title>Results</title>
<p>Fifteen subjects completed a two-stage decision-making task (<xref ref-type="fig" rid="pcbi-1003364-g002">Figure 2</xref>), in which each trial started with a choice between two key presses (first stage actions; A1 vs. A2). Each key press resulted in the appearance of either of two slot machines (denoted by S1 and S2 and distinguished by their colors) in a probabilistic manner. Next, at the slot machines, subjects again chose between two key presses (second stage actions; A1 versus A2), and, as a result, received an outcome; i.e., either a monetary reward or a neutral outcome. At the first stage, A1 most commonly led to S1, and A2 to S2 (common transitions; 70% of the time). In a minority of trials, A1 led to S2, and A2 to S1 (rare transitions; 30% of the time). This relationship was kept fixed throughout the test. Each of the second stage responses at the slot machines earned a reward either at a high probability (0.7) or a low probability (0.2). In order to ensure the subjects kept searching for the best keys and slot machines during the test, at each trial, with a small probability (1∶7), the rewarding probability of each key changed randomly to either the high or low probability. Each participant completed 270 trials.</p>
<fig id="pcbi-1003364-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g002</object-id><label>Figure 2</label><caption>
<title>Task description.</title>
<p>(A) Illustration of the timeline of events within a trial. Initially a black screen is presented, and the subject can choose between pressing A1, or A2 (first stage choice). After a key is pressed, one of the slot machines is presented, and the subject can again choose between pressing A1, and A2 (second stage choice). Choices at the second stage are reinforced by monetary reward. (B) Structure of the task. One of the key presses commonly leads to one of slot machines (70% of the time), and the other key commonly leads to the other slot machine. Choices at the second stage are reinforced either by a high probability (0.7) or a low probability (0.2). With a small probability (1/7), the rewarding probability of each key changes randomly to either the high or low probability.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g002" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Goal-directed and habitual performance on the two-stage task</title>
<p>In the analysis, we first sought to establish whether decision-making in this task is goal-directed, habitual or a mixture of both and, if both, to assess whether goal-directed and habitual control interact according to a flat structure or a hierarchical structure.</p>
<p>The first question can be answered by looking at the likelihood of the subjects repeating the same first stage action on each trial based on feedback received on the previous trial <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref>. Take for example a trial in which a subject presses A1 and transfers to the S2 slot machine (which is rare result of choosing A1). If the participant presses a button of that slot machine and receives a reward, this implies S2 is probably a good slot machine and, if the decision-making is goal-directed, in the first stage of the next trial the subject should try to reach this S2 slot machine again. It is expected therefore, that the probability that the subject will press A2 will increase because it is this key that (in this example) commonly leads to S2 (cf. <xref ref-type="fig" rid="pcbi-1003364-g003">Figure 3B</xref>). In contrast, if decisions are habitual, subjects should not be guided by contingencies between the responses and slot machines, and should tend to stay on the previously rewarded action, A1 (<xref ref-type="fig" rid="pcbi-1003364-g003">Figure 3A</xref>).</p>
<fig id="pcbi-1003364-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g003</object-id><label>Figure 3</label><caption>
<title>First-stage choices.</title>
<p>(A) Modeled habitual action control on the two–stage task: Under habitual control a first stage action that has been eventually reinforced (reward) in the previous trial is more likely to be repeated (higher stay probability), regardless of whether the repeated action commonly leads to the same slot machine (common) or not (rare). (B) Modeled goal-directed action control on the two–stage task: Under goal-directed action control a reinforced action is repeated if it commonly leads to the same slot machine in which reward is received, otherwise the other action is selected. (C) Data from the experiment: Actual stay probabilities averaged over all subjects and trials. When the previous trial was rewarded, stay probability was generally higher (as in habitual control), and was also higher when the previous trial was a common transition (as in goal-directed control). Thus, the responses of the subjects in the experiment were found to be a mixture of both habitual and goal-directed action control. Error bars: 1 SEM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g003" position="float" xlink:type="simple"/></fig>
<p>The results are presented in <xref ref-type="fig" rid="pcbi-1003364-g003">Figure 3C</xref>, which shows the probability of repeating the same action computed across all subjects and trials. We analyzed the data using mixed-effects logistic regression analyses by taking all coefficients as random effects across subjects (see Materials &amp; Methods: Behavioral Analysis). Results show that being rewarded in the previous trial increased the chance of staying on the same action, irrespective of whether it was a rare or a common transition (main effect of reward; coefficient estimate = 0.61; SE = 0.09; p&lt;3e-11), which suggests that habits constitute a component of the behavior. On the other hand, this increase was higher if the previous trial was a common transition (and lower after an unrewarded trial), suggesting that subjects also utilized their knowledge about the task structure (reward-transition interaction; coefficient estimate = 0.41; SE = 0.11; p&lt;5e-4). Therefore, the subjects' behavior was a mixture of both goal-directed and habitual actions. Also, as the figure shows, the probability of staying on the same action is generally higher than not staying on it, irrespective of reward and transition type in the previous trial (the intercept term is significantly positive; estimate = 1.52; SE = 0.20; p&lt;10e-14), which reflects a general tendency of animals and humans to repeat previous actions <xref ref-type="bibr" rid="pcbi.1003364-Ito1">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Lau1">[27]</xref>.</p>
<p>In previous studies, a hybrid model of model-free and model-based reinforcement learning (RL) was advanced to explain the behavior of subjects on this task based on the flat structure <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Glscher1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Otto1">[30]</xref>. According to this model, action values learned in model-free RL, roughly, reflect the frequency of the action rewarded on previous trials irrespective of the action-outcome contingency (i.e., in the current task, which key generates which slot machine) and, as such, these values underlie the habitual component of the model. These model-free values are then mixed with the values provided by the goal-directed system (modeled by a model-based RL) to produce the final values which guide action selection. As a consequence, and consistent with the above results, we should expect to see a combination of both habitual and goal-directed actions. The prediction from this hybrid model is illustrated in <xref ref-type="fig" rid="pcbi-1003364-g004">Figure 4A</xref>.</p>
<fig id="pcbi-1003364-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g004</object-id><label>Figure 4</label><caption>
<title>Simulation of the first-stage choices.</title>
<p>The probability of staying on the first stage action in simulations of: (A) the flat architecture; and, (B) the hierarchical architecture. Both architectures can model the pattern of data observed in the first stage stay probabilities on the task: i.e., a higher stay probability after being rewarded on the previous trial and an interaction between reward and transition.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g004" position="float" xlink:type="simple"/></fig>
<p>A hierarchical structure can, however, also be used to explain these results. For example, assume that a subject presses A1 in the first stage, and A2 in the second stage and receives a reward. As a result, the goal-directed system learns that contingency between the A1A2 action sequence and the reward is increased and so it should be more likely to repeat the action sequence in the next trial, whether or not the reward was received from the S1 or S2 slot machine (i.e., the common or rare transition). As the evaluation and performance of an action sequence is not guided by the task structure (i.e. the key-slot machine association), from this perspective it constitutes the habitual component of the behavior. All actions - either single action (e.g., A1) or action sequences (e.g., A1A2)-, will be subject to the goal-directed action selection process, such that actions with higher values will be selected with a higher probability. As a consequence, this implies that the behavior will be a mixture of habitual (when action sequences are selected) and goal-directed (when single actions are selected) actions and that this mix of actions can be generated without the need for the model-free component or an explicit arbitration mechanism used in the flat structure. This prediction is illustrated in <xref ref-type="fig" rid="pcbi-1003364-g004">Figure 4B</xref>.</p>
</sec><sec id="s2b">
<title>The interaction of goal-directed actions and habit sequences in stage 2 performance</title>
<p>Although both approaches are able to explain the mixture of behavioral control in the first stage, they make different predictions about second stage choices. This is because, if the observed habitual behavior is due to the execution of an action sequence, rather than cached values as the model-free account supposes, then we expect the subject to repeat the whole action sequence in the next trial, not just the first stage action.</p>
<p>Staying on the same first stage action in the next trial after being rewarded implies that this is probably a habitual response and so we expect the subject to repeat the second stage action as well, even if the slot machine is different from the one in the previous trial. In contrast, if the subject switches to the other first stage action, the previous action sequence is not repeated, and thus the second stage action is not expected to be repeated if the subject ends with a different slot machine in the next trial. In order to test this prediction, we looked at the trials that had a different slot machine to the one in their previous trial.</p>
<p><xref ref-type="fig" rid="pcbi-1003364-g005">Figure 5A</xref> shows the probability of repeating the same second stage action as a function of whether this action was rewarded on the previous trial and the subject had subsequently taken the same first stage action. Logistic regression conducted on second stage choices using factors of reward, separating rewarded and non-rewarded trials, and action, separating trials on which the first stage action was the same from those on which it differed, found neither an effect of reward (p&gt;0.05), nor of action (p&gt;0.05) but found a significant interaction between these factors (coefficient estimate = 1.02; SE = 0.38; p&lt;0.008), indicating that, during the execution of habitual responses, subjects tended to repeat the second stage action. This interaction remained significant even when we restricted the analysis either to trials after rare transitions (coefficient estimate = 1.33; SE = 0.60; p&lt;0.05) or after common transitions (coefficient estimate = 0.93; SE = 0.38; p&lt;0.05). Importantly, the fact that the effect of the reward was not significant rules out the possibility that the effect was due to the generalization of the values across slot machines.</p>
<fig id="pcbi-1003364-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g005</object-id><label>Figure 5</label><caption>
<title>Second-stage choices.</title>
<p>The probability of staying on the second stage action on trials for which the slot machine differs from the on one in the previous trial: (A) The observed stay probabilities. When the subjects are rewarded and stay on the same first stage action (same), the probability of staying on the same second stage action is higher. (B) Simulation of the flat architecture. Note that this is not consistent with the pattern in panel (A). (C) Simulation of the hierarchical architecture, which is consistent with the pattern observed in actual stay probabilities. Error bars: 1 SEM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g005" position="float" xlink:type="simple"/></fig>
<p>Simulations of the flat and hierarchical models are presented in <xref ref-type="fig" rid="pcbi-1003364-g005">Figure 5B and C</xref>, respectively. As predicted, the hierarchical structure captures the pattern of the subjects' second stage actions (the interaction between the reward and the same first stage action; p&lt;0.001), whereas the flat structure is not consistent with repeating the same action in the second stage (p&gt;0.05).</p>
<p>Previously, we focused on trials with a different slot machine to the one in the previous trial. This was because, in this condition, flat and hierarchical accounts provide different predictions. When the slot machine is the same, both accounts (flat and hierarchical) predict that being rewarded in the previous trial increases the probability of staying on the same second stage action. In addition to this prediction, the hierarchical account predicts that when the slot machine is the same as the one on the previous trial, this increase should be higher than the increase when the slot machine is different. This is because, when the slot machine is different, staying on the same second stage action is drive by execution of the previous action sequence whereas, when the slot machine is the same, executing either the previous action sequence or a goal-directed decision at the second stage can result in staying on the same second stage action.</p>
<p>As a consequence we looked at the effect of being rewarded in the previous trial, and whether the slot machine was the same as the one in the previous trial, on the probability of staying of the same second stage action (in the trials in which the first stage action was the same as the previous trial).</p>
<p><xref ref-type="fig" rid="pcbi-1003364-g006">Figure 6A</xref> shows the results. A significant main effect of reward was found (coefficient estimate = 0.69; SE = 0.21; p&lt;0.002) indicating that being rewarded in the previous trial increases the probability of taking the same second stage action, irrespective of whether the slot machine was the same as the previous trial or not, which is consistent with the hierarchical account. In addition, we found a significant interaction between the effect of reward and whether the slot machine being the same (coefficient estimate = 3.46; SE = 0.51; p&lt;3e-11), consistent with the finding that the probability of staying on the second stage action was higher when the second stage action was the same.</p>
<fig id="pcbi-1003364-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g006</object-id><label>Figure 6</label><caption>
<title>Second-stage choices in a same vs. different state.</title>
<p>The probability of staying on the second stage action when the same (A) or different (B) first stage action is taken, as a function of whether the previous trial is rewarded, and whether the second stage state is the same or different from the previous trial. Error bars: 1 SEM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g006" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003364-g006">Figure 6B</xref> shows the probability of staying on the same second stage action when the subject takes a different first stage action. As predicted, because the subject did not execute the previous action sequence, the main effect of reward was not significant (p&gt;0.05) but the interaction between reward and the second stage being the same was significant (coefficient estimate = 1.72; SE = 0.40; p&lt;3e-5) which means that subjects tend to take the same action on the same slot machine after being rewarded, as predicted by both accounts.</p>
</sec><sec id="s2c">
<title>Reaction times during habit execution</title>
<p>In the previous section we showed that if, after being rewarded, the subject repeats the same first stage action, they are probably repeating the previous action sequence and, as such, they tend to repeat the second stage action as well. However, even in the situation in which the subject is executing an action sequence there will be trials on which they might not repeat the same second stage action. In such conditions, we should suppose that either (i) the subject took an exploratory goal-directed action in the first stage, or (ii) the subject started an action sequence but its performance was inhibited and control returned to the evaluation system in the second stage. In both cases, the hierarchical account predicts that reaction times on trials in which the second stage action is not taken should be higher.</p>
<p><xref ref-type="fig" rid="pcbi-1003364-g007">Figure 7A</xref> illustrates these reaction times as a function of whether the previous trial was rewarded and the subject takes the same second stage action (only in trials on which the slot machine is different from that on the previous trial and the subject subsequently takes the same first stage action). If the previous trial is rewarded, reaction times were lower when a subject completes an action sequence than when the second stage action was not executed as a part of a sequence (coefficient estimate = −1.66; SE = 0.45; p&lt;3e-4). Importantly, the effect was not significant when the previous trial was not rewarded (p&gt;0.05), which rules out the possibility that the observed increase in the reaction times was because of the cost of switching to the other second stage action.</p>
<fig id="pcbi-1003364-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g007</object-id><label>Figure 7</label><caption>
<title>Reaction times in the second-stage.</title>
<p>(A) Reaction time (RT) in the second stage action when the same first stage action was taken as a function of whether the same second stage action was taken and whether pervious trial is rewarded (only calculated for trials on which the second stage state was different from the previous trial). (B) Predicted reaction times by the model (a.u. : arbitrary unit). Error bars: 1 SEM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g007" position="float" xlink:type="simple"/></fig>
<p>We further asked whether the model can predict the reaction times in the second stage. As mentioned above, at the first stage, the goal-directed process more frequently selects actions that have a higher contingency to reward (either single actions, or action sequences). As such, if an action sequence has a high value, it is likely to be selected for execution, and so we expect a low reaction time in the second stage. For example, assume the subject has executed action A1 in the first stage, and A2 in the second stage and the aim is to predict whether A2 has a high or low reaction time. It can be argued, if the value of the A1A2 action sequence is high, that it was probably executed in the first stage, and thus the execution of A2 is part of an action sequence (A1A2) started in the first stage, implying the subject should show a low reaction time. In general we assume that the reaction time in the second stage is inversely related to the value of the action sequence that contains that action (see Material &amp; Methods: Hierarchical sequence based, model based RL). In the case of this example we will have:<disp-formula id="pcbi.1003364.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e001" xlink:type="simple"/></disp-formula>Based on this, we calculated the predicted reaction time of the action taken by the subject in the conditions shown in <xref ref-type="fig" rid="pcbi-1003364-g007">Figure 7A</xref>. The results are shown in <xref ref-type="fig" rid="pcbi-1003364-g007">Figure 7B</xref>. As the figure shows, the predicted reaction times by the model are consistent with the pattern of reaction times observed in the data.</p>
<p>In general, the above analysis of stage 2 performance and this analysis of reaction times implies that (i) when the previous trial is rewarded, (ii) the same first stage action is taken, and (iii) the reaction time is low, then the subject is most likely performing an action sequence. As a consequence it is expected to repeat the same second stage action, even on a different slot machine to the one in the previous trial. In order to more closely examine this relationship we used conditional inference trees and partitioned second stage actions into whether they involved staying or switching to the other action based on the above three factors (see Materials &amp; Methods: Behavioral Analysis for more details). The results are shown in <xref ref-type="fig" rid="pcbi-1003364-g008">Figure 8</xref>. As the figure shows, when the previous trial was not rewarded (node # 1 ‘no reward’ condition), staying on the same second stage action was independent of either whether the first stage action was repeated or the reaction time was low (p&gt;0.05; permutation test). If the previous trial was rewarded (node # 1 ‘reward’ condition) then, if the reaction time was high (node #2 RT&gt;0.437s) or the reaction time was low but the subject doesn't repeat the first stage action (node #3 ‘different’ condition), then again the second stage action was not repeated. Only when: (i) the previous trial was rewarded, (ii) the subject took the same first stage action, and (iii) their reaction time was low (node #3 ‘same’ condition), did the subject repeat the second stage action, consistent with the prediction of the hierarchical account.</p>
<fig id="pcbi-1003364-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.g008</object-id><label>Figure 8</label><caption>
<title>Effect of reward, and reaction times on second-stage choices.</title>
<p>Partitioning the probability of staying on the same second stage action (stay: staying on the same second stage action; switch: switching to the other second stage action) as a function of (i) reward on the previous trial (node #1), (ii) whether the same first stage action is taken (stage 1 action; node #3), and (iii) reaction times (RT). ‘n’ represents the number of data points; p-values are calculated using a permutation test.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.g008" position="float" xlink:type="simple"/></fig></sec><sec id="s2d">
<title>Behavioral modeling: Bayesian model selection</title>
<p>The results described in the previous sections suggest that a hierarchical structure better characterizes the effect of feedback from the previous trial on performance on the subsequent trial. However, choices are generally guided by the feedback from all previous trials, not just the immediately prior trial. As such, it is still to be established which framework better captures behavior in this more general condition.</p>
<p>We used a Bayesian model selection method to establish which framework produces choices that are the most similar to the subjects' actions. Both flat and hierarchical architectures have different variants with different degrees of freedom. As such, we compared a family of flat models with a family of hierarchical models <xref ref-type="bibr" rid="pcbi.1003364-Penny1">[31]</xref>, where each family consists of a complex model, and its nested simpler models. The results (<xref ref-type="table" rid="pcbi-1003364-t001">Table 1</xref>) show that, given the subjects' data, the hierarchical family is more likely than the flat family to produces choices similar to those made by the subjects. We found that the exceedance probability in favor of the hierarchical family was 0.99 meaning, roughly, that we can be 99% confident that the hierarchical family generated the observed data.</p>
<table-wrap id="pcbi-1003364-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.t001</object-id><label>Table 1</label><caption>
<title>Model comparison between hierarchical and flat families.</title>
</caption><alternatives><graphic id="pcbi-1003364-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="2" align="left" rowspan="1">In Each Family</td>
<td colspan="3" align="left" rowspan="1">In Total</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Family</td>
<td align="left" rowspan="1" colspan="1">Free Parameters</td>
<td align="left" rowspan="1" colspan="1">-log(P(D|M))</td>
<td align="left" rowspan="1" colspan="1">p-r<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">Number Favoring Best Model</td>
<td align="left" rowspan="1" colspan="1">Exceedance Probability</td>
<td align="left" rowspan="1" colspan="1">Exceedance Probability</td>
<td align="left" rowspan="1" colspan="1">Number Favoring Best Model</td>
<td align="left" rowspan="1" colspan="1">Exceedance Probability</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">H</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e002" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4219.6</td>
<td align="left" rowspan="1" colspan="1">0.26</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">0.000</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">0.993</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e003" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4092.7</td>
<td align="left" rowspan="1" colspan="1">0.29</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1">0.000</td>
<td align="left" rowspan="1" colspan="1">0.003</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e004" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4189.3</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">0.001</td>
<td align="left" rowspan="1" colspan="1">0.005</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e005" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4127.9</td>
<td align="left" rowspan="1" colspan="1">0.29</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">0.003</td>
<td align="left" rowspan="1" colspan="1">0.013</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e006" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4074.9</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">0.002</td>
<td align="left" rowspan="1" colspan="1">0.011</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e007" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4078.0</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">0.011</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e008" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4110.3</td>
<td align="left" rowspan="1" colspan="1">0.29</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">0.000</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e009" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4058.7</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">0.986<xref ref-type="table-fn" rid="nt102">*</xref></td>
<td align="left" rowspan="1" colspan="1">0.911<xref ref-type="table-fn" rid="nt103">**</xref></td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">F</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e010" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4212.0</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">0.002</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">0.006</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e011" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4212.5</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e012" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4168.6</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">0.697<xref ref-type="table-fn" rid="nt102">*</xref></td>
<td align="left" rowspan="1" colspan="1">0.006</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e013" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4201.1</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">0.005</td>
<td align="left" rowspan="1" colspan="1">0.002</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e014" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4173.1</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">0.032</td>
<td align="left" rowspan="1" colspan="1">0.006</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e015" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4198.4</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">0.007</td>
<td align="left" rowspan="1" colspan="1">0.003</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e016" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4174.2</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">0.049</td>
<td align="left" rowspan="1" colspan="1">0.002</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e017" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4169.4</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">0.199</td>
<td align="left" rowspan="1" colspan="1">0.005</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>H: Hierarchical; F: Flat. Shown for each model: negative log model evidence −log(P(D|M)); a pseudo-r statistic (p – r<sup>2</sup>) which is a normalized measure of the degree of variance accounted for in comparison to a model with random choices; the number of subjects favoring the best fitting model based on the model evidence; The exceedance probability which represents the probability that each model (or family) is most likely among alternatives over the population.</p></fn><fn id="nt102"><label>*</label><p>best fitting model in each family.</p></fn><fn id="nt103"><label>**</label><p>best fitting model.</p></fn></table-wrap-foot></table-wrap>
<p>In the hierarchical family, the probabilities of taking actions in the second stage are partially based on the probability of taking an action sequence in the first stage. As these second stage choices are the canonical difference between the two families, we expected that removing the effect of action sequences on the second stage choices would reduce the fit of the hierarchical account to data. Thus we generated a family of hierarchical models similar to <xref ref-type="table" rid="pcbi-1003364-t001">Table 1</xref>. but with the effect of action sequences on the second stage actions removed, and compared the generated family with the family of hierarchical models presented in <xref ref-type="table" rid="pcbi-1003364-t001">Table 1</xref> (see Materials &amp; Methods: Hierarchical model-based, sequence-based RL). Results indicated that the exceedance probability in favor of the family in which the performance of action sequences was reflected in second stage choices was 0.99, confirming that the selection of an action sequence in the first stage increased the probability of taking the second element of the action sequence in the next stage.</p>
<p><xref ref-type="table" rid="pcbi-1003364-t001">Table 1</xref> represents the model comparison results within each family. The parameter estimates for the best fitting model from each family in terms of the exceedance probabilities <xref ref-type="bibr" rid="pcbi.1003364-Stephan1">[32]</xref> are presented in <xref ref-type="table" rid="pcbi-1003364-t002">Table 2</xref>. The best fitting models from each family were simulated in the task conditions to produce <xref ref-type="fig" rid="pcbi-1003364-g004">Figure 4</xref> and <xref ref-type="fig" rid="pcbi-1003364-g005">Figure 5</xref>.</p>
<table-wrap id="pcbi-1003364-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003364.t002</object-id><label>Table 2</label><caption>
<title>Best fitting parameter estimates for each family across subjects.</title>
</caption><alternatives><graphic id="pcbi-1003364-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003364.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="4" align="left" rowspan="1">Hierarchical</td>
<td colspan="4" align="left" rowspan="1">Flat</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Parameter</td>
<td align="left" rowspan="1" colspan="1">First Quartile</td>
<td align="left" rowspan="1" colspan="1">Median</td>
<td align="left" rowspan="1" colspan="1">Third Quartile</td>
<td align="left" rowspan="1" colspan="1">Parameter</td>
<td align="left" rowspan="1" colspan="1">First Quartile</td>
<td align="left" rowspan="1" colspan="1">Median</td>
<td align="left" rowspan="1" colspan="1">Third Quartile</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e018" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">4.24</td>
<td align="left" rowspan="1" colspan="1">5.80</td>
<td align="left" rowspan="1" colspan="1">6.96</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e019" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1.64</td>
<td align="left" rowspan="1" colspan="1">2.33</td>
<td align="left" rowspan="1" colspan="1">3.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e020" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.82</td>
<td align="left" rowspan="1" colspan="1">0.89</td>
<td align="left" rowspan="1" colspan="1">0.95</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e021" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.65</td>
<td align="left" rowspan="1" colspan="1">0.84</td>
<td align="left" rowspan="1" colspan="1">0.93</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e022" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1.25</td>
<td align="left" rowspan="1" colspan="1">1.66</td>
<td align="left" rowspan="1" colspan="1">2.25</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e023" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.84</td>
<td align="left" rowspan="1" colspan="1">0.94</td>
<td align="left" rowspan="1" colspan="1">1.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e024" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.25</td>
<td align="left" rowspan="1" colspan="1">0.46</td>
<td align="left" rowspan="1" colspan="1">0.62</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e025" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.40</td>
<td align="left" rowspan="1" colspan="1">0.59</td>
<td align="left" rowspan="1" colspan="1">0.68</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e026" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−0.50</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">0.70</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e027" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.69</td>
<td align="left" rowspan="1" colspan="1">0.93</td>
<td align="left" rowspan="1" colspan="1">0.95</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e028" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">0.14</td>
<td align="left" rowspan="1" colspan="1">0.29</td>
<td align="left" rowspan="1" colspan="1">0.77</td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">-</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec></sec><sec id="s3">
<title>Discussion</title>
<p>Although prior research has suggested that goal-directed and habitual actions should be conceived as single step actions organized according to a flat architecture (e.g. <xref ref-type="bibr" rid="pcbi.1003364-Daw1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref>), the results of the current experiment found that: (i) human subjects combined actions together to form action sequences, as revealed by the open-loop execution of sequences of actions and reaction times in the current task, and, therefore, that action sequences constituted a necessary component of behavior; (ii) the use of action sequences by human subjects was sufficient to explain habitual decisions on this task, meaning choices that were not guided by action-outcome contingencies; and, (iii) a goal-directed system assessing both actions and action sequences in a hierarchical manner explained behavior better than a flat model attributing habits to model-free evaluation.</p>
<p>Furthermore, although hierarchical models have had a longstanding role in decision-making <xref ref-type="bibr" rid="pcbi.1003364-Lashley1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Miller1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Newell1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Botvinick2">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Cooper2">[37]</xref>, here we provide direct experimental evidence for the role of these models in understanding the operation and interaction of goal-directed and habitual actions. We used a version of the two-stage discrimination task described by Daw et al <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref> in which the ambiguity of the first stage predictions by both actions and stimuli was reduced by removing the explicit predictive cues of previous versions. Using this task we found, as previously described, that action selection in the first stage reflected a mixture of goal-directed and habitual strategies. The two accounts diverge with respect to the status of the second stage actions; whereas the flat architecture/single step action perspective predicts that the status of action selection in the second stage should be independent of the first, we found that this was not true; habitual action selection in the first stage predicted continued habitual selection in the second stage as a sequence of actions, a finding predicted by a hierarchical goal-directed/habit sequence account <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>. According to this account, at the top of the hierarchy the goal-directed system evaluates and selects goals and then habits efficiently implement decisions made by the goal-directed system in the form of action sequences. In comparison to the other accounts, which posit a flat interaction between these two systems, we found that the hierarchical account provides more accurate predictions both in terms of the choices of the subjects, and in terms of their reaction times during action selection. When performing according to a habitual sequence of actions, subjects tended to repeat both previously reinforced sequences and to perform these sequences at significantly lower reaction times than when their actions were goal-directed.</p>
<sec id="s3a">
<title>Hierarchical decision-making and the two-stage task</title>
<p>A number of studies have previously investigated the relationship between hierarchical RL and decision-making <xref ref-type="bibr" rid="pcbi.1003364-Doya1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Botvinick1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-RibasFernandes1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Holroyd1">[43]</xref>. We extended these studies by showing how the formation of action sequences can lead to decisions that are insensitive to (i) the values of the outcomes <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref> and (ii) the contingency between specific actions and their outcomes (i.e. the key press–slot machine associations in this study), the two defining characteristics of the habitual behavior.</p>
<p>The other difference between the hierarchical RL model that we used here and previous work is that we assumed that performance of action sequences is insensitive to the feedback received during execution <xref ref-type="bibr" rid="pcbi.1003364-Pew1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Keele1">[17]</xref>, whereas, in general, previous work based on hierarchical RL theory has assumed that action selection is based on the state of the environment <xref ref-type="bibr" rid="pcbi.1003364-Barto1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Sutton1">[46]</xref>. Within this latter framework, one can posit that habits are hierarchically organized actions but that their performance is sensitive to the feedback received after execution of each individual action. Although this class of models can explain habitual behavior executed in the first stage of the current task, this approach predicts that second stage actions will, ultimately, be similar to those of the flat architecture discussed earlier, which is not consistent with the data observed in this study.</p>
<p>In the hierarchical account advanced here we assumed, based on the previous findings in rodents <xref ref-type="bibr" rid="pcbi.1003364-Ostlund1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Balleine2">[47]</xref>, that, similar to single actions, action sequences are also under goal-directed control. Alternatively, it is possible that the value of any action sequence is learned in a model-free manner (for example using Q-learning) without learning the identity of the particular outcome that it predicts. Our results are silent with respect to this latter assumption; nevertheless, whatever the case, the conclusion that habitual responses in the first stage were due to the execution of an action sequence still holds. One way to study this issue is to add another choice to the end of the task, making it a three stage task, and then asking whether performance of for example A1A2 action sequence is goal-directed or habitual, which can be answered by devaluation of outcome of A1A2, or using the same task structure that we used here to distinguish habitual and goal-directed actions. However, again, if it were found that the selection of the A1A2 sequence was not sensitive to environmental contingencies, or outcome values, this could be due either to the formation of A1A2A3 action sequence (since outcome of A1A2 falls within sequence boundaries <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>), or it could be because action sequences are open to model-free evaluation. Similar to the study here, these accounts can be distinguished by examining whether the subject selects A3 during habitual selection of A1A2 irrespective of the outcome of A1A2 performance. If so, it can be concluded that the observed habitual behavior is due to the formation of an action sequence, not model-free RL. Along the same lines, it is possible to assume that, in the current study, first stage habitual responses were guided by a flat model operating in parallel to the hierarchical model we propose here. Again, although the task results are neutral with respect to this assumption, adding a parallel model increases the model's complexity, is not required to account for the current data, and so it necessity should be motivated by additional behavioral data.</p>
<p>It might also be argued that, although the current predictions apply to the modified two-stage discrimination task used here, they may not apply to previous versions of the task. In previous versions, subjects at each stage chose between two symbols instead of two fixed actions and the symbols moved from side to side at each trial ensuring there was no consistent mapping between the button presses and the symbols. There are two points to make here: First, the fact that specific (e.g. left- or right-hand) actions are degraded in their contingency with the outcome on this version of the task raises the issue of stimulus control; either the stimuli exclusively mediate the predictions of second stage outcomes or the concept of action needs to be made more liberal to the selection of a symbol. The former approach would, of course, render the task Pavlovian, rather than instrumental, and the applicability of model-based control problematic. Second, and relatedly, in order to apply our hierarchical model to the earlier task, we also need to extend the concept of an ‘action’ from pressing a button (as in our task), to selecting a symbol; if this is accepted then, using the logic laid out earlier, the hierarchical goal-directed/habit sequences model can explain the results of the task. In the prior version of the task, symbols in the second stage were different from each other, for example in one of the second stage states subject could choose between symbols ‘C’ and ‘D’, but in the other second stage state, the choice was between symbols ‘E’ and ‘F’. As such, we cannot directly assess the probability of staying on the same second stage action if the subjects end up in a different second stage state. Nevertheless, the hierarchical theory predicts that if the subject selects same first stage action, and ends up with the same second stage state and selects the same second stage action, then the reaction time will be faster than when they end up with in a different second stage state.</p>
</sec><sec id="s3b">
<title>Deviations from prediction and the interpretation of the two-stage task</title>
<p>Predictions of both models (flat and hierarchical) were found to deviate from the behavior of the subjects in two cases. In the first case, if, after being rewarded, the subject switches to the other action then both accounts predict that the probability of staying on the second stage action should be on average 0.5 (<xref ref-type="fig" rid="pcbi-1003364-g005">Figure 5B,C</xref>). However, in the actual data it is below 0.5 (<xref ref-type="fig" rid="pcbi-1003364-g005">Figure 5A</xref>). In the second case, both accounts predict that the difference between stay probability in common and rare transitions should be equal in both the reward and no-reward conditions (<xref ref-type="fig" rid="pcbi-1003364-g004">Figure 4A, B</xref>), however, as <xref ref-type="fig" rid="pcbi-1003364-g003">Figure 3C</xref> shows, the difference is larger in the reward condition. It is possible to capture these two deviations by adding more free parameters to the models; however, since the deviations exist for both the flat and hierarchical families and so do not affect the comparison between them, we didn't add further parameters to account for these two deviations.</p>
<p>As in previous work, we interpreted the interaction between being rewarded and the type of transition in the previous trial (rare or common) as the evidence for goal-directed behavior. It should, however, be noted that, if there is a strong initial bias in total possible reward for one action vs. the other at the first-stage, and reward transitions are slow, then it is possible to observe an interaction between reward and transition type without engaging a goal-directed system. As a consequence of the higher overall probability of reward for taking, say, action ‘A1’ in the first stage, the subject can establish that action has a higher value (without relying on the task structure) and so will take that action, i.e. ‘A1’, more frequently than the other, i.e. action ‘A2’, which means that the probability of staying on action ‘A1’ will be higher than action ‘A2’ in general. At the same time, because action ‘A1’ is better than the other action, most of the rewarded common transitions and unrewarded rare transitions result from taking action ‘A1’. Likewise, most of the unrewarded common transitions and rewarded rare transitions will be the result of taking action ‘A2’. This fact, and the fact that stay probability on action ‘A1’ is generally higher, will produce a reward-transition interaction, without having a goal-directed system, at least in the period that action ‘A1’ is better than the other action. This bias is proportional to how fast the bias in first stage values changes and cannot account for the current data. It should also be noted that, as the comparison between the flat and hierarchical model families was based on model fit, those results don't suffer from this problem.</p>
</sec><sec id="s3c">
<title>Inhibitory interactions between goal-directed and habitual control</title>
<p>Although, on the hierarchical goal-directed/habit sequence model advanced here, habits are integrated with the goal-directed process to reach the goals selected by this latter system, competition can also occur between these two systems when the further execution of an ongoing habit sequence is found to be inappropriate by the goal-directed system and it attempts to take back control. This type of competition resembles the situation in an inhibitory control task, such as the stop-signal task, in which subjects must respond quickly when a ‘go’ signal appears but must stop the action if a stop-signal appears <xref ref-type="bibr" rid="pcbi.1003364-Verbruggen1">[48]</xref>. In the context of our task, seeing a slot machine in the second-stage is the ‘go’ signal, which causes the execution of the next action in the sequence. The stop signal comes from the goal-directed system when the pending response is identified as inappropriate. Consistent with this conception in conditions in which sequence performance is inhibited, reaction times are slower. In the stop-signal task, subjects are typically able to inhibit their responses when the stop signal is temporally close to the ‘go’ signal. Although the stop signal task is more global in terms of response inhibition, whereas in the current task the inhibition is specific to one as opposed to an alternative action, this implies that the ability of the goal-directed system to override habits depends on how fast it calculates the correct action: the faster it calculates, the higher the chance of taking control back before action execution.</p>
</sec><sec id="s3d">
<title>Habit sequences vs. stimulus-response habits</title>
<p>It is also interesting to consider the relationship between habit sequences and stimulus-response (S-R) theories of habit learning. The S-R theory of habit learning maintains that habits are responses that are elicited by antecedent stimuli rather than their consequences <xref ref-type="bibr" rid="pcbi.1003364-Guthrie1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Hull1">[50]</xref>. Such S-R theories maintain that stimuli trigger their associated behavioral responses due to an association between the stimulus and the response. According to the habit sequence theory, however, the stimulus instead signals that the next action in the sequence should be executed; i.e., in the context of our task, seeing a slot machine signals that it is time for the next action to be executed. Although the next action to be executed is determined by the sequence, the response is still stimulus-bound to some extent and is elicited only when the next expected stimulus is encountered. Nevertheless, these two theories provide different predictions. For example, S-R theory predicts that, in the presence of the appropriate stimulus the response will be performed, irrespective of whether that stimulus was encountered as part of the habit sequence or not. In contrast, habit sequence theory predicts that the individual will respond to the stimulus only when the appropriate habit sequence has already been launched by the goal-directed system.</p>
</sec><sec id="s3e">
<title>Action sequence formation, error signals and dopamine</title>
<p>In the two-stage task that we used in this study, there are few possible action sequences, and so it is easy for the subject to enumerate all of them during decision making. However, in general, the number of action sequences grows exponentially with number of individual actions, and, as such, it will rapidly become impractical to consider all of them at the choice point. As a consequence, the decision-maker needs to discover ‘useful’ action sequences, and to limit consideration to those for action selection rather than all the possible action sequences. In the context of the hierarchical RL literature, this problem is known as ‘option discovery’ and various methods has been proposed to address it (see <xref ref-type="bibr" rid="pcbi.1003364-Botvinick1">[19]</xref> for a review). In particular, we have previously shown how action sequences can be formed using a reward prediction error signal <xref ref-type="bibr" rid="pcbi.1003364-Dezfouli1">[8]</xref>, which has the benefit of forging a bridge between habit sequence formation, and reward prediction error which has been shown to be coded by the phasic activity of dopamine neurons in midbrain <xref ref-type="bibr" rid="pcbi.1003364-Schultz1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Schultz2">[52]</xref>.</p>
<p>The flat architecture also utilizes reward prediction error, but for the learning of S-R associations instead of action sequences <xref ref-type="bibr" rid="pcbi.1003364-Daw1">[5]</xref>. Here one critical difference lies in the fact that the hierarchical architecture maintains that the reward prediction error is not computed at the second stage when actions are executed habitually in contrast to the flat architecture according to which reward prediction errors are computed in all conditions.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants and behavioral task</title>
<p>Fifteen English speaking subjects (seven females; eight males; mean age 23.8 years [SD 4.3]) completed a two-stage decision-making task. After a description of the study, written consent was obtained. This study was approved by the Sydney University Ethics Committee.</p>
<p>Each subject completed 270 trials, with a break after the first 120 trials (<xref ref-type="fig" rid="pcbi-1003364-g002">Figure 2</xref>). Each trial started with the presentation of a black square and subjects could choose between pressing either ‘Z’ (using left hand) or ‘/’ (using right hand). After pressing the key, a slot machine appeared on the screen, and the subject could make the next response, which would result in either a monetary reward or no reward. The outcome was shown for two seconds and after that an inter trial interval started and lasted for one second, after which the next trial began.</p>
<p>The probability of earning money at each choice was randomly set to either 0.2 or 0.7 at the beginning of the session, and in each trial, with the chance of 1/7, they were again randomly set to 0.2 or 0.7. This later step was to encourage searching for the best keys throughout the session.</p>
<p>Subjects were instructed that the chance of reaching each slot machine by pressing each key will not change throughout the task, but the goodness of the keys in terms of leading to rewards will change over time.</p>
<p>If a first stage action is the best action (the maximum probability of receiving reward on the keys of the slot machine that it commonly leads to is greater than the other action), and slot machines reset in the next trial, the probability that the action remains the best action is 3/16. Based on this, and given that probability of resetting is 1/7, the average number of trials for which a first stage action remains the best action is as follows:<disp-formula id="pcbi.1003364.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e029" xlink:type="simple"/><label>(1)</label></disp-formula>The fact that a first stage action remains the best for a few numbers of trials ensures that reward-transition interaction does not emerge as the result of developing bias toward the best action.</p>
</sec><sec id="s4b">
<title>Behavioral analysis</title>
<p>For all the analyses, we used <italic>R</italic> <xref ref-type="bibr" rid="pcbi.1003364-R1">[53]</xref>, and the <italic>R</italic> package <italic>lme4</italic> <xref ref-type="bibr" rid="pcbi.1003364-Bates1">[54]</xref>. </p>
<p>In the analysis presented in the section headed ‘Goal-directed and habitual performance on the two-stage task’, we used mixed-effects logistic regression in which whether the previous first stage action is repeated was a dependent variable, and the transition type (rare or common), and reward received in the previous trial were explanatory variables. We treated all the explanatory variables as random effects.</p>
<p>In the analysis in the section headed ‘The interaction of goal-directed actions and habit sequences in stage 2 performance’, staying or switching to the other second stage action is the dependent variable, and the reward received in the previous trial and staying on the first stage action were the explanatory variables. Only trials in which the second stage states were different from previous trials were included in this analysis. All the explanatory variables were used as random effects. In the second analysis of this section, staying on the same second stage action is dependent variable, and whether second stage state is the same, and whether previous trial was rewarded, are explanatory variables, and also random effects. Only trials in which first stage action is the same as the previous trail were included in this analysis. The third analysis is similar to the third one, except that trials in which first stage action is not the same as the previous trial are included in the analysis.</p>
<p>For analysis of the model behavior in the section headed ‘The interaction of goal-directed actions and habit sequences in stage 2 performance’, each model was simulated 3000 trials in the task with the best fitting parameters of each individual (see the section headed ‘Computational Modeling’ below for more information). Then we analyzed data using linear mixed-effects regression in which the probability of selecting the same second stage action by the model was taken as the dependent variable, and the reward received in the previous trial and staying on the first stage actions were explanatory variables. The intercept was treated as the random effect, and reported p-values are MCMC-estimated using <italic>R</italic> package <italic>LanguageR</italic> <xref ref-type="bibr" rid="pcbi.1003364-Baayen1">[55]</xref>.</p>
<p>In the analysis in the first part of the section headed ‘Reaction times during habit execution’, staying on the same second stage action was a dependent variable, and the reaction time was an explanatory and random effect. Only trials in which the previous trial was rewarded (first analysis) or not rewarded (second analysis), the first stage action was repeated, and the second stage state was not the same, were included in this analysis.</p>
<p>In the second analysis of this section, we applied a recursive partitioning method by taking (i) whether the previous trial is rewarded, (ii) whether the same first stage action is being taken, and (iii) reaction time as covariates, and staying on the same second stage action as response. We used <italic>R</italic> package <italic>‘party’</italic> <xref ref-type="bibr" rid="pcbi.1003364-Hothorn1">[56]</xref> for the analysis which employs conditional inference trees for recursive partitioning. In short, the partitioning method works as follows: at each stage of partitioning the algorithm performs a significance test on independence between any of covariates and the response using permutation tests. If the hypothesis is rejected (in the current analysis p-value less than 0.05), it selects the covariate which has strongest association with the response, and performs a split on that covariate.</p>
</sec><sec id="s4c">
<title>Computational modeling</title>
<sec id="s4c1">
<title>Simulation environment</title>
<p>We assumed that the environment has five states; the initial state denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e030" xlink:type="simple"/></inline-formula>, (the black screen in <xref ref-type="fig" rid="pcbi-1003364-g001">Figure 1</xref>), slot machine states denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e031" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e032" xlink:type="simple"/></inline-formula>, the reward state denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e033" xlink:type="simple"/></inline-formula> and no-reward state denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e034" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c2">
<title>Model-based, model-free RL hybrid</title>
<p>For modeling the flat interaction, a family of hybrid models similar to the previous works was used <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Glscher1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Otto1">[30]</xref>. A model-based RL <xref ref-type="bibr" rid="pcbi.1003364-Sutton2">[57]</xref> model was used for modeling goal-directed behavior; and a Q-learning model <xref ref-type="bibr" rid="pcbi.1003364-Watkins1">[58]</xref> was used to model the habitual behavior. We assumed that actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e035" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e036" xlink:type="simple"/></inline-formula> are available in states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e037" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e039" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Model-based RL</italic>- we denote the transition function with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e040" xlink:type="simple"/></inline-formula> which is the probability of reaching state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e041" xlink:type="simple"/></inline-formula> after executing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e042" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e043" xlink:type="simple"/></inline-formula>. We assume that the transition function at the first stage is fixed (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e044" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e045" xlink:type="simple"/></inline-formula>) and it will not change during learning. For other states, after executing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e046" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e047" xlink:type="simple"/></inline-formula> and reaching state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e048" xlink:type="simple"/></inline-formula>, the transition function updates as follows:<disp-formula id="pcbi.1003364.e049"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e049" xlink:type="simple"/><label>(2)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e050" xlink:type="simple"/></inline-formula> is the update rate of the state-action-state transitions.</p>
<p>We assumed that the reward at state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e051" xlink:type="simple"/></inline-formula> is one (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e052" xlink:type="simple"/></inline-formula>), and zero in all other states. Based on this, the goal-directed value of taking action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e053" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e054" xlink:type="simple"/></inline-formula> is as follows:<disp-formula id="pcbi.1003364.e055"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e055" xlink:type="simple"/><label>(3)</label></disp-formula>Where:<disp-formula id="pcbi.1003364.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e056" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p><italic>Model-free RL</italic>- After taking action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e057" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e058" xlink:type="simple"/></inline-formula>, and reaching state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e059" xlink:type="simple"/></inline-formula>, model-free values update as follows:<disp-formula id="pcbi.1003364.e060"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e060" xlink:type="simple"/><label>(5)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e061" xlink:type="simple"/></inline-formula> is the learning rate, which can be different in the first stage and second stage actions. For the first stage actions (actions executed in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e062" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e063" xlink:type="simple"/></inline-formula>, and for the second stage actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e064" xlink:type="simple"/></inline-formula>. Also<disp-formula id="pcbi.1003364.e065"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e065" xlink:type="simple"/><label>(6)</label></disp-formula>In the trials in which the best action is executed in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e066" xlink:type="simple"/></inline-formula> the habitual value of the action executed in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e067" xlink:type="simple"/></inline-formula> also updates according to the outcome. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e068" xlink:type="simple"/></inline-formula> was to be the action which was taken in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e069" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e070" xlink:type="simple"/></inline-formula> the action taken in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e071" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e072" xlink:type="simple"/></inline-formula> the state visited after executing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e073" xlink:type="simple"/></inline-formula>, values update as follows:<disp-formula id="pcbi.1003364.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e074" xlink:type="simple"/><label>(7)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e075" xlink:type="simple"/></inline-formula> is the reinforcement eligibility parameter, and determines how the first stage action values are affected by receiving the outcome after executing the second stage actions.</p>
<p>Final values are then computed by combining the values provided by the habitual and goal-directed processes:<disp-formula id="pcbi.1003364.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e076" xlink:type="simple"/><label>(8)</label></disp-formula>Were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e077" xlink:type="simple"/></inline-formula> determines the relative contribution of habitual and goal-directed values into the final values.</p>
<p>Finally, the probability of selecting action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e078" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e079" xlink:type="simple"/></inline-formula> will be determined according to the soft-max rule:<disp-formula id="pcbi.1003364.e080"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e080" xlink:type="simple"/><label>(9)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e081" xlink:type="simple"/></inline-formula> is the action preservation parameter and captures the general tendency of taking the same action as the previous trial <xref ref-type="bibr" rid="pcbi.1003364-Ito1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-Lau1">[27]</xref>. Assuming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e082" xlink:type="simple"/></inline-formula> and <italic>a</italic> being the action taken in the previous trial in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e083" xlink:type="simple"/></inline-formula> state, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e084" xlink:type="simple"/></inline-formula>, otherwise it will be zero. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e085" xlink:type="simple"/></inline-formula> parameter controls the rate of exploration, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e086" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e087" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e088" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e089" xlink:type="simple"/></inline-formula>.</p>
<p>In the most general form, all the free parameters are included in the model: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e090" xlink:type="simple"/></inline-formula> (we assumed that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e091" xlink:type="simple"/></inline-formula>). We generated eight simpler models by setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e092" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e093" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e094" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c3">
<title>Hierarchical model-based, sequence-based RL</title>
<p>Implementation of the hierarchical structure is similar to hierarchical RL <xref ref-type="bibr" rid="pcbi.1003364-Barto1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1003364-Sutton1">[46]</xref>, with action sequences (A1A1, A1A2, etc) as options <xref ref-type="bibr" rid="pcbi.1003364-Sutton1">[46]</xref>. We assumed in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e095" xlink:type="simple"/></inline-formula>, actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e096" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e097" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e098" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e099" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e100" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e101" xlink:type="simple"/></inline-formula> are available. In states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e102" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e103" xlink:type="simple"/></inline-formula>, actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e105" xlink:type="simple"/></inline-formula> are available. After reaching a terminal state (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e106" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e107" xlink:type="simple"/></inline-formula>), transition functions of both the action sequence, and the single action that led to that state update according to <xref ref-type="disp-formula" rid="pcbi.1003364.e049">equation (2)</xref>. In the case of single actions, the transition function will be updated by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e108" xlink:type="simple"/></inline-formula> update rate, and in the case of action sequences, the transition function will be updated by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e109" xlink:type="simple"/></inline-formula> update rate. Based on the learned transition function, value of action <italic>a</italic> in state <italic>s</italic> is calculated by the goal-directed system using <xref ref-type="disp-formula" rid="pcbi.1003364.e055">equation (3)</xref>.</p>
<p>The probability of selecting each action will be as follows:<disp-formula id="pcbi.1003364.e110"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e110" xlink:type="simple"/><label>(10)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e111" xlink:type="simple"/></inline-formula> determines the relative preference for single actions instead of executing action sequences. If action <italic>a</italic> is a single action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e112" xlink:type="simple"/></inline-formula>, and if action <italic>a</italic> is an action sequence, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e113" xlink:type="simple"/></inline-formula>. As before, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e114" xlink:type="simple"/></inline-formula> captures action perseveration. We assumed that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e115" xlink:type="simple"/></inline-formula> if action <italic>a</italic> is a single action, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e116" xlink:type="simple"/></inline-formula> if action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e117" xlink:type="simple"/></inline-formula> is an action sequence. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e118" xlink:type="simple"/></inline-formula> is calculated using <xref ref-type="disp-formula" rid="pcbi.1003364.e055">equation (3)</xref>.</p>
<p>For calculating the probability of selecting actions in the second stage, given the first choice of the subject, we need to know whether that action is a part of an action sequence selected earlier, or is it under goal-directed control. Assume we know action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e119" xlink:type="simple"/></inline-formula> has been executed in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e120" xlink:type="simple"/></inline-formula> by the subject, the probability of this action being due to performing the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e121" xlink:type="simple"/></inline-formula> action sequence is:<disp-formula id="pcbi.1003364.e122"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e122" xlink:type="simple"/><label>(11)</label></disp-formula>Similarly, the probability of observing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e123" xlink:type="simple"/></inline-formula> due to selecting the single action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e124" xlink:type="simple"/></inline-formula> is:<disp-formula id="pcbi.1003364.e125"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e125" xlink:type="simple"/><label>(12)</label></disp-formula>Based on this, the probability that the model assigns to action <italic>a</italic> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e126" xlink:type="simple"/></inline-formula>, given that action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e127" xlink:type="simple"/></inline-formula> is being observed in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e128" xlink:type="simple"/></inline-formula> is:<disp-formula id="pcbi.1003364.e129"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e129" xlink:type="simple"/><label>(13)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e130" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e131" xlink:type="simple"/></inline-formula> are calculated using <xref ref-type="disp-formula" rid="pcbi.1003364.e122">equations (11)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003364.e125">(12)</xref> respectively. In the most general form, all the free parameters are included in the model: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e132" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e133" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e134" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e135" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e136" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e137" xlink:type="simple"/></inline-formula>. We generated eight simpler models by setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e138" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e139" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e140" xlink:type="simple"/></inline-formula>.</p>
<p>In the analyses in the section headed ‘Reaction times during habit execution’, we assumed that reaction times in the second stage are inversely related to the probability of executing an action sequence in the first stage. As such, if subject has taken action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e141" xlink:type="simple"/></inline-formula> in the first stage, and action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e142" xlink:type="simple"/></inline-formula> in the second stage, then model prediction of the reaction time of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e143" xlink:type="simple"/></inline-formula> will be:<disp-formula id="pcbi.1003364.e144"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e144" xlink:type="simple"/><label>(14)</label></disp-formula>For the second analysis in the section headed ‘Behavioral Modeling: Bayesian model selection’, we aimed to remove the effect of action sequences in the second stage choices. We used eight models same as above, but the probability that the model assigns to action <italic>a</italic> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003364.e145" xlink:type="simple"/></inline-formula>, was defined as:<disp-formula id="pcbi.1003364.e146"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003364.e146" xlink:type="simple"/><label>(15)</label></disp-formula>Which indicates probability of taking each action in each slot machine is guided only by the rewards earned on that slot machine, and not by the action sequences in the first stage.</p>
</sec><sec id="s4c4">
<title>Model selection</title>
<p>Since the two families of models that we are comparing are not nested in each other, we can't use classical model selection. Instead, we use a Bayesian model selection for comparing these two families of models <xref ref-type="bibr" rid="pcbi.1003364-Penny1">[31]</xref>. We first calculated the model evidence for each model using the Laplace approximation <xref ref-type="bibr" rid="pcbi.1003364-Daw3">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003364-MacKay1">[60]</xref>, and then calculated the exceedance probability favoring each family, (taking model identity as a random effect) using the ‘spm_compare_families’ routine in the spm8 software. Within each family, exceedance probabilities were calculated using the ‘spm_BMS’ routine <xref ref-type="bibr" rid="pcbi.1003364-Stephan1">[32]</xref>.</p>
<p>The Laplace approximation requires a prior assumption of probability distributions over the free parameters of models. Similar to the previous study <xref ref-type="bibr" rid="pcbi.1003364-Daw2">[23]</xref>, for parameters between zero and one (learning rates, reinforcement eligibility, weight parameter), we assumed a Beta(1.1, 1.1) distribution; for exploration-exploitation parameters we assumed a Gamma(1.2, 5) distribution, and for perseveration parameters, a Normal(0, 1) distribution was assumed. The Laplace approximation includes finding the maximum a posteriori (MAP) parameter estimates. For this purpose, we used the IPOPT software package <xref ref-type="bibr" rid="pcbi.1003364-Wchter1">[61]</xref> for nonlinear optimization, and the DerApproximator package <xref ref-type="bibr" rid="pcbi.1003364-Kroshko1">[62]</xref> in order to estimate the Hessian at the MAP point.</p>
</sec></sec></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003364-Adams1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adams</surname><given-names>CD</given-names></name> (<year>1981</year>) <article-title>Variations in the sensitivity of instrumental responding to reinforcer devalaution</article-title>. <source>Q J Exp Psychol</source> <volume>34B</volume>: <fpage>77</fpage>–<lpage>98</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Balleine1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name>, <name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name> (<year>2010</year>) <article-title>Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action</article-title>. <source>Neuropsychopharmacol Off Publ Am Coll Neuropsychopharmacol</source> <volume>35</volume>: <fpage>48</fpage>–<lpage>69</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19776734" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/19776734</ext-link>. Accessed 4 September 2010.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Dickinson1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Squire</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Varga</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>JW</given-names></name> (<year>1998</year>) <article-title>Omission learning after instrumental pretraining</article-title>. <source>Q J Exp Psychol B Comp Physiol Psychol</source> <volume>51</volume> (<issue>B</issue>)  <fpage>271</fpage>–<lpage>286</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Dickinson2"><label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">Dickinson A (1994) Instrumental conditioning. In: Mackintosh NJ, editor. Animal cognition and learning. London: Academic Press. pp. 4–79.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Daw1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/16286932" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/16286932</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Keramati1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002055</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3102758&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3102758&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 22 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Doya1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>1999</year>) <article-title>What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?</article-title> <source>Neural Networks</source> <volume>12</volume>: <fpage>961</fpage>–<lpage>974</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12662639" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/12662639</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Dezfouli1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dezfouli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2012</year>) <article-title>Habits, action sequences and reinforcement learning</article-title>. <source>Eur J Neurosci</source> <volume>35</volume>: <fpage>1036</fpage>–<lpage>1051</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/22487034" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/22487034</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Graybiel1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graybiel</surname><given-names>AM</given-names></name> (<year>2008</year>) <article-title>Habits, rituals, and the evaluative brain</article-title>. <source>Annu Rev Neurosci</source> <volume>31</volume>: <fpage>359</fpage>–<lpage>387</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18558860" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/18558860</ext-link>. Accessed 5 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Book1"><label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">Book W. (1908) ThePsychologyof Skill. Missoula, MT: MontanaPress.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Lashley1"><label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">Lashley KS (1951) The problem of serial order in behavior. In: Jeffress LA, editor. Cerebral Mechanisms in Behavior. New York: Wiley. pp. 112–136.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Pew1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pew</surname><given-names>RW</given-names></name> (<year>1966</year>) <article-title>Acquisition of hierarchical control over the temporal organization of a skill</article-title>. <source>J Exp Psychol</source> <volume>71</volume>: <fpage>764</fpage>–<lpage>771</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/5939722" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/5939722</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Miller1"><label>13</label>
<mixed-citation publication-type="book" xlink:type="simple">Miller GA, Galanter E, Pribram KH (1960) Plans and the structure of behavior. New York: Holt, Rinehart &amp; Winston.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Verwey1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verwey</surname><given-names>WB</given-names></name> (<year>1996</year>) <article-title>Buffer loading and chunking in sequential keypressing</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>22</volume>: <fpage>544</fpage>–<lpage>562</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.22.3.544" xlink:type="simple">http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.22.3.544</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Newell1"><label>15</label>
<mixed-citation publication-type="book" xlink:type="simple">Newell A, Simon HA (1963) GPS, a program that simulates human thought. In: Feigenbaum EA, Feldman J, editors. Computers and thought. New York: McGraw-Hill. pp. 279–293.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Dickinson3"><label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">Dickinson A, Balleine BW (2002) The Role of Learning in the Operation of Motivational Systems. In: Gallistel CR, editor. Steven's handbook of experimental psychology: Learning, motivation and emotion. 3<sup>rd</sup> edition. New York: John Wiley &amp; Sons, Vol. 3. pp. 497–534.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Keele1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keele</surname><given-names>SW</given-names></name> (<year>1968</year>) <article-title>Movement control in skilled motor performance</article-title>. <source>Psychol Bull</source> <volume>70</volume>: <fpage>387</fpage>–<lpage>403</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/h0026739" xlink:type="simple">http://doi.apa.org/getdoi.cfm?doi=10.1037/h0026739</ext-link>. Accessed 4 July 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Ostlund1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ostlund</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Winterbauer</surname><given-names>NE</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2009</year>) <article-title>Evidence of action sequence chunking in goal-directed instrumental conditioning and its dependence on the dorsomedial prefrontal cortex</article-title>. <source>J Neurosci Off J Soc Neurosci</source> <volume>29</volume>: <fpage>8280</fpage>–<lpage>8287</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19553467" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/19553467</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Botvinick1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name> (<year>2009</year>) <article-title>Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective</article-title>. <source>Cognition</source> <volume>113</volume>: <fpage>262</fpage>–<lpage>280</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 23 August 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Fermin1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fermin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Yoshida</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ito</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Yoshimoto</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2010</year>) <article-title>Evidence for model-based action planning in a sequential finger movement task</article-title>. <source>J Mot Behav</source> <volume>42</volume>: <fpage>371</fpage>–<lpage>379</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/21184355" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/21184355</ext-link>. Accessed 1 October 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Rangel1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rangel</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Camerer</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name> (<year>2008</year>) <article-title>A framework for studying the neurobiology of value-based decision making</article-title>. <source>Nat Rev Neurosci</source> <volume>9</volume>: <fpage>545</fpage>–<lpage>556</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2357" xlink:type="simple">10.1038/nrn2357</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003364-Redish1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>A unified framework for addiction: vulnerabilities in the decision process</article-title>. <source>Behav Brain Sci</source> <volume>31</volume>: <fpage>415</fpage>–<lpage>37; discussion 437–87</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0140525X0800472X" xlink:type="simple">10.1017/S0140525X0800472X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003364-Daw2"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2011</year>) <article-title>Model-based influences on humans' choices and striatal prediction errors</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/S0896627311001255</ext-link>. Accessed 8 August 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Nissen1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nissen</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Bullemer</surname><given-names>P</given-names></name> (<year>1987</year>) <article-title>Attentional Requirements of Learning: Performance Measures Evidence from</article-title>. <source>Cogn Psychol</source> <volume>19</volume>: <fpage>1</fpage>–<lpage>32</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/0010028587900028" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/0010028587900028</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Ito1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>9861</fpage>–<lpage>9874</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19657038" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/19657038</ext-link>. Accessed 28 February 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Kim1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sul</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Huh</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Jung</surname><given-names>MW</given-names></name> (<year>2009</year>) <article-title>Role of striatum in updating values of chosen actions</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>14701</fpage>–<lpage>14712</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19940165" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/19940165</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Lau1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname><given-names>PW</given-names></name> (<year>2005</year>) <article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title>. <source>J Exp Anal Behav</source> <volume>84</volume>: <fpage>555</fpage>–<lpage>579</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1389781&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1389781&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Glscher1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gläscher</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name> (<year>2010</year>) <article-title>States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>585</fpage>–<lpage>595</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627310002874" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/S0896627310002874</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Wunderlich1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wunderlich</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Smittenaar</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2012</year>) <article-title>Dopamine enhances model-based over model-free choice behavior</article-title>. <source>Neuron</source> <volume>75</volume>: <fpage>418</fpage>–<lpage>424</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3417237&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3417237&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 26 February 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Otto1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Otto</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Markman</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2013</year>) <article-title>The Curse of Planning: Dissecting multiple reinforcement learning systems by taxing the central executive</article-title>. <source>Psychol Sci</source> <volume>24</volume> (<issue>5</issue>)  <fpage>751</fpage>–<lpage>61</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Penny1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Rosa</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Comparing families of dynamic causal models</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000709</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2837394&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2837394&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 26 February 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Stephan1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Bayesian model selection for group studies</article-title>. <source>Neuroimage</source> <volume>46</volume>: <fpage>1004</fpage>–<lpage>1017</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2703732&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2703732&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Botvinick2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Plaut</surname><given-names>DC</given-names></name> (<year>2004</year>) <article-title>Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action</article-title>. <source>Psychol Rev</source> <volume>111</volume>: <fpage>395</fpage>–<lpage>429</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.111.2.395" xlink:type="simple">10.1037/0033-295X.111.2.395</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003364-Estes1"><label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">Estes WK (1972) An associative basis for coding and organization in memory. In: Melton AW, Martin E, editors. Coding processes in human memory. Washington, DC: V.H. Winston &amp; Sons. pp. 161–190.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Schneider1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneider</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Logan</surname><given-names>GD</given-names></name> (<year>2006</year>) <article-title>Hierarchical control of cognitive processes: switching tasks in sequences</article-title>. <source>J Exp Psychol Gen</source> <volume>135</volume>: <fpage>623</fpage>–<lpage>640</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/17087577" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/17087577</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Cooper1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname><given-names>RP</given-names></name>, <name name-style="western"><surname>Shallice</surname><given-names>T</given-names></name> (<year>2006</year>) <article-title>Hierarchical schemas and goals in the control of sequential behavior</article-title>. <source>Psychol Rev</source> <volume>113</volume>: <fpage>887</fpage>–<lpage>916; discussion 917–931</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://discovery.ucl.ac.uk/5924/" xlink:type="simple">http://discovery.ucl.ac.uk/5924/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Cooper2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Shallice</surname><given-names>T</given-names></name> (<year>2000</year>) <article-title>Contention scheduling and the control of routine activities</article-title>. <source>Cogn Neuropsychol</source> <volume>17</volume>: <fpage>297</fpage>–<lpage>338</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://discovery.ucl.ac.uk/4753/" xlink:type="simple">http://discovery.ucl.ac.uk/4753/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-RibasFernandes1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ribas-Fernandes</surname><given-names>JJF</given-names></name>, <name name-style="western"><surname>Solway</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diuk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>McGuire</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>A neural signature of hierarchical reinforcement learning</article-title>. <source>Neuron</source> <volume>71</volume>: <fpage>370</fpage>–<lpage>379</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/21791294" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/21791294</ext-link>. Accessed 28 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Diuk1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diuk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Tsai</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wallis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Botvinick</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2013</year>) <article-title>Hierarchical Learning Induces Two Simultaneous, But Separable, Prediction Errors in Human Basal Ganglia</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>5797</fpage>–<lpage>5805</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/23536092" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/23536092</ext-link>. Accessed 27 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Frank1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Badre</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Mechanisms of Hierarchical Reinforcement Learning in Corticostriatal Circuits 1: Computational Analysis</article-title>. <source>Cereb cortex</source> <volume>22</volume> (<issue>3</issue>)  <fpage>509</fpage>–<lpage>26</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Badre1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Badre</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>MJ</given-names></name> (<year>2012</year>) <article-title>Mechanisms of Hierarchical Reinforcement Learning in Cortico-Striatal Circuits 2: Evidence from fMRI</article-title>. <source>Cereb cortex</source> <volume>22</volume> (<issue>3</issue>)  <fpage>527</fpage>–<lpage>36</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Reynolds1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>O'Reilly</surname><given-names>RC</given-names></name> (<year>2009</year>) <article-title>Developing PFC representations using reinforcement learning</article-title>. <source>Cognition</source> <volume>113</volume>: <fpage>281</fpage>–<lpage>292</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783795&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783795&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Holroyd1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname><given-names>CB</given-names></name>, <name name-style="western"><surname>Yeung</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Motivation of extended behaviors by anterior cingulate cortex</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>121</fpage>–<lpage>127</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S1364661311002592" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/S1364661311002592</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Barto1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Mahadevan</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Recent advances in hierarchical reinforcement learning</article-title>. <source>Discret Event Dyn Syst</source> <volume>13</volume>: <fpage>41</fpage>–<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Dietterich1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dietterich</surname><given-names>TG</given-names></name> (<year>2000</year>) <article-title>Hierarchical reinforcement learning with the MAXQ value function decomposition</article-title>. <source>J Artif Intell Res</source> <volume>13</volume>: <fpage>227</fpage>–<lpage>303</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=1622262.1622268" xlink:type="simple">http://dl.acm.org/citation.cfm?id=1622262.1622268</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Sutton1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Precup</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>S</given-names></name> (<year>1999</year>) <article-title>Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning</article-title>. <source>Artif Intell</source> <volume>112</volume>: <fpage>181</fpage>–<lpage>211</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.citeulike.org/user/tdahl/article/2491074" xlink:type="simple">http://www.citeulike.org/user/tdahl/article/2491074</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Balleine2"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name>, <name name-style="western"><surname>Garner</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gonzalez</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name> (<year>1995</year>) <article-title>Motivational control of heterogeneous instrumental chains</article-title>. <source>J Exp Psychol Anim Behav Process</source> <volume>21</volume>: <fpage>203</fpage>–<lpage>217</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0097-7403.21.3.203" xlink:type="simple">http://doi.apa.org/getdoi.cfm?doi=10.1037/0097-7403.21.3.203</ext-link>. Accessed 7 March 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Verbruggen1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verbruggen</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Logan</surname><given-names>GD</given-names></name> (<year>2008</year>) <article-title>Response inhibition in the stop-signal paradigm</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>: <fpage>418</fpage>–<lpage>424</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2709177&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2709177&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Guthrie1"><label>49</label>
<mixed-citation publication-type="book" xlink:type="simple">Guthrie ER (1935) The Psychology of Learning. New York: Harpers.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Hull1"><label>50</label>
<mixed-citation publication-type="book" xlink:type="simple">Hull CL (1943) Principles of Behavior. New York: Appleton.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Schultz1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name> (<year>1997</year>) <article-title>A neural substrate of prediction and reward</article-title>. <source>Science (80-)</source> <volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Schultz2"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name> (<year>2000</year>) <article-title>Neuronal coding of prediction errors</article-title>. <source>Annu Rev Neurosci</source> <volume>23</volume>: <fpage>473</fpage>–<lpage>500</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.23.1.473" xlink:type="simple">10.1146/annurev.neuro.23.1.473</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003364-R1"><label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">R Core Team (2012) R: A Language and Environment for Statistical Computing. Available: <ext-link ext-link-type="uri" xlink:href="http://www.r-project.org/" xlink:type="simple">http://www.r-project.org/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Bates1"><label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Bates D, Maechler M (2009) lme4: Linear mixed-effects models using S4 classes. R package version 0.999375-32. Available: <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/lme4/index.html" xlink:type="simple">http://cran.r-project.org/web/packages/lme4/index.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Baayen1"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Baayen RH (2011) languageR: Data sets and functions with “Analyzing Linguistic Data: A practical introduction to statistics”. Available: <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/package=languageR" xlink:type="simple">http://cran.r-project.org/package=languageR</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Hothorn1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hothorn</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hornik</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zeileis</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Unbiased Recursive Partitioning: A Conditional Inference Framework</article-title>. <source>J Comput Graph Stat</source> <volume>15</volume>: <fpage>651</fpage>–<lpage>674</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://pubs.amstat.org/doi/abs/10.1198/106186006X133933" xlink:type="simple">http://pubs.amstat.org/doi/abs/10.1198/106186006X133933</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Sutton2"><label>57</label>
<mixed-citation publication-type="book" xlink:type="simple">Sutton RS, Barto AG (1998) Reinforcement learning: an introduction. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Watkins1"><label>58</label>
<mixed-citation publication-type="book" xlink:type="simple">Watkins CJCH (1989) Learning from Delayed Rewards. Cambridge University.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Daw3"><label>59</label>
<mixed-citation publication-type="book" xlink:type="simple">Daw ND (2010) Trial-by-trial data analysis using computational models. In: Phelps E, Robbins TW, Delgado MR, editors. Affect, Learning and Decision Making, Attention and Performance XXIII. Oxford: Oxford University Press. Available: <ext-link ext-link-type="uri" xlink:href="http://www.citeulike.org/user/garrettmd/article/7748989" xlink:type="simple">http://www.citeulike.org/user/garrettmd/article/7748989</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-MacKay1"><label>60</label>
<mixed-citation publication-type="book" xlink:type="simple">MacKay DJC (2003) Information Theory, Inference and Learning Algorithms. Cambridge University Press. Available: <ext-link ext-link-type="uri" xlink:href="http://books.google.com/books?id=AKuMj4PN_EMC&amp;pgis=1" xlink:type="simple">http://books.google.com/books?id=AKuMj4PN_EMC&amp;pgis=1</ext-link>. Accessed 6 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Wchter1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wächter</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Biegler</surname><given-names>LT</given-names></name> (<year>2005</year>) <article-title>On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming</article-title>. <source>Math Program</source> <volume>106</volume>: <fpage>25</fpage>–<lpage>57</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.springerlink.com/index/10.1007/s10107-004-0559-y" xlink:type="simple">http://www.springerlink.com/index/10.1007/s10107-004-0559-y</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003364-Kroshko1"><label>62</label>
<mixed-citation publication-type="other" xlink:type="simple">Kroshko D (n.d.) OpenOpt: Free scientific-engineering software for mathematical modeling and optimization. Available: <ext-link ext-link-type="uri" xlink:href="http://www.openopt.org/" xlink:type="simple">http://www.openopt.org/</ext-link>.</mixed-citation>
</ref>
</ref-list></back>
</article>