<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006327</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00007</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Electromagnetic radiation</subject><subj-group><subject>Light</subject><subj-group><subject>Visible light</subject><subj-group><subject>Luminance</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Scalp</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Scalp</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Electrochemistry</subject><subj-group><subject>Electrode potentials</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Shared spatiotemporal category representations in biological and artificial deep neural networks</article-title>
<alt-title alt-title-type="running-head">Shared category representations in brain and DNN</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0597-4715</contrib-id>
<name name-style="western">
<surname>Greene</surname>
<given-names>Michelle R.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hansen</surname>
<given-names>Bruce C.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Neuroscience Program, Bates College, Lewiston, ME, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Psychological and Brain Sciences, Neuroscience Program, Colgate University, Hamilton, NY, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">mgreene2@bates.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>7</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>7</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>7</issue>
<elocation-id>e1006327</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>6</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Greene, Hansen</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006327"/>
<abstract>
<p>Visual scene category representations emerge very rapidly, yet the computational transformations that enable such invariant categorizations remain elusive. Deep convolutional neural networks (CNNs) perform visual categorization at near human-level accuracy using a feedforward architecture, providing neuroscientists with the opportunity to assess one successful series of representational transformations that enable categorization <italic>in silico</italic>. The goal of the current study is to assess the extent to which sequential scene category representations built by a CNN map onto those built in the human brain as assessed by high-density, time-resolved event-related potentials (ERPs). We found correspondence both over time and across the scalp: earlier (0–200 ms) ERP activity was best explained by early CNN layers at all electrodes. Although later activity at most electrode sites corresponded to earlier CNN layers, activity in right occipito-temporal electrodes was best explained by the later, fully-connected layers of the CNN around 225 ms post-stimulus, along with similar patterns in frontal electrodes. Taken together, these results suggest that the emergence of scene category representations develop through a dynamic interplay between early activity over occipital electrodes as well as later activity over temporal and frontal electrodes.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>We categorize visual scenes rapidly and effortlessly, but still have little insight into the neural processing stages that enable this feat. In a parallel development, deep convolutional neural networks (CNNs) have been developed that perform visual categorization with human-like accuracy. We hypothesized that the stages of processing in a CNN may parallel the stages of processing in the human brain. We found that this is indeed the case, with early brain signals best explained by early stages of the CNN and later brain signals explained by later CNN layers. We also found that category-specific information seems to first emerge in sensory cortex and is then rapidly fed up to frontal areas. The similarities between biological brains and artificial neural networks provide neuroscientists with the opportunity to better understand the process of categorization by studying the artificial systems.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000088</institution-id>
<institution>Directorate for Social, Behavioral and Economic Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>1736274</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Hansen</surname>
<given-names>Bruce C.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded by National Science Foundation (1736274) to MRG and BCH (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov" xlink:type="simple">https://www.nsf.gov</ext-link>), and James S McDonnell Foundation (220020439) to BCH (<ext-link ext-link-type="uri" xlink:href="https://www.jsmf.org" xlink:type="simple">https://www.jsmf.org</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="17"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-08-03</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files may be found at the Open Science Framework: <ext-link ext-link-type="uri" xlink:href="http://osf.io/rf8sz" xlink:type="simple">http://osf.io/rf8sz</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Categorization, the act of grouping like with like, is a hallmark of human intelligence. Scene categorization in particular is incredibly rapid [<xref ref-type="bibr" rid="pcbi.1006327.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref002">2</xref>] and may even be automatic [<xref ref-type="bibr" rid="pcbi.1006327.ref003">3</xref>]. Despite the importance of this problem, little is known about the temporal dynamics of neural activity that give rise to categorization. A common conceptual framework in the visual neuroscience community considers representations in each visual area as a geometric space, with individual images represented as points within this space [<xref ref-type="bibr" rid="pcbi.1006327.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref005">5</xref>]. According to this account, early visual processing can be described as a tangled geometric surface in which individual categories cannot be easily separated, and that over the course of processing, the ventral visual stream disentangles these manifolds, allowing for categories to be distinguished [<xref ref-type="bibr" rid="pcbi.1006327.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref007">7</xref>]. Although this view has provided a useful descriptive framework, we still know little about how this disentangling occurs because it has been difficult to examine the representations at each stage of processing.</p>
<p>In a parallel development, work in computer vision has resulted in the creation of deep convolutional neural networks (CNNs) whose categorization abilities rival those of human observers [<xref ref-type="bibr" rid="pcbi.1006327.ref008">8</xref>]. Although CNNs do not explicitly seek to model the human visual system, their architectures are inspired by the structure of the human visual system [<xref ref-type="bibr" rid="pcbi.1006327.ref009">9</xref>]: like the human visual system, they are arranged hierarchically in discrete representational layers, and they apply both linear and nonlinear operations across the whole of visual space. As CNNs are explicitly trained for the purpose of visual categorization, and as they achieve near human-level performance at this task, they provide neuroscientists with an unprecedented opportunity to interrogate the types of intermediate level representations that are built en route to categorization. Indeed, there is a growing literature detailing the similarities between aspects of CNNs and activity in biological brains [<xref ref-type="bibr" rid="pcbi.1006327.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1006327.ref017">17</xref>].</p>
<p>Of particular interest to the current study is the correspondence between the stages of processing within a CNN and the temporal order of processing in the human brain, as assessed with M/EEG. Studies have demonstrated that the brain activity elicited by individual images can be predicted by the CNN [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref018">18</xref>] and that upper layers of the CNN can predict semantically-relevant spatial properties such as overall scene volume [<xref ref-type="bibr" rid="pcbi.1006327.ref019">19</xref>]. However, these studies pool across all sensors at a given time point, discarding potentially informative scalp patterns. While this capitalizes on the fine temporal scale of M/EEG, a complete understanding of the neural dynamics of scene understanding requires the characterization of information flow across the cortex. Additionally, key questions remain open, including understanding the development of scene category membership and how intermediate stages of visual representation allow for these complex abstractions to take place.</p>
<p>Therefore, the goal of the current study is to assess the extent to which sequential representations in each layer of a pre-trained deep convolutional neural network predict the sequential representations built by the human brain using high-density time-resolved event-related potentials (ERPs). Previewing our results, we show a spatiotemporal correspondence between sequential CNN layers and the order of processing in the human visual system. Specifically, earlier layers of the CNN correspond better to early time points in human visual processing and to electrodes over occipital and left temporal scalp regions. Later layers of the CNN correspond to later information and best match electrodes in the frontal half of the scalp, as well as over the right occipitotemporal cortex. The correspondence between computer models and human vision provides neuroscientists with the unique opportunity to probe intermediate-level representations <italic>in silico</italic>, allowing for a more complete understanding of the neural computations generating visual categorization.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We recorded high-density 256-channel EEG while 13 human participants viewed 2,250 photographs from 30 scene categories for 750 ms each while engaged in a three-alternative forced choice (3AFC) task. An additional 15 participants viewed the same images with the same procedure for 500 ms to serve as a full internal replication. Our approach is graphically illustrated in <xref ref-type="fig" rid="pcbi.1006327.g001">Fig 1</xref>. To compare scene category representations in a pre-trained (Places-205 with 8-layer “AlexNet” architecture [<xref ref-type="bibr" rid="pcbi.1006327.ref020">20</xref>]) deep convolutional neural network (CNN) to those of human observers, we used the representational similarity analysis (RSA) framework [<xref ref-type="bibr" rid="pcbi.1006327.ref005">5</xref>]. This allowed us to directly compare models and neuroelectric signals by abstracting both into a similarity space.</p>
<fig id="pcbi.1006327.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The image set consisted of 75 images from each of 30 scene categories.</title>
<p>Example images are shown on the left. Activations for each of the eight layers of a pre-trained deep convolutional neural network (CNN) were extracted for each of the 2250 images. We averaged across exemplars to create 30x30 representational dissimilarity matrices (RDMs) for each layer of the CNN. For each participant, and for each of the 256 electrodes, we ran a 40 ms sliding window from the 100 ms before stimulus presentation, and running through the 750 ms where image was on screen. At each time point, a 30x30 RDM was created from the average voltage values at the given electrode and compared to each layer of the CNN using correlation. Note: although full matrices are shown, only the lower triangle was used in analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g001" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>CNN decoding results</title>
<p>In order to understand the potential contributions of each CNN layer to category-specific neural responses, we assessed the extent to which each CNN layer contained decodable category information. All layers contained above-chance information for decoding the 30 scene categories (see <xref ref-type="fig" rid="pcbi.1006327.g002">Fig 2</xref>). The classification accuracy ranged from 40.8% in the first layer (Conv1) to 90.3% correct in the first fully-connected layer (FC6). Somewhat unexpectedly, this is higher accuracy than we observed in the top layer (FC8: 84.6%). It is also noteworthy that this level of classification is higher than the 69–70% top-1 classification accuracy reported in [<xref ref-type="bibr" rid="pcbi.1006327.ref020">20</xref>]. This is likely due to the fact that some of the images in our dataset were from the training set for this CNN. In sum, we can expect to see category-specific information in each of the eight layers of the CNN, with maximum category information coming from the fully connected layers, peaking in the sixth layer.</p>
<fig id="pcbi.1006327.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Category decoding accuracy for each layer of the CNN.</title>
<p>Error bars reflect 95% confidence intervals. Horizontal line indicates chance level (1/30).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Time-resolved encoding results</title>
<p>To get an overall picture of human/CNN correspondence, we examined the extent to which the set of all CNN layers have explanatory power for visual ERPs. Our data-driven clustering method identified five groups of electrodes. We averaged ERPs within each cluster and then examined the variance explained by all layers of the CNN within that cluster’s representational dissimilarity matrices (RDMs) over time. For each cluster, we examined the onset of significant explained variance, the maximum explained variance, and the latency of maximum explained variance. We observed no statistically significant differences between the clusters for onset (F(4,60)&lt;1), maximum explained variance (F(4,60) = 1.11, p = 0.36), nor latency at maximum (F(4,60)&lt;1). Thus, we show the average of all 256 electrodes in <xref ref-type="fig" rid="pcbi.1006327.g003">Fig 3</xref>. We found that the CNN could predict neuroelectric activity starting at 54 ms after scene presentation (55 ms for internal replication set), and it achieved maximal predictive power 93 ms after stimulus onset (75 ms for internal replication set).</p>
<fig id="pcbi.1006327.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Variance explained over time for all eight CNN layers together for the average of all 256 electrodes.</title>
<p>Black line indicates mean with shaded gray region reflecting 95% confidence interval. Top line indicates statistical significance.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g003" xlink:type="simple"/>
</fig>
<p>To assess how much of the explainable ERP variance was captured by the CNN, we estimated the noise ceiling using a method suggested by [<xref ref-type="bibr" rid="pcbi.1006327.ref021">21</xref>]. We found that the maximum explainable variance to be 48–54% (internal replication set: 45%-48%). Therefore, the average maximum <italic>r</italic><sup>2</sup> of 0.10 across all electrodes does not account for all variability in this dataset. As CNNs are exclusively feedforward models, the remaining variability to be explained may reflect the role of feedback in human scene categorization [<xref ref-type="bibr" rid="pcbi.1006327.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref023">23</xref>].</p>
<p>In order to gain additional insight into the relative successes and failures of the CNN as a model for visual scene categorization, we examined the regression residuals between 50 and 250 ms after stimulus onset. We averaged the residuals over participant, resulting in a 30x30 matrix. We then averaged over superordinate category (indoor, urban outdoor, and natural outdoor) in order to visualize broad patterns of CNN/neuroelectric difference. As seen in <xref ref-type="fig" rid="pcbi.1006327.g004">Fig 4</xref>, we observed negative residuals among and between the two outdoor superordinate categories. This pattern indicates that the CNN predicted <italic>more</italic> similarity between these categories than what was observed neurally. In other words, that the human category representations of outdoor categories are more distinct compared to the predictions made by the CNN. By contrast, the residuals between indoor and urban categories were positive, suggesting that the CNN had a finer-grained representation of these category differences compared to the human brain. The residuals for the replication set demonstrated the same broad pattern (see Supporting Material).</p>
<fig id="pcbi.1006327.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Residuals as a function of superordinate-level scene category for all eight layers of the CNN between 50 and 250 ms post-stimulus onset.</title>
<p>Negative values indicate over-prediction of neural similarity, while positive values indicate under-prediction of neural similarity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g004" xlink:type="simple"/>
</fig>
<p>In order to examine the correspondence between the processing stages of the CNN and the representational stages in human visual processing, we next examined the variance explained by each individual CNN layer. Averaged across all 256 electrodes, we observed a correspondence between the onset of explained variability and CNN layer, with earlier layers explaining ERP variability earlier than later layers (r = 0.38, p&lt;0.001 for the main set and r = 0.40, p&lt;0.001 for the replication set, see <xref ref-type="fig" rid="pcbi.1006327.g005">Fig 5</xref>). Additionally, we observed a negative relationship between CNN layer and explained variability, with earlier layers explaining more ERP variability than later layers. This effect was pronounced in the first 100 ms post-stimulus (r = -0.51, p&lt;0.0001 for the main set and r = -0.49, p&lt;0.0001 for the replication set), and was also observed between 101 and 200 ms (r = -0.26, p&lt;0.005 for the main set and r = -0.23, p&lt;0.05 for the replication set). A one-way ANOVA found a significant difference in the onset of significant explained variability as a function of CNN layer (F(7,96) = 5.46, p&lt;0.0001) for the main dataset and F(7,112) = .4.81, p&lt;0.001 for the replication set) Additionally, a one-way ANOVA revealed significant differences in the maximum explained variability across CNN layers: F(7,96) = 15.7, p&lt;0.0001 for the main dataset and F(7,112) = 10.1, p&lt;0.0001 for the replication set).</p>
<fig id="pcbi.1006327.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Main plot: Variance explained in each of the eight layers of the CNN taken alone.</title>
<p>Each waveform represents the average of all 256 electrodes. Lowest layers are in darker colors, and horizontal lines represent statistical significance. Insets, clockwise from top: (1) Onset of statistically significant encoding as a function of CNN layer; (2) maximum variance explained between 101 and 200 ms as a function of CNN layer; (3) maximum variance explained between 0 and 100 ms as a function of CNN layer; (4) maximum variance explained between 101 and 200 ms as a function of CNN layer. Taken together, we can see that early CNN layers explain more variance in ERP patterns, and do so earlier than later CNN layers. All error bars reflect +/- 1 SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g005" xlink:type="simple"/>
</fig>
<p>A chief advantage of conducting the encoding analyses at each electrode, rather than collapsing across all sensors as has been done in previous work [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref018">18</xref>], is that we are able to examine the spatiotemporal patterns of explained variability rather than just temporal. Using data-driven electrode clustering (see <xref ref-type="sec" rid="sec005">Methods</xref>), we identified five groups of spatially contiguous electrodes with differing voltage patterns. The clustering took microvolt patterns in the topographic maps at each time point as input and tested for spatially continuous groups of electrodes that had similar microvolt patterns. We tested these against a chance level defined by a baseline topographic map. In each group, we examined the maximum explained variability for each layer of the CNN. As shown in <xref ref-type="fig" rid="pcbi.1006327.g006">Fig 6</xref>, we observed a striking dissociation. Central occipital electrodes were better explained by the earlier layers of the CNN at all time points (Spearman’s rank-order correlation between layer and maximum explained variance: (rho = -0.64, t(12) = -13.5, p&lt;0.0001 for main set; rho = -0.68, t(14) = -13.3, p&lt;0.0001 for replication set). A similar trend was seen in left occipitotemporal electrodes for time points before 200 ms in the main dataset, with a trend in the replication dataset (80–110 ms: rho = -0.53, t(12) = -6.31, p&lt;0.0001 for main set; rho = -0.78, t(14) = -10.3, p&lt;0.0001 for replication set; 120–200 ms: rho = -0.54, t(12) = -6.4, p&lt;0.0001 for main dataset, rho = -0.38, t(14) = 1.2, p = 0.13). By contrast, the right occipitotemporal cluster was best explained by early layers in early time bins (80–110 ms: rho = -0.57, t(12) = -8.37, p&lt;0.0001 for the main dataset and rho = -0.88, t(14) = 10.5, p&lt;0.0001 for the replication dataset), then by mid-to-late-level layers between 120–200 ms (peak in layer 4 for main set, layer 5 for replication set; rho = 0.44, t(12) = 1.29, p = 0.11 for main; rho = -0.08, t(14)&lt;1 for replication) and between 200–250 ms post stimulus onset (maximum in layer 6 for main set, maximum in layer 3 for replication set; rho = 0.17, t(12)&lt;1). As evident in <xref ref-type="fig" rid="pcbi.1006327.g006">Fig 6</xref>, the later time bins did not have a significant rank-order correlation because of non-monotonic patterns between CNN layer and explained variability. Post-hoc t-tests (Benjamini-Hochberg corrected for multiple comparisons) revealed that for 120–200 ms, the numerical peak in layer 4 was statistically significant from both earlier layer 1 (t(12) = 4.78, p&lt;0.001) and later layer 7 (t(12) = 3.66, p&lt;0.001) and layer 8 (t(12) = 4.08, p&lt;0.001). For 200–300 ms, the numerical peak in layer 6 was significantly different from layer 1 (t(12) = 5.17, p&lt;0.0001) and layer 8 (t(12) = 3.3, p&lt;0.005). Statistics for the replication set can be found in the Supporting Materials.</p>
<fig id="pcbi.1006327.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006327.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Encoding time course for each of the five identified clusters.</title>
<p>For each plot, main graph shows variability explained over time. Bar charts show maximum explained variability for each of eight CNN layers in the post-stimulus time window indicated above.–A- Frontal cluster.–B- Central cluster.–C- Left occipitotemporal cluster.–D- Central occipital cluster.–E- Right occipitotemporal cluster.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.g006" xlink:type="simple"/>
</fig>
<p>Similarly, while the frontal cluster best reflected lower layers of the CNN early 120–200 ms time bin (rho = -0.62, t(12) = -4.98, p&lt;0.0005; see Supporting Materials for replication set), it best reflected information from the later layers, particularly the sixth CNN layer (FC6) in the 200–300 ms time bin (rho: 0.48, t(12)&lt;1.) Post-hoc t-tests (corrected) revealed significantly more variance explained by layer 3 compared with layer 1 (t(12) = 2.29, p&lt;0.05), and more in layer 6 compared with layer 7 (t(12) = 1.92, p&lt;0.05) or layer 8 (t(12) = 1.87, p&lt;0.05). Similarly, the replication dataset also had a local maximum of explained variance at the sixth CNN layer (see Supporting Materials for more information). Interestingly, the sixth layer had the maximum decodable category information (see <xref ref-type="fig" rid="pcbi.1006327.g002">Fig 2</xref>), suggesting that these signals reflect category-specific information. When comparing the latencies of maximum explained variability in these two clusters, we observed that the right occipitotemporal cluster peaked about 10 ms earlier than the frontal cluster (227 versus 236 ms). However, this difference was not statistically reliable (t(12)&lt;1).</p>
</sec>
</sec>
<sec id="sec005" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec006">
<title>Ethics statement</title>
<p>Human subjects research was approved by the Colgate University Internal Review Board (IRB).</p>
</sec>
<sec id="sec007">
<title>Participants</title>
<p>Fourteen observers (6 female, 13 right-handed) participated in the study. One of the participant’s data contained fewer than half valid trials following artifact rejection and was removed from all analyses. An additional 15 participants (9 female, 12 right-handed) were recruited as part of an internal replication study (specific results from those participants are reported as Supporting Material). The age of all participants ranged from 18 to 22 (mean age = 19.1 for main experiment, and 19.4 for replication study). All participants had normal (or corrected to normal) vision as determined by standard ETCRS acuity charts. All participants gave Institutional Review Board-approved written informed consent before participating, and were compensated for their time.</p>
</sec>
<sec id="sec008">
<title>Stimuli</title>
<p>The stimuli consisted of 2250 color images of real-world photographs from 30 different scene categories (75 exemplars in each category) taken from the SUN database [<xref ref-type="bibr" rid="pcbi.1006327.ref024">24</xref>]. Categories were chosen to include 10 indoor categories, 10 urban categories, and 10 natural landscape categories, and were selected from the larger set of 908 categories from the SUN database on the basis of making maximally different RDMs when examining three different types of visual information (layer 7 features from a CNN, a bag-of-words object model, and a model of a scene’s functions, see [<xref ref-type="bibr" rid="pcbi.1006327.ref025">25</xref>] for more details on category selection). When possible, images were taken from the SUN database. In cases where this database did not have 75 images, we sampled from the internet (copyright-free images). Six of the 30 categories (bamboo forest, bar, butte, skyscraper, stadium, and volcano) were represented in the Places-205 database that comprised the training set for the CNN. Although the SUN and Places databases were designed to be complementary with few overlapping images [<xref ref-type="bibr" rid="pcbi.1006327.ref020">20</xref>], it is possible that some images in our set were included in the training set for the Places CNN. Care was taken to omit images with salient faces in them. All images had a resolution of 512 x 512 pixels (which subtended 20.8° of visual angle) and were processed to possess the same root-mean-square (RMS) contrast (luminance and color) as well as mean luminance. All images were fit with a circular linear edge-ramped window to obscure the square frame of the images, thereby uniformly distributing contrast changes around the circular edge of the stimulus [<xref ref-type="bibr" rid="pcbi.1006327.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref027">27</xref>].</p>
</sec>
<sec id="sec009">
<title>Apparatus</title>
<p>All stimuli were presented on a 23.6” VIEWPixx/EEG scanning LED-backlight LCD monitor with 1ms black-to-white pixel response time. Maximum luminance output of the display was 100 cd/m<sup>2</sup>, with a frame rate of 120 Hz and resolution of 1920 x 1080 pixels. Single pixels subtended 0.0406° of visual angle (i.e. 2.43 arc min.) as viewed from 32 cm. Head position was maintained with an Applied Science Laboratories (ASL) chin rest.</p>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Experimental procedure</title>
<p>Participants engaged in a 3 alternative forced-choice (3AFC) categorization task with each of the 2250 images. As it was not feasible for participants to view all images in one sitting, all images were randomly split into two sets, keeping roughly equal numbers of images within each category. Each image set was presented within a different ~50-minute recording session, run on separate days. The image set was counterbalanced across participants, and image order within each set was randomized. Participants viewed the windowed scenes against a mean luminance background under darkened conditions (i.e. the only light source in the testing chamber was the monitor). All trials began with a 500 ms fixation followed by a variable duration (500–750 ms) blank mean luminance screen to enable any fixation-driven activity to dissipate. Next, a scene image was presented for 750 ms followed by a variable 100–250 ms blank mean luminance screen, followed by a response screen consisting of the image’s category name and the names of two distractor categories presented laterally in random order (distractor category labels were randomly sampled from the set of 29 and therefore varied on a trial-by-trial basis). Observers selected their choice by using a mouse to click on the correct category name. Performance feedback was not given.</p>
</sec>
<sec id="sec011">
<title>EEG recording and processing</title>
<p>Continuous EEGs were recorded in a Faraday chamber using EGI’s Geodesic EEG acquisition system (GES 400) with Geodesic Hydrocel sensor nets consisting of a dense array of 256 channels (electrolytic sponges). The on-line reference was at the vertex (Cz), and the impedances were maintained below 50 kΩ (EGI amplifiers are high-impedance amplifiers–this value is optimized for this system). All EEG signals were amplified and sampled at 1000 Hz. The digitized EEG waveforms were band-pass filtered offline from 0.1 Hz to 45 Hz to remove the DC offset and eliminate 60 Hz line noise.</p>
<p>All continuous EEGs were divided into 850 ms epochs (100 ms before stimulus onset and the 750 ms of stimulus-driven data). Trials that contained eye movements or eye blinks during data epochs were excluded from analysis. Further, all epochs were subjected to algorithmic artifact rejection of voltage exceeding ± 100 μV. These trial rejection routines resulted in no more than 10% of the trials being rejected on a participant-by-participant basis. Each epoch was then re-referenced offline to the net average, and baseline corrected to the last 100 ms of the luminance blank interval that preceded the image. Grand average event-related potentials (ERPs) were assembled by averaging all re-referenced and baseline corrected epochs across participants. Topographic plots were generated for all experimental conditions using EEGLAB [<xref ref-type="bibr" rid="pcbi.1006327.ref028">28</xref>] version 13.4.4b in MATLAB (ver. R2016a, The MathWorks, MA).</p>
<p>For all analyses, we improved the signal-to-noise ratio of the single trial data by using a bootstrapping approach to build sub-averages across trials for each trial (e.g., [<xref ref-type="bibr" rid="pcbi.1006327.ref019">19</xref>]). Specifically, for each trial within a given scene category, we randomly selected 20% of the trials within that category and averaged those to yield a sub-averaged ERP for that trial. This was repeated until all valid trials within each category were built. This process was repeated separately for each participant. This approach is desirable as we are primarily interested in category-level neuroelectric signals that are time-locked to the stimulus.</p>
</sec>
<sec id="sec012">
<title>Deep convolutional neural network (CNN)</title>
<p>In order to assess the representations available in a deep convolutional neural network at each processing stage, we extracted the activations in each of eight layers in a pre-trained network. Specifically, we used a CNN based on the AlexNet architecture [<xref ref-type="bibr" rid="pcbi.1006327.ref029">29</xref>] that was pre-trained on the Places database [<xref ref-type="bibr" rid="pcbi.1006327.ref020">20</xref>] and implemented in Caffe [<xref ref-type="bibr" rid="pcbi.1006327.ref030">30</xref>]. The first five layers of this neural network are convolutional, and the last three are fully connected. The convolutional layers have three operations: convolution, pooling, and a rectified linear (ReLu) nonlinearity. For these layers, we extracted features after the convolution step. This CNN was chosen because it is optimized for 205-category scene classification, and because the eight-layer architecture, loosely inspired by biological principles, is most frequently used when comparing CNNs and brain activity [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref016">16</xref>]. For each layer, we averaged across images within a category, creating 30-category by N-feature matrices.</p>
<p>In order to assess the amount of category-related information available in each layer of the CNN, we performed a decoding procedure on the feature vectors from each CNN layer using a linear multi-class support vector machine (SVM) implemented as LIBSVM in Matlab [<xref ref-type="bibr" rid="pcbi.1006327.ref031">31</xref>]. Decoding accuracies for each layer were calculated using 5-fold cross validation.</p>
<p>For all analyses, statistical testing was done via permutation testing. Specifically, we fully exchanged row and column labels for the RDMs (1000 permutation samples per participant) to create an empirical chance distribution. To correct for multiple comparisons, we used cluster extent with a threshold of p&lt;0.05, Bonferroni-corrected for multiple comparisons (similar to [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>]).</p>
</sec>
<sec id="sec013">
<title>Time-resolved encoding analysis</title>
<p>A forward pass of activations through the CNN was collected for each image and each layer, and reshaped into a feature vector. Feature vectors were averaged across category to create eight 30-category by N-feature matrices. From these feature matrices, we created 30-category by 30-category correlation matrices. Representational dissimilarity matrices (RDMs) were created by transforming the correlation matrices into distance matrices using the metric of 1-Spearman rho [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref014">14</xref>]. Each RDM is a symmetrical matrix with an undefined diagonal. Therefore, in all subsequent analyses, we will use only the lower triangle of each RDM to represent patterns of category similarity.</p>
<p>To create neural RDMs, for each participant and for each electrode, we extracted ERP signals within a 40 ms sliding window beginning 100 ms before image presentation, and extending to the entire 750 ms image duration. For each 40 ms window, we created a 30 x 30 correlation matrix from the voltage values at that electrode, averaged over category. A 30 x 30 RDM using the same 1-minus-correlation distance metric described above. The window size was truncated at the end of each trial as to not extend beyond image presentation. Thus, RDMs were created from each of 256 electrodes for each of 850 ms time points. The upper and lower bounds of the noise ceiling for the data were computed as recommended in [<xref ref-type="bibr" rid="pcbi.1006327.ref021">21</xref>]. It is worth noting that while previous MEG RDM results have performed time-resolved analyses on a time point by time point manner taking the value at each sensor at each time point as a feature (e.g. [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>]), our approach allows for the understanding of feature correspondence at each electrode, also enabling spatiotemporal analysis.</p>
<p>We computed a noise ceiling for the results following the method detailed in [<xref ref-type="bibr" rid="pcbi.1006327.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref021">21</xref>]. Briefly, the noise ceiling contains both lower- and upper-bound estimates of the group-average correlation with an RDM that would be predicted by an unknown true model. The upper bound consisted of the average correlation of each single participant to the group mean. As this includes all participants, it is overfit and therefore an overestimation of the true model’s fit. By contrast, the lower bound used a leave-one-participant out approach, computing each single-participant RDM’s correlation with the average RDM of the other participants. This avoids overfitting, but underestimates the true model’s average correlation due to the limited data.</p>
</sec>
<sec id="sec014">
<title>Electrode clustering</title>
<p>In order to examine spatial relations in encoding patterns across electrodes, we adopted a data-driven approach based on random field theory. We modified an algorithm initially created by Chauvin and colleagues used to assess statistical significance of pixel clusters in classification images [<xref ref-type="bibr" rid="pcbi.1006327.ref032">32</xref>]. This allowed us to identify spatially contiguous electrode clusters based on voltage differences, while remaining agnostic to any encoding differences with the CNN. Specifically, we submitted participant-averaged and z-transformed voltage difference topographic maps to the algorithm time point by time point. The spatial clustering algorithm then grouped electrodes that contained normalized voltage differences that significantly deviated from baseline noise (p &lt; .001). As this spatial grouping procedure is based on assessing the statistically significant peaks and troughs of the voltage patterns at a given point of time, its advantage is that we did not need to set the number of clusters in advance. We selected clusters that persisted for more than 20 ms, resulting in five clusters: an early central occipital cluster (100–125 ms), an early central cluster (80–105 ms), two bilateral occipitotemporal clusters (70–500 ms), and a large frontal cluster (135–500 ms). We assessed the relative encoding strength of each of the eight CNN layers within each of these five clusters in all analyses.</p>
</sec>
</sec>
<sec id="sec015" sec-type="conclusions">
<title>Discussion</title>
<p>In this work, we demonstrated that there is a substantial resemblance between the sequential scene category representations from each layer of a pre-trained deep convolutional neural network (CNN), and those in human ERP activity while observers are engaged in categorization. Early layers of the CNN best predicted early ERP activity, while later layers best predicted later activity. Furthermore, the total variability captured by the CNN was slightly less than half of the variability given by the noise ceiling of the data.</p>
<p>Furthermore, we observed a spatial correspondence between CNN layers and ERP variability. While electrodes over central- and left- occipitotemporal cortex were robustly predicted early by early CNN layers, electrodes over right occipitotemporal cortex had sequential representations that resembled the sequential representations of the CNN. Specifically, activity in these electrodes was best predicted by the second and third CNN layers before 100 ms post-stimulus, by the fourth layer between 120–200 ms, and by the sixth layer after 200 ms. A similar striking dissociation was observed over the frontal electrode cluster: early CNN layers best predicted early activity, but the more conceptual, fully-connected layers captured activity at the frontal electrodes about 100 ms later. Taken together, these results suggest a deeper homology between the human visual system and the deep neural network than the few biological principles that inspired the architecture [<xref ref-type="bibr" rid="pcbi.1006327.ref029">29</xref>]. This homology provides the unique intellectual opportunity to probe the mid- and late- stages of visual representation that have thus far been difficult to ascertain.</p>
<p>Because the CNN is an exclusively feedforward model, comparing the representations in the CNN and the human brain allows us to speculate on the extent to which human category representations are processed in a feedforward manner (such as [<xref ref-type="bibr" rid="pcbi.1006327.ref033">33</xref>]). We observed that earlier layers of the CNN explained more ERP variance compared with later layers of the CNN. This may be evidence that earlier visual processing is predominantly feedforward, while later visual processing requires feedback, consistent with other relatively early accounts of top-down feedback [<xref ref-type="bibr" rid="pcbi.1006327.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006327.ref036">36</xref>]. However, we also observed that the neural variability explained by the CNN only reached about half of the maximum possible explained variability given by the noise ceiling. This may suggest that feedback or recurrent processing plays a significant role in categorization [<xref ref-type="bibr" rid="pcbi.1006327.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref037">37</xref>]. Of course, it is possible that a different feedforward model, or a different weighting of features within this model may explain more variability in human ERP patterns. However, recent literature suggests that performance differences between different deep CNN architectures is smaller than the difference between CNNs and human observers, or between CNNs and previous models [<xref ref-type="bibr" rid="pcbi.1006327.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref038">38</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref039">39</xref>]. Future work will examine both recurrent and feedforward architectures to disentangle these possibilities.</p>
<p>In examining the residuals of the model fits, we found that the CNN over-estimated the similarity of natural landscape images while simultaneously under-estimating the similarity between different types of manufactured environments (indoor versus urban). This suggests a coarser-grained representation for natural landscape images in the CNN compared to human observers. This pattern may reflect the fact that only 29% of images and 28% of scene categories in Places-205 are natural landscape environments [<xref ref-type="bibr" rid="pcbi.1006327.ref020">20</xref>]. Having more training data for the CNN may have resulted in the ability to form finer-grained representations for manufactured environments.</p>
<p>Our results are largely in agreement with previous MEG studies that examined the correspondence between CNNs and time-resolved neural signals [<xref ref-type="bibr" rid="pcbi.1006327.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref018">18</xref>]. These studies examined whole-brain signals in a time point by time point manner, losing any spatial response pattern. By contrast, by using a sliding window on each electrode, we were able to retain the patterns across the scalp while still examining time-resolved data. Given the known representational differences between MEG and EEG [<xref ref-type="bibr" rid="pcbi.1006327.ref040">40</xref>], this work provides unique but complementary information about visual category representations. Specifically, [<xref ref-type="bibr" rid="pcbi.1006327.ref040">40</xref>] found that compared to EEG, MEG signals had decodable information earlier, and more driven by early visual cortex. This suggests that the time points reported here might constitute an upper bound on information availability.</p>
<p>A second difference between this work and previous is that we examined category-specific information instead of image-level information. As the act of recognition is generally an act of categorization [<xref ref-type="bibr" rid="pcbi.1006327.ref041">41</xref>], and because the CNN we used was pre-trained specifically to make category-level distinctions [<xref ref-type="bibr" rid="pcbi.1006327.ref042">42</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref043">43</xref>], we argue that this is the most natural comparison for human and artificial neural networks. Accordingly, we assessed the amount of category-level information available in each layer of the CNN. While it is unsurprising that significant category information exists in all eight layers, or that the amount of information increases across layers, we were surprised to observe that category information peaked in the sixth layer. Given that the utility of layer depth is still controversial within the computer vision community [<xref ref-type="bibr" rid="pcbi.1006327.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006327.ref045">45</xref>], this result may be of interest to this community as well. Furthermore, that both right occipitotemporal and frontal electrodes also had explained variability that peaked in the sixth CNN layer corroborates the view that a shallower artificial neural network might outperform deeper ones on scene classification tasks.</p>
<p>Although CNNs differ significantly from biological networks, they are of interest to neuroscientists because they allow us to see a solution to a difficult information-processing problem in a step-by-step manner. The extent to which hard problems such as visual recognition have unique solutions is an open question. Thus, the growing number of similarities between biological and neural networks may indicate that artificial neural networks have honed in on the same solution found by evolution and biology.</p>
</sec>
<sec id="sec016">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006327.s001" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Overall variance explained by all eight layers of the CNN for the supplementary dataset.</title>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s002" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Overall variance explained by all eight layers of the CNN as a function of electrode group.</title>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s003" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Variance explained by each layer of the CNN alone.</title>
<p>As in <xref ref-type="fig" rid="pcbi.1006327.g005">Fig 5</xref> in the main text, CNN layers are ordered darkest (Conv 1) to lightest (FC8).</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s004" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Onset of significant explained variance as a function of CNN layer.</title>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s005" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Latency of maximum explained variability as a function of CNN layer for replication dataset.</title>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s006" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Residuals across superordinate category for replication set.</title>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s007" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Explained variability in frontal cluster between 200–300 ms after stimulus onset.</title>
<p>Each trace reflects an individual participant.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s008" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s008" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Text of supporting information, part 1.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006327.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006327.s009" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Text of supporting information, part 2.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006327.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>The Briefest of Glances: The Time Course of Natural Scene Understanding</article-title>. <source>Psychol Sci</source>. <year>2009</year> <month>Apr</month>;<volume>20</volume>:<fpage>464</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2009.02316.x" xlink:type="simple">10.1111/j.1467-9280.2009.02316.x</ext-link></comment> <object-id pub-id-type="pmid">19399976</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potter</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Wyble</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hagmann</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>McCourt</surname> <given-names>ES</given-names></name>. <article-title>Detecting meaning in RSVP at 13 ms per picture</article-title>. <source>Atten Percept Psychophys</source>. <year>2014</year>;<fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name>. <article-title>Visual categorization is automatic and obligatory: Evidence from Stroop-like paradigm</article-title>. <source>J Vis</source>. <year>2014</year> <month>Jan</month> <day>16</day>;<volume>14</volume>(<issue>1</issue>):<fpage>14</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.1.14" xlink:type="simple">10.1167/14.1.14</ext-link></comment> <object-id pub-id-type="pmid">24434626</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Edelman</surname> <given-names>S</given-names></name>. <article-title>Representation is representation of similarities</article-title>. <source>Behav Brain Sci</source>. <year>1998</year> <month>Aug</month>;<volume>21</volume>(<issue>4</issue>):<fpage>449</fpage>–<lpage>67</lpage>; discussion 467–498. <object-id pub-id-type="pmid">10097019</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational Similarity Analysis–Connecting the Branches of Systems Neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>;<volume>2</volume>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn Sci</source>. <year>2007</year> <month>Aug</month>;<volume>11</volume>(<issue>8</issue>):<fpage>333</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2007.06.010" xlink:type="simple">10.1016/j.tics.2007.06.010</ext-link></comment> <object-id pub-id-type="pmid">17631409</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year> <month>Nov</month>;<volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russakovsky</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Krause</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Satheesh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>Int J Comput Vis</source>. <year>2015</year> <month>Apr</month> <day>11</day>;<fpage>1</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukushima</surname> <given-names>K</given-names></name>. <article-title>Neocognitron: A hierarchical neural network capable of visual pattern recognition</article-title>. <source>Neural Netw</source>. <year>1988</year>;<volume>1</volume>(<issue>2</issue>):<fpage>119</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agrawal</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Stansbury</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <source>Pixels to Voxels: Modeling Visual Representation in the Human Brain</source>. ArXiv14075104 Cs Q-Bio [Internet]. <year>2014</year> <month>Jul</month> <day>18</day> [cited 2016 Aug 2]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1407.5104" xlink:type="simple">http://arxiv.org/abs/1407.5104</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ardila</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <etal>et al</etal>. <article-title>Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition</article-title>. <source>PLOS Comput Biol</source>. <year>2014</year> <month>Dec</month> <day>18</day>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003963</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003963" xlink:type="simple">10.1371/journal.pcbi.1003963</ext-link></comment> <object-id pub-id-type="pmid">25521294</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Sci Rep</source>. <year>2016</year> <month>Jun</month> <day>10</day>;<volume>6</volume>:<fpage>27755</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep27755" xlink:type="simple">10.1038/srep27755</ext-link></comment> <object-id pub-id-type="pmid">27282108</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Gerven</surname> <given-names>MAJ van</given-names></name>. <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>J Neurosci</source>. <year>2015</year> <month>Jul</month> <day>8</day>;<volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLOS Comput Biol</source>. <year>2014</year> <month>Nov</month> <day>6</day>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bracci</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Beeck</surname> <given-names>HPO de</given-names></name>. <article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title>. <source>PLOS Comput Biol</source>. <year>2016</year> <month>Apr</month> <day>28</day>;<volume>12</volume>(<issue>4</issue>):<fpage>e1004896</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004896" xlink:type="simple">10.1371/journal.pcbi.1004896</ext-link></comment> <object-id pub-id-type="pmid">27124699</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <chapter-title>Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</chapter-title>. In: <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>26</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2013</year> [cited 2016 Aug 2]. p. <fpage>3093</fpage>–<lpage>3101</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4991-hierarchical-modular-optimization-of-convolutional-networks-achieves-representations-similar-to-macaque-it-and-human-ventral-stream.pdf" xlink:type="simple">http://papers.nips.cc/paper/4991-hierarchical-modular-optimization-of-convolutional-networks-achieves-representations-similar-to-macaque-it-and-human-ventral-stream.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nat Neurosci</source>. <year>2016</year> <month>Mar</month>;<volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4244" xlink:type="simple">10.1038/nn.4244</ext-link></comment> <object-id pub-id-type="pmid">26906502</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seeliger</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Fritsche</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Schoenmakers</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>J-M</given-names></name>, <name name-style="western"><surname>Bosch</surname> <given-names>SE</given-names></name>, <etal>et al</etal>. <article-title>Convolutional neural network-based encoding and decoding of visual object recognition in space and time</article-title>. <source>NeuroImage</source>. <year>2017</year> <month>Jul</month> <day>16</day>;</mixed-citation></ref>
<ref id="pcbi.1006327.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Dynamics of scene representations in the human brain revealed by magnetoencephalography and deep neural networks</article-title>. <source>NeuroImage</source>. <year>2017</year> <month>Jun</month>;<volume>153</volume>:<fpage>346</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.03.063" xlink:type="simple">10.1016/j.neuroimage.2016.03.063</ext-link></comment> <object-id pub-id-type="pmid">27039703</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lapedriza</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>. <article-title>Places: A 10 million Image Database for Scene Recognition</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;PP(<issue>99</issue>):<fpage>1</fpage>–<lpage>1</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLOS Comput Biol</source>. <year>2014</year> <month>Apr</month> <day>17</day>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kassam</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Ghuman</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Boshyan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schmid</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>Top-down facilitation of visual recognition</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2006</year> <month>Jan</month> <day>10</day>;<volume>103</volume>(<issue>2</issue>):<fpage>449</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0507062103" xlink:type="simple">10.1073/pnas.0507062103</ext-link></comment> <object-id pub-id-type="pmid">16407167</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Devereux</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Randall</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tyler</surname> <given-names>LK</given-names></name>. <article-title>Predicting the Time Course of Individual Objects with MEG</article-title>. <source>Cereb Cortex</source>. <year>2015</year> <month>Oct</month> <day>1</day>;<volume>25</volume>(<issue>10</issue>):<fpage>3602</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhu203" xlink:type="simple">10.1093/cercor/bhu203</ext-link></comment> <object-id pub-id-type="pmid">25209607</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xiao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ehinger</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Hays</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>SUN Database: Exploring a Large Collection of Scene Categories</article-title>. <source>Int J Comput Vis</source>. <year>2014</year> <month>Aug</month> <day>13</day>;<fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname> <given-names>II</given-names></name>, <name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Baldassano</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>. <article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title>. <source>eLife</source>. <year>2018</year> <month>Mar</month> <day>7</day>;<fpage>7</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Essock</surname> <given-names>EA</given-names></name>. <article-title>A horizontal bias in human visual processing of orientation and its correspondence to the structural components of natural scenes</article-title>. <source>J Vis</source>. <year>2004</year> <month>Dec</month> <day>1</day>;<volume>4</volume>(<issue>12</issue>):<fpage>5</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Hess</surname> <given-names>RF</given-names></name>. <article-title>Discrimination of amplitude spectrum slope in the fovea and parafovea and the local amplitude distributions of natural scene imagery</article-title>. <source>J Vis</source>. <year>2006</year> <month>Jun</month> <day>2</day>;<volume>6</volume>(<issue>7</issue>):<fpage>3</fpage>–<lpage>3</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Delorme</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Makeig</surname> <given-names>S</given-names></name>. <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J Neurosci Methods</source>. <year>2004</year> <month>Mar</month> <day>15</day>;<volume>134</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2003.10.009" xlink:type="simple">10.1016/j.jneumeth.2003.10.009</ext-link></comment> <object-id pub-id-type="pmid">15102499</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref029"><label>29</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title>. In: <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>25</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2012</year> [cited 2014 Jul 1]. p. <fpage>1097</fpage>–<lpage>1105</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" xlink:type="simple">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, et al. Caffe: Convolutional Architecture for Fast Feature Embedding. In: Proceedings of the 22Nd ACM International Conference on Multimedia [Internet]. New York, NY, USA: ACM; 2014 [cited 2017 Jan 10]. p. 675–678. (MM ‘14). Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/2647868.2654889" xlink:type="simple">http://doi.acm.org/10.1145/2647868.2654889</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>C-J</given-names></name>. <article-title>LIBSVM: A Library for Support Vector Machines</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year> <month>May</month>;<volume>2</volume>(<issue>3</issue>):<fpage>27:1</fpage>–<lpage>27:27</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chauvin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Arguin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>F</given-names></name>. <article-title>Accurate statistical tests for smooth classification images</article-title>. <source>J Vis</source>. <year>2005</year> <month>Oct</month> <day>5</day>;<volume>5</volume>(<issue>9</issue>):<fpage>659</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/5.9.1" xlink:type="simple">10.1167/5.9.1</ext-link></comment> <object-id pub-id-type="pmid">16356076</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci</source>. <year>2007</year>;<volume>104</volume>(<issue>15</issue>):<fpage>6424</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0700622104" xlink:type="simple">10.1073/pnas.0700622104</ext-link></comment> <object-id pub-id-type="pmid">17404214</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goddard</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Dermody</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Woolgar</surname> <given-names>A</given-names></name>. <article-title>Representational dynamics of object recognition: Feedforward and feedback information flows</article-title>. <source>NeuroImage</source>. <year>2016</year> <month>Mar</month>;<volume>128</volume>:<fpage>385</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.01.006" xlink:type="simple">10.1016/j.neuroimage.2016.01.006</ext-link></comment> <object-id pub-id-type="pmid">26806290</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name>. <article-title>The reverse hierarchy theory of visual perceptual learning</article-title>. <source>Trends Cogn Sci</source>. <year>2004</year>;<volume>8</volume>(<issue>10</issue>):<fpage>457</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2004.08.011" xlink:type="simple">10.1016/j.tics.2004.08.011</ext-link></comment> <object-id pub-id-type="pmid">15450510</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peyrin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Michel</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Seghier</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Landis</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>The Neural Substrates and Timing of Top–Down Processes during Coarse-to-Fine Categorization of Visual Scenes: A Combined fMRI and ERP Study</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year> <month>Jan</month> <day>4</day>;<volume>22</volume>(<issue>12</issue>):<fpage>2768</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2010.21424" xlink:type="simple">10.1162/jocn.2010.21424</ext-link></comment> <object-id pub-id-type="pmid">20044901</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>. <article-title>View from the top: Hierarchies and reverse hierarchies in the visual system</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>:<fpage>791</fpage>–<lpage>804</lpage>. <object-id pub-id-type="pmid">12467584</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geirhos</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Janssen</surname> <given-names>DHJ</given-names></name>, <name name-style="western"><surname>Schütt</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Rauber</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name>. <article-title>Comparing deep neural networks against humans: object recognition when the signal gets weaker</article-title>. <source>ArXiv170606969 Cs Q-Bio Stat</source> [Internet]. <year>2017</year> <month>Jun</month> <day>21</day> [cited 2017 Dec 31]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.06969" xlink:type="simple">http://arxiv.org/abs/1706.06969</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jozwik</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Storrs</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>. <article-title>Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments</article-title>. <source>Front Psychol</source> [Internet]. <year>2017</year> [cited 2018 Mar 31];8. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726/full" xlink:type="simple">https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726/full</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>. <article-title>Multivariate pattern analysis of MEG and EEG: A comparison of representational structure in time and space</article-title>. <source>NeuroImage</source>. <year>2017</year> <month>Sep</month> <day>1</day>;<volume>158</volume>(<issue>Supplement C</issue>):<fpage>441</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006327.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruner</surname> <given-names>JS</given-names></name>. <article-title>On perceptual readiness</article-title>. <source>Psychol Rev</source>. <year>1957</year>;<volume>64</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>52</lpage>. <object-id pub-id-type="pmid">13420288</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year> <month>May</month> <day>28</day>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1006327.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lapedriza</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Xiao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <chapter-title>Learning Deep Features for Scene Recognition using Places Database</chapter-title>. In: <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lawrence</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>27</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2014</year> [cited 2015 Oct 13]. p. <fpage>487</fpage>–<lpage>495</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf" xlink:type="simple">http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ba</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Caruana</surname> <given-names>R</given-names></name>. <chapter-title>Do Deep Nets Really Need to be Deep?</chapter-title> In: <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lawrence</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>27</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2014</year>. p. <fpage>2654</fpage>–<lpage>2662</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf" xlink:type="simple">http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006327.ref045"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition [Internet]. 2016 [cited 2017 Nov 15]. p. 770–8. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" xlink:type="simple">https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>