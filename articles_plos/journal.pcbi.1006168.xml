<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01321</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006168</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Evolutionary processes</subject><subj-group><subject>Convergent evolution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Electronics</subject><subj-group><subject>Signal decoders</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kalman filter</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kalman filter</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical mechanics</subject><subj-group><subject>Kinematics</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Optimizing the learning rate for adaptive estimation of neural encoding models</article-title>
<alt-title alt-title-type="running-head">Learning rate calibration algorithm for adaptive estimation</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6373-703X</contrib-id>
<name name-style="western">
<surname>Hsieh</surname> <given-names>Han-Lin</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0544-7720</contrib-id>
<name name-style="western">
<surname>Shanechi</surname> <given-names>Maryam M.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Ming Hsieh Department of Electrical Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Neuroscience Graduate Program, University of Southern California, Los Angeles, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Stein</surname> <given-names>Carlos</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">shanechi@usc.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>5</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>29</day>
<month>5</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>5</issue>
<elocation-id>e1006168</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>8</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Hsieh, Shanechi</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006168"/>
<abstract>
<p>Closed-loop neurotechnologies often need to adaptively learn an encoding model that relates the neural activity to the brain state, and is used for brain state decoding. The speed and accuracy of adaptive learning algorithms are critically affected by the learning rate, which dictates how fast model parameters are updated based on new observations. Despite the importance of the learning rate, currently an analytical approach for its selection is largely lacking and existing signal processing methods vastly tune it empirically or heuristically. Here, we develop a novel analytical calibration algorithm for optimal selection of the learning rate in adaptive Bayesian filters. We formulate the problem through a fundamental trade-off that learning rate introduces between the steady-state error and the convergence time of the estimated model parameters. We derive explicit functions that predict the effect of learning rate on error and convergence time. Using these functions, our calibration algorithm can keep the steady-state parameter error covariance smaller than a desired upper-bound while minimizing the convergence time, or keep the convergence time faster than a desired value while minimizing the error. We derive the algorithm both for discrete-valued spikes modeled as point processes nonlinearly dependent on the brain state, and for continuous-valued neural recordings modeled as Gaussian processes linearly dependent on the brain state. Using extensive closed-loop simulations, we show that the analytical solution of the calibration algorithm accurately predicts the effect of learning rate on parameter error and convergence time. Moreover, the calibration algorithm allows for fast and accurate learning of the encoding model and for fast convergence of decoding to accurate performance. Finally, larger learning rates result in inaccurate encoding models and decoders, and smaller learning rates delay their convergence. The calibration algorithm provides a novel analytical approach to predictably achieve a desired level of error and convergence time in adaptive learning, with application to closed-loop neurotechnologies and other signal processing domains.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Closed-loop neurotechnologies for treatment of neurological disorders often require adaptively learning an encoding model to relate the neural activity to the brain state and decode this state. Fast and accurate adaptive learning is critically affected by the learning rate, a key variable in any adaptive algorithm. However, existing signal processing algorithms select the learning rate empirically or heuristically due to the lack of a principled approach for learning rate calibration. Here, we develop a novel analytical calibration algorithm to optimally select the learning rate. The learning rate introduces a trade-off between the steady-state error and the convergence time of the estimated model parameters. Our calibration algorithm can keep the steady-state parameter error smaller than a desired value while minimizing the convergence time, or keep the convergence time faster than a desired value while minimizing the error. Using extensive closed-loop simulations, we show that the calibration algorithm allows for fast learning of accurate encoding models, and consequently for fast convergence of decoder performance to high values for both discrete-valued spike recordings and continuous-valued recordings such as local field potentials. The calibration algorithm can achieve a predictable level of speed and accuracy in adaptive learning, with significant implications for neurotechnologies.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>CCF-1453868</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0544-7720</contrib-id>
<name name-style="western">
<surname>Shanechi</surname> <given-names>Maryam M.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>The authors acknowledge support of the Army Research Office (ARO) under contract W911NF-16-1-0368 to MMS (<ext-link ext-link-type="uri" xlink:href="https://www.arl.army.mil/www/" xlink:type="simple">https://www.arl.army.mil/www/</ext-link>). This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative (MURI). The authors also acknowledge support of the National Science Foundation under CAREER Award CCF-1453868 to MMS (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/" xlink:type="simple">https://www.nsf.gov/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="0"/>
<page-count count="34"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-06-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>This manuscript is based on simulated data. All simulated data is available at URL <ext-link ext-link-type="uri" xlink:href="https://nseip.usc.edu/publications/" xlink:type="simple">https://nseip.usc.edu/publications/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Recent technological advances have enabled the real-time recording and processing of different invasive neural signal modalities, including the electrocorticogram (ECoG), local field potentials (LFP), and spiking activity [<xref ref-type="bibr" rid="pcbi.1006168.ref001">1</xref>]. This real-time recording capability has allowed for the development of various neurotechnologies to treat neurological disorders. For example, motor brain-machine interfaces (BMI) have the potential to restore movement to disabled patients by recording the neural activity—such as ECoG, LFP, or spikes—in real time, decoding from this activity the motor intent of the subject, and using the decoded intent to actuate and control an external device [<xref ref-type="bibr" rid="pcbi.1006168.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref012">12</xref>]. Closed-loop deep brain stimulation (DBS) systems, e.g., for treatment of Parkinson’s disease, use recordings such as ECoG or LFP to decode the underlying diseased state of the brain and adjust the electrical stimulation pattern to an appropriate brain region, e.g., the subthalamic nucleus (STN) [<xref ref-type="bibr" rid="pcbi.1006168.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref016">16</xref>]. These neurotechnologies are examples of closed-loop neural systems.</p>
<p>Closed-loop neural systems need to learn an encoding model that relates the neural signal (e.g., spikes) to the underlying brain state (e.g., motor intent) for each subject. The encoding model is often taken as a parametric function and is used to derive mathematical algorithms, termed decoders, that estimate the subject’s brain state from their neural activity. These closed-loop neural systems run in real time and often require the encoding model parameters to be learned in closed loop, online and adaptively (<xref ref-type="fig" rid="pcbi.1006168.g001">Fig 1</xref>). For example, in motor BMIs, neural encoding can differ for movement of the BMI compared to that of the native arm or to imagined movements [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref020">20</xref>]. Hence encoding model parameters are better learned adaptively in closed-loop BMI operation [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref030">30</xref>]. Another reason for real-time adaptive learning could be the non-stationary nature of neural activity patterns over time, for example due to learning in motor BMIs [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref019">19</xref>], due to new experience in the hippocampus [<xref ref-type="bibr" rid="pcbi.1006168.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref032">32</xref>], or due to stimulation-induced plasticity in DBS systems [<xref ref-type="bibr" rid="pcbi.1006168.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref034">34</xref>]. Adaptive learning algorithms in closed-loop neural systems, such as adaptive Kalman filters (KF), are typically batch-based. They collect batches of neural activity, fit a new set of parameters in each batch using maximum-likelihood techniques, and update the model parameters [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref027">27</xref>]. In addition to these methods, adaptive point process filters (PPF) have also been developed for tracking plasticity in offline datasets [<xref ref-type="bibr" rid="pcbi.1006168.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>]. Recently, control-based state-space algorithms have been designed for adaptive learning of point process spike models during closed-loop BMI operation, and have improved the speed of real-time parameter convergence compared with batch-based methods [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>].</p>
<fig id="pcbi.1006168.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Closed-loop neural system.</title>
<p>Closed-loop neural systems often need to learn an encoding model adaptively and in real time. The encoding model describes the relationship between neural recordings and the brain state. For example, the relevant brain state in motor BMIs is the intended velocity and in DBS systems is the disease state, e.g., in Parkinson’s disease. The neural system uses the learned encoding model to decode the brain state. This decoded brain state is then used, for example, to move a prosthetic in motor BMIs while providing visual feedback to the subject, or to control the stimulation pattern applied to the brain in DBS systems. A critical parameter for any adaptive learning algorithm is the learning rate, which dictates how fast the encoding model parameters are updated as new neural observations are received. An analytical calibration algorithm will enable achieving a predictable level of accuracy and speed in adaptive learning to improve the transient and steady-state operation of neural systems.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g001" xlink:type="simple"/>
</fig>
<p>A critical design parameter in any adaptive algorithm is the learning rate, which dictates how fast model parameters are updated based on a new observation of neural activity (<xref ref-type="fig" rid="pcbi.1006168.g001">Fig 1</xref>). The learning rate introduces a trade-off between the convergence time and the steady-state error of the estimated model parameters [<xref ref-type="bibr" rid="pcbi.1006168.ref037">37</xref>]. Increasing the learning rate decreases the convergence time, allowing for parameter estimates to reach their final values faster. However, this faster convergence comes at the price of a larger steady-state parameter estimation error. Similarly, a smaller learning rate will decrease the steady-state error, but lower the speed of convergence. Hence principled calibration of the learning rate is critical for fast and accurate learning of the encoding model, and consequently for both the transient and the steady-state performance of the decoder.</p>
<p>To date, however, adaptive algorithms have chosen the learning rate empirically. For example, in batch-based methods, once a new batch estimate is obtained, the parameter estimates from previous batches are either replaced with these new estimates [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>] or are smoothly changed by weighted-averaging based on a desired half-life [<xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref027">27</xref>]. In adaptive state-space algorithms, such as adaptive PPF, learning rate is dictated by the choice of the noise covariance in the prior model of the parameter decoder, which is again chosen empirically [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref038">38</xref>]. Given the significant impact of the learning rate on both the transient and the steady-state performance of closed-loop neurotechnologies, it is important to develop a principled learning rate calibration algorithm that can meet a desired error or convergence time performance for any neural recording modality (such as spikes, ECoG, and LFP) and across applications. In addition to neurotechnologies, designing such a calibration algorithm is also of great importance in general signal processing applications. Prior adaptive signal processing methods have largely focused on non-Bayesian gradient decent algorithms. These algorithms, however, do not predict the effect of the learning rate on error or convergence time (except for a limited case of scalar linear models; see <xref ref-type="sec" rid="sec021">Discussions</xref>) and hence can only provide heuristics for tuning the learning rate [<xref ref-type="bibr" rid="pcbi.1006168.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref040">40</xref>]. A calibration algorithm that can write an explicit function for the effect of the learning rate on error and/or convergence time for both linear and nonlinear observation models would also provide a novel approach for learning rate selection in other signal processing domains [<xref ref-type="bibr" rid="pcbi.1006168.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref047">47</xref>]. For example, in image processing [<xref ref-type="bibr" rid="pcbi.1006168.ref043">43</xref>], in electrocardiography [<xref ref-type="bibr" rid="pcbi.1006168.ref041">41</xref>], in anesthesia control [<xref ref-type="bibr" rid="pcbi.1006168.ref044">44</xref>], in automated heart beat detection [<xref ref-type="bibr" rid="pcbi.1006168.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref047">47</xref>], and in unscented Kalman filters [<xref ref-type="bibr" rid="pcbi.1006168.ref042">42</xref>], adaptive filters with learning rates are used in decoding system states or in learning system parameters in real time (see <xref ref-type="sec" rid="sec021">Discussions</xref>).</p>
<p>Here, we develop a mathematical framework to optimally calibrate the learning rate for Bayesian adaptive learning of neural encoding models. We derive the calibration algorithm both for learning a nonlinear point process model for discrete-valued spiking activity—which we term point process encoding model—, and for learning a linear model with Gaussian noise for continuous-valued neural activities (e.g., LFP or ECoG)—which we term Gaussian encoding model. Our framework derives an explicit analytical function for the effect of learning rate on parameter estimation error and/or convergence time. Minimizing the convergence time and the steady-state error covariance are competing requirements. We thus formulate the calibration problem through the fundamental trade-off that the learning rate introduces between the convergence time and the steady-state error, and derive the optimal calibration algorithm for two alternative objectives: satisfying a user-specified upper-bound on the steady-state parameter error covariance while minimizing the convergence time, and vice versa. For both objectives, we derive analytical solutions for the learning rate. The calibration algorithm can pre-compute the learning rate prior to start of real-time adaptation.</p>
<p>We show that the calibration algorithm can analytically solve for the optimal learning rate for both point process and Gaussian encoding models. We use extensive Monte-Carlo simulations of adaptive Bayesian filters operating on both discrete-valued spikes and continuous-valued neural observations to validate the analytical predictions of the calibration algorithm. With these simulations, we demonstrate that the learning rate selected analytically by the calibration algorithm minimizes the convergence time while satisfying an upper-bound on the steady-state error covariance or vice versa. Thus the algorithm results in fast and accurate learning of the encoding model. In addition to the encoding model, we also examine the influence of the calibration algorithm on decoding by taking a motor BMI system, which uses discrete-valued spikes or continuous-valued neural activity (e.g., ECoG or LFP), as an example. We perform extensive closed-loop BMI simulations [<xref ref-type="bibr" rid="pcbi.1006168.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>] that closely conform to our non-human primate BMI experiments [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref051">51</xref>] (see <xref ref-type="sec" rid="sec021">Discussions</xref>). Using these simulations, we show that analytically selecting the optimal learning rate can improve both the transient operation of the BMI by allowing its decoding performance to converge faster, and the steady-state performance of the BMI by allowing it to learn a more accurate decoder. We also demonstrate that large learning rates lead to inaccurate encoding models and decoders, and small learning rates delay the convergence of encoding models and decoder performance. By providing a novel analytical approach for learning rate optimization, this calibration algorithm has significant implications for closed-loop neurotechnologies and for other signal processing applications (see <xref ref-type="sec" rid="sec021">Discussions</xref>).</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<p>We derive the calibration algorithm for adaptation of two widely-used neural encoding models—the linear model with Gaussian noise for continuous-valued signals such as LFP and ECoG, and the nonlinear point process model for the spiking activity. In the former case, the calibration algorithm adjusts the learning rate of an adaptive KF, and in the latter case it adjusts the learning rate of an adaptive PPF. We design the calibration algorithm for adaptive PPF and KF, as these filters have been validated in closed-loop non-human primate and human experiments both in our work and in other studies (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref030">30</xref>]). However, to date, the learning rates in these filters have been selected using empirical tuning. Instead, the new calibration algorithm provides a novel analytical approach for selecting the learning rate to achieve a predictable and desired level of parameter error and convergence time in these widely-used adaptive filters.</p>
<p>In both the adaptive PPF and the adaptive KF, the learning rate is dictated by the noise covariance of the decoder’s prior model for the parameters. In what follows, we derive calibration algorithms for two possible objectives: to keep the steady-state parameter error covariance smaller than a user-specified upper-bound while minimizing the convergence time, or to keep the convergence time faster than a user-specified upper-bound while minimizing the steady-state error covariance. We first derive analytical expressions for both the steady-state error covariance and the convergence time as a function of the learning rate by writing the recursive error dynamics and the corresponding recursive error covariance equations for the adaptive PPF and adaptive KF. By taking the limit of these recursions as time goes to infinity, we find the analytical expressions for the steady-state error covariance and the convergence time as a function of the learning rate. We then find the inverse maps of these functions, which provide the optimal learning rate for a desired objective. We also introduce the numerical simulation setup used to evaluate the effect of the calibration algorithm on both encoding models and decoding. The flowchart of the calibration algorithm is in <xref ref-type="fig" rid="pcbi.1006168.g002">Fig 2</xref>. Readers mainly interested in the results can skip the rest of this section.</p>
<fig id="pcbi.1006168.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Flowchart of the calibration algorithm.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g002" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>The calibration algorithm for continuous neural signals</title>
<p>In this section, we derive the calibration algorithm for continuous signals such as LFP and ECoG. We first present the observation model and the adaptive KF for these signals. We then find the steady-state error covariance and the convergence time as functions of the learning rate. Finally, we derive the inverse functions to select the optimal learning rate.</p>
<sec id="sec004">
<title>Adaptive KF</title>
<p>We denote the continuous observation signal, such as ECoG or LFP, from channel <italic>c</italic> by <inline-formula id="pcbi.1006168.e001"><alternatives><graphic id="pcbi.1006168.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. This continuous signal can, for example, be taken as the LFP or ECoG log-power in a desired frequency band as these powers have been shown to be related to the underlying brain states [<xref ref-type="bibr" rid="pcbi.1006168.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref053">53</xref>]. As in various previous work (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref055">55</xref>]), we construct the continuous observation model as a linear function of the underlying brain state with Gaussian noise
<disp-formula id="pcbi.1006168.e002"><alternatives><graphic id="pcbi.1006168.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">ψ</mml:mi></mml:mrow> <mml:mi>c</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
The above equation constitutes the neural encoding model for continuous neural signals where ⋅′ indicates the transpose operation, and <inline-formula id="pcbi.1006168.e003"><alternatives><graphic id="pcbi.1006168.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is a column vector with <bold>v</bold><sub><italic>t</italic></sub> denoting the encoded brain state. Also, <bold><italic>ψ</italic></bold><sup><italic>c</italic></sup> = [<italic>ξ</italic><sup><italic>c</italic></sup>, (<bold><italic>η</italic></bold><sup><italic>c</italic></sup>)′]′ is a column vector containing the encoding model parameters to be learned. In particular, <italic>ξ</italic><sup><italic>c</italic></sup> is the baseline log-power and <bold><italic>η</italic></bold><sup><italic>c</italic></sup> depends on the application. Finally, <inline-formula id="pcbi.1006168.e004"><alternatives><graphic id="pcbi.1006168.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is a white Gaussian noise with variance <italic>Z</italic><sup><italic>c</italic></sup>. As an example, in motor BMIs, we take the brain state <bold>v</bold><sub><italic>t</italic></sub> as the intended velocity command whether in moving one’s arm or in moving a BMI. We thus select <inline-formula id="pcbi.1006168.e005"><alternatives><graphic id="pcbi.1006168.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow> <mml:mi>c</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi>η</mml:mi> <mml:mi>x</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>η</mml:mi> <mml:mi>y</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>∥</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow> <mml:mi>c</mml:mi></mml:msup> <mml:mo>∥</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>θ</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>θ</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> with ‖<bold><italic>η</italic></bold><sup><italic>c</italic></sup>‖ the modulation depth and <italic>θ</italic><sup><italic>c</italic></sup> the preferred direction of channel <italic>c</italic>. The goal of adaptation is to learn the encoding model parameters in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref>, i.e., <bold><italic>ψ</italic></bold><sup><italic>c</italic></sup>. In some cases, it may also be desired to learn <italic>Z</italic><sup><italic>c</italic></sup> adaptively. Here, we first focus on adaptive learning of the parameters <bold><italic>ψ</italic></bold><sup><italic>c</italic></sup> and the derivation of the calibration algorithm. We then present a method to learn <italic>Z</italic><sup><italic>c</italic></sup> concurrently with the parameters.</p>
<p>We write a recursive Bayesian decoder to learn the parameters <bold><italic>ψ</italic></bold><sup><italic>c</italic></sup> recursively in real time. In neurotechnologies, such as BMIs, neural encoding model parameters are either time-invariant or change substantially slower compared with the time-scales of parameter learning (days compared with minutes, respectively; see e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>]). Thus neural encoding model parameters in the adaptive learning algorithm can be largely assumed to be essentially fixed within relevant time-scales of parameter adaptation (e.g., minutes) in BMIs [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref057">57</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref064">64</xref>]. While one application of recursive Bayesian decoders (e.g., KF or PPF) is to track time-varying parameters, these filters have also been used to estimate parameters that are fixed but unknown and their application in this context has been studied extensively [<xref ref-type="bibr" rid="pcbi.1006168.ref065">65</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref069">69</xref>]. For example, the KF has been used to estimate unknown fixed parameters in prior applications such as climate modeling, control of fluid dynamics, spacecraft control, and robotics [<xref ref-type="bibr" rid="pcbi.1006168.ref066">66</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref069">69</xref>]. The PPF has also been used to estimate fixed unknown parameters [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>].</p>
<p>Assuming that all channels are conditionally independent [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>] (see <xref ref-type="sec" rid="sec021">Discussions</xref>), we can adapt the parameters for each channel separately. For convenience, we drop the superscript of the channel in what follows. A recursive Bayesian decoder consists of a prior model for the parameters, which models their uncertainty; it also consists of an observation model that relates the parameters to the neural activity. The observation model is given by <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref>. We build the prior model by modeling the uncertainty of <bold><italic>ψ</italic></bold> as a random-walk [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]
<disp-formula id="pcbi.1006168.e006"><alternatives><graphic id="pcbi.1006168.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
Here <bold>s</bold><sub><italic>t</italic></sub> is a white Gaussian noise with covariance matrix <bold>S</bold> = <italic>s</italic><bold>I</bold><sub><italic>n</italic></sub>(<italic>s</italic> &gt; 0), where <bold>I</bold><sub><italic>n</italic></sub> is the identity matrix and <italic>n</italic> is the parameter dimension. Note that <bold>s</bold><sub><italic>t</italic></sub> is simply used to model our uncertainty at time <italic>t</italic> about the unknown parameter <bold><italic>ψ</italic></bold> and thus is not representing a biophysical noise. Consequently, the covariance parameter <italic>s</italic> is not a biophysical parameter to be learned; rather, <italic>s</italic> is a design choice that controls how fast parameter estimates are updated and thus serves as a tool to control the convergence time and error covariance in learning the neural encoding model parameters <bold><italic>ψ</italic></bold> adaptively in real time (see Appendix A in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref> for details). We define <italic>s</italic> as the learning rate since it dictates how fast parameters are updated in the Bayesian decoder as new neural observations are made [<xref ref-type="bibr" rid="pcbi.1006168.ref070">70</xref>] (see <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> below and Appendix A in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref> for details). Our goal is to solve for the optimal <italic>s</italic> that achieves a desired trade-off between the steady-state error covariance and convergence time.</p>
<p>Combining <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e006">(2)</xref> and since both the prior and observation models are linear and Gaussian, we can derive a recursive KF to estimate <bold><italic>ψ</italic></bold><sub><italic>t</italic></sub> from <italic>y</italic><sub>1</sub>, ⋯, <italic>y</italic><sub><italic>t</italic></sub>. KF finds the minimum mean-squared error (MMSE) estimate of the parameters, which is given by the mean of the posterior density. Denoting the posterior and prediction means by <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> and <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic>−1</sub>, and their covariances by <bold>S</bold><sub><italic>t</italic>|<italic>t</italic></sub> and <bold>S</bold><sub><italic>t</italic>|<italic>t</italic>−1</sub>, respectively, the KF recursions are given as
<disp-formula id="pcbi.1006168.e007"><alternatives><graphic id="pcbi.1006168.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula> <disp-formula id="pcbi.1006168.e008"><alternatives><graphic id="pcbi.1006168.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">S</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1006168.e009"><alternatives><graphic id="pcbi.1006168.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msup><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula> <disp-formula id="pcbi.1006168.e010"><alternatives><graphic id="pcbi.1006168.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:msup><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>Note that <bold>S</bold><sub><italic>t</italic>|<italic>t</italic></sub> specifies the relative weight of the neural observation <italic>y</italic><sub><italic>t</italic></sub> compared with the previous parameter estimate in updating the current parameter estimate and thus determines how fast <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> is learned in <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref>. Since <bold>S</bold><sub><italic>t</italic>|<italic>t</italic></sub> is a function of <italic>s</italic>, which is the only design choice in our control, we call <italic>s</italic> the learning rate. As shown in Appendix A in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>, as <italic>s</italic> increases, parameters are updated faster. Hence given the encoded brain/behavioral state <inline-formula id="pcbi.1006168.e011"><alternatives><graphic id="pcbi.1006168.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> in a training session, we can learn the parameters adaptively using <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref>. To enable parameter adaptation and learning, a training session is often used in which the encoded state is measured or inferred. In our motor BMI example, the encoded brain state is the intended velocity and can be either observed or inferred behaviorally using a supervised training session in which subjects perform instructed BMI movements (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>]) as we describe in the Numerical Simulations section. In applications such as motor BMIs, there is typically a second decoder that takes the estimated parameters from <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> to decode the brain state, e.g., the kinematics (<xref ref-type="fig" rid="pcbi.1006168.g001">Fig 1</xref>; see Appendix B in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>). However, this brain state decoder does not affect the parameter decoder [<xref ref-type="bibr" rid="pcbi.1006168.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]. We discuss the simulation details later in the section.</p>
</sec>
<sec id="sec005">
<title>Overview of the two objectives for the calibration algorithm</title>
<p>We define <bold><italic>ψ</italic></bold>* as the unknown true value of the parameters <bold><italic>ψ</italic></bold> to be learned. Under mild conditions given in Appendix C in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>, which are satisfied in our problem setup, <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> is an asymptotically unbiased estimator (lim<sub><italic>t</italic> → ∞</sub> <italic>E</italic>[<bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>] = <bold><italic>ψ</italic></bold>*). There are two objectives that the calibration algorithm can be designed for. First, we can minimize the convergence time—defined as the time it takes for the difference (<bold><italic>ψ</italic></bold>*−<italic>E</italic>[<bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>]) to converge to <bold>0</bold>—subject to an upper-bound constraint on the steady-state error covariance of the estimated parameters. Second, we can minimize the steady-state error covariance of the estimated parameters, i.e., the Euclidean 2-norm ‖<italic>Cov</italic>[<bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>]‖, while keeping the convergence time below a desired upper-bound. We derive the calibration algorithm for each of these objectives and provide them in Theorems 1 and 2.</p>
</sec>
<sec id="sec006">
<title>Calibration algorithm: Analytical functions to predict the effect of learning rate on parameter error and convergence time</title>
<p>Regardless of the objective, to derive the calibration algorithm we first need to write the error dynamics in terms of the learning rate <italic>s</italic>. We denote the estimation error by <bold>g</bold><sub><italic>t</italic></sub> = <bold><italic>ψ</italic></bold>*−<bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>. We denote the estimation error covariance at time <italic>t</italic> by <inline-formula id="pcbi.1006168.e012"><alternatives><graphic id="pcbi.1006168.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">g</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> since <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> is asymptotically unbiased by Appendix C in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. We denote the limit of <inline-formula id="pcbi.1006168.e013"><alternatives><graphic id="pcbi.1006168.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> in time, which is the steady-state error covariance, by <inline-formula id="pcbi.1006168.e014"><alternatives><graphic id="pcbi.1006168.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>. Our goal is to express the steady-state error covariance <inline-formula id="pcbi.1006168.e015"><alternatives><graphic id="pcbi.1006168.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and the convergence time of <italic>E</italic>[<bold>g</bold><sub><italic>t</italic></sub>] as functions of the learning rate <italic>s</italic>.</p>
<p>To find the steady-state error covariance <inline-formula id="pcbi.1006168.e016"><alternatives><graphic id="pcbi.1006168.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> as a function of <italic>s</italic>, we first derive a recursive equation to compute <inline-formula id="pcbi.1006168.e017"><alternatives><graphic id="pcbi.1006168.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> as a function of the learning rate. By solving this recursive equation and taking the limit as <italic>t</italic> → ∞ with some approximations, we express <inline-formula id="pcbi.1006168.e018"><alternatives><graphic id="pcbi.1006168.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> as a function of the learning rate <italic>s</italic>. Similarly, by finding a recursive equation for <italic>E</italic>[<bold>g</bold><sub><italic>t</italic></sub>] as a function of <italic>s</italic> and solving it using an approximation, we express the convergence time of E[<bold>g</bold><sub><italic>t</italic></sub>] as a function of the learning rate <italic>s</italic>. To make the derivation rigorous, we assume that the encoded behavioral state <bold>v</bold><sub><italic>t</italic></sub> during the training session (i.e., the experimental session in which parameters are being learned adaptively) is periodic with period <italic>T</italic>. This holds in many cases, for example in motor BMIs in which the training session involves making periodic center-out-and-back movements [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]. We will show later that even in cases where the behavioral state is not periodic, our derivations of the steady-state error covariance as a function of the learning rate allow for accurate calibration to achieve the desired objectives. The derivations are lengthy and are thus provided in Appendix D in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. Also a detailed explanation of why the periodicity assumption is used in rigorous derivations, and why the approach still extends to non-periodic cases is provided in Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. Below we present the conclusions of the derivations in the following theorem. This theorem is the basis for the calibration algorithm in the case of adaptive KF for continuous neural signal modalities.</p>
<p><bold>Theorem 1</bold>. <italic>Assume that the encoded state</italic> <bold>v</bold><sub><italic>t</italic></sub> <italic>in</italic> <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> <italic>is periodic with period T. We define</italic> <inline-formula id="pcbi.1006168.e019"><alternatives><graphic id="pcbi.1006168.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msup><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> <italic>and write its eigenvalue decomposition as</italic> <bold>H</bold><sub><italic>ave</italic></sub> = <bold>U</bold> <italic>diag</italic>(<italic>h</italic><sub>1</sub>, …, <italic>h</italic><sub><italic>n</italic></sub>)<bold>U</bold>′ <italic>with</italic> (0 &lt; <italic>h</italic><sub><italic>i</italic></sub> ≤ <italic>h</italic><sub><italic>i</italic>+ 1</sub>). <italic>We also define</italic> <disp-formula id="pcbi.1006168.e020"><alternatives><graphic id="pcbi.1006168.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow></mml:msqrt> <mml:mo>-</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p><italic>The steady-state error covariance</italic>, <inline-formula id="pcbi.1006168.e021"><alternatives><graphic id="pcbi.1006168.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, <italic>can be expressed as a function of the learning rate s as</italic> <disp-formula id="pcbi.1006168.e022"><alternatives><graphic id="pcbi.1006168.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula> <italic>where</italic> <inline-formula id="pcbi.1006168.e023"><alternatives><graphic id="pcbi.1006168.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow> <mml:mi>s</mml:mi></mml:mfrac></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> <italic>is monotonically increasing with respect to s</italic>.</p>
<p><italic>The convergence dynamics of the expected error</italic> E[<bold>g</bold><sub><italic>t</italic></sub>] <italic>can be expressed as a function of the learning rate s as</italic> <disp-formula id="pcbi.1006168.e024"><alternatives><graphic id="pcbi.1006168.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mfrac><mml:msub><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>×</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula> <italic>where</italic> <inline-formula id="pcbi.1006168.e025"><alternatives><graphic id="pcbi.1006168.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mfrac><mml:msub><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>κ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow></mml:msqrt> <mml:mo>-</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> <italic>is monotonically decreasing with respect to s</italic>.</p>
<p><italic>From</italic> <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref>, <italic>the behavior of the expectation of the estimation error</italic> E[<bold>g</bold><sub><italic>t</italic></sub>] <italic>across time is dominated by the largest diagonal term</italic>, <inline-formula id="pcbi.1006168.e026"><alternatives><graphic id="pcbi.1006168.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mfrac><mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:msub><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, <italic>whose inverse we define as the convergence rate</italic>.</p>
<p>Since <bold>U</bold> is the unitary matrix of the eigenvalue decomposition of <bold>H</bold><sub><italic>ave</italic></sub>, which is not related to <italic>s</italic>, <bold>U</bold> is independent of the learning rate <italic>s</italic> and the diagonal terms of <inline-formula id="pcbi.1006168.e027"><alternatives><graphic id="pcbi.1006168.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> are strictly increasing functions of <italic>s</italic>. This is intuitively sound since a higher learning rate results in a larger error covariance at steady state. Also, the inverse of convergence rate in <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> is monotonically decreasing with respect to <italic>s</italic>. This monotonically decreasing relationship is also intuitively sound: a faster convergence rate requires a larger learning rate. These relationships clearly show the trade-off between the steady-state error covariance <inline-formula id="pcbi.1006168.e028"><alternatives><graphic id="pcbi.1006168.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and the convergence time. All these properties will be confirmed in the Results section. Finally note that computing <bold>H</bold><sub><italic>ave</italic></sub> does not require complete knowledge of <inline-formula id="pcbi.1006168.e029"><alternatives><graphic id="pcbi.1006168.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> but simply the expectation (average) of a function of <inline-formula id="pcbi.1006168.e030"><alternatives><graphic id="pcbi.1006168.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> (e.g., simply knowing what the supervised training trajectories look like on average rather than exactly knowing the trajectories.)</p>
<p>Now that we have an analytical expression for the steady-state error covariance and the convergence rate as functions of the learning rate <italic>s</italic> (<xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref>, respectively), all we need to do is to find the inverse of these functions to solve for the optimal learning rate <italic>s</italic> from a given upper-bound on <inline-formula id="pcbi.1006168.e031"><alternatives><graphic id="pcbi.1006168.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> or on the convergence time.</p>
</sec>
<sec id="sec007">
<title>Calibration algorithm: The inverse functions to compute the learning rate</title>
<p>We now derive the inverse functions of Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> to compute the optimal learning rate <italic>s</italic> for each of the two objectives in the calibration algorithm. To derive the inverse function for computing the learning rate corresponding to a given steady-state error covariance, we formulate the optimization problem as that of calculating the largest learning rate <italic>s</italic> that satisfies <inline-formula id="pcbi.1006168.e032"><alternatives><graphic id="pcbi.1006168.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mrow><mml:mo>∥</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∥</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">lim</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>∥</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>V</italic><sub><italic>bd</italic></sub> is the desired upper-bound on the steady-state error covariance. We want the largest learning rate that satisfies this relationship because the convergence time is a decreasing function of the learning rate and hence will benefit from larger rates. The key step in solving this inequality is observing that the 2-norm <inline-formula id="pcbi.1006168.e033"><alternatives><graphic id="pcbi.1006168.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mrow><mml:mo>∥</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∥</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">lim</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>∥</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the largest singular value of <inline-formula id="pcbi.1006168.e034"><alternatives><graphic id="pcbi.1006168.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, which is also the largest eigenvalue of <inline-formula id="pcbi.1006168.e035"><alternatives><graphic id="pcbi.1006168.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> due to its positive definite property. Since the eigenvalues of <inline-formula id="pcbi.1006168.e036"><alternatives><graphic id="pcbi.1006168.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> are analytic functions of the learning rate in Theorem 1, we can solve the inequality analytically. The details of this derivation are shown in Appendix F in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>.</p>
<p>For the learning rate optimization to satisfy a given convergence time upper-bound, the goal is to calculate the smallest learning rate <italic>s</italic> that makes <inline-formula id="pcbi.1006168.e037"><alternatives><graphic id="pcbi.1006168.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow></mml:mfrac> <mml:mo>≤</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> within the given time <italic>C</italic><sub><italic>bd</italic></sub>, where <italic>C</italic><sub><italic>bd</italic></sub> is the upper-bound of the convergence time and <italic>E</italic><sub><italic>rest</italic></sub> is the relative estimation error (e.g., 5%) at which point we consider the parameters to have reached steady state. We want the smallest learning rate that satisfies the convergence time constraint because the steady-state error decreases with smaller learning rates. The key in solving this inequality is noting that ‖E[<bold>g</bold><sub><italic>t</italic></sub>]‖ converges exponentially with the inverse convergence rate defined in Theorem 1. So <inline-formula id="pcbi.1006168.e038"><alternatives><graphic id="pcbi.1006168.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="normal">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">g</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> can be written as a function of the learning rate <italic>s</italic> explicitly. The derivation details are in Appendix F in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>.</p>
<p>We provide the conclusions of the above derivations resulting in the inverse functions for both objectives in the following theorem:</p>
<p><bold>Theorem 2</bold> <italic>Calibration objective 1 to constrain steady-state error: Assume that the time-step (i.e., sampling time) in</italic> <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> <italic>is</italic> Δ <italic>seconds and h</italic><sub>1</sub> <italic>is the smallest eigenvalue of</italic> <bold>H</bold><sub><italic>ave</italic></sub> <italic>defined in Theorem 1. The optimal learning rate to achieve an upper-bound V<sub>bd</sub> on the steady-state error covariance while allowing for the fastest convergence time is given by</italic> <disp-formula id="pcbi.1006168.e039"><alternatives><graphic id="pcbi.1006168.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>V</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>d</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>h</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>V</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>d</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>&gt;</mml:mo> <mml:msubsup><mml:mi>h</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p><italic>Calibration objective 2 to constrain convergence time: Define</italic> <inline-formula id="pcbi.1006168.e040">
<alternatives>
<graphic id="pcbi.1006168.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e040" xlink:type="simple"/>
<mml:math display="inline" id="M40">
<mml:mrow>
<mml:msub>
<mml:mi>C</mml:mi>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>m</mml:mi>
<mml:mi>e</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mn>4</mml:mn>
<mml:msub>
<mml:mi>h</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:mo>×</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mfrac>
<mml:mo>Δ</mml:mo>
<mml:msub>
<mml:mi>C</mml:mi>
<mml:mrow>
<mml:mi>b</mml:mi>
<mml:mi>d</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mfrac>
</mml:msup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>, <italic>which is independent of the learning rate s. The optimal learning rate to achieve an upper-bound C<sub>bd</sub> on the convergence time, defined to be the time-point at which the relative parameter error is E<sub>rest</sub>, is given by</italic> <disp-formula id="pcbi.1006168.e041"><alternatives><graphic id="pcbi.1006168.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mn>4</mml:mn> <mml:msubsup><mml:mi>h</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>×</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>4</mml:mn> <mml:msub><mml:mi>h</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula></p>
<p>To summarize, if the objective is to bound the steady-state error covariance, then the user will select the upper-bound <italic>V</italic><sub><italic>bd</italic></sub>, calculate <bold>H</bold><sub><italic>ave</italic></sub> defined in Theorem 1, and apply <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> to find the optimal learning rate <italic>s</italic>. If the objective is to bound the convergence time, the user will select the upper-bound <italic>C</italic><sub><italic>bd</italic></sub>, what percentage of error at convergence time they are willing to tolerate <italic>E</italic><sub><italic>rest</italic></sub>, calculate <bold>H</bold><sub><italic>ave</italic></sub>, and use <xref ref-type="disp-formula" rid="pcbi.1006168.e041">(10)</xref> to find the optimal learning rate <italic>s</italic>.</p>
</sec>
<sec id="sec008">
<title>Concurrent estimation of the noise variance <italic>Z</italic></title>
<p>So far we have assumed that the observation noise variance, <italic>Z</italic>, in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> is known (for example through offline learning). However, this variance may need to be estimated online just like the encoding parameters <bold><italic>ψ</italic></bold>. We can address this scenario by using our knowledge of the range of possible <italic>Z</italic>’s, i.e., (<italic>Z</italic><sub><italic>min</italic></sub> and <italic>Z</italic><sub><italic>max</italic></sub>) and use the calibration algorithm to compute the learning rate for both <italic>Z</italic><sub><italic>min</italic></sub> and <italic>Z</italic><sub><italic>max</italic></sub>. Then for the first calibration objective, we can select the smaller of the two <italic>s</italic>’s corresponding to <italic>Z</italic><sub><italic>min</italic></sub> and <italic>Z</italic><sub><italic>max</italic></sub>. This smaller <italic>s</italic> gives the most conservative choice to assure a given upper-bound for the steady-state error covariance. Similarly, for the second calibration objective, we can select the larger of the two <italic>s</italic>’s to assure a given upper-bound on the convergence time. This method is valid since the learning rate is a monotonic function of <italic>Z</italic>. We can see this by noting that <bold>H</bold><sub><italic>ave</italic></sub> in Theorem 1 is monotonic with respect to <italic>Z</italic>, and so are its eigenvalues (<italic>h</italic><sub>1</sub>, …, <italic>h</italic><sub><italic>n</italic></sub>). From <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e041">(10)</xref>, the learning rate <italic>s</italic> is also a monotonic function of <italic>h</italic><sub>1</sub>. Together, these imply that the learning rate is a monotonic function of <italic>Z</italic>.</p>
<p>Finally, to adaptively estimate <italic>Z</italic> in real time, we can use the covariance matching technique [<xref ref-type="bibr" rid="pcbi.1006168.ref071">71</xref>]. Denoting <inline-formula id="pcbi.1006168.e042"><alternatives><graphic id="pcbi.1006168.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi>q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, we can estimate <italic>Z</italic> adaptively by adding the following equation to the recursions in <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref>:
<disp-formula id="pcbi.1006168.e043"><alternatives><graphic id="pcbi.1006168.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Z</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>q</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>L</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>|</mml:mo> <mml:mi>j</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <inline-formula id="pcbi.1006168.e044"><alternatives><graphic id="pcbi.1006168.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>L</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup> <mml:msub><mml:mi>q</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the sample mean, and <italic>L</italic> is the number of samples used in estimating <italic>Z</italic>. Here <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref> is derived using the covariance matching technique. The derivation detail can be found in [<xref ref-type="bibr" rid="pcbi.1006168.ref071">71</xref>]. Since <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref> only uses the prediction mean <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic>−1</sub> and the prediction covariance <bold>S</bold><sub><italic>t</italic>|<italic>t</italic>−1</sub>, we use <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref> right after the prediction step of the KF. This means that we run the KF by first calculating the predictions using <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e008">(4)</xref>, then estimating <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> using <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref>, and finally substituting <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> for <italic>Z</italic> in <xref ref-type="disp-formula" rid="pcbi.1006168.e009">(5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> to get the updated parameters <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>.</p>
</sec>
</sec>
<sec id="sec009">
<title>The calibration algorithm for discrete-valued spikes</title>
<p>We now follow the same formulation used for continuous-valued signals, such as LFP or ECoG, to derive the calibration algorithm for the discrete-valued spiking activity. The derivation follows similar steps but, due to the nonlinearity in the observation model, has some differences that we point out. Given the nonlinearities, in this case, the calibration algorithm can be derived for the main first objective, i.e., to keep the steady-state error covariance below a desired upper-bound while minimizing convergence time (<xref ref-type="fig" rid="pcbi.1006168.g002">Fig 2</xref>; see <xref ref-type="sec" rid="sec021">Discussions</xref>).</p>
<sec id="sec010">
<title>Adaptive PPF</title>
<p>The spiking activity can be modeled as a time-series of 0’s and 1’s, representing the lack or presence of spikes in consecutive time-steps, respectively. This discrete-time binary time-series can be modeled as a point process [<xref ref-type="bibr" rid="pcbi.1006168.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref076">76</xref>]. A point process is specified by its instantaneous rate function. Prior work have used generalized linear models (GLM) to model the firing rate as a log-linear function of the encoded state <bold>v</bold><sub><italic>t</italic></sub> [<xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref074">74</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>], e.g., the intended velocity in a motor BMI [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>]. Denoting the binary spike event of neuron <italic>c</italic> at time <italic>t</italic> by <inline-formula id="pcbi.1006168.e045"><alternatives><graphic id="pcbi.1006168.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, and the time-step by Δ as before, the point process likelihood function is given by [<xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>]
<disp-formula id="pcbi.1006168.e046"><alternatives><graphic id="pcbi.1006168.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mo>λ</mml:mo> <mml:mi>c</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup></mml:msup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msup><mml:mo>λ</mml:mo> <mml:mi>c</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
The above equation constitutes the neural encoding model for discrete spiking activity; here λ<sup><italic>c</italic></sup>(⋅) is the firing rate of neuron <italic>c</italic> and is taken as
<disp-formula id="pcbi.1006168.e047"><alternatives><graphic id="pcbi.1006168.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mo>λ</mml:mo> <mml:mi>c</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow> <mml:mi>c</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>ϕ</italic><sup><italic>c</italic></sup> = [<italic>β</italic><sup><italic>c</italic></sup>, (<italic>α</italic><sup><italic>c</italic></sup>)′]′ are the encoding model parameters to be learned. Note that the normalization constant in <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref> is approximately 1 because the time-bin Δ in the discrete-time point process for spikes is taken to be small enough to at most contain one spike as shown in [<xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>]. Thus for a small Δ, the probability of having 2 or more spikes, i.e., <inline-formula id="pcbi.1006168.e048"><alternatives><graphic id="pcbi.1006168.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup> <mml:mo>≥</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is negligibly small and can be ignored. So <inline-formula id="pcbi.1006168.e049"><alternatives><graphic id="pcbi.1006168.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>t</mml:mi> <mml:mi>c</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> can only be either 0 or 1 and the normalization constant for 0 or 1 spikes is exactly 1. The details of this approximation can be found in [<xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>].</p>
<p>For spikes, a PPF can estimate the parameters using data in a training session in which the encoded state can be either observed or inferred [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref077">77</xref>]. For example, adaptive PPF has been used to track neural plasticity in the rat hippocampus [<xref ref-type="bibr" rid="pcbi.1006168.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref077">77</xref>]. For motor BMIs, a closed-loop adaptive PPF has been developed to learn <italic>ϕ</italic><sup><italic>c</italic></sup> using an optimal feedback-control model to infer the intended velocity, resulting in fast and robust parameter convergence [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>]. As in the adaptive KF case, the adaptive PPF assumes that all neurons are conditionally independent so every <italic>ϕ</italic><sup><italic>c</italic></sup> can be updated separately [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref077">77</xref>] (see <xref ref-type="sec" rid="sec021">Discussions</xref>). From now on, we remove the superscript <italic>c</italic> for convenience. Denote the true unknown value of <italic>ϕ</italic> by <italic>ϕ</italic>*. We model our uncertainty about <italic>ϕ</italic> in time as a random-walk [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]
<disp-formula id="pcbi.1006168.e050"><alternatives><graphic id="pcbi.1006168.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where <bold>q</bold><sub><italic>t</italic></sub> is a white Gaussian noise with covariance matrix <bold>Q</bold> = <italic>r</italic><bold>I</bold><sub><italic>n</italic></sub>(<italic>r</italic> &gt; 0) and <italic>r</italic> is the learning rate here. Note that similar to the case of KF, <bold>q</bold><sub><italic>t</italic></sub> is simply used to model our uncertainty at time <italic>t</italic> about the unknown parameter <italic>ϕ</italic> and thus is not representing a biophysical noise. Consequently, the covariance parameter <italic>r</italic> is not a biophysical parameter to be learned but is a design choice that controls how fast neural encoding model parameters <italic>ϕ</italic> are learned. Thus <italic>r</italic> serves as the learning rate as shown in detail in Appendix A in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. Similar to the KF, the PPF has already been shown to be successful in estimating unknown fixed parameters in neurotechnologies [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>].</p>
<p>Given the observation model in <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref> and the prior model in <xref ref-type="disp-formula" rid="pcbi.1006168.e050">(14)</xref>, adaptive PPF is derived using the Laplace approximation, which assumes that the posterior density is Gaussian. Denoting the posterior and prediction means by <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub> and <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic>−1</sub>, and their covariances by <bold>Q</bold><sub><italic>t</italic>|<italic>t</italic></sub> and <bold>Q</bold><sub><italic>t</italic>|<italic>t</italic>−1</sub>, respectively, the adaptive PPF—derived using the Laplace Gaussian approximation to the posterior density—is given by the following recursions [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]
<disp-formula id="pcbi.1006168.e051"><alternatives><graphic id="pcbi.1006168.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula> <disp-formula id="pcbi.1006168.e052"><alternatives><graphic id="pcbi.1006168.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e052" xlink:type="simple"/><mml:math display="block" id="M52"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula> <disp-formula id="pcbi.1006168.e053"><alternatives><graphic id="pcbi.1006168.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula> <disp-formula id="pcbi.1006168.e054"><alternatives><graphic id="pcbi.1006168.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
Similar to <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> in the KF, <bold>Q</bold><sub><italic>t</italic>|<italic>t</italic></sub> determines the relative weight of the neural observation <italic>N</italic><sub><italic>t</italic></sub> compared with the previous parameter estimate in updating the current parameter estimate and thus determines how fast <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub> is learned in <xref ref-type="disp-formula" rid="pcbi.1006168.e054">(18)</xref>. Because <bold>Q</bold><sub><italic>t</italic>|<italic>t</italic></sub> is governed by <italic>r</italic>, which is in our control, we refer to <italic>r</italic> as the learning rate for the PPF. As <italic>r</italic> increases, parameters are updated faster. Details are provided in Appendix A in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. Here <inline-formula id="pcbi.1006168.e055"><alternatives><graphic id="pcbi.1006168.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and as before <inline-formula id="pcbi.1006168.e056"><alternatives><graphic id="pcbi.1006168.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where <bold>v</bold><sub><italic>t</italic></sub> is the encoded behavioral/brain state (e.g., rat position in a maze or intended velocity in BMI), which is either observed or inferred. In studying the hippocampal place cell plasticity, for example, rat position can be observed. In motor BMIs, the intended velocity can be inferred using a supervised training session in which subjects perform instructed BMI movements [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>] as we present in the Numerical Simulations section. We now derive a calibration algorithm for the learning rate <italic>r</italic> in the adaptive PPF <xref ref-type="disp-formula" rid="pcbi.1006168.e051">(15)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e054">(18)</xref>. The calibration algorithm minimizes the estimated parameter convergence time of <italic>E</italic>[<italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub>]→<italic>ϕ</italic>* under a given upper-bound constraint on the steady-state error covariance‖<italic>Cov</italic>[<italic>ϕ</italic>* − <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub>]‖.</p>
</sec>
<sec id="sec011">
<title>Calibration algorithm: Analytical function and inverse function</title>
<p>Learning rate calibration for spikes can again be posed as an optimization problem. We denote the error vector by <bold>e</bold><sub><italic>t</italic></sub> = <italic>ϕ</italic>* − <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub> and the error covariance by <inline-formula id="pcbi.1006168.e057"><alternatives><graphic id="pcbi.1006168.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">e</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. We can show that <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub>, which is PPF’s estimate of the parameters, is asymptotically unbiased (lim<sub><italic>t</italic> → ∞</sub> <italic>E</italic>[<italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub>] = <italic>ϕ</italic>*) under some mild conditions (Appendix G in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>). We define the steady-state error covariance as <inline-formula id="pcbi.1006168.e058"><alternatives><graphic id="pcbi.1006168.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">lim</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Thus the goal of the optimization problem is to select the optimal learning rate <italic>r</italic> that minimizes the convergence time of <italic>E</italic>[<bold>e</bold><sub><italic>t</italic></sub>]→<bold>0</bold> while keeping the 2-norm of the steady-state error covariance <inline-formula id="pcbi.1006168.e059"><alternatives><graphic id="pcbi.1006168.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> smaller than the user-defined upper-bound.</p>
<p>We derive the calibration algorithm similar to the case of continuous signals. We first find a recursive equation for <inline-formula id="pcbi.1006168.e060"><alternatives><graphic id="pcbi.1006168.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> using <xref ref-type="disp-formula" rid="pcbi.1006168.e051">(15)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e054">(18)</xref>. We then solve this equation and take the limit <italic>t</italic> → ∞ with some approximations to write the steady-state error covariance <inline-formula id="pcbi.1006168.e061"><alternatives><graphic id="pcbi.1006168.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> as an analytic function of the learning rate <italic>r</italic>. For rigorousness in derivations, for now we assume that the behavioral state in the training set, e.g., the intended velocity {<bold>v</bold><sub><italic>t</italic></sub>}, is periodic with period <italic>T</italic>. As we also mentioned in the case of continuous signals, this assumption is reasonable in many applications such as motor BMI. However, we will show in the Results section that the calibration algorithm still works even when this assumption is violated. Also in Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref> we show why the approach also extends to non-periodic cases. The derivation detail is presented in Appendix H in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. The derivation shows that the steady-state error covariance <inline-formula id="pcbi.1006168.e062"><alternatives><graphic id="pcbi.1006168.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> can be written as a function of the learning rate <italic>r</italic> as follows:</p>
<p><bold>Theorem 3</bold>. <italic>Assume the encoded state</italic> <bold>v</bold><sub><italic>t</italic></sub> <italic>in</italic> <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref> <italic>is periodic with period T. We write the eigenvalue decomposition of</italic> <inline-formula id="pcbi.1006168.e063"><alternatives><graphic id="pcbi.1006168.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>λ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> <italic>as</italic> <bold>U</bold> <italic>diag</italic>(<italic>a</italic><sub>1</sub>, …, <italic>a</italic><sub><italic>n</italic></sub>)<bold>U</bold>′ <italic>with</italic> (0 &lt; <italic>a</italic><sub><italic>i</italic></sub> ≤ <italic>a</italic><sub><italic>i</italic>+ 1</sub>) <italic>and we denote</italic> <disp-formula id="pcbi.1006168.e064"><alternatives><graphic id="pcbi.1006168.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e064" xlink:type="simple"/><mml:math display="block" id="M64"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>b</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn> <mml:msub><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>r</mml:mi></mml:mrow></mml:msqrt> <mml:mo>-</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <italic>The steady-state error covariance</italic>, <inline-formula id="pcbi.1006168.e065"><alternatives><graphic id="pcbi.1006168.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, <italic>can be expressed as a function of the learning rate r as</italic> <disp-formula id="pcbi.1006168.e066"><alternatives><graphic id="pcbi.1006168.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>b</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>b</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>b</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mspace width="0.277778em"/><mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>Compared with the steady-state error covariance <inline-formula id="pcbi.1006168.e067"><alternatives><graphic id="pcbi.1006168.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:msubsup><mml:mi mathvariant="bold">S</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> for continuous signals in <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref>, the steady-state error covariance for spikes <inline-formula id="pcbi.1006168.e068"><alternatives><graphic id="pcbi.1006168.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi> <mml:mo>+</mml:mo> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref> has exactly the same form when replacing <italic>h</italic><sub><italic>i</italic></sub> with <italic>a</italic><sub><italic>i</italic></sub> and <italic>s</italic> with <italic>r</italic>. Hence to compute the optimal learning rate <italic>r</italic> from <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref>, we can again apply <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> while replacing <italic>h</italic><sub><italic>i</italic></sub> with <italic>a</italic><sub><italic>i</italic></sub> and <italic>s</italic> with <italic>r</italic>. Note that <bold>M</bold><sub><italic>ave</italic></sub> includes the firing rate λ(<italic>t</italic>|<italic>ϕ</italic>*), which is related to the unknown true parameter <italic>ϕ</italic>*. Since λ(<italic>t</italic>|<italic>ϕ</italic>*)Δ in <bold>M</bold><sub><italic>ave</italic></sub> has the same role as <italic>Z</italic><sup>−1</sup> in <bold>H</bold><sub><italic>ave</italic></sub> for KF, and since <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref> has the same form as <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref>, the learning rate <italic>r</italic> is a monotonic function of λ(<italic>t</italic>|<italic>ϕ</italic>*)Δ similar to the case of <italic>Z</italic> for KF. Thus we use our knowledge of the minimum and maximum possible firing rates to calculate the extreme values of the learning rate <italic>r</italic> from <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref>, and select the minimum of the two <italic>r</italic>’s as the most conservative value to keep the steady-state error covariance under the given bound <italic>V</italic><sub><italic>bd</italic></sub>.</p>
</sec>
</sec>
<sec id="sec012">
<title>Calibration algorithm for non-periodic state evolution</title>
<p>For both discrete and continuous signals, we considered a periodic behavioral state (e.g., intended velocity) in the training data for the derivations to satisfy the mild conditions in Appendix C in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>. However, the derivation of <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref>, <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref> are based on <bold>H</bold><sub><italic>ave</italic></sub> and <bold>M</bold><sub><italic>ave</italic></sub> for the continuous and discrete signals, respectively, which are simply the average values of functions of the state {<bold>v</bold><sub><italic>t</italic></sub>}. So the core information needed in the calibration algorithm is not the state periodicity, but its expected value, which we can compute empirically for any state evolution. As detailed in Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>, the periodicity of <bold>v</bold><sub><italic>t</italic></sub> is simply required to ensure that the mean of the prediction covariance <bold>S</bold><sub><italic>t</italic>+ 1|<italic>t</italic></sub> is well-defined at steady state. If we ignore some mathematical rigorousness and instead assume that <bold>S</bold><sub><italic>t</italic>+ 1|<italic>t</italic></sub> has bounded steady-state moments (which is a relatively mild requirement), then this calibration algorithm can be generalized to the case with non-periodic <bold>v</bold><sub><italic>t</italic></sub> directly. That is precisely why, as we show using simulations in the Results section, the calibration algorithm works even in the case of random evolution for the states {<bold>v</bold><sub><italic>t</italic></sub>} in the training experiment. Periodicity is simply required to guarantee the <italic>existence</italic> of the mean of <bold>S</bold><sub><italic>t</italic>+ 1|<italic>t</italic></sub> at steady state (instead of assuming this existence) in the derivations, as detailed in Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>.</p>
</sec>
<sec id="sec013">
<title>Numerical simulations</title>
<p>To validate the calibration algorithm, we run extensive closed-loop numerical simulations. We show that the calibration algorithm allows for fast and precise learning of encoding model parameters, and subsequently for a desired transient and steady-state behavior of the decoders (<xref ref-type="fig" rid="pcbi.1006168.g001">Fig 1</xref>). While the calibration algorithm can be applied to learn encoding models and decoders for any brain state, as a concrete example, we use a motor BMI to validate the algorithm.</p>
<p>In motor BMIs, the relevant brain state is the intended movement. The BMI needs to learn an encoding model that relates the neural activity to the subject’s intended movement. We simulate a closed-loop BMI within a center-out-and-back reaching task with 8 targets. In this task, the subject needs to take a cursor on a computer screen to one of 8 peripheral targets, and then return it to the center to initiate another trial [<xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>]. To simulate how subjects generate a pattern of neural activity to control the cursor, we use an optimal feedback-control (OFC) model of the BMI that has been devised and validated in prior experiments [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>] and is inspired by the OFC models of the natural sensorimotor system [<xref ref-type="bibr" rid="pcbi.1006168.ref078">78</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref080">80</xref>]. We then simulate the spiking activity as a point process using the nonlinear encoding model in <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref> and simulate the ECoG/LFP log-powers as a Gaussian process linearly dependent on the brain state [<xref ref-type="bibr" rid="pcbi.1006168.ref055">55</xref>] using the linear encoding model in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref>. We test the calibration algorithm for adaptive learning of the ECoG/LFP and the spike model parameters. We assess the ability of the calibration algorithm to enable fast and accurate learning of the encoding models, and to lead to a desired transient and steady-state performance of the decoder.</p>
<p>To simulate the intended movement, we use the OFC model. We assume that movement evolves according to a linear dynamical model [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>]
<disp-formula id="pcbi.1006168.e069"><alternatives><graphic id="pcbi.1006168.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e069" xlink:type="simple"/><mml:math display="block" id="M69"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">A</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
where <inline-formula id="pcbi.1006168.e070"><alternatives><graphic id="pcbi.1006168.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">d</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the kinematic state at time <italic>t</italic>, with <bold>d</bold><sub><italic>t</italic></sub> and <bold>v</bold><sub><italic>t</italic></sub> being the position and velocity vectors in the two-dimensional space, respectively. Here <bold>u</bold><sub><italic>t</italic></sub> is the control signal that the brain decides on to move the cursor and <bold>w</bold><sub><italic>t</italic></sub> is white Gaussian noise with covariance matrix <bold>W</bold>. Also, <bold>A</bold> and <bold>B</bold> are coefficient matrices that are often fitted to subjects’ manual movements [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref080">80</xref>]. Similar to prior work [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>], we write <xref ref-type="disp-formula" rid="pcbi.1006168.e069">(20)</xref> as
<disp-formula id="pcbi.1006168.e071"><alternatives><graphic id="pcbi.1006168.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>Δ</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>Δ</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mi>α</mml:mi></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mi>α</mml:mi></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
where Δ is the time-step and <italic>α</italic> is selected according to our prior non-human primate data [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>].</p>
<p>The OFC model assumes that the brain quantifies the task goal within a cost function and decides on its control commands by minimizing this cost. For the center-out movement task, the cost function can be quantified as [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref078">78</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref080">80</xref>]
<disp-formula id="pcbi.1006168.e072"><alternatives><graphic id="pcbi.1006168.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e072" xlink:type="simple"/><mml:math display="block" id="M72"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>J</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo> <mml:msub><mml:mi mathvariant="bold">d</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>v</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>∥</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>∥</mml:mo> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
where <bold>d</bold>* is the target position, and <italic>w</italic><sub><italic>v</italic></sub> and <italic>w</italic><sub><italic>r</italic></sub> are weights selected to fit the profile of manual movements. For the linear dynamics in <xref ref-type="disp-formula" rid="pcbi.1006168.e069">(20)</xref> and the quadratic cost in <xref ref-type="disp-formula" rid="pcbi.1006168.e072">(22)</xref>, the optimal control command is given by the well-known infinite horizon linear quadratic Gaussian (LQG) solution as [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref081">81</xref>]
<disp-formula id="pcbi.1006168.e073"><alternatives><graphic id="pcbi.1006168.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e073" xlink:type="simple"/><mml:math display="block" id="M73"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
where <bold>x</bold>* = [<bold>d</bold>*′, <bold>0</bold>′]′ is the target for position and velocity (as the subject needs to reach the target position and stop there). Here <bold>L</bold> is the gain matrix, which can be found recursively and offline by solving the discrete-time Riccati equation [<xref ref-type="bibr" rid="pcbi.1006168.ref081">81</xref>]. By substituting <xref ref-type="disp-formula" rid="pcbi.1006168.e073">(23)</xref> in <xref ref-type="disp-formula" rid="pcbi.1006168.e069">(20)</xref>, we can compute the intended kinematics of the subject in response to visual feedback of the current decoded cursor kinematics <bold>x</bold><sub><italic>t</italic></sub> in our simulations [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>]. Details are provided in our prior work [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>]. Note that we use a single OFC model to simulate the brain strategy throughout all closed-loop numerical simulations—i.e., both during training experiments in which parameters are being learned in parallel to the kinematics being decoded (<xref ref-type="fig" rid="pcbi.1006168.g001">Fig 1</xref>), or after training is complete and during pure decoding experiments when the learned parameters are fixed and the learned decoder is used to move the cursor. Indeed prior work have suggested that the brain strategy in closed-loop control largely remains consistent, e.g., regardless of whether parameters are being adapted or not (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref083">83</xref>]).</p>
<p>The subject’s intended velocity <bold>v</bold><sub><italic>t</italic></sub> is in turn encoded in neural activity. We first test the performance of the calibration algorithm for continuous ECoG/LFP recordings. We then test this performance for discrete spike recordings.</p>
<p>For the continuous signals, we simulate 30 LFP/ECoG features whose baseline powers and preferred directions in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> are randomly selected in [1, 6] dB and [0, 2<italic>π</italic>], respectively. The modulation depth, ‖<bold><italic>η</italic></bold>‖, in each channel is randomly-selected in [<xref ref-type="bibr" rid="pcbi.1006168.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref010">10</xref>] and the noise variances are randomly-selected in [320, 380]. The initial value, <bold><italic>ψ</italic></bold><sub>0|0</sub>, and the true value, <bold><italic>ψ</italic></bold>*, of each channel are selected randomly and independently. The eight targets are around a circle with radius 0.3. Each trial including the forward and the back movement for a selected target in the center-out-and-back task takes 2 secs. During the training experiment, the subject reaches the targets in the counter-clockwise order repeatedly. To assess whether the calibration algorithm can analytically compute the steady-state error covariance and convergence time for a given learning rate accurately, we simulate 3000 trials under each learning rate considered.</p>
<p>For spikes, we simulate 30 neurons. Here since the state <bold>v</bold><sub><italic>t</italic></sub> is the intended velocity, we can also interpret <xref ref-type="disp-formula" rid="pcbi.1006168.e047">(13)</xref> as a modified cosine-tuning model [<xref ref-type="bibr" rid="pcbi.1006168.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref084">84</xref>] by writing it as
<disp-formula id="pcbi.1006168.e074"><alternatives><graphic id="pcbi.1006168.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e074" xlink:type="simple"/><mml:math display="block" id="M74"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mo>λ</mml:mo> <mml:mi>c</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>∥</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow> <mml:mi>c</mml:mi></mml:msup> <mml:mo>∥</mml:mo></mml:mrow> <mml:mrow><mml:mo>∥</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∥</mml:mo></mml:mrow> <mml:mo form="prefix">cos</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi>θ</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
where <italic>θ</italic><sub><italic>t</italic></sub> is the direction of <bold>v</bold><sub><italic>t</italic></sub>, <italic>θ</italic><sup><italic>c</italic></sup> is the preferred direction of the neuron (or direction of <bold><italic>α</italic></bold><sup><italic>c</italic></sup> = ‖<bold><italic>α</italic></bold><sup><italic>c</italic></sup>‖[cos <italic>θ</italic><sup><italic>c</italic></sup>, sin <italic>θ</italic><sup><italic>c</italic></sup>]′), and finally ‖<bold><italic>α</italic></bold><sup><italic>c</italic></sup>‖ is the modulation depth. For each neuron, we select the baseline firing rate randomly between [4, 10] Hz and the maximum firing rate randomly between [40, 80] Hz. We select each neuron’s preferred direction in <xref ref-type="disp-formula" rid="pcbi.1006168.e074">(24)</xref> randomly between [0, 2<italic>π</italic>]. The task setup is equivalent to the continuous signal case. We simulate 1000 trials for each learning rate considered.</p>
<p>We also examine the effect of the calibration algorithm on kinematic decoding. For continuous signals, we use a KF kinematic decoder as in prior work (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref055">55</xref>]). For the discrete spike signals, we use a PPF kinematic decoder as in prior work (e.g., in real-time BMIs [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>]). Kinematic decoder details have also been provided in Appendix B in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref> for convenience.</p>
</sec>
</sec>
<sec id="sec014" sec-type="results">
<title>Results</title>
<p>We first investigate whether the calibration algorithm can analytically approximate two quantities well: the steady-state error covariance and the convergence time of the encoding model parameters as a function of the learning rate. We do so by running multiple closed-loop BMI simulations with different learning rates. These Monte-Carlo simulations allow us to compute the true value of the two quantities. We then compare these true values with the analytically-computed values from the calibration algorithm. We find that, for both continuous and discrete signals, the calibration algorithm accurately computes its desired quantity (i.e, either the error covariance or the convergence time) for any type of behavioral state trajectory in the training data (i.e., periodic or not). Thus the calibration algorithm can find the optimal learning rate for a desired trade-off between the parameter convergence time and error covariance. We also show how the inverse function can be used to compute the learning rate for a desired trade-off. Moreover, we examine how the calibration algorithm—and consequently the learned encoding model—affects decoding performance. We show that, by finding the optimal learning rate, the calibration algorithm results in fast and accurate decoding. In particular, compared to the optimal rate, larger learning rates could result in inaccurate steady-state decoding performance and smaller learning rates result in slow convergence of the decoding performance.</p>
<sec id="sec015">
<title>The calibration algorithm computes the convergence time and error covariance accurately with continuous signals</title>
<p>We first assess the accuracy of the analytically-computed error covariance and convergence time by the calibration algorithm. As described in detail in Numerical Simulation section, we run a closed-loop BMI simulation in which the subject performs a center-out-and-back task to eight targets in counter-clockwise order. We simulate 30 LFP/ECoG features.</p>
<p>We define the convergence time as the time when the estimated parameters reach within 5% of their true values, i.e., ‖<bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> − <bold><italic>ψ</italic></bold>*‖≤0.05 × ‖<bold><italic>ψ</italic></bold><sub>0|0</sub> − <bold><italic>ψ</italic></bold>*‖ (so <italic>E</italic><sub><italic>rest</italic></sub> = 0.05; as defined before <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub>, <bold><italic>ψ</italic></bold>*, and <bold><italic>ψ</italic></bold><sub>0|0</sub> are the current parameter estimate, the true parameter value, and the initial parameter estimate, respectively.) <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3A</xref> shows the true and the analytically-computed error covariance and convergence time as a function of the learning rate, across a wide range of learning rates. The analytically-computed values are close to the true values. From <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3A</xref>, the average normalized root-mean-squared errors (RMSE) between the true and the analytically-computed values for the convergence time and the steady-state error covariance are 3.6% and 1.6%, respectively (where normalization is done by dividing by the range of possible convergence time and covariance values). <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3A</xref> shows that as the learning rate <italic>s</italic> increases, the error covariance increases and the convergence time decreases. Also, the error covariance is inversely related to the convergence time. These trends also demonstrate the fundamental trade-off between steady-state error covariance and convergence time.</p>
<fig id="pcbi.1006168.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The calibration algorithm accurately computes the steady-state error covariance and convergence time as a function of learning rate for continuous signals.</title>
<p>(A) The analytically-computed and the true error covariance and convergence time of the encoding model parameters (baseline, <bold><italic>η</italic></bold><sub><italic>x</italic></sub>, and <bold><italic>η</italic></bold><sub><italic>y</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref>) for different learning rates <italic>s</italic>, across a wide range of <italic>s</italic>. The top left panel shows the relation between the three quantities. The other three panels are projections of this plot to three planes, showing each of the three pair-wise relationships. All axes are in log scale. True quantities are computed from BMI simulations with periodic center-out-and-back training datasets. The analytically-computed values are obtained by the calibration algorithm according to Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref>. The analytically-computed and true values match tightly across a wide range of learning rates, showing that the calibration algorithm can accurately compute the learning rate for a desired trade-off between steady-state error and convergence time. (B) Adaptive estimation of the unknown observation noise variance using <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref> under different learning rates <italic>s</italic>. The bottom three panels are zoomed-in versions of the top panels to show the transient behavior of the estimated noise variance, which converges to its true value in all cases.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g003" xlink:type="simple"/>
</fig>
<p>In the above analysis, we considered estimating the encoding model parameters <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref>. As derived in <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref>, when the noise variance <italic>Z</italic> in <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> is unknown, we can also estimate this variance in real time and simultaneously with the parameters. We thus repeated our closed-loop BMI simulations, this time simultaneously estimating the noise variance <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> to show that it converges to the true value regardless of the learning rate <italic>s</italic>. <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3B</xref> shows that <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> converges to the true value with all tested learning rates, which cover a large range (5 × 10<sup>−7</sup> to 5 × 10<sup>−3</sup>). Moreover, even when estimating both <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> and the noise variance <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> jointly, the analytically-computed error covariance is still close to the true one (normalized RMSE is 4.5%). Overall, the analytically-computed error covariance is robust to the uncertainty in <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> because <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> converges to the true value at steady state regardless of the learning rate (<xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3B</xref>).</p>
</sec>
<sec id="sec016">
<title>Use of the inverse function to compute the learning rate</title>
<p>Here we show how the inverse functions in Theorem 2 can be used to select the learning rate. In our example, we require the 95% confidence bound of the estimated encoding model parameters (i.e., ±2 standard deviations of error) to be within 10% of their average value. Thus this constraint provides the desired upper-bound on the steady-state error covariance <italic>V</italic><sub><italic>bd</italic></sub>. In general, <italic>V</italic><sub><italic>bd</italic></sub> can be selected in any manner desired by the user. Once <italic>V</italic><sub><italic>bd</italic></sub> is specified, we use <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> and find the optimal value of the learning rate as <italic>s</italic><sub>1</sub> = 5.6 × 10<sup>−5</sup>. Hence the calibration algorithm dictates that the learning rate should be smaller than <italic>s</italic><sub>1</sub> to satisfy the desired error covariance upper-bound.</p>
<p>Let’s now suppose that we want to ensure that the convergence time is within a given range. In our example, we require the estimation error to converge within 7 minutes, where convergence is defined as reaching within 5% of the true value (<italic>E</italic><sub><italic>rest</italic></sub> = 0.05). This constraint sets the upper-bound on the convergence time to be <italic>C</italic><sub><italic>bd</italic></sub> = 7min = 420 sec. The calibration algorithm using <xref ref-type="disp-formula" rid="pcbi.1006168.e041">(10)</xref> dictates that the learning rate needs to be larger than 4.75 × 10<sup>−5</sup>.</p>
<p>Taken together, for the above constraints for error covariance and convergence time, any learning rate 4.75 × 10<sup>−5</sup> &lt; <italic>s</italic> &lt; 5.6 × 10<sup>−5</sup> is admissible. For conciseness and as an illustrative example, we select the learning rate <italic>s</italic> = 5 × 10<sup>−5</sup>, which satisfies both criteria above. In the next section, we examine the effect of this learning rate on the estimated model parameters over time, i.e., on the adaptation profiles (<xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4</xref>).</p>
<fig id="pcbi.1006168.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Parameter adaptation profiles confirm the accuracy of the calibration algorithm with continuous signals.</title>
<p>(A–C) show sample adaptation profiles of the model parameters <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> for different learning rates <italic>s</italic> in ascending order. For each learning rate, the estimated parameters are within the analytically-computed 95% confidence bounds by the calibration algorithm about 96% of the time, demonstrating the accuracy of the calibration algorithm.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec017">
<title>Parameter adaptation profiles confirm the accuracy of the calibration algorithm</title>
<p>We also examined the evolution of the estimated encoding model parameters <bold><italic>ψ</italic></bold><sub><italic>t</italic>|<italic>t</italic></sub> in time, which we refer to as the parameter adaptation profiles. Plotting the adaptation profile provides a direct way of investigating the influence of the learning rate on the estimated encoding model. We plot the adaptation profiles for the optimal learning rate in our example above, i.e., <italic>s</italic> = 5 × 10<sup>−5</sup>. We also show these profiles for a smaller and a larger learning rate (<xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4</xref>). We used these adaptation profiles to further assess the accuracy of the calibration algorithm.</p>
<p>The adaptation profiles confirm the accuracy of the calibration algorithm as expected from <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3A</xref>. We used <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref> to find the steady-state error covariance for each learning rate in <xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4</xref> and consequently to compute the 95% confidence bounds for the parameter estimates (which are equal to ±2 square-root of the analytically-computed error covariance). We then empirically found the percentage of time during which the steady-state parameter estimates were within this 95% bound. If the covariance matrix is accurately computed by the calibration algorithm, then this percentage should be close to 95%. We found that about 96% of the time, the steady-state estimated parameters lie within the 95% confidence bound calculated by the calibration algorithm for all learning rates. Finally, we also simulated the case where the parameters may shift from day to day (see <xref ref-type="sec" rid="sec021">Discussions</xref>) to see the application of the calibration algorithm in this case. We confirmed, as shown in <xref ref-type="supplementary-material" rid="pcbi.1006168.s001">S1 Fig</xref>, that the same KF with a learning rate calculated from the calibration algorithm (<xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4B</xref>) can track the parameters and satisfy the criteria on steady-state error and convergence time on both days.</p>
</sec>
<sec id="sec018">
<title>The calibration algorithm generalizes to different state evolution profiles</title>
<p>In the algorithm derivation and for rigorousness to ensure the existence of the mean of the prediction covariance <bold>S</bold><sub><italic>t</italic>+ 1|<italic>t</italic></sub> at steady state (instead of simply assuming this existence; Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>), we assume that the evolution of behavioral state {<bold>v</bold><sub><italic>t</italic></sub>}, e.g., the trajectory, is periodic in the training data. However, in computing the error covariance and the convergence time, the only aspect of <bold>v</bold><sub><italic>t</italic></sub> needed by the calibration algorithm is not periodicity, but an average of a function of <bold>v</bold><sub><italic>t</italic></sub> over time, which is <bold>H</bold><sub><italic>ave</italic></sub>. Indeed, if we assume <bold>S</bold><sub><italic>t</italic>+ 1|<italic>t</italic></sub> has bounded steady-state moments, then our derivation directly applies to the general non-periodic case (Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006168.s002">S2 Fig</xref>). To show that the calibration algorithm also extends to the case of non-periodic state evolutions, we run a closed-loop BMI simulation with a non-periodic trajectory. In this simulation, in each trial, one of eight targets is instructed randomly according to a uniform distribution over the targets. So the trajectory is no longer periodic (in contrast to when the targets are instructed one by one and in counter-clockwise order). The comparison between the true error covariance and convergence time and their values computed analytically by the calibration algorithm are shown in <xref ref-type="fig" rid="pcbi.1006168.g005">Fig 5A</xref>, across a wide range of learning rates. The analytically-computed values are still close to the true values, with an average normalized RMSE of 2.1% and 7.4% for the steady-state error covariance and the convergence time, respectively. Similarly, when the noise variance <italic>Z</italic> needs to be estimated, its estimate <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> from <xref ref-type="disp-formula" rid="pcbi.1006168.e043">(11)</xref> still converges to the true value for all learning rates (<xref ref-type="fig" rid="pcbi.1006168.g005">Fig 5B</xref>). Even when estimating <italic>Z</italic><sub><italic>t</italic>|<italic>t</italic></sub> simultaneously with parameters, the calibration algorithm can approximate the error covariance well (normalized RMSE is 2.6%). Taken together, these results demonstrate that the calibration algorithm can generalize to a wide range of problems since the training state-evolution when adapting the encoding models could have a general form.</p>
<fig id="pcbi.1006168.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The calibration algorithm generalizes to training datasets with non-periodic state trajectories.</title>
<p>Figure convention is the same as <xref ref-type="fig" rid="pcbi.1006168.g003">Fig 3</xref>. Here the true quantities are computed in closed-loop BMI simulations with a non-periodic trajectory generated by selecting targets randomly and uniformly. The analytically-computed error covariance and convergence times given by the calibration algorithm closely match their true values across a wide range of the learning rate <italic>s</italic>, showing that the calibration algorithm extends across training datasets with different state-evolution trajectories.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec019">
<title>The calibration algorithm for discrete spiking activity</title>
<p>We also validate the calibration algorithm for discrete-valued spiking observations. We run multiple closed-loop BMI simulations with either a periodic or a non-periodic trajectory. The simulation setting is the same as that for continuous signals and given in Numerical Simulation section. <xref ref-type="fig" rid="pcbi.1006168.g006">Fig 6</xref> shows that the analytically-computed error covariance is close to its true value across a wide range of learning rates with any type of trajectory (i.e., periodic or not). The average normalized RMSE between the true and the analytically-computed error covariance is around 5% with either periodic or non-periodic trajectory. This result shows that the calibration algorithm can also accurately compute the learning rate effect for a nonlinear point process model of spiking activity. The result also verifies the generality of the calibration algorithm to different state evolution profiles during adaptation, as was the case for continuous signals.</p>
<fig id="pcbi.1006168.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g006</object-id>
<label>Fig 6</label>
<caption>
<title>The calibration algorithm accurately computes the steady-state error covariance for discrete spiking activity.</title>
<p>(A) The analytically-computed and the true steady-state error covariance as a function of the learning rate <italic>r</italic>. True values are found from closed-loop BMI simulations with a periodic center-out-and-back trajectory. The calibration algorithm analytically computes the covariance based on <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref>. The calibration algorithm closely approximates the steady-state error covariance as demonstrated by the closeness of the analytically-computed and true curves across a wide range of <italic>r</italic>. (B) Figure convention is the same as (A) except that all true values are computed in closed-loop BMI simulations with a non-periodic trajectory generated by selecting one of the eight targets randomly and uniformly in each trial. The calibration algorithm can again closely approximate the steady-state error covariance, demonstrating the generalizability of the approach to training datasets with varying state-evolution trajectories.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g006" xlink:type="simple"/>
</fig>
<p>In the case of spikes, the inverse function can again be used to select the learning rate for a given upper-bound on the steady-state error covariance. For example, we can require the error covariance to be within 7% of the average values for all parameters, which provides the value of <italic>V</italic><sub><italic>bd</italic></sub>. Again, <italic>V</italic><sub><italic>bd</italic></sub> can be selected as desired by the user. Once <italic>V</italic><sub><italic>bd</italic></sub> is specified, we use the inverse function using Theorem 3 and <xref ref-type="disp-formula" rid="pcbi.1006168.e039">Eq (9)</xref> and find that the corresponding optimal learning rate <italic>r</italic> is 10<sup>−7</sup>.</p>
<p>We also confirm the accuracy of the calibration algorithm using the parameter adaptation profiles. We plot three realizations of the estimated point process parameters, <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub>, under different learning rates <italic>r</italic> to examine whether the 95% confidence bounds computed by the calibration algorithm are accurate (<xref ref-type="fig" rid="pcbi.1006168.g007">Fig 7</xref>; similar analysis to the case of continuous signals). Note that the confidence bounds are given by twice the square-root of the analytically-computed covariance matrix. We use the optimal learning rate computed for our example above, i.e., <italic>r</italic> = 10<sup>−7</sup>, and a smaller and a larger learning rate in <xref ref-type="fig" rid="pcbi.1006168.g007">Fig 7</xref>. We find that at steady state, the estimated parameters are within the 95% confidence bound about 96% of time. This shows the accuracy of the analytically-computed confidence bound (if this bound is correct, about 95% of the time the estimates should be within confidence bounds). This result is consistent with the good match between the true and analytically-computed covariances in <xref ref-type="fig" rid="pcbi.1006168.g006">Fig 6</xref>.</p>
<fig id="pcbi.1006168.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Parameter adaptation profiles confirm the accuracy of the calibration algorithm with discrete spiking activity.</title>
<p>(A)–(C) show sample adaptation profiles of model parameters <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub> in a closed-loop BMI simulation under different learning rates <italic>r</italic> in ascending order. Increasing the learning rate increases the error covariance. Also, about 96% of the time, the parameter estimates at steady state are within the 95% confidence bounds computed by the calibration algorithm; this demonstrates that the calibration algorithm can closely approximate the error covariance and consequently the confidence bounds.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g007" xlink:type="simple"/>
</fig>
<p>Finally, even though the convergence time cannot be analytically obtained in the case of spike observations, it is still significantly affected by the learning rate <italic>r</italic>. For a small learning rate (<italic>r</italic> = 10<sup>−9</sup>), the parameter estimate <italic>ϕ</italic><sub><italic>t</italic>|<italic>t</italic></sub> does not converge to its true value even in 2000 sec. In comparison, this convergence time is only about 200 sec for an intermediate learning rate (<italic>r</italic> = 10<sup>−7</sup>). Hence to allow for fast convergence, it is critical to select the maximum possible learning rate that satisfies a desired upper-bound constraint on error covariance. This was the basis for the calibration algorithm.</p>
</sec>
<sec id="sec020">
<title>The effect of learning rate on decoding</title>
<p>The selection of the optimal learning rate is critical not only for fast and accurate estimation of the encoding model, but also for accurate decoding of the brain state. Here we show that the selection of the appropriate learning rate by the calibration algorithm can improve both the transient and the steady-state operation of decoders. We simulate closed-loop BMI decoding under various learning rates. Since the optimal trajectory for reaching a target in a center-out task should be close to a straight line connecting the center to the target, as the measure of decoding accuracy we use the RMSE between the decoded trajectory and these straight lines [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>] (the error is the perpendicular distance of the decoded position to the straight line at each time).</p>
<p>To study the effect of the learning rate on steady-state BMI decoding, we adaptively estimate the encoding model parameters under different learning rates. We fix the estimated parameters after varying amounts of adaptation time. We then use the obtained fixed models to run the closed-loop BMI simulations without adaptation. We run these simulations both for continuous LFP/ECoG observations decoded with a KF kinematic decoder, and for discrete spike observations decoded with a PPF kinematic decoder (Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9</xref>, respectively).</p>
<fig id="pcbi.1006168.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Learning rate calibration affects both the transient and the steady-state performance of closed-loop BMI decoders with continuous neural activity.</title>
<p>(A) The evolution of the decoded trajectory as the adaptation time is increased under different learning rates <italic>s</italic>. Note that the decoder is fixed after a given adaptation time is completed (as noted on each row). The fixed decoder is then used to generate the displayed trajectories. Each color corresponds to one learning rate. Decoding performance is unstable when the learning rate is large (<italic>s</italic> = 5 × 10<sup>−1</sup>) even at steady state; this means that depending on exactly when we stop the adaptation and fix the decoder, performance widely oscillates due to the large steady-state model parameter error. (B) RMSE of the decoded trajectory under different learning rates for different adaptation times. RMSE is computed for a fixed decoder that was obtained by stopping the adaptation at various times (different colors). RMSE converges faster as the learning rate is increased (<italic>s</italic> = 5 × 10<sup>−5</sup> to 5 × 10<sup>−3</sup>, for example). However, if the learning rate is selected too large (<italic>s</italic> = 5 × 10<sup>−1</sup>), RMSE oscillates depending on when adaptation is stopped, without converging to a stable number. These results show that appropriately calibrating the learning rate is important not only for encoding model estimation but also for a desired trade-off between convergence time and steady-state RMSE in decoding.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g008" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006168.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006168.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Learning rate calibration affects both the transient and the steady-state performance of closed-loop BMI decoders with discrete spiking activity.</title>
<p>Figure conventions are the same as <xref ref-type="fig" rid="pcbi.1006168.g008">Fig 8</xref>. (A) The evolution of the decoded trajectory across time under different learning rates <italic>r</italic>. Each color corresponds to one learning rate. As in <xref ref-type="fig" rid="pcbi.1006168.g008">Fig 8</xref>, the decoder is fixed after a given adaptation time is completed (as noted on each row). The fixed decoder is then used to generate the displayed trajectories. The decoding performance is unstable when the learning rate is large (<italic>r</italic> = 10<sup>−3</sup>), i.e., the performance widely oscillates. (B) RMSE of the decoded trajectory under different learning rates for different adaptation times. RMSE is computed for a fixed decoder that was obtained by stopping the adaptation at various times (different colors). RMSE converges faster as the learning rate is increased (<italic>r</italic> = 10<sup>−7</sup> to 10<sup>−5</sup>, for example). However, if the learning rate is selected too large (<italic>r</italic> = 10<sup>−3</sup>), RMSE oscillates without converging to a stable number. These results again demonstrate the importance of calibrating the learning rate for fast convergence and accuracy of decoding.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.g009" xlink:type="simple"/>
</fig>
<p>By comparing the small and medium learning rates, we find that a small learning rate results in a slow rate of convergence for the decoder performance, without improving the steady-state performance (two-sided t-test <italic>P</italic> &gt; 0.36; Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9</xref>). Moreover, large learning rates result in poor and unstable steady-state decoding due to inaccurate estimation of the model parameters. This is evident by observing that for large learning rates, BMI decoding RMSE widely oscillates as a function of time at which adaptation stops for both continuous ECoG/LFP observations and discrete spike observations (Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8B</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9B</xref>, respectively). This result shows that due to the large steady-state error, steady-state parameter estimates change widely depending on exactly when we stop the adaptation. Thus the decoder does not converge to a stable performance. Taken together, optimally selecting the learning rate to achieve a desired level of steady-state parameter error covariance is also important for fast convergence and accuracy of decoding.</p>
<p>It is interesting to note that due to feedback-correction in closed-loop BMI, the decoder can tolerate a larger steady-state parameter error than we would typically allow if our only goal is to track the encoding model parameters. This is evident by noting, for example, that using a learning rate of <italic>s</italic> = 5 × 10<sup>−3</sup> for continuous signals results in a relatively large steady-state parameter error as shown in <xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4</xref> (The 95% confidence bound is about ±30% of the modulation depth). However, for the purpose of BMI decoding, this learning rate results in no loss of performance at steady state compared to smaller learning rates, and allows for a faster convergence time (<xref ref-type="fig" rid="pcbi.1006168.g008">Fig 8</xref>). Hence the user-defined upper-bound on the steady-state error covariance is dependent on the application and the goal of adaptation. For closed-loop decoding, a larger error covariance could be tolerated, and as a result, a faster convergence time can be achieved. In contrast, if the goal is to accurately track the evolution of encoding models over time, for example to study learning and plasticity, a lower steady-state error covariance should be targeted. Regardless of the desired upper-bound on the error covariance, the calibration algorithm can closely approximate the corresponding learning rate that satisfies this upper-bound while allowing for the fastest possible convergence.</p>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>Discussion</title>
<p>Developing invasive closed-loop neurotechnologies to treat various neurological disorders requires adaptively learning accurate encoding models that relate the recorded activity—whether in the form of spikes, LFP, or ECoG—to the underlying brain state. Fast and accurate adaptive learning of encoding models is critically affected by the choice of the learning rate [<xref ref-type="bibr" rid="pcbi.1006168.ref037">37</xref>], which introduces a fundamental trade-off between the steady-state error and the convergence time of the estimated model parameters. Despite the importance of the learning rate, currently a principled approach for its calibration is lacking. Here, we developed a principled analytical calibration algorithm for optimal selection of the learning rate in adaptive methods. We designed the calibration algorithm for two possible user-specified adaptation objectives, either to keep the parameter estimation error covariance smaller than a desired value while minimizing convergence time, or to keep the parameter convergence time faster than a given value while minimizing error. We also derived the calibration algorithm both for discrete-valued spikes modeled as point processes nonlinearly dependent on the brain state, and for continuous-valued neural recordings, such as LFP and ECoG, modeled as Gaussian processes linearly dependent on the brain state. We showed that the calibration algorithm allows for fast and accurate learning of encoding model parameters (Figs <xref ref-type="fig" rid="pcbi.1006168.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1006168.g007">7</xref>), and enables fast convergence of decoding performance and accurate steady-state decoding (Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9</xref>). We also demonstrated that larger learning rates make the encoding model and the decoding performance inaccurate, and smaller learning rates delay their convergence. The calibration algorithm provides an analytical approach to predict the effect of the learning rate in advance, and thus to select its optimal value prior to real-time adaptation in closed-loop neurotechnologies.</p>
<p>To derive the calibration algorithm, we introduced a formulation based on the fundamental trade-off that the learning rate dictates between the steady-state error and the convergence time of the estimated parameters. Calibrating the learning rate analytically requires deriving two functions that describe how the learning rate affects the convergence time and the steady-state error covariance, respectively. However, currently no explicit functions exist for these two relationships for Bayesian filters, such as the Kalman filter or the point process filter. We showed that the two functions can be analytically derived (Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref>, <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e066">(19)</xref>) and can accurately predict the effect of the learning rate (Figs <xref ref-type="fig" rid="pcbi.1006168.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006168.g006">6</xref>). We obtained the calibration algorithm by deriving two inverse functions that solve for the learning rate based on a given upper-bound of the error covariance (<xref ref-type="disp-formula" rid="pcbi.1006168.e039">Eq (9)</xref>) or the convergence time (<xref ref-type="disp-formula" rid="pcbi.1006168.e041">Eq (10)</xref>), respectively.</p>
<p>To allow for rigorous derivations in finding tractable analytical solutions for the learning rate, we performed the derivations for the case in which the behavioral state in the training experiment evolved periodically over time. This is the case in many applications; for example, in motor BMIs, models are often learned during a training session in which subjects perform a periodic center-out-and-back movement. However, we found that the calibration algorithm only depended on an average value of the behavioral state rather than on its periodic characteristics. Indeed, we showed that with a simplifying assumption, the derivation extends to the general non-periodic case (Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006168.s002">S2 Fig</xref>); moreover, using extensive numerical simulations, we demonstrated that the calibration algorithm can accurately predict the effect of the learning rate on parameter error and convergence time for a general behavioral state evolution in the training experiments (Figs <xref ref-type="fig" rid="pcbi.1006168.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1006168.g006">6B</xref>). The match between the analytical prediction of the calibration algorithm and the simulation results suggest the generalizability of the calibration algorithm across various behavioral state evolutions.</p>
<p>We derived the calibration algorithm for Bayesian adaptive filters, i.e., KF for continuous-valued activity and PPF for discrete-valued spikes. Here the KF and PPF were used to adaptively learn the neural encoding model parameters, which were assumed to be unknown but essentially fixed within the time-scales of parameter learning. This scenario is largely the case that arises in neurotechnologies for learning encoding models/decoders for two reasons. First, in neurotechnologies, such as BMIs, the parameters of the encoding models are initially unknown because they need to be learned in real time during closed-loop operation (cannot be learned offline and a-priori before actually using the BMI). Second, even though these parameters are unknown, they are largely fixed at least within relevant time-scales of parameter learning (e.g., minutes) in BMIs (and even typically within time-scales of BMI operation in a day, e.g., hours; see for example [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref057">57</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref064">64</xref>]). Even in scenarios where these parameters may change over time for example due to plasticity or task learning, the time-scale of parameter variation will be substantially slower than the time-scale of parameter estimation/learning in the KF or PPF. For example, as we show here and as observed in prior experiments through trial and error, with a well-calibrated adaptive algorithm the parameters can typically be learned within several minutes (e.g., [<xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>]). In contrast, the time-scale of changes in encoding model parameters is typically on the order of days [<xref ref-type="bibr" rid="pcbi.1006168.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>]. So even in the case that parameters may be changing, for the purpose of selecting the learning rate in the adaptive algorithm, they can be considered as essentially constant. We also showed that the calibration algorithm combined with the Bayesian adaptive filter can be used on an as-needed basis to re-learn parameters in case they shift over these relevant longer time-scales, e.g., from day to day. Finally, while Bayesian adaptive filters such as the KF and PPF can be used to track time-varying parameters, they can also be used to estimate fixed but unknown parameters as shown both in neurotechnologies and in other applications such as climate modeling, control of fluid dynamics, and robotics [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref065">65</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref069">69</xref>], and confirmed in our derivations and simulations here.</p>
<p>In deriving the calibration algorithm, we assumed that recorded signals (whether continuous or discrete) are conditionally independent over channels and in time, similar to prior work [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref054">54</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>]. This assumption enables the derivation of tractable real-time decoders (i.e., KF and PPF), adaptive algorithms, and in our case the analytical calibration algorithm, for both linear and nonlinear observation models (Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref>) for continuous neural signals and binary spike events, respectively. While conditional dependencies could exist in general, prior experiments have shown that algorithms derived with these conditional independence assumptions work well for neural data analysis [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref054">54</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref072">72</xref>]. Finally, given the high dimensionality of neural recordings obtained in current neurotechnologies, modeling correlations between channels would introduce a large number of unknown neural parameters that need to be learned in real time. This real-time learning becomes computationally quite expensive, and would require more data (and thus longer time in real-time applications) for parameters to be learned without overfitting. Thus the conditional independence assumption makes the parameter learning algorithms and setups amenable for real-time applications by reducing the number of model parameters and complexity.</p>
<p>The selected learning rate in the calibration algorithm depends on the user-specified upper-bound on the error covariance or convergence time. The values of these upper-bounds could be chosen by the user based on the goal of adaptation. If the adaptation goal is to accurately estimate the encoding model parameters (e.g., to study learning), then the acceptable error upper-bound may be selected to be small. In such a case, the calibration algorithm would select a small learning rate. However, we showed that if the goal of calibration is to enable accurate decoding in a closed-loop BMI, then larger errors in the estimated parameters may be tolerated. This is due to feedback-correction in BMIs, which can compensate for the parameter estimation error (Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9</xref>). The calibration algorithm would then select larger learning rates to improve how fast decoding performance converges to high values. However, even in this case, there is a limit to how large the learning rate can be. A learning rate that is too large will result in unstable and inaccurate performance of the decoder (Figs <xref ref-type="fig" rid="pcbi.1006168.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1006168.g009">9</xref>). This result shows the importance of the calibration algorithm regardless of the goal of adaptation.</p>
<p>The calibration algorithm may also serve as a tool to help examine the interaction between model adaptation and neural adaptation. In closed-loop neurotechnologies, neural representations can change over time resulting in neural adaptation, e.g., due to learning over multiple days. For example, in motor BMIs, the brain can change its encoding of movement (e.g., the directional tuning of neurons) to improve neuroprosthetic control [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref085">85</xref>]. Neural and model adaptation result in a “two-learner system” and can interact [<xref ref-type="bibr" rid="pcbi.1006168.ref056">56</xref>]. It is important to study whether model adaptation interferes with neural adaptation in these closed-loop systems, and if so whether this interference depends on how fast models are adapted. By accurately adjusting the convergence time and hence the speed of model adaptation, the calibration algorithm may provide a useful tool in studying such interference in careful experiments. Moreover, if neural adaptation is significantly affected by the speed of model adaptation, the calibration algorithm could help carefully adjust this speed for a desired neural adaptation outcome. It is also important to examine this interference problem theoretically [<xref ref-type="bibr" rid="pcbi.1006168.ref086">86</xref>].</p>
<p>To validate the calibration algorithm, we used a motor BMI as an example. The calibration algorithm, however, can be applied to other closed-loop neurotechnologies that need to decode various brain states, for example, interest score in closed-loop cortically-coupled computer vision for image search [<xref ref-type="bibr" rid="pcbi.1006168.ref087">87</xref>] or mood in closed-loop DBS systems [<xref ref-type="bibr" rid="pcbi.1006168.ref088">88</xref>]. Also, while our main goal was to derive the calibration algorithm for closed-loop neurotechnologies, this algorithm can be used in other domains of signal processing. We derived the calibration algorithm to select the learning rate and predict its effect on error and convergence time in Bayesian adaptive filters. Prior work in other signal processing applications have focused vastly on the non-Bayesian LMS or steepest-decent adaptive filters [<xref ref-type="bibr" rid="pcbi.1006168.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref040">40</xref>]. However, LMS is only applicable to linear observation models [<xref ref-type="bibr" rid="pcbi.1006168.ref037">37</xref>]. Moreover, steepest-decent filters that use non-linear cost functions to specify the goal of adaptation cannot predict the effect of learning rate on error or convergence time and thus only provide heuristics for learning rate selection [<xref ref-type="bibr" rid="pcbi.1006168.ref037">37</xref>]. Finally, LMS or steepest-decent filters are not Bayesian filters, unlike the KF or the PPF (Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e007">(3)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e010">(6)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e051">(15)</xref>–<xref ref-type="disp-formula" rid="pcbi.1006168.e054">(18)</xref>). Using a Bayesian filter for parameter adaptation has the advantage that it can extend to nonlinear stochastic observation models (such as the point process model of spikes) [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref036">36</xref>]. Here, we derived a learning rate calibration algorithm for Bayesian filters both with continuous linear observation models (KF) and with discrete nonlinear observations models (PPF). Importantly, we derived explicit analytical functions <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e041">(10)</xref> to predict the effect of the learning rate on steady-state error and convergence time for a Bayesian filter. This allowed us to analytically compute an optimal value for the learning rate to achieve a desired user-specified performance metric.</p>
<p>Our main contribution is the derivation of a novel analytical calibration algorithm for both nonlinear point process and linear Gaussian encoding models (Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e002">(1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e046">(12)</xref>); this calibration algorithm optimally selects the learning rate based on the trade-off between convergence time and steady-state error covariance. In deriving closed-form expressions for the calibration algorithm, we needed to <italic>analytically</italic> compute the steady-state error covariance in both the PPF and the KF. Note that, even in the case of the KF, this analytical computation cannot be achieved through the general steady-state analysis of the KF. First, the steady-state analysis of the KF does not formulate a tradeoff between the steady-state error covariance and convergence time, and thus does not provide a calibration algorithm. Second, in order to derive the calibration algorithm, we need to derive novel <italic>analytical closed-form</italic> expressions for the steady-state error covariance and convergence time in the KF (so that we can find the inverse function to compute the optimal learning rate for a given covariance or convergence time). To obtain these expressions, we need to find an analytical solution for a special form of the discrete Riccati equation (DRE) [<xref ref-type="bibr" rid="pcbi.1006168.ref089">89</xref>]. While the DRE is solved numerically and recursively in the general steady-state analysis of a KF, there exists no analytical solution with a closed-form expression for a DRE in general. Obtaining such an analytical solution is critical for calculating the optimal learning rate in <xref ref-type="disp-formula" rid="pcbi.1006168.e039">(9)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e041">(10)</xref>. Therefore, unlike the steady-state analysis of KF, we additionally had to derive the analytic solution of a special form of DRE first (Appendix J in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref>). Third, we also needed analytical expressions for the convergence time of the KF during the transient phase, which again the steady-state analysis of the KF does not provide. Finally, note that we also provide the calibration algorithm for the point process model of the binary spike time-series and thus for the nonlinear PPF in addition to the linear KF.</p>
<p>Here our focus was on deriving an analytical calibration algorithm for both nonlinear point process and linear Gaussian encoding models for spikes and continuous neural recordings, respectively. Thus to validate our analytical approach, we used extensive closed-loop Monte-Carlo simulations. These simulations allowed us to examine the generalizability of the calibration algorithm across different neural signal modalities. The closed-loop simulations closely conformed to our prior non-human primate experiments [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref029">29</xref>]. Prior studies have shown that these closed-loop simulations can mimic the observed experimental effects and thus provide a useful validation testbed for algorithms [<xref ref-type="bibr" rid="pcbi.1006168.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref090">90</xref>]. Moreover, the calibration algorithm adjusted the learning rate of adaptive PPF and adaptive KF decoders, which have been shown to be successful for real-time BMI training and control using spikes or LFP in non-human primate and human experiments both in our work and other studies [<xref ref-type="bibr" rid="pcbi.1006168.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1006168.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006168.ref055">55</xref>]. However, prior experiments, including ours, selected the learning rates empirically in these decoders. Given that the calibration algorithm is run prior to experiments, and based on the success of adaptive PPF and KF in prior animal and human experiments, we expect our calibration algorithm to be seamlessly incorporated in BMIs regardless of the neural signal modality. The calibration algorithm allows the optimal learning rate to be computed prior to running the adaptation experiments to achieve a predictable speed and accuracy in adaptive learning. Implementing the calibration algorithm in animal models of adaptive BMIs using both spikes and LFP is the topic of our future investigation.</p>
<p>Finally, the calibration algorithm has the potential to be generalized to Bayesian filters beyond the KF and PPF, e.g., the unscented Kalman filter [<xref ref-type="bibr" rid="pcbi.1006168.ref042">42</xref>], an adaptive filter with a binomial distribution as the observation model [<xref ref-type="bibr" rid="pcbi.1006168.ref044">44</xref>], or hybrid spike-LFP filters [<xref ref-type="bibr" rid="pcbi.1006168.ref091">91</xref>]. The derivations of Eqs <xref ref-type="disp-formula" rid="pcbi.1006168.e022">(7)</xref> and <xref ref-type="disp-formula" rid="pcbi.1006168.e024">(8)</xref> in theorems 1 and 3 are based on the recursive equation for estimation error dynamics, which is derived from the desired Bayesian filter. This implies that for other observation models different from a linear model with Gaussian noise in KF or a nonlinear point process model in PPF, once we write down their corresponding Bayesian adaptive filters [<xref ref-type="bibr" rid="pcbi.1006168.ref092">92</xref>], we can derive the calibration algorithms by writing the corresponding recursive error equations. Thus this calibration algorithm has the potential to be generalized and applied to other types of signals with various stochastic models. This will be a topic of our future investigation.</p>
</sec>
<sec id="sec022">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006168.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>The calibration algorithm along with the recursive Bayesian decoder can be used on an as-needed basis to re-learn encoding models as parameters shift over time.</title>
<p>Simulation of a BMI system in which parameters are estimated at the beginning of each day and fixed for the rest of the day. This is the setup used in the vast majority of BMI systems because encoding model parameters are either largely time-invariant or change much slower compared with the relevant time-scales of parameter adaptive learning (e.g., minutes) in BMIs and even the time-scale of BMI operation in a day (e.g., hours) (see <xref ref-type="sec" rid="sec021">Discussion</xref>). Figure convention is the same as in <xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4</xref>. Here we show the example of the KF whose learning rate is selected using the calibration algorithm to satisfy user-specified criteria on steady-state error and convergence time as described in Results and shown in <xref ref-type="fig" rid="pcbi.1006168.g004">Fig 4B</xref>. As the task is the same on both days and since <bold>H</bold><sub><italic>ave</italic></sub> is simply an expectation (average) of a function of <inline-formula id="pcbi.1006168.e075"><alternatives><graphic id="pcbi.1006168.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and does not need knowledge of <inline-formula id="pcbi.1006168.e076"><alternatives><graphic id="pcbi.1006168.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006168.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> values, we used the same <bold>H</bold><sub><italic>ave</italic></sub> based on the same average quantity to compute the optimal learning rate on both days. The calibration algorithm satisfies the user-specified criteria on parameter estimates on day 1. We then assume that on day 2 parameters have shifted. On day 2, parameters can again be estimated using the same Kalman filter whose learning rate is selected with the calibration algorithm. Similar to day 1, on day 2 the requirements on steady-state error and convergence time are again satisfied.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006168.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Sketch of the derivation of the calibration algorithm.</title>
<p>The derivation of the calibration algorithm with a periodic encoded state <bold>v</bold><sub><italic>t</italic></sub> during the training session follows the blue arrows. If we assume that the prediction covariance <bold>S</bold><sub><italic>t</italic>+1|<italic>t</italic></sub> has bounded steady-state moments, then the proof generalizes to the non-periodic <bold>v</bold><sub><italic>t</italic></sub> as shown by the red arrows (see Appendix E in <xref ref-type="supplementary-material" rid="pcbi.1006168.s003">S1 Text</xref> and <xref ref-type="fig" rid="pcbi.1006168.g005">Fig 5</xref>). Similarly for the PPF, if we assume that the prediction covariance <bold>Q</bold><sub><italic>t</italic>+1|<italic>t</italic></sub> has bounded steady-state moments, then the proof generalizes to the non-periodic <bold>v</bold><sub><italic>t</italic></sub> (<xref ref-type="fig" rid="pcbi.1006168.g006">Fig 6B</xref>) and the mean of <bold>Q</bold><sub><italic>t</italic>+1|<italic>t</italic></sub> at steady state can be approximated using <bold>M</bold><sub><italic>ave</italic></sub> in Theorem 3 to find the optimal learning rate. Here DRE refers to the discrete Riccati equation.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006168.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006168.s003" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>All appendixes (A–J).</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006168.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Anastassiou</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>The origin of extracellular fields and currents—EEG, ECoG, LFP and spikes</article-title>. <source>Nature reviews neuroscience</source>. <year>2012</year>;<volume>13</volume>(<issue>6</issue>):<fpage>407</fpage>–<lpage>420</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3241" xlink:type="simple">10.1038/nrn3241</ext-link></comment> <object-id pub-id-type="pmid">22595786</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>. <article-title>Brain–machine interface control algorithms</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>. <year>2017</year>;<volume>25</volume>(<issue>10</issue>):<fpage>1725</fpage>–<lpage>1734</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNSRE.2016.2639501" xlink:type="simple">10.1109/TNSRE.2016.2639501</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Brandman</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Cash</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>LR</given-names></name>. <chapter-title>Review: Human intracortical recording and neural decoding for brain-computer interfaces</chapter-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>. <year>2017</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>XT</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>DW</given-names></name>. <article-title>Brain-controlled interfaces: movement restoration with neural prosthetics</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>52</volume>(<issue>1</issue>):<fpage>205</fpage>–<lpage>220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.09.019" xlink:type="simple">10.1016/j.neuron.2006.09.019</ext-link></comment> <object-id pub-id-type="pmid">17015237</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>. <article-title>Brain–machine interfaces: past, present and future</article-title>. <source>TRENDS in Neurosciences</source>. <year>2006</year>;<volume>29</volume>(<issue>9</issue>):<fpage>536</fpage>–<lpage>546</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2006.07.004" xlink:type="simple">10.1016/j.tins.2006.07.004</ext-link></comment> <object-id pub-id-type="pmid">16859758</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>K</given-names></name>. <article-title>Brain-Computer Interfaces [from the guest editors]</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2008</year>;<volume>25</volume>(<issue>1</issue>):<fpage>16</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MSP.2008.4408438" xlink:type="simple">10.1109/MSP.2008.4408438</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>. <article-title>Bridging the brain to the world: a perspective on neural interface systems</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>(<issue>3</issue>):<fpage>511</fpage>–<lpage>521</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.10.037" xlink:type="simple">10.1016/j.neuron.2008.10.037</ext-link></comment> <object-id pub-id-type="pmid">18995827</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>. <article-title>Principles of neural ensemble physiology underlying the operation of brain-machine interfaces</article-title>. <source>Nature reviews Neuroscience</source>. <year>2009</year>;<volume>10</volume>(<issue>7</issue>):<fpage>530</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2653" xlink:type="simple">10.1038/nrn2653</ext-link></comment> <object-id pub-id-type="pmid">19543222</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Suminski</surname> <given-names>AJ</given-names></name>. <article-title>Sensing with the motor cortex</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>3</issue>):<fpage>477</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.10.020" xlink:type="simple">10.1016/j.neuron.2011.10.020</ext-link></comment> <object-id pub-id-type="pmid">22078507</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thakor</surname> <given-names>NV</given-names></name>. <article-title>Translating the brain-machine interface</article-title>. <source>Science translational medicine</source>. <year>2013</year>;<volume>5</volume>(<issue>210</issue>):<fpage>210ps17</fpage>–<lpage>210ps17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/scitranslmed.3007303" xlink:type="simple">10.1126/scitranslmed.3007303</ext-link></comment> <object-id pub-id-type="pmid">24197734</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Kellis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Klaes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Aflalo</surname> <given-names>T</given-names></name>. <article-title>Toward more versatile and intuitive cortical brain–machine interfaces</article-title>. <source>Current Biology</source>. <year>2014</year>;<volume>24</volume>(<issue>18</issue>):<fpage>R885</fpage>–<lpage>R897</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.07.068" xlink:type="simple">10.1016/j.cub.2014.07.068</ext-link></comment> <object-id pub-id-type="pmid">25247368</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Combining decoder design and neural adaptation in brain-machine interfaces</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>84</volume>(<issue>4</issue>):<fpage>665</fpage>–<lpage>680</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.08.038" xlink:type="simple">10.1016/j.neuron.2014.08.038</ext-link></comment> <object-id pub-id-type="pmid">25459407</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marceglia</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rossi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Foffani</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bianchi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cerutti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Priori</surname> <given-names>A</given-names></name>. <article-title>Basal ganglia local field potentials: applications in the development of new deep brain stimulation devices for movement disorders</article-title>. <source>Expert review of medical devices</source>. <year>2007</year>;<volume>4</volume>(<issue>5</issue>):<fpage>605</fpage>–<lpage>614</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1586/17434440.4.5.605" xlink:type="simple">10.1586/17434440.4.5.605</ext-link></comment> <object-id pub-id-type="pmid">17850195</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Little</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pogosyan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Neal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zavala</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Zrinzo</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hariz</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Adaptive deep brain stimulation in advanced Parkinson disease</article-title>. <source>Annals of neurology</source>. <year>2013</year>;<volume>74</volume>(<issue>3</issue>):<fpage>449</fpage>–<lpage>457</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/ana.23951" xlink:type="simple">10.1002/ana.23951</ext-link></comment> <object-id pub-id-type="pmid">23852650</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Priori</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Foffani</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rossi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marceglia</surname> <given-names>S</given-names></name>. <article-title>Adaptive deep brain stimulation (aDBS) controlled by local field potential oscillations</article-title>. <source>Experimental neurology</source>. <year>2013</year>;<volume>245</volume>:<fpage>77</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.expneurol.2012.09.013" xlink:type="simple">10.1016/j.expneurol.2012.09.013</ext-link></comment> <object-id pub-id-type="pmid">23022916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Santaniello</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Montgomery</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Gale</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Kopell</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sarma</surname> <given-names>SV</given-names></name>. <article-title>Therapeutic mechanisms of high-frequency stimulation in Parkinson’s disease and neural restoration via loop-based reinforcement</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>6</issue>):<fpage>E586</fpage>–<lpage>E595</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1406549111" xlink:type="simple">10.1073/pnas.1406549111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taylor</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Tillery</surname> <given-names>SIH</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>. <article-title>Direct cortical control of 3D neuroprosthetic devices</article-title>. <source>Science</source>. <year>2002</year>;<volume>296</volume>(<issue>5574</issue>):<fpage>1829</fpage>–<lpage>1832</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1070291" xlink:type="simple">10.1126/science.1070291</ext-link></comment> <object-id pub-id-type="pmid">12052948</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Crist</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Santucci</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Dimitrov</surname> <given-names>DF</given-names></name>, <etal>et al</etal>. <article-title>Learning to Control a Brain-Machine Interface for Reaching and Grasping by Primates</article-title>. <source>PLoS Biol</source>. <year>2003</year>;<volume>1</volume>(<issue>2</issue>):<fpage>e42</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0000042" xlink:type="simple">10.1371/journal.pbio.0000042</ext-link></comment> <object-id pub-id-type="pmid">14624244</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ganguly</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Emergence of a Stable Cortical Map for Neuroprosthetic Control</article-title>. <source>PLoS Biol</source>. <year>2009</year> <month>Jul</month>;<volume>7</volume>(<issue>7</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1000153" xlink:type="simple">10.1371/journal.pbio.1000153</ext-link></comment> <object-id pub-id-type="pmid">19621062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baranauskas</surname> <given-names>G</given-names></name>. <article-title>What limits the performance of current invasive Brain Machine Interfaces?</article-title> <source>Frontiers in Systems Neuroscience</source>. <year>2014</year>;<volume>8</volume>(<issue>68</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2014.00068" xlink:type="simple">10.3389/fnsys.2014.00068</ext-link></comment> <object-id pub-id-type="pmid">24808833</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Velliste</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Spalding</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Whitford</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>. <article-title>Cortical control of a prosthetic arm for self-feeding</article-title>. <source>Nature</source>. <year>2008</year> <month>Jun</month>;<volume>453</volume>:<fpage>1098</fpage>–<lpage>1101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06996" xlink:type="simple">10.1038/nature06996</ext-link></comment> <object-id pub-id-type="pmid">18509337</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gilja</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Chestek</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Fan</surname> <given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>A High-Performance Neural Prosthesis Enabled by Control Algorithm Design</article-title>. <source>Nat Neurosci</source>. <year>2012</year> <month>Dec</month>;<volume>15</volume>:<fpage>1752</fpage>–<lpage>1757</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3265" xlink:type="simple">10.1038/nn.3265</ext-link></comment> <object-id pub-id-type="pmid">23160043</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Dangi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Closed-Loop Decoder Adaptation on Intermediate Time-Scales Facilitates Rapid BMI Performance Improvements Independent of Decoder Initialization Conditions</article-title>. <source>IEEE Trans Neural Syst Rehabil Eng</source>. <year>2012</year> <month>Jul</month>;<volume>20</volume>(<issue>4</issue>):<fpage>468</fpage>–<lpage>477</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNSRE.2012.2185066" xlink:type="simple">10.1109/TNSRE.2012.2185066</ext-link></comment> <object-id pub-id-type="pmid">22772374</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Collinger</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Wodlinger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Downey</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>DJ</given-names></name>, <etal>et al</etal>. <article-title>High-performance neuroprosthetic control by an individual with tetraplegia</article-title>. <source>The Lancet</source>. <year>2013</year> <month>Feb</month>;<volume>381</volume>(<issue>9866</issue>):<fpage>557</fpage>–<lpage>564</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0140-6736(12)61816-9" xlink:type="simple">10.1016/S0140-6736(12)61816-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mahmoudi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sanchez</surname> <given-names>JC</given-names></name>. <article-title>A symbiotic brain-machine interface through value-based decision making</article-title>. <source>PLOS ONE</source>. <year>2011</year> <month>Apr</month>;<volume>6</volume>(<issue>3</issue>):<fpage>e14760</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0014760" xlink:type="simple">10.1371/journal.pone.0014760</ext-link></comment> <object-id pub-id-type="pmid">21423797</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hochberg</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Bacher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jarosiewicz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Masse</surname> <given-names>NY</given-names></name>, <name name-style="western"><surname>Simeral</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Vogel</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Reach and grasp by people with tetraplegia using a neurally controlled robotic arm</article-title>. <source>Nature</source>. <year>2012</year> <month>May</month>;<volume>485</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11076" xlink:type="simple">10.1038/nature11076</ext-link></comment> <object-id pub-id-type="pmid">22596161</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dangi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gowda</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>So</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shanechi</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Continuous Closed-Loop Decoder Adaptation with a Recursive Maximum Likelihood Algorithm Allows for Rapid Performance Acquisition in Brain-Machine Interfaces</article-title>. <source>Neural Comput</source>. <year>2014</year> <month>Sep</month>;<volume>26</volume>(<issue>9</issue>):<fpage>1811</fpage>–<lpage>1839</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00632" xlink:type="simple">10.1162/NECO_a_00632</ext-link></comment> <object-id pub-id-type="pmid">24922501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Robust Brain-Machine Interface Design Using Optimal Feedback Control Modeling and Adaptive Point Process Filtering</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>4</issue>):<fpage>e1004730</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004730" xlink:type="simple">10.1371/journal.pcbi.1004730</ext-link></comment> <object-id pub-id-type="pmid">27035820</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gowda</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Dangi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Rapid Control and Feedback Rates Enhance Neuroprosthetic Control</article-title>. <source>Nature Communications</source>. <year>2017</year>;(<issue>8</issue>):<fpage>13825</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13825" xlink:type="simple">10.1038/ncomms13825</ext-link></comment> <object-id pub-id-type="pmid">28059065</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gilja</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Pandarinath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Blabe</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Simeral</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Sarma</surname> <given-names>AA</given-names></name>, <etal>et al</etal>. <article-title>Clinical translation of a high-performance neural prosthesis</article-title>. <source>Nature medicine</source>. <year>2015</year>;<volume>21</volume>(<issue>10</issue>):<fpage>1142</fpage>–<lpage>1145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nm.3953" xlink:type="simple">10.1038/nm.3953</ext-link></comment> <object-id pub-id-type="pmid">26413781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Solo</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>Contrasting patterns of receptive field plasticity in the hippocampus and the entorhinal cortex: an adaptive filtering approach</article-title>. <source>Journal of Neuroscience</source>. <year>2002</year>;<volume>22</volume>(<issue>9</issue>):<fpage>3817</fpage>–<lpage>3830</lpage>. <object-id pub-id-type="pmid">11978857</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Stanley</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>Hippocampal plasticity across multiple days of exposure to novel environments</article-title>. <source>Journal of Neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>35</issue>):<fpage>7681</fpage>–<lpage>7689</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1958-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1958-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15342735</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Van Hartevelt</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Cabral</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Møller</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Aziz</surname> <given-names>TZ</given-names></name>, <etal>et al</etal>. <article-title>Neural plasticity in human brain connectivity: the effects of long term deep brain stimulation of the subthalamic nucleus in Parkinson’s disease</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>1</issue>):<fpage>e86496</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0086496" xlink:type="simple">10.1371/journal.pone.0086496</ext-link></comment> <object-id pub-id-type="pmid">24466120</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Santaniello</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Montgomery</surname> <given-names>EB</given-names> <suffix>Jr</suffix></name>, <name name-style="western"><surname>Gale</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Sarma</surname> <given-names>SV</given-names></name>. <article-title>Non-stationary discharge patterns in motor cortex under subthalamic nucleus deep brain stimulation</article-title>. <source>Frontiers in integrative neuroscience</source>. <year>2012</year>;<volume>6</volume>:<fpage>35</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnint.2012.00035" xlink:type="simple">10.3389/fnint.2012.00035</ext-link></comment> <object-id pub-id-type="pmid">22754509</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Nguyen</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Solo</surname> <given-names>V</given-names></name>. <article-title>An analysis of neural receptive field plasticity by point process adaptive filtering</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2001</year>;<volume>98</volume>(<issue>21</issue>):<fpage>12261</fpage>–<lpage>12266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.201409398" xlink:type="simple">10.1073/pnas.201409398</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Solo</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>Dynamic analysis of neural encoding by point process adaptive filtering</article-title>. <source>Neural Comput</source>. <year>2004</year>;<volume>16</volume>:<fpage>971</fpage>–<lpage>998</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976604773135069" xlink:type="simple">10.1162/089976604773135069</ext-link></comment> <object-id pub-id-type="pmid">15070506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref037">
<label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haykin</surname> <given-names>SS</given-names></name>. <source>Adaptive filter theory</source>. <publisher-name>Pearson Education India</publisher-name>; <year>2008</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Shanechi MM, Carmena JM. Optimal feedback-controlled point process decoder for adaptation and assisted training in brain-machine interfaces. In: Neural Engineering (NER), 2013 6th International IEEE/EMBS Conference on. IEEE; 2013. p. 653–656.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jacobs</surname> <given-names>RA</given-names></name>. <article-title>Increased rates of convergence through learning rate adaptation</article-title>. <source>Neural networks</source>. <year>1988</year>;<volume>1</volume>(<issue>4</issue>):<fpage>295</fpage>–<lpage>307</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0893-6080(88)90003-2" xlink:type="simple">10.1016/0893-6080(88)90003-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Luo</surname> <given-names>ZQ</given-names></name>. <article-title>On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks</article-title>. <source>Neural Computation</source>. <year>1991</year>;<volume>3</volume>(<issue>2</issue>):<fpage>226</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1991.3.2.226" xlink:type="simple">10.1162/neco.1991.3.2.226</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Xue</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>YH</given-names></name>, <name name-style="western"><surname>Tompkins</surname> <given-names>WJ</given-names></name>. <article-title>Neural-network-based adaptive matched filtering for QRS detection</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>1992</year>;<volume>39</volume>(<issue>4</issue>):<fpage>317</fpage>–<lpage>329</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/10.126604" xlink:type="simple">10.1109/10.126604</ext-link></comment> <object-id pub-id-type="pmid">1592397</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Wan EA, Van Der Merwe R. The unscented Kalman filter for nonlinear estimation. In: Adaptive Systems for Signal Processing, Communications, and Control Symposium 2000. AS-SPCC. The IEEE 2000. Ieee; 2000. p. 153–158.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Polesel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ramponi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mathews</surname> <given-names>VJ</given-names></name>. <article-title>Image enhancement via adaptive unsharp masking</article-title>. <source>IEEE transactions on image processing</source>. <year>2000</year>;<volume>9</volume>(<issue>3</issue>):<fpage>505</fpage>–<lpage>510</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/83.826787" xlink:type="simple">10.1109/83.826787</ext-link></comment> <object-id pub-id-type="pmid">18255421</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>. <article-title>An adaptive and generalizable closed-loop system for control of medically induced coma and other states of anesthesia</article-title>. <source>Journal of neural engineering</source>. <year>2016</year>;<volume>13</volume>(<issue>6</issue>):<fpage>066019</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/13/6/066019" xlink:type="simple">10.1088/1741-2560/13/6/066019</ext-link></comment> <object-id pub-id-type="pmid">27819255</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Chemali</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Liberman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Solt</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>A Brain-Machine Interface for Control of Medically-Induced Coma</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year> <month>Oct</month>;<volume>9</volume>(<issue>10</issue>):<fpage>e1003284</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003284" xlink:type="simple">10.1371/journal.pcbi.1003284</ext-link></comment> <object-id pub-id-type="pmid">24204231</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Citi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>. <article-title>A real-time automated point-process method for the detection and correction of erroneous and ectopic heartbeats</article-title>. <source>IEEE transactions on biomedical engineering</source>. <year>2012</year>;<volume>59</volume>(<issue>10</issue>):<fpage>2828</fpage>–<lpage>2837</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TBME.2012.2211356" xlink:type="simple">10.1109/TBME.2012.2211356</ext-link></comment> <object-id pub-id-type="pmid">22875239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref047">
<label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Citi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>. <chapter-title>A point process local likelihood algorithm for robust and automated heart beat detection and correction</chapter-title>. In: <source>Computing in Cardiology, 2011</source>. <publisher-name>IEEE</publisher-name>; <year>2011</year>. p. <fpage>293</fpage>–<lpage>296</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Wornell</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>Feedback-controlled parallel point process filter for estimation of goal-directed movements from neural signals</article-title>. <source>IEEE Trans Neural Syst Rehabil Eng</source>. <year>2013</year> <month>Jan</month>;<volume>21</volume>:<fpage>129</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNSRE.2012.2221743" xlink:type="simple">10.1109/TNSRE.2012.2221743</ext-link></comment> <object-id pub-id-type="pmid">23047892</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Wornell</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Powers</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>A real-time brain-machine interface combining motor target and trajectory intent using an optimal feedback control design</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>4</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0059049" xlink:type="simple">10.1371/journal.pone.0059049</ext-link></comment> <object-id pub-id-type="pmid">23593130</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>ZM</given-names></name>. <article-title>A cortical-spinal prosthesis for targeted limb movement in paralysed primate avatars</article-title>. <source>Nat Commun</source>. <year>2014</year> <month>Feb</month>;<volume>5</volume> (<issue>3237</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms4237" xlink:type="simple">10.1038/ncomms4237</ext-link></comment> <object-id pub-id-type="pmid">24549394</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Powers</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wornell</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>ZM</given-names></name>. <article-title>Neural population partitioning and a concurrent brain-machine interface for sequential motor function</article-title>. <source>Nat Neurosci</source>. <year>2012</year> <month>Dec</month>;<volume>15</volume>(<issue>12</issue>):<fpage>1715</fpage>–<lpage>1722</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3250" xlink:type="simple">10.1038/nn.3250</ext-link></comment> <object-id pub-id-type="pmid">23143511</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Belitski</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gretton</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Magri</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Murayama</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Montemurro</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <etal>et al</etal>. <article-title>Low-Frequency Local Field Potentials and Spikes in Primary Visual Cortex Convey Independent Visual Information</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>22</issue>):<fpage>5696</fpage>–<lpage>5709</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://jneurosci.org/content/28/22/5696" xlink:type="simple">http://jneurosci.org/content/28/22/5696</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0009-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0009-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18509031</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Einevoll</surname> <given-names>GT</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>. <article-title>Modelling and analysis of local field potentials for studying the function of cortical circuits</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2013</year>;<volume>14</volume>:<fpage>770</fpage>–<lpage>785</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3599" xlink:type="simple">10.1038/nrn3599</ext-link></comment> <object-id pub-id-type="pmid">24135696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pistohl</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ball</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schulze-Bonhage</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Aertsen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mehring</surname> <given-names>C</given-names></name>. <article-title>Prediction of arm movement trajectories from ECoG-recordings in humans</article-title>. <source>Journal of neuroscience methods</source>. <year>2008</year>;<volume>167</volume>(<issue>1</issue>):<fpage>105</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2007.10.001" xlink:type="simple">10.1016/j.jneumeth.2007.10.001</ext-link></comment> <object-id pub-id-type="pmid">18022247</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>So</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dangi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Gastpar</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Subject-specific modulation of local field potential spectral power during brain-machine interface control in primates</article-title>. <source>J Neural Eng</source>. <year>2014</year> <month>Feb</month>;<volume>11</volume>:<fpage>026002</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/11/2/026002" xlink:type="simple">10.1088/1741-2560/11/2/026002</ext-link></comment> <object-id pub-id-type="pmid">24503623</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Orsborn</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Overduin</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Dimitrov</surname> <given-names>DF</given-names></name>, <name name-style="western"><surname>Carmena</surname> <given-names>JM</given-names></name>. <article-title>Closed-loop decoder adaptation shapes neural plasticity for skillful neuroprosthetic control</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>:<fpage>1380</fpage>–<lpage>1392</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.04.048" xlink:type="simple">10.1016/j.neuron.2014.04.048</ext-link></comment> <object-id pub-id-type="pmid">24945777</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Simeral</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Black</surname> <given-names>MJ</given-names></name>. <article-title>Neural control of computer cursor velocity by decoding motor cortical spiking activity in humans with tetraplegia</article-title>. <source>J Neural Eng</source>. <year>2008</year>;<volume>5</volume>:<fpage>455</fpage>–<lpage>476</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/5/4/010" xlink:type="simple">10.1088/1741-2560/5/4/010</ext-link></comment> <object-id pub-id-type="pmid">19015583</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mulliken</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Musallam</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>Decoding Trajectories from Posterior Parietal Cortex Ensembles</article-title>. <source>J Neurosci</source>. <year>2008</year> <month>Nov</month>;<volume>28</volume>(<issue>48</issue>):<fpage>12913</fpage>–<lpage>12926</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1463-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1463-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19036985</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Suminski</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Tkach</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Fagg</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name>. <article-title>Incorporating Feedback from Multiple Sensory Modalities Enhances Brain-Machine Interface Control</article-title>. <source>J Neurosci</source>. <year>2010</year> <month>Dec</month>;<volume>30</volume>(<issue>50</issue>):<fpage>16777</fpage>–<lpage>16787</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3967-10.2010" xlink:type="simple">10.1523/JNEUROSCI.3967-10.2010</ext-link></comment> <object-id pub-id-type="pmid">21159949</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ethier</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Oby</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Bauman</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>LE</given-names></name>. <article-title>Restoration of grasp following paralysis through brain-controlled stimulation of muscles</article-title>. <source>Nature</source>. <year>2012</year> <month>May</month>;<volume>485</volume>:<fpage>368</fpage>–<lpage>371</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature10987" xlink:type="simple">10.1038/nature10987</ext-link></comment> <object-id pub-id-type="pmid">22522928</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hauschild</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mulliken</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Fineman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Loeb</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>Cognitive signals for brain-machine interfaces in posterior parietal cortex include continuous 3D trajectory commands</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>;<volume>109</volume>:<fpage>17075</fpage>–<lpage>17080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1215092109" xlink:type="simple">10.1073/pnas.1215092109</ext-link></comment> <object-id pub-id-type="pmid">23027946</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Musallam</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Corneil</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Greger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Scherberger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>Cognitive control signals for neural prosthetics</article-title>. <source>Science</source>. <year>2004</year> <month>Jul</month>;<volume>305</volume>:<fpage>258</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1097938" xlink:type="simple">10.1126/science.1097938</ext-link></comment> <object-id pub-id-type="pmid">15247483</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Flint</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>ZA</given-names></name>, <name name-style="western"><surname>Scheid</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Slutzky</surname> <given-names>MW</given-names></name>. <article-title>Long term, stable brain machine interface performance using local field potentials and multiunit spikes</article-title>. <source>Journal of neural engineering</source>. <year>2013</year>;<volume>10</volume>(<issue>5</issue>):<fpage>056005</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/10/5/056005" xlink:type="simple">10.1088/1741-2560/10/5/056005</ext-link></comment> <object-id pub-id-type="pmid">23918061</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chestek</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Gilja</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Fan</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <etal>et al</etal>. <article-title>Long-term stability of neural prosthetic control signals from silicon cortical arrays in rhesus macaque motor cortex</article-title>. <source>Journal of neural engineering</source>. <year>2011</year>;<volume>8</volume>(<issue>4</issue>):<fpage>045005</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/8/4/045005" xlink:type="simple">10.1088/1741-2560/8/4/045005</ext-link></comment> <object-id pub-id-type="pmid">21775782</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Evensen</surname> <given-names>G</given-names></name>. <article-title>The ensemble Kalman filter: Theoretical formulation and practical implementation</article-title>. <source>Ocean dynamics</source>. <year>2003</year>;<volume>53</volume>(<issue>4</issue>):<fpage>343</fpage>–<lpage>367</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10236-003-0036-9" xlink:type="simple">10.1007/s10236-003-0036-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moradkhani</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sorooshian</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>HV</given-names></name>, <name name-style="western"><surname>Houser</surname> <given-names>PR</given-names></name>. <article-title>Dual state–parameter estimation of hydrological models using ensemble Kalman filter</article-title>. <source>Advances in water resources</source>. <year>2005</year>;<volume>28</volume>(<issue>2</issue>):<fpage>135</fpage>–<lpage>147</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.advwatres.2004.09.002" xlink:type="simple">10.1016/j.advwatres.2004.09.002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Annan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hargreaves</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Edwards</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Marsh</surname> <given-names>R</given-names></name>. <article-title>Parameter estimation in an intermediate complexity earth system model using an ensemble Kalman filter</article-title>. <source>Ocean modelling</source>. <year>2005</year>;<volume>8</volume>(<issue>1</issue>):<fpage>135</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ocemod.2003.12.004" xlink:type="simple">10.1016/j.ocemod.2003.12.004</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>VanDyke</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>CD</given-names></name>, <etal>et al</etal>. <article-title>Unscented Kalman filtering for spacecraft attitude state and parameter estimation</article-title>. <source>Advances in the Astronautical Sciences</source>. <year>2004</year>;<volume>118</volume>(<issue>1</issue>):<fpage>217</fpage>–<lpage>228</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Van Der Merwe R, Wan EA. The square-root unscented Kalman filter for state and parameter-estimation. In: Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on. vol. 6. IEEE; 2001. p. 3461–3464.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref070">
<label>70</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haykin</surname> <given-names>S</given-names></name>. <source>Kalman filtering and neural networks</source>. <volume>vol. 47</volume>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Myers</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tapley</surname> <given-names>B</given-names></name>. <article-title>Adaptive sequential estimation with unknown noise statistics</article-title>. <source>IEEE Transactions on Automatic Control</source>. <year>1976</year>;<volume>21</volume>(<issue>4</issue>):<fpage>520</fpage>–<lpage>523</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TAC.1976.1101260" xlink:type="simple">10.1109/TAC.1976.1101260</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Quirk</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>A statistical paradigm for neural spike train decoding applied to position prediction from ensemble firing patterns of rat hippocampal place cells</article-title>. <source>J Neurosci</source>. <year>1998</year> <month>Sep</month>;<volume>18</volume>(<issue>18</issue>):<fpage>7411</fpage>–<lpage>7425</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.18-18-07411.1998" xlink:type="simple">10.1523/JNEUROSCI.18-18-07411.1998</ext-link></comment> <object-id pub-id-type="pmid">9736661</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Ventura</surname> <given-names>V</given-names></name>. <article-title>A Spike-Train Probability Model</article-title>. <source>Neural Computation</source>. <year>2001</year>;<volume>13</volume>(<issue>8</issue>):<fpage>1713</fpage>–<lpage>1720</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/08997660152469314" xlink:type="simple">10.1162/08997660152469314</ext-link></comment> <object-id pub-id-type="pmid">11506667</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ventura</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>L</given-names></name>. <article-title>The time-rescaling theorem and its application to neural spike train data analysis</article-title>. <source>Neural Comput</source>. <year>2001</year>;<volume>14</volume>:<fpage>325</fpage>–<lpage>346</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/08997660252741149" xlink:type="simple">10.1162/08997660252741149</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Truccolo</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>J Neurophysiol</source>. <year>2005</year>;<volume>93</volume>:<fpage>1074</fpage>–<lpage>1089</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00697.2004" xlink:type="simple">10.1152/jn.00697.2004</ext-link></comment> <object-id pub-id-type="pmid">15356183</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Citi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ba</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Barbieri</surname> <given-names>R</given-names></name>. <article-title>Likelihood methods for point processes with refractoriness</article-title>. <source>Neural computation</source>. <year>2014</year>;<volume>26</volume>(<issue>2</issue>):<fpage>237</fpage>–<lpage>263</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00548" xlink:type="simple">10.1162/NECO_a_00548</ext-link></comment> <object-id pub-id-type="pmid">24206384</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Nguyen</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Solo</surname> <given-names>V</given-names></name>. <article-title>An analysis of neural receptive field plasticity by point process adaptive filtering</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2001</year>;<volume>98</volume>(<issue>21</issue>):<fpage>12261</fpage>–<lpage>12266</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/98/21/12261.abstract" xlink:type="simple">http://www.pnas.org/content/98/21/12261.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.201409398" xlink:type="simple">10.1073/pnas.201409398</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>. <article-title>Optimality principles in sensorimotor control</article-title>. <source>Nat Neurosci</source>. <year>2004</year> <month>Sep</month>;p. <fpage>907</fpage>–<lpage>915</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1309" xlink:type="simple">10.1038/nn1309</ext-link></comment> <object-id pub-id-type="pmid">15332089</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>. <article-title>Optimal Feedback Control as a Theory of Motor Coordination</article-title>. <source>Nat Neurosci</source>. <year>2002</year> <month>Nov</month>;<volume>5</volume>(<issue>11</issue>):<fpage>1226</fpage>–<lpage>1235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn963" xlink:type="simple">10.1038/nn963</ext-link></comment> <object-id pub-id-type="pmid">12404008</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref080">
<label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>. <article-title>Evidence for the flexible sensorimotor strategies predicted by optimal feedback control</article-title>. <source>J Neurosci</source>. <year>2007</year> <month>Aug</month>;p. <fpage>9354</fpage>–<lpage>9368</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1110-06.2007" xlink:type="simple">10.1523/JNEUROSCI.1110-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17728449</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref081">
<label>81</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Jeffrey</surname> <given-names>BB</given-names></name>. <source>Linear Optimal Control</source>. <publisher-name>Addison Wesley</publisher-name>; <year>1999</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref082">
<label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>A computational neuroanatomy for motor control</article-title>. <source>Exp Brain Res</source>. <year>2008</year>;p. <fpage>359</fpage>–<lpage>381</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-008-1280-5" xlink:type="simple">10.1007/s00221-008-1280-5</ext-link></comment> <object-id pub-id-type="pmid">18251019</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref083">
<label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Golub</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Byron</surname> <given-names>MY</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>SM</given-names></name>. <article-title>Internal models for interpreting neural population activity during sensorimotor control</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e10015</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.10015" xlink:type="simple">10.7554/eLife.10015</ext-link></comment> <object-id pub-id-type="pmid">26646183</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref084">
<label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moran</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>. <article-title>Motor Cortical Representation of Speed and Direction During Reaching</article-title>. <source>J Neurophysiol</source>. <year>1999</year>;<volume>82</volume>:<fpage>2676</fpage>–<lpage>2692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1999.82.5.2676" xlink:type="simple">10.1152/jn.1999.82.5.2676</ext-link></comment> <object-id pub-id-type="pmid">10561437</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref085">
<label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sadtler</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Quick</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Golub</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <etal>et al</etal>. <article-title>Neural constraints on learning</article-title>. <source>Nature</source>. <year>2014</year>;<volume>512</volume>(<issue>7515</issue>):<fpage>423</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature13665" xlink:type="simple">10.1038/nature13665</ext-link></comment> <object-id pub-id-type="pmid">25164754</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref086">
<label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Merel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pianto</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Encoder-Decoder Optimization for Brain-Computer Interfaces</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year> <month>6</month>;<volume>11</volume>(<issue>6</issue>):<fpage>e1004288</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004288" xlink:type="simple">10.1371/journal.pcbi.1004288</ext-link></comment> <object-id pub-id-type="pmid">26029919</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref087">
<label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pohlmeyer</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jangraw</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Lou</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>SF</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Closing the loop in cortically-coupled computer vision: a brain–computer interface for searching image databases</article-title>. <source>Journal of neural engineering</source>. <year>2011</year>;<volume>8</volume>(<issue>3</issue>):<fpage>036025</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/8/3/036025" xlink:type="simple">10.1088/1741-2560/8/3/036025</ext-link></comment> <object-id pub-id-type="pmid">21562364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref088">
<label>88</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sani</surname> <given-names>OG</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Shanechi</surname> <given-names>MM</given-names></name>. <chapter-title>Real-time decoding of mood from human large-scale ECoG activity</chapter-title>. In: <source>Society for Neuroscience (SFN) Abstract</source>, <publisher-loc>Washington, DC</publisher-loc>; <year>2017</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref089">
<label>89</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bittanti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Laub</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Willems</surname> <given-names>JC</given-names></name>. <source>The Riccati Equation</source>. <publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref090">
<label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gilja</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Chestek</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>. <article-title>A Closed-Loop Human Simulator for Investigating the Role of Feedback Control in Brain-Machine Interfaces</article-title>. <source>J Neurophysiol</source>. <year>2011</year>;<volume>105</volume>:<fpage>1932</fpage>–<lpage>1949</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00503.2010" xlink:type="simple">10.1152/jn.00503.2010</ext-link></comment> <object-id pub-id-type="pmid">20943945</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006168.ref091">
<label>91</label>
<mixed-citation publication-type="other" xlink:type="simple">Hsieh HL, Shanechi MM. Multiscale brain-machine interface decoders. In: Engineering in Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of the. IEEE; 2016. p. 6361–6364.</mixed-citation>
</ref>
<ref id="pcbi.1006168.ref092">
<label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Arulampalam</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Maskell</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gordon</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Clapp</surname> <given-names>T</given-names></name>. <article-title>A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking</article-title>. <source>IEEE Transactions on signal processing</source>. <year>2002</year>;<volume>50</volume>(<issue>2</issue>):<fpage>174</fpage>–<lpage>188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/78.978374" xlink:type="simple">10.1109/78.978374</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>