<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2004473</article-id>
<article-id pub-id-type="publisher-id">pbio.2004473</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Short Reports</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject><subj-group><subject>Syllables</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject><subj-group><subject>Phonemes</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Motor system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Motor system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Motor system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Syntax</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title>
<alt-title alt-title-type="running-head">Speech tracking in auditory and motor regions</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4498-0146</contrib-id>
<name name-style="western">
<surname>Keitel</surname>
<given-names>Anne</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gross</surname>
<given-names>Joachim</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kayser</surname>
<given-names>Christoph</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Institute of Neuroscience and Psychology, University of Glasgow, Glasgow, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Institute for Biomagnetism and Biosignalanalysis, University of Münster, Münster, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Cognitive Neuroscience, Bielefeld University, Bielefeld, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bizley</surname>
<given-names>Jennifer</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University College London, United Kingdom of Great Britain and Northern Ireland</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">anne.keitel@glasgow.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>12</day>
<month>3</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2018</year>
</pub-date>
<volume>16</volume>
<issue>3</issue>
<elocation-id>e2004473</elocation-id>
<history>
<date date-type="received">
<day>9</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Keitel et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2004473"/>
<abstract>
<p>During online speech processing, our brain tracks the acoustic fluctuations in speech at different timescales. Previous research has focused on generic timescales (for example, delta or theta bands) that are assumed to map onto linguistic features such as prosody or syllables. However, given the high intersubject variability in speaking patterns, such a generic association between the timescales of brain activity and speech properties can be ambiguous. Here, we analyse speech tracking in source-localised magnetoencephalographic data by directly focusing on timescales extracted from statistical regularities in our speech material. This revealed widespread significant tracking at the timescales of phrases (0.6–1.3 Hz), words (1.8–3 Hz), syllables (2.8–4.8 Hz), and phonemes (8–12.4 Hz). Importantly, when examining its perceptual relevance, we found stronger tracking for correctly comprehended trials in the left premotor (PM) cortex at the phrasal scale as well as in left middle temporal cortex at the word scale. Control analyses using generic bands confirmed that these effects were specific to the speech regularities in our stimuli. Furthermore, we found that the phase at the phrasal timescale coupled to power at beta frequency (13–30 Hz) in motor areas. This cross-frequency coupling presumably reflects top-down temporal prediction in ongoing speech perception. Together, our results reveal specific functional and perceptually relevant roles of distinct tracking and cross-frequency processes along the auditory–motor pathway.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>How we comprehend speech—and how the brain encodes information from a continuous speech stream—is of interest for neuroscience, linguistics, and research on language disorders. Previous work that examined dynamic brain activity has addressed the issue of comprehension only indirectly, by contrasting intelligible speech with unintelligible speech or baseline activity. Recent work, however, suggests that brain areas can show similar stimulus-driven activity but differently contribute to perception or comprehension. To directly address the perceptual relevance of dynamic brain activity for speech encoding, we used a straightforward, single-trial comprehension measure. Furthermore, previous work has been vague regarding the analysed timescales. We therefore base our analysis directly on the timescales of phrases, words, syllables, and phonemes of our speech stimuli. By incorporating these two conceptual innovations, we demonstrate that different areas of the brain track acoustic information at the time-scales of words and phrases. Moreover, our results suggest that the motor cortex uses a cross-frequency coupling mechanism to predict the timing of phrases in ongoing speech. Our findings suggest spatially and temporally distinct brain mechanisms that directly shape our comprehension.</p>
</abstract>
<funding-group>
<funding-statement>BBSRC <ext-link ext-link-type="uri" xlink:href="http://www.bbsrc.ac.uk/" xlink:type="simple">http://www.bbsrc.ac.uk/</ext-link> (grant number BB/L027534/1) to CK and JG. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. European Research Council <ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu/" xlink:type="simple">https://erc.europa.eu/</ext-link> (grant number 646657) to CK. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Wellcome Trust <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/" xlink:type="simple">https://wellcome.ac.uk/</ext-link> (grant number 098433) to JG. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="0"/>
<page-count count="19"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-03-22</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data underlying the results presented in the manuscript are available from the Dryad database (doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">10.5061/dryad.1qq7050</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<p>Speech consists of hierarchically organised linguistic segments such as phrases, words, syllables, and phonemes [<xref ref-type="bibr" rid="pbio.2004473.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>]. To comprehend speech, a listener needs to parse the continuous stream into these segments [<xref ref-type="bibr" rid="pbio.2004473.ref003">3</xref>]. One mechanism that has been proposed to fulfil such a role is the tracking of speech information in dynamic brain activity (often termed speech-to-brain entrainment) [<xref ref-type="bibr" rid="pbio.2004473.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref005">5</xref>]. Such tracking is observed as a precise alignment of rhythmic brain activity to the temporal characteristics of speech. Previous studies commonly focused on brain activity at the timescales of traditional delta (1–4 Hz) and theta (4–8 Hz) bands [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref007">7</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref008">8</xref>]. These have been suggested to reflect the neural processing of prosodic and syllabic speech features [e.g., <xref ref-type="bibr" rid="pbio.2004473.ref003">3</xref>]. However, such a general association between timescales in brain activity and speech properties is difficult [<xref ref-type="bibr" rid="pbio.2004473.ref009">9</xref>]. First, there are large interindividual differences in speaking rate and use of prosody. For example, the syllabic rate (sometimes interchangeably used with speech modulation rate) has been found within a range of 2 to 20 Hz across studies [<xref ref-type="bibr" rid="pbio.2004473.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref013">13</xref>]. This indicates that the syllabic rate does not always fall into the often used theta frequency band. Second, the association between specific timescales and linguistic or phonological features remains ambiguous because multiple properties have been associated with delta (stress, intonation, phrase structure) [<xref ref-type="bibr" rid="pbio.2004473.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref015">15</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref016">16</xref>] and theta bands (syllables, jaw movements) [<xref ref-type="bibr" rid="pbio.2004473.ref017">17</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref018">18</xref>]. In the present study, we therefore take a different approach and first extract linguistic timescales from the speech corpus, based on stimulus-specific regularities. We then use these to investigate the neural mechanisms underlying speech encoding. We believe that any observed neural or perceptual effects at these linguistically specific timescales allow a more straightforward and mechanistically more specific interpretation than effects at generic timescales.</p>
<p>The functional interpretation of speech-to-brain entrainment is further hampered by a lack of a systematic assessment of where (in the brain) and at which timescale it is relevant for a perceptual outcome. By combining neural recordings with a behavioural measure, it has recently been shown that much distributed neural activity represents processes contributing to sensory encoding and perception, and only some focal activity is causally related to perceptual decisions [<xref ref-type="bibr" rid="pbio.2004473.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref020">20</xref>]. Brain areas can therefore have similar stimulus-driven responses but different causal relationships with perceptual decisions [<xref ref-type="bibr" rid="pbio.2004473.ref020">20</xref>]. By contrast, most studies attempting to link comprehension and speech tracking have done so only indirectly (i.e., without probing perceptual decisions) (but see [<xref ref-type="bibr" rid="pbio.2004473.ref021">21</xref>]). These studies typically vary speech intelligibility by using background noise [<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref023">23</xref>], noise vocoding [<xref ref-type="bibr" rid="pbio.2004473.ref007">7</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref024">24</xref>], reversed speech [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref025">25</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref026">26</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref027">27</xref>], or by shuffling syllables [<xref ref-type="bibr" rid="pbio.2004473.ref028">28</xref>]. Thereby, they have revealed a link between neural delta and theta band tracking and intelligibility during listening to continuous speech [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref021">21</xref>]. Interestingly, some of these studies have demonstrated widespread low-frequency tracking across the brain, going beyond auditory cortex and encompassing prefrontal and motor areas [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>]. However, because many studies contrast brain activity during actual speech encoding with a surrogate dataset reflecting no speech–brain relation, these only demonstrate the existence of a speech-to-brain encoding process but not its functional or perceptual relevance. This notion is also supported by the finding that auditory entrainment can be observed for subthreshold stimuli [<xref ref-type="bibr" rid="pbio.2004473.ref029">29</xref>], suggesting that there is no necessary perceptual consequence of this tracking process. Therefore, it remains uncertain whether and where the tracking of speech by dynamic brain activity is perceptually relevant for comprehension at the single-trial level.</p>
<p>In the present study, participants performed a comprehension task on natural, structurally identical sentences embedded in noise. Sentences were semantically unpredictable, but meaningful. Using magnetoencephalography (MEG), we analysed speech tracking (quantified by mutual information [MI]) in source-localised data at the single-trial level. We then directly tested the perceptual relevance of speech tracking at timescales mapping onto linguistic categories such as phrasal elements, words, syllables, and phonemes.</p>
<sec id="sec001" sec-type="results">
<title>Results</title>
<sec id="sec002">
<title>Behavioural results</title>
<p>Participants listened to single sentences and indicated after each trial which (out of 4) target words occurred in the sentence. Participants reported the correct target word, on average, in 69.7 ± 7.1% (mean ± SD) of trials, with chance level being 25%. Performance of individual participants ranged from 56.1% to 81.1%, allowing a comparison between correct and incorrect trials for each participant.</p>
</sec>
<sec id="sec003">
<title>Overall speech tracking</title>
<p>The MI between the source-localised, Hilbert-transformed MEG time series and the Hilbert-transformed speech envelope was computed within 4 frequency bands. These reflected the rates of phrases (0.6–1.3 Hz), words (1.8–3 Hz), syllables (2.8–4.8 Hz), and phonemes (8–12.4 Hz) in the stimulus corpus (for an example sentence, see <xref ref-type="fig" rid="pbio.2004473.g001">Fig 1</xref>). The boundaries of each band were defined based on the slowest and fastest event per linguistic category across sentences (see <xref ref-type="sec" rid="sec012">Materials and methods</xref>). When compared to surrogate data, speech MI was significant in all analysed frequency bands (<xref ref-type="fig" rid="pbio.2004473.g002">Fig 2A</xref>; phrases: <italic>T</italic><sub>sum</sub>(19) = 32,262.9, <italic>p</italic> &lt; .001; words: <italic>T</italic><sub>sum</sub>(19) = 22,2243.8, <italic>p</italic> &lt; .001; syllables: <italic>T</italic><sub>sum</sub>(19) = 13,428.6, <italic>p</italic> &lt; .001; phonemes: <italic>T</italic><sub>sum</sub>(19) = 1,294.0, <italic>p</italic> = .018). As in previous studies, MI was strongest in early auditory areas [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref030">30</xref>] and decreased with increasing frequency [<xref ref-type="bibr" rid="pbio.2004473.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>]. Tracking of phrases, words, and syllables was reflected in a bilateral cluster, whereas phoneme tracking was only significant in the right hemisphere. These results confirm the previously reported existence of speech encoding in rhythmic brain activity versus a null hypothesis of no encoding but do not speak on the perceptual relevance.</p>
<fig id="pbio.2004473.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2004473.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Example sentence from the stimulus material.</title>
<p>Shown is the acoustic waveform (black line) as well as its segmentation into phrases, words, syllables, and phonemes (last word only). Example sentence deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.g001" xlink:type="simple"/>
</fig>
<fig id="pbio.2004473.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2004473.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Overall and perceptually relevant speech tracking in the 4 stimulus-tailored frequency bands.</title>
<p>(A) Significant areas for comparison between true MI values and surrogate data (<italic>t</italic> test, cluster-corrected). The used frequency bands map onto timescales for phrases (0.6–1.3 Hz), words (1.8–3 Hz), syllables (2.8–4.8 Hz), and phonemes (8–12.4 Hz). (B) Clusters where speech tracking MI was larger for correctly comprehended compared to incorrectly comprehended trials (<italic>t</italic> test, cluster-corrected). Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>]. MI, mutual information; n.s., not significant.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Perceptually relevant speech tracking</title>
<p>To localise cortical regions where entrainment was functionally relevant for comprehension, we statistically compared MI between correct and incorrect trials within each band (hereafter called ‘perceptually relevant’). This yielded significant left-hemispheric clusters in 2 frequency bands (<xref ref-type="fig" rid="pbio.2004473.g002">Fig 2B</xref>). For the phrasal timescale (0.6–1.3 Hz), MI was larger for correctly versus incorrectly comprehended trials in a cluster comprising left pre- and postcentral regions, supramarginal gyrus, and Heschl gyrus (HG; <italic>T</italic><sub>sum</sub>(19) = 568.00, <italic>p</italic><sub>cluster</sub> = .045; 205 grid points). The effect peaked in the left premotor (PM) cortex (left Brodmann area [BA] 6). For the word timescale (1.8–3 Hz), MI was larger in a cluster comprising left superior, middle, and inferior temporal gyrus as well as supramarginal gyrus and HG (<italic>T</italic><sub>sum</sub>(19) = 739.59, <italic>p</italic><sub>cluster</sub> = .018; 263 grid points). The effect peaked in the left middle temporal gyrus (MTG; left BA 21). There was a small overlap of the clusters for phrasal and word timescales, peaking in the left HG (left BA 41, see <xref ref-type="fig" rid="pbio.2004473.g003">Fig 3A</xref>).</p>
<fig id="pbio.2004473.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2004473.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Summary of perceptually relevant effects and control analyses.</title>
<p>(A) Brain regions with perceptually relevant speech tracking. For phrases (blue), the effect was strongest in the left PM area. For words (turquoise), it was strongest in the left MTG. Areas that were perceptually relevant for both phrases and words (yellow) include left HG and supramarginal gyrus. Peak grid points are denoted with circles. (B) Comparison of tracking MI values at peak grid points in PM cortex, MTG, and HG for phrase and word scales. Boxes denote interquartile range with median line; error bars show minimum and maximum, excluding outliers. (C) MI between the phase at the phrasal timescale (0.6–1.3 Hz) and power in beta, alpha, and theta bands (blue plots), as well as the phase at the word timescale (1.8–3 Hz) and beta power (turquoise plot) for correctly and incorrectly comprehended trials, averaged across all grid points in the motor cluster (cluster shown in inlet). Only PAC between the phase at the phrasal timescale and beta power was perceptually relevant. (D) Whole-brain analysis across all 12,337 grid points confirming that PAC between phrasal phase and beta power is indeed confined to motor regions. Coloured area denotes the cluster where PAC between phrasal phase and beta power was larger for correctly comprehended than uncomprehended trials. Significance in panel B and C is denoted with: *** = <italic>p</italic> &lt; .001, ** = <italic>p</italic> &lt; .01, * = <italic>p</italic> &lt; .05. Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>]. HG, Heschl gyrus; MI, mutual information; MTG, middle temporal gyrus; n.s., not significant; PAC, phase-amplitude coupling; PM, premotor.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.g003" xlink:type="simple"/>
</fig>
<p>We performed several further analyses to determine whether these effects were specific to those timescales extracted from the stimulus corpus. First, we performed posthoc <italic>t</italic> tests at the peak grid points of each cluster to see whether phrasal and word effects were indeed significant only for the respective timescales (<xref ref-type="fig" rid="pbio.2004473.g003">Fig 3B</xref>). As expected, MI differed between correct and incorrect trials at the phrasal timescale in left PM cortex and HG (<italic>t</italic>(19) = 4.90, <italic>p</italic><sub>FDR</sub> &lt; .001 and <italic>t</italic>(19) = 2.53, <italic>p</italic><sub>FDR</sub> = .031, respectively). Likewise, MI differed at the word timescale in the left MTG and HG (<italic>t</italic>(19) = 5.22, <italic>p</italic><sub>FDR</sub> &lt; .001 and <italic>t</italic>(19) = 3.48, <italic>p</italic><sub>FDR</sub> = .005, respectively). MI neither differed at the phrasal scale in MTG (<italic>t</italic>(19) = −1.78, <italic>p</italic><sub>FDR</sub> = .11) nor at the word scale in PM cortex (<italic>t</italic>(19) = −0.57, <italic>p</italic><sub>FDR</sub> = .58). We also compared correct and incorrect trials at the same peak grid points for syllable and phoneme timescales, although the whole-brain analysis did not indicate that effects were perceptually relevant for the task at these timescales. This was to make sure that effects had not been overlooked due to corrections for multiple comparisons. None of the comparisons were significant (all <italic>p</italic><sub>FDR</sub> &gt; .56, see <xref ref-type="supplementary-material" rid="pbio.2004473.s001">S1 Fig</xref>), indicating that none of the peak grid points in PM, HG, or MTG showed perceptual relevance at the faster scales of syllables or phonemes.</p>
<p>Second, we compared brain-wide MI values between correctly and incorrectly comprehended trials in 7 generic, 2 Hz–wide frequency bands (from 0–8 Hz, in 1-Hz steps) to confirm that the above intelligibility-related effects were indeed specific to frequency bands matched to the specific temporal structure of the speech material. This is an important contrast because most previous studies used generic bands with a predefined fixed frequency spacing. Perceptually relevant effects were found in only 2 bands (<xref ref-type="supplementary-material" rid="pbio.2004473.s002">S2A Fig</xref>). For the 1–3 Hz band, which largely overlaps with the word scale, MI was larger for comprehended than uncomprehended trials in a cluster centred around auditory cortex (<italic>T</italic><sub>sum</sub>(19) = 1,078.85, <italic>p</italic><sub>cluster</sub> = .030), confirming the relevance of auditory regions for word-level encoding. For the 2–4 Hz band, which spans the scale of words and syllables, MI was only marginally enhanced for comprehended sentences in a cluster covering middle and inferior temporal cortex (<italic>T</italic><sub>sum</sub>(19) = 751.93, <italic>p</italic><sub>cluster</sub> = .046).</p>
<p>Third, using posthoc statistics, we also verified that the MI at the previously identified peak grid points (see <xref ref-type="fig" rid="pbio.2004473.g003">Fig 3A</xref> for peaks) differed between correct and incorrect trials only at those timescales that matched the stimulus-specific bands (<xref ref-type="supplementary-material" rid="pbio.2004473.s002">S2B Fig</xref>). The motor cortex was not found to be perceptually relevant in any of the probed generic bands, which suggests that exact frequency boundaries are necessary to detect phrase tracking in motor areas.</p>
</sec>
<sec id="sec005">
<title>Phase-amplitude coupling in motor cortex</title>
<p>Rhythmic brain activity represents neuronal excitability changes [<xref ref-type="bibr" rid="pbio.2004473.ref032">32</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref033">33</xref>]. In auditory areas, this mechanism has been suggested to reflect a segmentation of the incoming sensory stream [<xref ref-type="bibr" rid="pbio.2004473.ref034">34</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref035">35</xref>]. But what is the role of slow excitability changes in the motor system? The motor system plays a role in the temporal prediction of rhythms and beats [<xref ref-type="bibr" rid="pbio.2004473.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref037">37</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref038">38</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref039">39</xref>]. Previous studies have suggested that these predictions rely on the coupling of delta phase to rhythmic activity in the beta band [<xref ref-type="bibr" rid="pbio.2004473.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref041">41</xref>]. We therefore hypothesised that speech entrainment at the phrasal scale—and its perceptual relevance—is directly linked to phase-amplitude coupling (PAC) with motor cortical beta activity. Indeed, we found that the coupling of the phase of phrasal-scale activity (0.6–1.3 Hz) to beta power (13–30 Hz) was significantly stronger for correctly versus incorrectly comprehended trials in our motor cluster (<italic>t</italic>(19) = 2.96, <italic>p</italic><sub>FDR</sub> = .032; <xref ref-type="fig" rid="pbio.2004473.g003">Fig 3C</xref>). In contrast, there was no such cross-frequency coupling for the phase of word-scale activity relative to beta power (<italic>t</italic>(19) = 1.14, <italic>p</italic><sub>FDR</sub> = .356) or of phrasal phase to either alpha (<italic>t</italic>(19) = 1.38, <italic>p</italic><sub>FDR</sub> = .356) or theta power (<italic>t</italic>(19) = −0.38, <italic>p</italic><sub>FDR</sub> = .708).</p>
<p>To confirm that the PAC between phrasal phase and beta power is confined to motor regions, we performed a further whole-brain analysis, comparing PAC between correct and incorrect trials. This analysis yielded 1 cluster in which PAC was larger for comprehended than uncomprehended trials (<italic>T</italic><sub>sum</sub> = 203.94, <italic>p</italic><sub>cluster</sub> = .030, one-sided; <xref ref-type="fig" rid="pbio.2004473.g003">Fig 3D</xref>). The cluster included left pre- and postcentral regions. Therefore, perceptually relevant PAC was confined to the left motor system, overlapping with the speech tracking effect in left motor areas.</p>
</sec>
<sec id="sec006">
<title>Phrasal tracking and PAC in additional dataset</title>
<p>The sentence structure in the present study was relatively rigid and predictable, which could have emphasised effects at the phrasal timescale. We therefore tested the presence of the phrasal tracking effect and the PAC between phrasal phase and beta power in the motor cortex in an additional dataset in which the sentence and phrase structure were more variable. Here, participants listened to a natural 7-min narration while their MEG was recorded [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref042">42</xref>]. The phrasal rate of the narration ranged between 0.1 Hz and 1.5 Hz. We specifically tested phrase tracking and PAC in the motor cluster, defined by results in the main dataset above, by contrasting the actual MI with surrogate data. Phrase tracking was larger in actual data (<italic>t</italic>(22) = 6.22, <italic>p</italic><sub>FDR</sub> &lt; .001), as was PAC between phrasal phase and beta power (<italic>t</italic>(22) = 4.52, <italic>p</italic><sub>FDR</sub> &lt; .001; <xref ref-type="fig" rid="pbio.2004473.g004">Fig 4</xref>). These results suggest that the found mechanisms in the present study also exist in an unrelated dataset with highly variable sentence structure.</p>
<fig id="pbio.2004473.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2004473.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Phrase tracking and PAC in motor cortex in an additional dataset.</title>
<p>MI values for phrase tracking and PAC were compared with surrogate data. Surrogate data were created by reversing the time series for speech and computing MI between forward brain time series and reversed speech time series. Surrogate data represent values that would be expected by chance. (A) Speech tracking at the phrasal time-scale (0.1–1.5 Hz) for real and surrogate data in the motor cortex (see inlet for analysed area). Phrase tracking was significantly larger in real data than in surrogate data. (B) MI between the phase at the phrasal timescale (0.1–1.5 Hz) and power in the beta band (13–30 Hz). PAC was significantly larger in real data than in surrogate data. Significance is denoted with: *** = <italic>p</italic> &lt; .001. Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>]. MI, mutual information; PAC, phase-amplitude coupling.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>By focusing on stimulus-specific timescales and measuring comprehension on individual trials, we show that distinct neural and linguistic timescales relate to speech encoding along the auditory–motor pathway. Our findings provide specific functional roles of speech tracking at two timescales within the delta band, one relevant in motor areas—accompanied by modulations in delta–beta coupled oscillations—and one relevant in temporal areas. Previous studies typically show that speech tracking peaks in early auditory areas, independent of the analysed frequency band [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref030">30</xref>]. Although a topographical distinction between hierarchical time-scales, as shown in the present data, has been hinted at before [<xref ref-type="bibr" rid="pbio.2004473.ref001">1</xref>], this has not been straightforwardly demonstrated.</p>
<sec id="sec008">
<title>The motor system predicts phrasal timing using beta oscillations</title>
<p>The motor system plays a causal role in speech perception [<xref ref-type="bibr" rid="pbio.2004473.ref043">43</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref044">44</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref045">45</xref>]. Previous studies have attributed functions for simulating speech production [<xref ref-type="bibr" rid="pbio.2004473.ref046">46</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref047">47</xref>] or sensorimotor speech processing [<xref ref-type="bibr" rid="pbio.2004473.ref048">48</xref>] to the motor system. Furthermore, the PM cortex and the motor system generally have been associated with generating temporal predictions [<xref ref-type="bibr" rid="pbio.2004473.ref049">49</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref051">51</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref052">52</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref053">53</xref>] and the processing of rhythms and beats [<xref ref-type="bibr" rid="pbio.2004473.ref037">37</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref038">38</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref039">39</xref>]. In the present study, we increase the knowledge about its role for natural speech processing by uncovering two specific neural mechanisms. The first mechanism is a perceptually relevant speech tracking specifically at the phrasal timescale, peaking in the left PM cortex. Notably, the timing of phrasal elements in the used stimulus corpus was relatively predictable because all sentences followed the same structure. The phrasal structure was also defined by prominent pauses between phrasal elements (evident in the clear peak in the frequency spectrum, <xref ref-type="supplementary-material" rid="pbio.2004473.s003">S3A Fig</xref>). On the other hand, words (and therefore syllables and phonemes) were not semantically—or temporally—predictable due to the recombination of words across sentences. The motor system likely exploited the temporally predictive phrasal information for parsing and segmenting the sentences, thus facilitating comprehension by providing a temporal prediction of when the relevant target word was likely to occur. Our results therefore suggest that perceptually relevant speech entrainment emerges not only at the time-scale of the directly task-relevant feature (here words) but also at those time-scales that can be exploited to better detect or encode this feature. We confirmed this motor mechanism in a second dataset, which featured a less stereotyped phrasal structure. Yet it is possible that this mechanism is not specific to the phrasal structure per se. Instead, it could be that the motor system would exploit any temporal regularities [<xref ref-type="bibr" rid="pbio.2004473.ref038">38</xref>], regardless of their linguistic or metalinguistic relevance. Future research is required to directly compare acoustic and linguistic regularities and their relevance for speech tracking.</p>
<p>It has been suggested that delta entrainment to speech in the left hemisphere reflects a motor-driven top-down modulation [<xref ref-type="bibr" rid="pbio.2004473.ref042">42</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref054">54</xref>]. These top-down modulations have been associated with beta oscillations [<xref ref-type="bibr" rid="pbio.2004473.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref055">55</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref056">56</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref057">57</xref>], which are prevalent in the motor system [<xref ref-type="bibr" rid="pbio.2004473.ref058">58</xref>]. Beta power in the motor system has also been related to speech comprehension [<xref ref-type="bibr" rid="pbio.2004473.ref057">57</xref>]. The finding that the temporal prediction of tone sequences is mediated by prestimulus delta–beta coupled oscillations further supports this hypothesis ([<xref ref-type="bibr" rid="pbio.2004473.ref036">36</xref>], see also [<xref ref-type="bibr" rid="pbio.2004473.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref041">41</xref>]). Here, we show—to our knowledge, for the first time—that such a cross-frequency mechanism also operates during the encoding and perception of continuous speech. This coupling is (i) specific to phrasal delta phase (0.6–1.3 Hz) and beta power (13–30 Hz) and (ii) only perceptually relevant in left motor areas. Furthermore, in an additional dataset, we show that this phrasal delta–beta coupling is also present during the processing of a natural, spontaneous narration. Based on the above-mentioned findings [<xref ref-type="bibr" rid="pbio.2004473.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref041">41</xref>], we speculate that this cross-frequency motor coupling reflects top-down temporal prediction, which is relevant both for the perception of simple sounds [<xref ref-type="bibr" rid="pbio.2004473.ref036">36</xref>] and speech.</p>
</sec>
<sec id="sec009">
<title>Word segmentation in the temporal cortex</title>
<p>Speech tracking at the word scale was perceptually relevant across the entire midtemporal gyrus, peaking in MTG and including superior and inferior temporal gyrus as well as inferior supramarginal gyrus. Previous MEG studies that have localised tracking processes typically show that this peaks in early auditory areas independent of the frequency band (for example, when contrasted with the null hypothesis of no speech encoding) [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref030">30</xref>]. Only by using a direct comprehension measure can we show that perceptually relevant word segmentation peaks in the left MTG. The MTG is associated with lexical semantic processes [<xref ref-type="bibr" rid="pbio.2004473.ref059">59</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref060">60</xref>] and is one endpoint of the ventral auditory pathway, mapping sound to meaning [<xref ref-type="bibr" rid="pbio.2004473.ref061">61</xref>]. It is plausible that stronger speech tracking, and therefore better word-scale segmentation in these regions, is directly linked to comprehension performance. The result that the effect at the word scale extends dorsally to supramarginal gyrus seems to contradict models of a ventral focus of word comprehension. However, it is consistent with the notion of a dorsal lexicon, thought to store articulatorily organised word form representations [<xref ref-type="bibr" rid="pbio.2004473.ref062">62</xref>].</p>
</sec>
<sec id="sec010">
<title>Specificity of linguistic timescales</title>
<p>An analysis of 2 Hz–wide generic bands showed that (i) the activity in the motor system was not predictive for comprehension in any generic band; (ii) the 1–3 Hz band, which largely overlaps with the word scale, yielded a similar pattern as the word-specific timescale; and (iii) the 2–4 Hz band also overlapped with the effect at the word timescale, albeit only minimally significantly (<xref ref-type="supplementary-material" rid="pbio.2004473.s002">S2 Fig</xref>). These results suggest that perceptually relevant speech tracking in the motor system is specific to the phrasal timescale in the stimulus material. In temporal regions, perceptually relevant tracking was found in the delta band (above 1 Hz and below 4 Hz), independent of the specific boundaries of the used bands (although 0–2 Hz did not yield a significant effect). This suggests that speech tracking in temporal areas emerges at more widespread timescales, perhaps because word length is more variable than phrasal length in the present stimulus material. Analyses of the coefficient of variation (<italic>c</italic><sub>v</sub>) supported this interpretation: when compared with phrases (<italic>c</italic><sub>v</sub> = 0.27), words varied in length almost twice as much (<italic>c</italic><sub>v</sub> = 0.48).</p>
<p>We chose to base the timescales on linguistic categories of phrases, words, syllables, and phonemes. This is the most pragmatic approach because the language system ultimately has to parse the speech stream into these segments. However, one could argue that these linguistic categories overlap with other metalinguistic elements that also follow temporal modulations below 4 Hz such as prosodic features [<xref ref-type="bibr" rid="pbio.2004473.ref016">16</xref>]. The most relevant prosodic features for speech segmentation are pauses, stress, and intonation [<xref ref-type="bibr" rid="pbio.2004473.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref063">63</xref>]. The phrases in the stimulus material are defined by pauses, and therefore phrasal timescale and timing of pauses can be considered one and the same. The interaction between linguistic categories and lexical stress is more complex. If we consider every third syllable as stressed [<xref ref-type="bibr" rid="pbio.2004473.ref064">64</xref>], one can derive a ‘stress timescale’ of 0.9 to 1.6 Hz, which partly overlaps with the phrasal timescale (0.6–1.3 Hz). The role of stress is manifold in speech (disambiguation of phonemically identical words, highlighting the meaning of words, metrical stress), and we cannot rule out that stressed syllables are reflected in neural activity. However, the segmentation into phrases does not typically have stressed syllables as boundaries because this would often yield nonsense phrases. Therefore, although stress is important and useful in speech comprehension, focusing on the phrasal timescale (as opposed to the ‘stress timescale’) is a direct way to address phrase segmentation. Fluctuations in pitch, or intonation, also occur in the delta band (see <xref ref-type="supplementary-material" rid="pbio.2004473.s003">S3B Fig</xref> for spectral analysis of pitch, or its acoustic correlate the fundamental frequency). Pitch fluctuations can signal phrasal boundaries [<xref ref-type="bibr" rid="pbio.2004473.ref065">65</xref>], and an overlap with the phrasal timescale is therefore not surprising. Because the auditory system is able to track pitch fluctuations [<xref ref-type="bibr" rid="pbio.2004473.ref009">9</xref>] and fundamental frequency and intensity are related, we cannot completely disentangle pitch tracking from envelope tracking. But language comprehension requires the grouping of words into phrases [<xref ref-type="bibr" rid="pbio.2004473.ref015">15</xref>], and focusing the analysis on the phrasal timescale is the most direct way of analysing phrasal processing. Future research needs to address the question of how much phrasal segmentation relies on the acoustic envelope, pitch fluctuations, or both. Taken together, linguistic and metalinguistic events in natural speech have a tendency to co-occur [<xref ref-type="bibr" rid="pbio.2004473.ref066">66</xref>], and their interaction is complex. However, for natural speech processing, the division into linguistic categories, as done in the present study, seems the most pragmatic and ecologically valid solution to gain specificity about speech comprehension effects.</p>
<p>Finally, in the present study, the average speech rate was approximately 130 words per minute. In two other studies that reported speech rate, it was considerably higher, at approximately 160 words per minute [<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>] and approximately 210 words per minute [<xref ref-type="bibr" rid="pbio.2004473.ref025">25</xref>]. The rate of syllables is typically associated with frequencies between 4 and 8 Hz [<xref ref-type="bibr" rid="pbio.2004473.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref067">67</xref>]. In the present study, it was 2.8 to 4.8 Hz, and in another study, it was even lower at 2 to 4 Hz [<xref ref-type="bibr" rid="pbio.2004473.ref004">4</xref>]. These differences demonstrate that, even in experimental contexts, speech rates can deviate from the assumed standard. Furthermore, a recent study has shown that the auditory system is not limited by traditionally imposed frequency bands [<xref ref-type="bibr" rid="pbio.2004473.ref057">57</xref>]. It therefore is highly beneficial to calculate stimulus-specific speech regularities for speech tracking analyses instead of applying generic frequency bands (cf. [<xref ref-type="bibr" rid="pbio.2004473.ref068">68</xref>] for visual modality).</p>
</sec>
<sec id="sec011">
<title>Not all speech tracking processes are perceptually relevant</title>
<p>Our results regarding overall speech tracking (compared with chance) replicate previous reports of widespread speech-to-brain entrainment at multiple timescales [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>]. However, in our data, only speech tracking in specific bands within the delta frequency range differed between trials with correct and incorrect comprehension and was therefore likely relevant for the perceptual outcome. One interpretation of why only the slow timescales were directly perceptually relevant is that the comprehension task focused on words, thus stressing the word timescale. Furthermore, the stereotyped phrasal structure of the sentence provided a temporal structure on which the emergence of the target word could be expected. Therefore, participants may have relied on the encoding of the phrasal structure to exploit its regularity, thereby stressing the phrasal timescale. However, it has been suggested that speech tracking in the delta and theta bands index different functional roles for speech perception [<xref ref-type="bibr" rid="pbio.2004473.ref069">69</xref>], such that theta tracking reflects the analysis of acoustic features and delta tracking reflects linguistic representations. In line with the this is the notion that only speech-to-brain entrainment in the delta band reflects active speech-specific processing, as opposed to a passive, low-level synchronisation to acoustic properties at other timescales [<xref ref-type="bibr" rid="pbio.2004473.ref030">30</xref>]. Therefore, these and our findings tentatively support the conclusion that only speech tracking in the delta band might indicate a speech-specific, perceptually relevant process during continuous speech processing.</p>
<p>Recent findings also highlight the distinction between widely distributed versus focal (but perceptually relevant) auditory encoding [<xref ref-type="bibr" rid="pbio.2004473.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref020">20</xref>] that could contribute to this pattern of results. In these accounts, perceptual choices are determined by the efficient readout of a restricted neural area, whereas widespread neural activity represents collateral processes of sensory processing. Therefore, such distributed processes could also explain the widespread speech tracking at all timescales we found when compared to chance level. In the present data, speech tracking at the syllabic and phonetic scales did not differ between trials with correct and incorrect comprehension. But for the participants to comprehend target words correctly, at least some syllables must have been encoded phonetically. It could be that the use of a noisy background prevented the robust encoding of individual syllables or phonemes, thus reducing the tracking at these timescales or reducing the statistical power in detecting between-trial differences. Furthermore, as mentioned above, the use of a word-related task could have highlighted effects at the word level and obscured effects at faster timescales. Additional work is required to understand whether speech tracking at the syllabic and phonetic timescales is indeed a robust marker of the actual neural encoding of these features or whether only speech tracking at timescales below the syllabic rate directly indexes functionally and perceptually relevant processes. Furthermore, the left-lateralised perceptually relevant speech tracking at slow timescales stands in contrast with bilateral overall speech tracking at these scales (<xref ref-type="fig" rid="pbio.2004473.g002">Fig 2</xref>). This supports the notion that ‘early’ acoustic processes are bilateral, whereas ‘higher-order’ speech comprehension is left-lateralised [<xref ref-type="bibr" rid="pbio.2004473.ref070">70</xref>].</p>
</sec>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec013">
<title>Ethics statement</title>
<p>All participants provided written informed consent prior to testing and received monetary compensation of £10 per h. The experiment was approved by a local ethics committee (College of Science and Engineering, University of Glasgow, application number 300140078) and conducted in compliance with the Declaration of Helsinki.</p>
</sec>
<sec id="sec014">
<title>Participants and data acquisition</title>
<p>Following previous sample sizes of MEG studies that used MI to study speech tracking [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref022">22</xref>], as well as previous recommendations [<xref ref-type="bibr" rid="pbio.2004473.ref071">71</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref072">72</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref073">73</xref>], 20 healthy, native British participants took part in the study (9 female, age 23.6 ± 5.8 years [mean ± SD], age range: 18 to 39 years). All participants were right-handed [Edinburgh Handedness Inventory; <xref ref-type="bibr" rid="pbio.2004473.ref074">74</xref>], had normal hearing [Quick Hearing Check; <xref ref-type="bibr" rid="pbio.2004473.ref075">75</xref>], and normal or corrected-to-normal vision. Furthermore, participants had no self-reported current or previous neurological or language disorders.</p>
<p>MEG was recorded with a 248-magnetometer, whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging, San Diego, CA) at a sampling rate of 1 KHz. Head positions were measured at the beginning and end of each run, using 5 coils placed on the participants’ heads. Coil positions were codigitised with head shape (FASTRAK, Polhemus Inc., Colchester, VT). Participants sat upright and fixated at a fixation point projected centrally on screen with a DLP projector. Sounds were transmitted binaurally through plastic earpieces, and 3.7 m–long plastic tubes connected to a sound pressure transducer. Stimulus presentation was controlled with Psychophysics toolbox [<xref ref-type="bibr" rid="pbio.2004473.ref076">76</xref>] for MATLAB (The MathWorks, Inc., Natick, MA).</p>
</sec>
<sec id="sec015">
<title>Stimuli</title>
<p>The stimulus material consisted of 2 structurally equivalent sets of 90 sentences (180 unique sentences in total) that were spoken by a trained, male, native British actor. The speaker was instructed to speak clearly and naturally. Sentences were constructed to be meaningful but unpredictable. Each sentence consisted of the same basic elements and therefore had the same structure. For example, the sentence ‘Did you notice, on Sunday night, Graham offered ten fantastic books’ consists of a ‘filler’ phrase (‘Did you notice’), a time phrase (‘on Sunday night’), a name, a verb, a numeral, an adjective, and a noun. There were 18 possible names, verbs, numerals, adjectives, and nouns that were each repeated 10 times. Sentence elements were randomly combined within a set of 90 sentences. To measure comprehension, a target word was included that was either the adjective in 1 set of sentences (‘fantastic’ in the above example or ‘beautiful’ in <xref ref-type="fig" rid="pbio.2004473.g001">Fig 1</xref>) or the number in the other set (for example, ‘twenty-one’). The duration of sentences ranged from 4.2 s to 6.5 s (5.4 ± 0.4 s [mean ± SD]). Sentences were presented at a sampling rate of 22,050 Hz.</p>
<p>During the experiment, speech stimuli were embedded in noise. The noise consisted of ecologically valid environmental sounds (traffic, car horns, people talking), combined into a mixture of 50 different background noises. The individual noise level for each participant was determined with a staircase procedure that was designed to yield a performance of around 70% correct. For the staircase procedure, only the 18 possible target words were used instead of whole sentences. Participants were presented with single target words embedded in noise and subsequently saw 2 alternatives on screen. They indicated by button press which word they had heard. Depending on whether their choice was correct or incorrect, the noise level was increased or decreased (one-up-three-down procedure) until a reliable level was reached. The average signal-to-noise ratio across participants was approximately −6 dB.</p>
</sec>
<sec id="sec016" sec-type="materials|methods">
<title>Experimental design</title>
<p>The 180 sentences were presented in 4 blocks with 45 sentences each. In each block, participants either indicated the comprehended adjective or the comprehended number, resulting in 2 ‘adjective blocks’ and 2 ‘number blocks’. The order of sentences and blocks was randomised for each participant. The first trial of each block was a ‘dummy’ trial that was discarded for subsequent analysis; this trial was repeated at the end of the block. After each sentence, participants were prompted with 4 target words (either adjectives or numbers) on the screen. They then had to indicate which one they heard by pressing 1 of 4 buttons on a button box. After 2 s, the next trial started automatically. Each block lasted approximately 10 min, and participants could rest in between blocks. The session, including instructions, questionnaires, preparation, staircase procedure, and 4 blocks, took approximately 3 to 3.5 hours.</p>
</sec>
<sec id="sec017">
<title>Speech preprocessing</title>
<p>For each sentence, we computed the wideband speech envelope at a sampling rate of 150 Hz following procedures of previous studies [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref077">77</xref>]. Acoustic waveforms were first filtered into 8 frequency bands (between 100 and 8,000 Hz; third-order Butterworth filter; forward and reverse) that were equidistant on the cochlear frequency map [<xref ref-type="bibr" rid="pbio.2004473.ref077">77</xref>]. From these 8 individual bands, the wideband speech envelope was extracted by averaging the magnitude of the Hilbert transformed signals from each band.</p>
<p>To define the timescales on which to probe speech encoding, we evaluated the rates of phrases, words, syllables, and phonemes in the stimulus material. For this, the duration between onsets of linguistic categories (i.e., phrases, words, and phonemes) was calculated. The exact onset timing was extracted from the speech signals using Penn Phonetics Lab Forced Aligner (P2FA; [<xref ref-type="bibr" rid="pbio.2004473.ref078">78</xref>]). Phrases were defined as the first 2 clauses in each sentence (for example, ‘I have heard’ and ‘on Tuesday night’). These phrases had distinct pauses (see <xref ref-type="fig" rid="pbio.2004473.g001">Fig 1</xref> for an example sentence) that determined the rhythm of the sentence (also visible in the frequency spectrum <xref ref-type="supplementary-material" rid="pbio.2004473.s003">S3A Fig</xref>). The syllable rate is generally difficult to assess [<xref ref-type="bibr" rid="pbio.2004473.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref079">79</xref>]. Here, we chose to count the actually produced syllables for each sentence. Finally, timescales were converted to frequencies, and the specific frequency bands for each category were then defined as the minimum and maximum frequencies across all 180 sentences. This led to the following bands: 0.6–1.3 Hz (phrases), 1.8–3.0 Hz (words), 2.8–4.8 Hz (syllables), and 8–12.4 Hz (phonemes). Mean and standard deviations for linguistic categories were as follows: 1.0 ± 0.1 Hz for phrases, 2.4 ± 0.3 Hz for words, 3.8 ± 0.4 Hz for syllables, and 10.4 ± 0.8 Hz for phonemes. Furthermore, the fundamental frequency for each sentence was extracted using Praat [<xref ref-type="bibr" rid="pbio.2004473.ref080">80</xref>]. This was used to determine the frequency spectrum of the pitch fluctuations (see <xref ref-type="supplementary-material" rid="pbio.2004473.s003">S3B Fig</xref>).</p>
</sec>
<sec id="sec018">
<title>MEG preprocessing</title>
<p>Preprocessing of MEG data was carried out in MATLAB (The MathWorks, Inc., Natick, MA) using the Fieldtrip toolbox [<xref ref-type="bibr" rid="pbio.2004473.ref081">81</xref>]. The 4 experimental blocks were preprocessed separately. Single trials were extracted from continuous data starting 2 s before sound onset and until 10 s after sound onset. MEG data were denoised using the reference signal. Known faulty channels (<italic>N</italic> = 7) were removed before further preprocessing. Trials with SQUID jumps (3.5% of trials) were detected and removed using Fieldtrip procedures with a cutoff <italic>z</italic>-value of 30. Before further artifact rejection, data were filtered between 0.2 and 150 Hz (fourth-order Butterworth filters, forward and reverse) and down-sampled to 300 Hz. Data were visually inspected to find noisy channels (4.37 ± 3.38 on average across blocks and participants) and trials (0.66 ± 1.03 on average across blocks and participants). Finally, heart and eye-movement artifacts were removed by performing an independent component analysis with 30 principal components. Data were further down-sampled to 150 Hz to match the sampling rate of the speech signal.</p>
</sec>
<sec id="sec019">
<title>Source localisation</title>
<p>Source localisation was performed using Fieldtrip, SPM8, and the Freesurfer toolbox. We acquired T1-weighted structural magnetic resonance images (MRIs) for each participant. These were coregistered to the MEG coordinate system using a semiautomatic procedure [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref006">6</xref>]. MRIs were then segmented and linearly normalised to a template brain (MNI space). A volume conduction model was constructed using a single-shell model [<xref ref-type="bibr" rid="pbio.2004473.ref082">82</xref>]. We projected sensor-level waveforms into source space using frequency-specific linear constraint minimum variance (LCMV) beamformers [<xref ref-type="bibr" rid="pbio.2004473.ref083">83</xref>] with a regularisation parameter of 7% and optimal dipole orientation (singular value decomposition method). Grid points had a spacing of 6 mm, resulting in 12,337 points covering the whole brain.</p>
</sec>
<sec id="sec020">
<title>Analysis of speech tracking in brain activity</title>
<p>We quantified the statistical dependency between the speech envelope and the source-localised MEG data using MI [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref034">34</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref084">84</xref>]. The speech envelopes, as well as MEG data, were filtered in the 4 frequency bands reflecting the rates of each linguistic category using third-order (for delta and theta bands) forward and reverse Butterworth filters. Within these bands, we computed the Hilbert transform and used real and imaginary parts for further analysis. Both parts were normalised separately and combined as a two-dimensional variable for the MI calculation [<xref ref-type="bibr" rid="pbio.2004473.ref084">84</xref>]. To take into account the stimulus–brain lag, we computed MI at 5 different lags (from 60 to 140 ms in 20-ms steps) and summed the MI values across lags. This procedure prevents spurious results that can occur when using a single lag. First, we calculated the overall MI for each source grid point. For a robust computation of MI values, we concatenated MEG and speech data from all trials. The resulting MI values were compared with surrogate data to determine their statistical significance. Surrogate data were created by randomly shuffling trials 50 times and averaging surrogate MI values across iterations. This repetition was necessary because all sentences followed the same structure and their envelope was often comparable, especially when filtered at low frequencies. We used a dependent <italic>t</italic> test for statistical comparison for each grid point and corrected for multiple comparisons with cluster-based permutation. Specifically, we used Monte-Carlo randomisation with 1,000 permutations and a critical <italic>t</italic> value of 2.1, which represents the critical value of the Student <italic>t</italic> distribution for 20 participants and a two-tailed probability of <italic>p</italic> = .05. The significance level for accepting clusters was 5%. We report summed <italic>t</italic> values (<italic>T</italic><sub>sum</sub>) as indicator of effect size.</p>
<p>For the analysis of perceptual relevance, we compared MI between trials in which participants responded correctly and incorrectly. Because the number of trials differed between these samples (on average, approximately 70% correct and 30% incorrect), we performed the calculations based on 80% of the minimally available number of trials. This way, the number of compared correct and incorrect trials was equal. However, because this included only a small part of all available trials, we repeated the analysis 20 times with a random selection of trials to yield representative values. The resulting MI values were averaged. Again, trials were concatenated to yield robust MI values. MI values between correct and incorrect trials were compared using the same method and parameters as for the comparison between overall MI and surrogate MI.</p>
<p>To examine the specificity of the effects, we compared MI between correct and incorrect trials for all peak grid points in both frequency bands (i.e., phrasal and word timescales). Peak grid points were those with the largest <italic>t</italic> values in each cluster and the largest summed <italic>t</italic> values for the overlap of grid points. This led to 12 comparisons (3 peak grid points × 4 frequency bands). MI values were compared using dependent sample <italic>t</italic> tests, corrected for multiple comparisons using the FDR method [<xref ref-type="bibr" rid="pbio.2004473.ref085">85</xref>].</p>
</sec>
<sec id="sec021">
<title>PAC</title>
<p>To examine the hypothesis that beta power is coupled with delta phase in the motor cluster and that this is perceptually relevant, we quantified PAC using the MI between beta power and delta phase. Phase and power were derived from Hilbert-transformed time series and filtered in the phrasal (0.6–1.3 Hz) and beta band (13–30 Hz). Phase was expressed as a unit magnitude complex number. To get an equal number of trials for correct and incorrect trials, we again took 80% of trials of the smaller sample, concatenated trials, and repeated the calculation 50 times. This was done for all grid points within the motor cluster (<italic>N</italic> = 205) and then averaged across grid points and iterations. PAC was compared between correct and incorrect trials across participants using a dependent sample <italic>t</italic> test.</p>
<p>We performed 3 control analyses within the motor cluster to address the frequency specificity of the effect. First, we analysed PAC between phrasal phase (0.6–1.3 Hz) and alpha power (8–12 Hz) as well as theta power (4–8 Hz). Second, we analysed PAC between the word phase (1.8–3 Hz) and beta power. All <italic>p</italic>-values were corrected for multiple comparisons using the FDR method [<xref ref-type="bibr" rid="pbio.2004473.ref085">85</xref>].</p>
<p>To address the spatial specificity of the delta–beta PAC, we also performed a whole-brain analysis. Based on the results in the motor cluster, we hypothesised that PAC should be larger in correct than incorrect trials. PAC between phrasal delta phase (0.6–1.3 Hz) and beta power (13–30 Hz) was compared between correct and incorrect trials, again equalling sample sizes by using 80% of the minimally available number of trials and repeating the analysis 20 times. PAC MI was averaged across all iterations and then compared between correct and incorrect trials across participants using a dependent sample <italic>t</italic> test for each grid point. To correct for multiple comparisons, we used the same parameters for cluster correction as in all previous analyses except that the significance level to choose significant clusters was one-sided, due to the clear hypothesis.</p>
</sec>
<sec id="sec022">
<title>Analysis of a previously published dataset</title>
<p>We analysed an additional and previously published dataset to confirm the present effects in the motor cortex. For this, we used data from 23 participants [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2004473.ref042">42</xref>] who passively listened to a 7-min continuous natural narration. Preprocessing and analysis were identical to the procedures of the main data. We compared (i) phrasal tracking and (ii) PAC between phrasal phase and beta power with surrogate data in the motor cortex. The phrasal rate of the speech stimulus was 0.5 ± 0.26 Hz (mean ± SD) and ranged between 0.1 Hz and 1.5 Hz. Surrogate data were created by reversing the time series for speech and computing MI between forward brain time series and reversed speech time series. This represents values that would be expected by chance [<xref ref-type="bibr" rid="pbio.2004473.ref002">2</xref>]. Values were computed for all grid points in the motor cluster and then spatially averaged. Actual MI values and surrogate data were compared using a dependent <italic>t</italic> test, and <italic>p</italic>-values for both tests were FDR corrected.</p>
<p>Data were deposited in the Dryad repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link>) [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>].</p>
</sec>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pbio.2004473.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Comparison of MI values at peak grid points in PM gyrus, MTG, and HG for syllable and phoneme scales.</title>
<p>Boxes denote interquartile range with median line; error bars show minimum and maximum, excluding outliers. None of the comparisons reached significance (all <italic>p</italic><sub>FDR</sub> &gt; .56, <italic>p</italic><sub>uncorrected</sub> &gt; .20). Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>]. FDR, false discovery rate; HG, Heshl gyrus; MI, mutual information; MTG, middle temporal gyrus; PM, premotor.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2004473.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Analyses in generic 2 Hz–wide bands.</title>
<p>Seven overlapping frequency bands were analysed (from 0–8 Hz, in 2 Hz–wide bands, in 1-Hz steps). The first 3 of these bands are displayed here. (A) Perceptually relevant tracking (larger MI for correctly comprehended than incorrectly comprehended trials) was found at the 1–3 Hz scale (<italic>T</italic><sub>sum</sub>(19) = 1,078.85, <italic>p</italic><sub>cluster</sub> = .030) and at the 2–4 Hz scale (<italic>T</italic><sub>sum</sub>(19) = 751.93, <italic>p</italic><sub>cluster</sub> = .046). Effects in all other bands were <italic>p</italic> &gt; .11. (B) Analysis of the peak grid points that showed the strongest effect in stimulus-specific bands. Larger MI for correctly than incorrectly comprehended trials is found in HG in the generic 1–3 Hz band (<italic>t</italic>(19) = 4.54, <italic>p</italic><sub>FDR</sub> = .002) and in MTG in the 2–4 Hz band (<italic>t</italic>(19) = 3.38, <italic>p</italic><sub>FDR</sub> = .014). All other comparisons are <italic>p</italic> &gt; .08. The peak grid point in the PM cortex does not show a comprehension modulation in any of the generic bands. Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>]. FDR, false discovery rate; HG, Heschl gyrus; MTG, middle temporal gyrus; PM, premotor; <italic>T</italic><sub>sum</sub>, summed <italic>t</italic> values.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2004473.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2004473.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Power spectral density estimates of speech envelope and pitch.</title>
<p>Welch’s periodograms are shown for speech envelopes (A) and fundamental frequency (F0-contours/pitch) (B) of all 180 stimulus sentences (thin gray lines) and their average (thick black line), for frequencies between 0.1 and 12 Hz (in 0.1-Hz steps). For envelope spectra, visible peaks that correspond to rates used in the analysis are marked with arrows (i.e., for phrases, words, and—less pronounced—syllables). Data deposited in the Dryad repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link> [<xref ref-type="bibr" rid="pbio.2004473.ref031">31</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Christian Keitel and Edwin Robertson for valuable comments on earlier versions of this manuscript, and Christoph Daube for providing the stimulus onsets in the seven-minute narration.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>BA</term>
<def><p>Brodmann area</p></def>
</def-item>
<def-item><term><italic>c</italic><sub>v</sub></term>
<def><p>coefficient of variation</p></def>
</def-item>
<def-item><term>FDR</term>
<def><p>false discovery rate</p></def>
</def-item>
<def-item><term>HG</term>
<def><p>Heschl gyrus</p></def>
</def-item>
<def-item><term>LCMV</term>
<def><p>linear constraint minimum variance</p></def>
</def-item>
<def-item><term>MEG</term>
<def><p>magnetoencephalography</p></def>
</def-item>
<def-item><term>MI</term>
<def><p>mutual information</p></def>
</def-item>
<def-item><term>MRI</term>
<def><p>magnetic resonance image</p></def>
</def-item>
<def-item><term>MTG</term>
<def><p>middle temporal gyrus</p></def>
</def-item>
<def-item><term>n.s.</term>
<def><p>not significant</p></def>
</def-item>
<def-item><term>PAC</term>
<def><p>phase-amplitude coupling</p></def>
</def-item>
<def-item><term>PM</term>
<def><p>premotor</p></def>
</def-item>
<def-item><term><italic>T</italic><sub>sum</sub></term>
<def><p>summed <italic>t</italic> values</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2004473.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Melloni</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name> (<year>2016</year>) <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nat Neurosci</source> <volume>19</volume>: <fpage>158</fpage>–<lpage>164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4186" xlink:type="simple">10.1038/nn.4186</ext-link></comment> <object-id pub-id-type="pmid">26642090</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hoogenboom</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title>. <source>PLoS Biol</source> <volume>11</volume>(<issue>12</issue>): <fpage>e1001752</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001752" xlink:type="simple">10.1371/journal.pbio.1001752</ext-link></comment> <object-id pub-id-type="pmid">24391472</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name> (<year>2012</year>) <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nat Neurosci</source> <volume>15</volume>: <fpage>511</fpage>–<lpage>517</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3063" xlink:type="simple">10.1038/nn.3063</ext-link></comment> <object-id pub-id-type="pmid">22426255</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doelling</surname> <given-names>KB</given-names></name>, <name name-style="western"><surname>Arnal</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Ghitza</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name> (<year>2014</year>) <article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title>. <source>Neuroimage</source> <volume>85</volume> <issue>Pt 2</issue>: <fpage>761</fpage>–<lpage>768</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name> (<year>2014</year>) <article-title>Cortical entrainment to continuous speech: functional roles and interpretations</article-title>. <source>Front Hum Neurosci</source> <volume>8</volume>: <fpage>311</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2014.00311" xlink:type="simple">10.3389/fnhum.2014.00311</ext-link></comment> <object-id pub-id-type="pmid">24904354</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keitel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name> (<year>2017</year>) <article-title>Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title>. <source>Neuroimage</source> <volume>147</volume>: <fpage>32</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.11.062" xlink:type="simple">10.1016/j.neuroimage.2016.11.062</ext-link></comment> <object-id pub-id-type="pmid">27903440</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chatterjee</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name> (<year>2014</year>) <article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title>. <source>Neuroimage</source> <volume>88C</volume>: <fpage>41</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name> (<year>2010</year>) <article-title>Auditory cortex tracks both auditory and visual stimulus dynamics using low-frequency neuronal phase modulation</article-title>. <source>PLoS Biol</source> <volume>8</volume>(<issue>8</issue>): <fpage>e1000445</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1000445" xlink:type="simple">10.1371/journal.pbio.1000445</ext-link></comment> <object-id pub-id-type="pmid">20711473</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Obleser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Herrmann</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Henry</surname> <given-names>MJ</given-names></name> (<year>2012</year>) <article-title>Neural Oscillations in Speech: Don't be Enslaved by the Envelope</article-title>. <source>Front Hum Neurosci</source> <volume>6</volume>: <fpage>250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2012.00250" xlink:type="simple">10.3389/fnhum.2012.00250</ext-link></comment> <object-id pub-id-type="pmid">22969717</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greenberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Carvey</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hitchcock</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>SY</given-names></name> (<year>2003</year>) <article-title>Temporal properties of spontaneous speech—a syllable-centric perspective</article-title>. <source>Journal of Phonetics</source> <volume>31</volume>: <fpage>465</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pellegrino</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Coupe</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Marsico</surname> <given-names>E</given-names></name> (<year>2011</year>) <article-title>A cross-language perspective on speech information rate</article-title>. <source>Language</source> <volume>87</volume>: <fpage>539</fpage>–<lpage>558</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname> <given-names>AA</given-names></name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>(<issue>7</issue>): <fpage>e1000436</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000436" xlink:type="simple">10.1371/journal.pcbi.1000436</ext-link></comment> <object-id pub-id-type="pmid">19609344</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Patel</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Butler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>C</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Temporal modulations in speech and music</article-title>. <source>Neurosci Biobehav Rev</source> <volume>81</volume>: <fpage>181</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2017.02.011" xlink:type="simple">10.1016/j.neubiorev.2017.02.011</ext-link></comment> <object-id pub-id-type="pmid">28212857</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghitza</surname> <given-names>O</given-names></name> (<year>2017</year>) <article-title>Acoustic-driven delta rhythms as prosodic markers</article-title>. <source>Lang Cogn Neurosci</source> <volume>32</volume>: <fpage>545</fpage>–<lpage>561</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Henry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Gaston</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schmuck</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Friederici</surname> <given-names>AD</given-names></name> (<year>2016</year>) <article-title>Linguistic bias modulates interpretation of speech via neural delta-band oscillations</article-title>. <source>Cereb Cortex</source>.</mixed-citation></ref>
<ref id="pbio.2004473.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goswami</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Leong</surname> <given-names>V</given-names></name> (<year>2013</year>) <article-title>Speech rhythm and temporal structure: Converging perspectives?</article-title> <source>Laboratory Phonology</source> <volume>4</volume>: <fpage>67</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghitza</surname> <given-names>O</given-names></name> (<year>2013</year>) <article-title>The theta-syllable: a unit of speech information defined by cortical function</article-title>. <source>Front Psychol</source> <volume>4</volume>: <fpage>138</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00138" xlink:type="simple">10.3389/fpsyg.2013.00138</ext-link></comment> <object-id pub-id-type="pmid">23519170</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cummins</surname> <given-names>F</given-names></name> (<year>2012</year>) <article-title>Oscillators and syllables: a cautionary note</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>364</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00364" xlink:type="simple">10.3389/fpsyg.2012.00364</ext-link></comment> <object-id pub-id-type="pmid">23060833</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bouton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chambon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tyrand</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Guggisberg</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Seeck</surname> <given-names>M</given-names></name>, <etal>et al</etal>. (<year>2018</year>) <article-title>Focal versus distributed temporal cortex activity for speech sound category assignment</article-title>. <source>Proc Natl Acad Sci U S A</source>.</mixed-citation></ref>
<ref id="pbio.2004473.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsunada</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>YE</given-names></name> (<year>2016</year>) <article-title>Causal contribution of primate auditory cortex to auditory perceptual decision-making</article-title>. <source>Nat Neurosci</source> <volume>19</volume>: <fpage>135</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4195" xlink:type="simple">10.1038/nn.4195</ext-link></comment> <object-id pub-id-type="pmid">26656644</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahissar</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Nagarajan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Protopapas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mahncke</surname> <given-names>H</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Speech comprehension is correlated with temporal response patterns recorded from auditory cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>98</volume>: <fpage>13367</fpage>–<lpage>13372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.201400998" xlink:type="simple">10.1073/pnas.201400998</ext-link></comment> <object-id pub-id-type="pmid">11698688</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giordano</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RAA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title>. <source>Elife</source> <volume>6</volume>.</mixed-citation></ref>
<ref id="pbio.2004473.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name> (<year>2013</year>) <article-title>Adaptive temporal encoding leads to a background-insensitive cortical representation of speech</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>5728</fpage>–<lpage>5735</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5297-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5297-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23536086</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelle</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>MH</given-names></name> (<year>2013</year>) <article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title>. <source>Cereb Cortex</source> <volume>23</volume>: <fpage>1378</fpage>–<lpage>1387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs118" xlink:type="simple">10.1093/cercor/bhs118</ext-link></comment> <object-id pub-id-type="pmid">22610394</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Di Liberto</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>O'Sullivan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Lalor</surname> <given-names>EC</given-names></name> (<year>2015</year>) <article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title>. <source>Curr Biol</source> <volume>25</volume>: <fpage>2457</fpage>–<lpage>2465</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.08.030" xlink:type="simple">10.1016/j.cub.2015.08.030</ext-link></comment> <object-id pub-id-type="pmid">26412129</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peña</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Melloni</surname> <given-names>L</given-names></name> (<year>2012</year>) <article-title>Brain oscillations during spoken sentence processing</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>24</volume>: <fpage>1149</fpage>–<lpage>1164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00144" xlink:type="simple">10.1162/jocn_a_00144</ext-link></comment> <object-id pub-id-type="pmid">21981666</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zoefel</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>VanRullen</surname> <given-names>R</given-names></name> (<year>2015</year>) <article-title>Selective perceptual phase entrainment to speech rhythm in the absence of spectral energy fluctuations</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>: <fpage>1954</fpage>–<lpage>1964</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3484-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3484-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25653354</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Melloni</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>W</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Characterizing neural entrainment to hierarchical linguistic units using electroencephalography (EEG)</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>11</volume>.</mixed-citation></ref>
<ref id="pbio.2004473.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ten Oever</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>van Atteveldt</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mehta</surname> <given-names>AD</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Low-frequency cortical oscillations entrain to subthreshold rhythmic auditory stimuli</article-title>. <source>J Neurosci</source> <volume>37</volume>: <fpage>4903</fpage>–<lpage>4912</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3658-16.2017" xlink:type="simple">10.1523/JNEUROSCI.3658-16.2017</ext-link></comment> <object-id pub-id-type="pmid">28411273</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Molinaro</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lizarazu</surname> <given-names>M</given-names></name> (<year>2018</year>) <article-title>Delta (but not theta)‐band cortical entrainment involves speech‐specific processing</article-title>. <source>European Journal of Neuroscience</source>. [Cited 18 January 2018]. Available from: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/ejn.13811" xlink:type="simple">10.1111/ejn.13811</ext-link></comment> <object-id pub-id-type="pmid">29283465</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keitel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name> (<year>2018</year>) <article-title>Data from: Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title>. <source>Dryad Digital Repository</source>. [Cited 2 March 2018]. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.1qq7050" xlink:type="simple">https://doi.org/10.5061/dryad.1qq7050</ext-link>.</mixed-citation></ref>
<ref id="pbio.2004473.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name> (<year>2010</year>) <article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title>. <source>Physiol Rev</source> <volume>90</volume>: <fpage>1195</fpage>–<lpage>1268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/physrev.00035.2008" xlink:type="simple">10.1152/physrev.00035.2008</ext-link></comment> <object-id pub-id-type="pmid">20664082</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Safaai</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sakata</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name> (<year>2015</year>) <article-title>Rhythmic auditory cortex activity at multiple timescales shapes stimulus-response gain and background firing</article-title>. <source>J Neurosci</source> <volume>35</volume>: <fpage>7750</fpage>–<lpage>7762</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0268-15.2015" xlink:type="simple">10.1523/JNEUROSCI.0268-15.2015</ext-link></comment> <object-id pub-id-type="pmid">25995464</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name> (<year>2015</year>) <article-title>Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha</article-title>. <source>J Neurosci</source> <volume>35</volume>: <fpage>14691</fpage>–<lpage>14701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2243-15.2015" xlink:type="simple">10.1523/JNEUROSCI.2243-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26538641</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Shah</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Knuth</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Ulbert</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Karmos</surname> <given-names>G</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex</article-title>. <source>J Neurophysiol</source> <volume>94</volume>: <fpage>1904</fpage>–<lpage>1911</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00263.2005" xlink:type="simple">10.1152/jn.00263.2005</ext-link></comment> <object-id pub-id-type="pmid">15901760</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnal</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Doelling</surname> <given-names>KB</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name> (<year>2015</year>) <article-title>Delta-beta coupled oscillations underlie temporal prediction accuracy</article-title>. <source>Cereb Cortex</source> <volume>25</volume>: <fpage>3077</fpage>–<lpage>3085</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhu103" xlink:type="simple">10.1093/cercor/bhu103</ext-link></comment> <object-id pub-id-type="pmid">24846147</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bengtsson</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Ullen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ehrsson</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Hashimoto</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kito</surname> <given-names>T</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Listening to rhythms activates motor and premotor cortices</article-title>. <source>Cortex</source> <volume>45</volume>: <fpage>62</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2008.07.002" xlink:type="simple">10.1016/j.cortex.2008.07.002</ext-link></comment> <object-id pub-id-type="pmid">19041965</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grahn</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Brett</surname> <given-names>M</given-names></name> (<year>2007</year>) <article-title>Rhythm and beat perception in motor areas of the brain</article-title>. <source>J Cogn Neurosci</source> <volume>19</volume>: <fpage>893</fpage>–<lpage>906</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2007.19.5.893" xlink:type="simple">10.1162/jocn.2007.19.5.893</ext-link></comment> <object-id pub-id-type="pmid">17488212</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morillon</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Wyart</surname> <given-names>V</given-names></name> (<year>2014</year>) <article-title>Motor contributions to the temporal precision of auditory attention</article-title>. <source>Nat Commun</source> <volume>5</volume>: <fpage>5255</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms6255" xlink:type="simple">10.1038/ncomms6255</ext-link></comment> <object-id pub-id-type="pmid">25314898</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cravo</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Rohenkohl</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wyart</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nobre</surname> <given-names>AC</given-names></name> (<year>2011</year>) <article-title>Endogenous modulation of low frequency oscillations by temporal expectations</article-title>. <source>J Neurophysiol</source> <volume>106</volume>: <fpage>2964</fpage>–<lpage>2972</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00157.2011" xlink:type="simple">10.1152/jn.00157.2011</ext-link></comment> <object-id pub-id-type="pmid">21900508</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saleh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Reimer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Penn</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ojakangas</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name> (<year>2010</year>) <article-title>Fast and slow oscillations in human primary motor cortex predict oncoming behaviorally relevant cues</article-title>. <source>Neuron</source> <volume>65</volume>: <fpage>461</fpage>–<lpage>471</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.02.001" xlink:type="simple">10.1016/j.neuron.2010.02.001</ext-link></comment> <object-id pub-id-type="pmid">20188651</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name> (<year>2015</year>) <article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title>. <source>Curr Biol</source> <volume>25</volume>: <fpage>1649</fpage>–<lpage>1653</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.04.049" xlink:type="simple">10.1016/j.cub.2015.04.049</ext-link></comment> <object-id pub-id-type="pmid">26028433</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Saygin</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Iacoboni</surname> <given-names>M</given-names></name> (<year>2004</year>) <article-title>Listening to speech activates motor areas involved in speech production</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>701</fpage>–<lpage>702</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1263" xlink:type="simple">10.1038/nn1263</ext-link></comment> <object-id pub-id-type="pmid">15184903</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watkins</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Strafella</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Paus</surname> <given-names>T</given-names></name> (<year>2003</year>) <article-title>Seeing and hearing speech excites the motor system involved in speech production</article-title>. <source>Neuropsychologia</source> <volume>41</volume>: <fpage>989</fpage>–<lpage>994</lpage>. <object-id pub-id-type="pmid">12667534</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fadiga</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Craighero</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Buccino</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rizzolatti</surname> <given-names>G</given-names></name> (<year>2002</year>) <article-title>Speech listening specifically modulates the excitability of tongue muscles: a TMS study</article-title>. <source>Eur J Neurosci</source> <volume>15</volume>: <fpage>399</fpage>–<lpage>402</lpage>. <object-id pub-id-type="pmid">11849307</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meister</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Deblieck</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Iacoboni</surname> <given-names>M</given-names></name> (<year>2007</year>) <article-title>The essential role of premotor cortex in speech perception</article-title>. <source>Curr Biol</source> <volume>17</volume>: <fpage>1692</fpage>–<lpage>1696</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2007.08.064" xlink:type="simple">10.1016/j.cub.2007.08.064</ext-link></comment> <object-id pub-id-type="pmid">17900904</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iacoboni</surname> <given-names>M</given-names></name> (<year>2008</year>) <article-title>The role of premotor cortex in speech perception: Evidence from fmri and rtms</article-title>. <source>Journal of Physiology-Paris</source> <volume>102</volume>: <fpage>31</fpage>–<lpage>34</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>McGettigan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Eisner</surname> <given-names>F</given-names></name> (<year>2009</year>) <article-title>A little more conversation, a little less action—candidate roles for the motor cortex in speech perception</article-title>. <source>Nature Reviews Neuroscience</source> <volume>10</volume>: <fpage>295</fpage>–<lpage>302</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2603" xlink:type="simple">10.1038/nrn2603</ext-link></comment> <object-id pub-id-type="pmid">19277052</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schubotz</surname> <given-names>RI</given-names></name> (<year>2007</year>) <article-title>Prediction of external events with our motor system: towards a new framework</article-title>. <source>Trends Cogn Sci</source> <volume>11</volume>: <fpage>211</fpage>–<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2007.02.006" xlink:type="simple">10.1016/j.tics.2007.02.006</ext-link></comment> <object-id pub-id-type="pmid">17383218</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnal</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name> (<year>2012</year>) <article-title>Cortical oscillations and sensory predictions</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>390</fpage>–<lpage>398</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.05.003" xlink:type="simple">10.1016/j.tics.2012.05.003</ext-link></comment> <object-id pub-id-type="pmid">22682813</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Radman</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Scharfman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name> (<year>2010</year>) <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>: <fpage>172</fpage>–<lpage>176</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2010.02.010" xlink:type="simple">10.1016/j.conb.2010.02.010</ext-link></comment> <object-id pub-id-type="pmid">20307966</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grahn</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Rowe</surname> <given-names>JB</given-names></name> (<year>2013</year>) <article-title>Finding and feeling the musical beat: striatal dissociations between detection and prediction of regularity</article-title>. <source>Cereb Cortex</source> <volume>23</volume>: <fpage>913</fpage>–<lpage>921</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs083" xlink:type="simple">10.1093/cercor/bhs083</ext-link></comment> <object-id pub-id-type="pmid">22499797</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breska</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Deouell</surname> <given-names>LY</given-names></name> (<year>2017</year>) <article-title>Neural mechanisms of rhythm-based temporal prediction: Delta phase-locking reflects temporal predictability but not rhythmic entrainment</article-title>. <source>PLoS Biol</source> <volume>15</volume>(<issue>2</issue>): <fpage>e2001665</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.2001665" xlink:type="simple">10.1371/journal.pbio.2001665</ext-link></comment> <object-id pub-id-type="pmid">28187128</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morillon</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hackett</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Kajikawa</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name> (<year>2015</year>) <article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title>. <source>Curr Opin Neurobiol</source> <volume>31</volume>: <fpage>230</fpage>–<lpage>238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2014.12.005" xlink:type="simple">10.1016/j.conb.2014.12.005</ext-link></comment> <object-id pub-id-type="pmid">25594376</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name> (<year>2010</year>) <article-title>Beta-band oscillations—signalling the status quo?</article-title> <source>Curr Opin Neurobiol</source> <volume>20</volume>: <fpage>156</fpage>–<lpage>165</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2010.02.015" xlink:type="simple">10.1016/j.conb.2010.02.015</ext-link></comment> <object-id pub-id-type="pmid">20359884</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morillon</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Baillet</surname> <given-names>S</given-names></name> (<year>2017</year>) <article-title>Motor origin of temporal predictions in auditory attention</article-title>. <source>Proceedings of the National Academy of Sciences</source>: 201705373.</mixed-citation></ref>
<ref id="pbio.2004473.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pefkou</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Arnal</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Fontolan</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name> (<year>2017</year>) <article-title>theta-band and beta-band neural activity reflects independent syllable tracking and comprehension of time-compressed speech</article-title>. <source>J Neurosci</source> <volume>37</volume>: <fpage>7930</fpage>–<lpage>7938</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2882-16.2017" xlink:type="simple">10.1523/JNEUROSCI.2882-16.2017</ext-link></comment> <object-id pub-id-type="pmid">28729443</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keitel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name> (<year>2016</year>) <article-title>Individual human brain areas can be identified from their characteristic spectral activation fingerprints</article-title>. <source>PLoS Biol</source> <volume>14</volume>(<issue>6</issue>): <fpage>e1002498</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002498" xlink:type="simple">10.1371/journal.pbio.1002498</ext-link></comment> <object-id pub-id-type="pmid">27355236</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname> <given-names>AD</given-names></name> (<year>2012</year>) <article-title>The cortical language circuit: from auditory perception to sentence comprehension</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>262</fpage>–<lpage>268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2012.04.001" xlink:type="simple">10.1016/j.tics.2012.04.001</ext-link></comment> <object-id pub-id-type="pmid">22516238</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leonard</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name> (<year>2014</year>) <article-title>Dynamic speech representations in the human temporal lobe</article-title>. <source>Trends in Cognitive Sciences</source> <volume>18</volume>: <fpage>472</fpage>–<lpage>479</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.05.001" xlink:type="simple">10.1016/j.tics.2014.05.001</ext-link></comment> <object-id pub-id-type="pmid">24906217</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saur</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kreher</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Schnell</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kummerer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kellmeyer</surname> <given-names>P</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Ventral and dorsal pathways for language</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>105</volume>: <fpage>18035</fpage>–<lpage>18040</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0805234105" xlink:type="simple">10.1073/pnas.0805234105</ext-link></comment> <object-id pub-id-type="pmid">19004769</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gow</surname> <given-names>DW</given-names></name> (<year>2012</year>) <article-title>The cortical organization of lexical knowledge: a dual lexicon model of spoken language processing</article-title>. <source>Brain Lang</source> <volume>121</volume>: <fpage>273</fpage>–<lpage>288</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandl.2012.03.005" xlink:type="simple">10.1016/j.bandl.2012.03.005</ext-link></comment> <object-id pub-id-type="pmid">22498237</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref063"><label>63</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Vaissière</surname> <given-names>J</given-names></name> (<year>1983</year>) <chapter-title>Language-independent prosodic features</chapter-title>. <source>Prosody: Models and measurements</source>: <publisher-name>Springer</publisher-name>. pp. <fpage>53</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kochanski</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Grabe</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Coleman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rosner</surname> <given-names>B</given-names></name> (<year>2005</year>) <article-title>Loudness predicts prominence: fundamental frequency lends little</article-title>. <source>J Acoust Soc Am</source> <volume>118</volume>: <fpage>1038</fpage>–<lpage>1054</lpage>. <object-id pub-id-type="pmid">16158659</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref065"><label>65</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Vaissière</surname> <given-names>J</given-names></name> (<year>2005</year>) <chapter-title>Perception of intonation</chapter-title>. <publisher-name>The handbook of speech perception</publisher-name>: <fpage>236</fpage>–<lpage>263</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lehiste</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Lass</surname> <given-names>NJ</given-names></name> (<year>1976</year>) <article-title>Suprasegmental features of speech</article-title>. <source>Contemporary issues in experimental phonetics</source> <volume>225</volume>: <fpage>239</fpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghitza</surname> <given-names>O</given-names></name> (<year>2012</year>) <article-title>On the role of theta-driven syllabic parsing in decoding speech: intelligibility of speech with a manipulated modulation spectrum</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>238</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00238" xlink:type="simple">10.3389/fpsyg.2012.00238</ext-link></comment> <object-id pub-id-type="pmid">22811672</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keitel</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Thut</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name> (<year>2017</year>) <article-title>Visual cortex responses reflect temporal structure of continuous quasi-rhythmic sensory stimulation</article-title>. <source>Neuroimage</source> <volume>146</volume>: <fpage>58</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.11.043" xlink:type="simple">10.1016/j.neuroimage.2016.11.043</ext-link></comment> <object-id pub-id-type="pmid">27867090</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kösem</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Van Wassenhove</surname> <given-names>V</given-names></name> (<year>2017</year>) <article-title>Distinct contributions of low-and high-frequency neural oscillations to speech comprehension</article-title>. <source>Language, Cognition and Neuroscience</source> <volume>32</volume>: <fpage>536</fpage>–<lpage>544</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelle</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>MH</given-names></name> (<year>2012</year>) <article-title>Neural Oscillations Carry Speech Rhythm through to Comprehension</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>320</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00320" xlink:type="simple">10.3389/fpsyg.2012.00320</ext-link></comment> <object-id pub-id-type="pmid">22973251</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simmons</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Simonsohn</surname> <given-names>U</given-names></name> (<year>2011</year>) <article-title>False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title>. <source>Psychological science</source> <volume>22</volume>: <fpage>1359</fpage>–<lpage>1366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797611417632" xlink:type="simple">10.1177/0956797611417632</ext-link></comment> <object-id pub-id-type="pmid">22006061</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bieniek</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name> (<year>2016</year>) <article-title>A robust and representative lower bound on object processing speed in humans</article-title>. <source>European Journal of Neuroscience</source> <volume>44</volume>: <fpage>1804</fpage>–<lpage>1814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/ejn.13100" xlink:type="simple">10.1111/ejn.13100</ext-link></comment> <object-id pub-id-type="pmid">26469359</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poldrack</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Durnez</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gorgolewski</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Matthews</surname> <given-names>PM</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>Scanning the horizon: towards transparent and reproducible neuroimaging research</article-title>. <source>Nature Reviews Neuroscience</source> <volume>18</volume>: <fpage>115</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2016.167" xlink:type="simple">10.1038/nrn.2016.167</ext-link></comment> <object-id pub-id-type="pmid">28053326</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oldfield</surname> <given-names>RC</given-names></name> (<year>1971</year>) <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>: <fpage>97</fpage>–<lpage>113</lpage>. <object-id pub-id-type="pmid">5146491</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koike</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Hurst</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Wetmore</surname> <given-names>SJ</given-names></name> (<year>1994</year>) <article-title>Correlation between the American-Academy-of-Otolaryngology-Head-and-Neck-Surgery 5-minute hearing test and standard audiological data</article-title>. <source>Otolaryngology-Head and Neck Surgery</source> <volume>111</volume>: <fpage>625</fpage>–<lpage>632</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/019459989411100514" xlink:type="simple">10.1177/019459989411100514</ext-link></comment> <object-id pub-id-type="pmid">7970802</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Delgutte</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name> (<year>2002</year>) <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source> <volume>416</volume>: <fpage>87</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/416087a" xlink:type="simple">10.1038/416087a</ext-link></comment> <object-id pub-id-type="pmid">11882898</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yuan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liberman</surname> <given-names>M</given-names></name> (<year>2008</year>) <article-title>Speaker identification on the SCOTUS corpus</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>123</volume>: <fpage>3878</fpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strauss</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>JL</given-names></name> (<year>2017</year>) <article-title>The syllable in the light of motor skills and neural oscillations</article-title>. <source>Language Cognition and Neuroscience</source> <volume>32</volume>: <fpage>562</fpage>–<lpage>569</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boersma</surname> <given-names>P</given-names></name> (<year>2001</year>) <article-title>Praat, a system for doing phonetics by computer</article-title>. <source>Glot International</source> <volume>5</volume>: <fpage>341</fpage>–<lpage>345</lpage>.</mixed-citation></ref>
<ref id="pbio.2004473.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oostenveld</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>JM</given-names></name> (<year>2011</year>) <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Comput Intell Neurosci</source> <volume>2011</volume>: <fpage>156869</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2011/156869" xlink:type="simple">10.1155/2011/156869</ext-link></comment> <object-id pub-id-type="pmid">21253357</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nolte</surname> <given-names>G</given-names></name> (<year>2003</year>) <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Phys Med Biol</source> <volume>48</volume>: <fpage>3637</fpage>–<lpage>3652</lpage>. <object-id pub-id-type="pmid">14680264</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Veen</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>van Drongelen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yuchtman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Suzuki</surname> <given-names>A</given-names></name> (<year>1997</year>) <article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title>. <source>IEEE Trans Biomed Eng</source> <volume>44</volume>: <fpage>867</fpage>–<lpage>880</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/10.623056" xlink:type="simple">10.1109/10.623056</ext-link></comment> <object-id pub-id-type="pmid">9282479</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Giordano</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>J</given-names></name>, <etal>et al</etal>. (<year>2017</year>) <article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title>. <source>Hum Brain Mapp</source> <volume>38</volume>: <fpage>1541</fpage>–<lpage>1573</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.23471" xlink:type="simple">10.1002/hbm.23471</ext-link></comment> <object-id pub-id-type="pmid">27860095</object-id></mixed-citation></ref>
<ref id="pbio.2004473.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name> (<year>1995</year>) <article-title>Controlling the False Discovery Rate—a Practical and Powerful Approach to Multiple Testing</article-title>. <source>Journal of the Royal Statistical Society Series B-Methodological</source> <volume>57</volume>: <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>