<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01398</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006446</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Network motifs</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Network reciprocity</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Scale-free networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Dimensionality in recurrent spiking networks: Global trends in activity and local origins in connectivity</article-title>
<alt-title alt-title-type="running-head">Dimensionality in recurrent spiking networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3576-9261</contrib-id>
<name name-style="western">
<surname>Recanatesi</surname> <given-names>Stefano</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9627-9576</contrib-id>
<name name-style="western">
<surname>Ocker</surname> <given-names>Gabriel Koch</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Buice</surname> <given-names>Michael A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Center for Computational Neuroscience, University of Washington, Seattle, Washington, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Allen Institute for Brain Science, Seattle, Washington, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Applied Mathematics, University of Washington, Seattle, Washington, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Diedrichsen</surname> <given-names>Jörn</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Western University, CANADA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">stefanor@uw.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>7</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>12</day>
<month>7</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>7</issue>
<elocation-id>e1006446</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>4</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Recanatesi et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006446"/>
<abstract>
<p>The dimensionality of a network’s collective activity is of increasing interest in neuroscience. This is because dimensionality provides a compact measure of how coordinated network-wide activity is, in terms of the number of modes (or degrees of freedom) that it can independently explore. A low number of modes suggests a compressed low dimensional neural code and reveals interpretable dynamics [<xref ref-type="bibr" rid="pcbi.1006446.ref001">1</xref>], while findings of high dimension may suggest flexible computations [<xref ref-type="bibr" rid="pcbi.1006446.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref003">3</xref>]. Here, we address the fundamental question of how dimensionality is related to connectivity, in both autonomous and stimulus-driven networks. Working with a simple spiking network model, we derive three main findings. First, the dimensionality of global activity patterns can be strongly, and systematically, regulated by local connectivity structures. Second, the dimensionality is a better indicator than average correlations in determining how constrained neural activity is. Third, stimulus evoked neural activity interacts systematically with neural connectivity patterns, leading to network responses of either greater or lesser dimensionality than the stimulus.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>New recording technologies are producing an amazing explosion of data on neural activity. These data reveal the simultaneous activity of hundreds or even thousands of neurons. In principle, the activity of these neurons could explore a vast space of possible patterns. This is what is meant by high-dimensional activity: the number of degrees of freedom (or “modes”) of multineuron activity is large, perhaps as large as the number of neurons themselves. In practice, estimates of dimensionality differ strongly from case to case, and do so in interesting ways across experiments, species, and brain areas. The outcome is important for much more than just accurately describing neural activity: findings of low dimension have been proposed to allow data compression, denoising, and easily readable neural codes, while findings of high dimension have been proposed as signatures of powerful and general computations. So what is it about a neural circuit that leads to one case or the other? Here, we derive a set of principles that inform how the connectivity of a spiking neural network determines the dimensionality of the activity that it produces. These show that, in some cases, highly localized features of connectivity have strong control over a network’s global dimensionality—an interesting finding in the context of, e.g., learning rules that occur locally. We also show how dimension can be much different than first meets the eye with typical “pairwise” measurements, and how stimuli and intrinsic connectivity interact in shaping the overall dimension of a network’s response.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>DMS-1514743</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>ES-B is supported by the NSF grant DMS-1514743. We gratefully acknowledge the support of the Swartz Foundation, through the Swartz Center for Theoretical Neuroscience at the University of Washington. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-07-24</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the manuscript and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A fundamental step toward understanding neural circuits is relating the structure of their dynamics to the structure of their connectivity [<xref ref-type="bibr" rid="pcbi.1006446.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref014">14</xref>]. However, the underlying networks are typically so complex that it is apriori unclear what features of the connectivity will matter most (and least) in driving network activity, and how the impacts of different connectivity features interact. Recent theoretical work has made progress in identifying rich and distinct roles for several different features of network connectivity: local connection structures [<xref ref-type="bibr" rid="pcbi.1006446.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref020">20</xref>], spatial profiles of coupling [<xref ref-type="bibr" rid="pcbi.1006446.ref021">21</xref>], low-rank connection structures [<xref ref-type="bibr" rid="pcbi.1006446.ref022">22</xref>], subnetwork statistics [<xref ref-type="bibr" rid="pcbi.1006446.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref024">24</xref>], clustered organization [<xref ref-type="bibr" rid="pcbi.1006446.ref025">25</xref>] (addressing connectivity properties in a complementary fashion from our present focus).</p>
<p>Here we focus on linking network connectivity to collective activity as quantified by the dimensionality of the neural response. This dimensionality summarizes the number of collective modes, or degrees of freedom, that the network’s activity explores. We use the “participation ratio” dimension, which is directly computable from the pairwise covariances among all cells in a population [<xref ref-type="bibr" rid="pcbi.1006446.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref028">28</xref>]. This connection is useful because the structure of pairwise covariance has been linked, in turn, to the fidelity of the neural code, both at the single neuron [<xref ref-type="bibr" rid="pcbi.1006446.ref029">29</xref>], and at the population levels [<xref ref-type="bibr" rid="pcbi.1006446.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref034">34</xref>]. Overall, the participation ratio has proven useful in interpreting properties of multi-units neuronal recordings [<xref ref-type="bibr" rid="pcbi.1006446.ref027">27</xref>], and has yielded a remarkable perspective on neural plasticity and how high dimensional responses can be optimal for general computations [<xref ref-type="bibr" rid="pcbi.1006446.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref002">2</xref>].</p>
<p>Two factors arise in our efforts to understand what it is about a network’s connectivity that determines the dimensionality of its activity. First, this process requires untangling two leading contributions to collective spiking: the reverberation of internal activity within the circuit, and its modulation by external inputs [<xref ref-type="bibr" rid="pcbi.1006446.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref037">37</xref>]. Experiments point out that both have strong effects [<xref ref-type="bibr" rid="pcbi.1006446.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref040">40</xref>], and they interact in rich ways that our analysis will begin to dissect.</p>
<p>Second, beyond providing general formulas, the understanding we seek demands that we identify relatively simple “observables” of complex network connectivity that systematically determine the dimensionality they produce. A natural approach is based on connection paths through networks, and how these can in turn be decomposed into local circuit micro-circuits, or “motifs” [<xref ref-type="bibr" rid="pcbi.1006446.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref042">42</xref>]. This is attractive because such local connectivity structures can be measured in tractable “multi-patch” type experiments, are limited in their complexity, and are controlled by local plasticity mechanisms. The prevalence of motifs, characterized in terms of connection probabilities and strengths, has achieved success in predicting the average levels of pairwise correlation among spiking cells—a measure of coordinated activity related to dimensionality in interesting ways that we will further explore below ([<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref020">20</xref>]; see also [<xref ref-type="bibr" rid="pcbi.1006446.ref043">43</xref>]). Here we deploy this framework to compute the dimensionality of spontaneous and stimulus-driven neural activity. We find that expressions based on just the details of small (and hence local) connection motifs give correct qualitative, and in some (but not other) cases quantitative, predictions of trends in dimensionality of global activity patterns. This underlines the utility of local network motifs as building blocks in bridging from network connectomics to network dynamics.</p>
<p>Our main findings are threefold: First, the dimensionality of global activity patterns can be strongly, and systematically, regulated by local connectivity structures. Second, for a wide range of networks this dimensionality can be surprisingly low (indicating strongly coordinated activity) even when the average correlations among pairs of neurons are very weak, cfr. [<xref ref-type="bibr" rid="pcbi.1006446.ref044">44</xref>]. Third, the dimensionality of stimulus evoked neural activity is controlled systematically by neural connectivity, leading to network responses that have either expanded or reduced the dimension of the original stimulus.</p>
<p>In what follows we will start by introducing the underlying theoretical framework. We describe the mathematical model, a spiking network of linearly interacting point process cells (a “Poisson linear network”, linearized GLM, or Hawkes process), together with the measure of dimensionality we use. We show how this dimensionality can be expressed in terms of connectivity motifs. We continue by analyzing the dimensionality of the spontaneous (internally generated) activity of an excitatory randomly connected network, and move to stimulus-driven networks of this type. Finally, we generalize our results to consider different connectivity topologies as well as excitatory-inhibitory balanced networks. We hinge the discussion around the question of how a network can modulate the dimensionality of its response to external stimuli by leveraging its local connectivity.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>In recent years, neuroscientists have developed a flexible framework for predicting how spike train correlations are guided by the structure of recurrent connections [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref045">45</xref>]. Here we present and extend this framework to compute the dimensionality of spontaneous and stimulus-driven activity. The expert reader, who may be well acquainted with all this material, may be able to start reading from later sections. In the same spirit we encourage the reader for whom the idea of network motifs is novel, to follow the more detailed presentation found in the Suppl. Mat. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> up to the result expressed in <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref>.</p>
<p>Throughout the paper bold lower-case letters will identify vectors, while bold upper case letters identify matrices. Non-bold letters identify scalar numbers.</p>
<sec id="sec003">
<title>The theoretical framework</title>
<p>Consider a recurrent neural network of <italic>N</italic> neurons where the activity <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) of neuron <italic>i</italic> at time <italic>t</italic> occurs around a baseline rate of irregular firing, which is set by the internal connectivity of the network <bold><italic>W</italic></bold> and an external input <bold><italic>ξ</italic></bold>(<italic>t</italic>). The spike train of neuron <italic>i</italic> is given by <inline-formula id="pcbi.1006446.e001"><alternatives><graphic id="pcbi.1006446.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where each spike is sampled from a Poisson distribution with instantaneous mean rate (intensity) <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>). The response of the whole network <bold><italic>y</italic></bold>(<italic>t</italic>) can then be captured by linearizing its dynamics around the baseline rates, giving the equation:
<disp-formula id="pcbi.1006446.e002"><alternatives><graphic id="pcbi.1006446.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>∞</mml:mi></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mspace width="-0.166667em"/><mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi mathvariant="bold-italic">s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="4pt"/><mml:mi mathvariant="normal">d</mml:mi> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mo>*</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi mathvariant="bold-italic">s</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where each entry of the vector <bold><italic>y</italic></bold>(<italic>t</italic>) is the instantaneous firing rate of neuron <italic>i</italic> at time <italic>t</italic> with baseline firing rate <bold><italic>y</italic></bold><sub>0</sub>. Here <italic>W</italic><sub><italic>ij</italic></sub> is the synaptic strength between neuron <italic>i</italic> and neuron <italic>j</italic>, and <bold><italic>A</italic></bold> is a diagonal matrix where <italic>A</italic><sub><italic>ii</italic></sub> is the postsynaptic filter which encapsulates the timecourse of the postsynaptic response. Thus, <italic>G</italic><sub><italic>ij</italic></sub> = <italic>A</italic><sub><italic>ii</italic></sub> ⋅ <italic>W</italic><sub><italic>ij</italic></sub> defines an effective connectivity matrix. Finally, <bold><italic>ξ</italic></bold> is the external input to the network. This model is pictured in <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1a</xref>, where the input <bold><italic>ξ</italic></bold> contributes to the baseline activity of each neuron, and the recurrent feedback is linearized.</p>
<fig id="pcbi.1006446.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Dimensionality of the activity of a generalized linear recurrent network (a “linearized inhomogeneous Poisson GLM.” [<xref ref-type="bibr" rid="pcbi.1006446.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref047">47</xref>]).</title>
<p><bold>a</bold>) Schematic of a generalized linear recurrent neural network. <bold>b</bold>) Spike train generated by the model, showing activity of the neurons in the model and binning procedure over time windows of length <italic>τ</italic>. <bold>c</bold>) Point cloud representation of the binned spike train in, neural space with coordinates as activity of single neurons. <bold>d</bold>) Example of a symmetric distribution of activities for three neurons, while the rest are silent. <bold>e</bold>) Example of an asymmetric distribution of the activities. <bold>f</bold>) Dimensionality of the neural activities as a function of average connectivity in SONET networks, with varied average connectivity and motif statistics.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g001" xlink:type="simple"/>
</fig>
<p>The stochastic spiking dynamics induced by <xref ref-type="disp-formula" rid="pcbi.1006446.e002">Eq 1</xref> leads (cfr. Supp. Mat.) to an equation for the covariance matrix <bold><italic>C</italic></bold> of the network response. For simplicity we present the result as a matrix of spike train auto- and cross-spectra at frequency <italic>ω</italic>, <bold><italic>C</italic></bold>(<italic>ω</italic>). This is the matrix of the Fourier transforms of the familiar auto- and cross-covariance functions; its zero mode <bold><italic>C</italic></bold>(0) is the the usual covariance matrix on which we will focus for the rest of this work [<xref ref-type="bibr" rid="pcbi.1006446.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref049">49</xref>]. Very usefully, this mode has been shown to yield an accurate approximation of correlations over any time window that is long enough to encompass the structure of neural correlograms [<xref ref-type="bibr" rid="pcbi.1006446.ref050">50</xref>]. The linearized dynamics, <xref ref-type="disp-formula" rid="pcbi.1006446.e002">Eq 1</xref> give rise to the covariance matrix as:
<disp-formula id="pcbi.1006446.e003"><alternatives><graphic id="pcbi.1006446.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>⟨</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>⟩</mml:mo> <mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">A</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi> <mml:mspace width="4pt"/><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munder> <mml:mo>+</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">A</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>*</mml:mo></mml:msup></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi> <mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:munder> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>The first term of <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> expresses how the variability in the activity of single neurons (the baseline covariance <bold><italic>C</italic></bold><sub>0</sub>) propagates through the network to induce internally-generated covariability. Similarly, external inputs with covariance <bold><italic>C</italic></bold><sub><italic>inp</italic></sub> give rise to covariances ((<bold><italic>A</italic></bold>(<italic>ω</italic>)<bold><italic>C</italic></bold><sub><italic>inp</italic></sub>(<italic>ω</italic>)<bold><italic>A</italic></bold>(<italic>ω</italic>)*) in the externally induced term), which then propagate through the network. (External inputs with low-rank correlations could reflect global fluctuations due to shifts in attention, vigilance state, or motor activity [<xref ref-type="bibr" rid="pcbi.1006446.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref051">51</xref>].)</p>
<p>Above we also introduced <inline-formula id="pcbi.1006446.e004"><alternatives><graphic id="pcbi.1006446.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where Δ<sub><italic>ij</italic></sub> is called a propagator as it reflects how a spike in neuron <italic>j</italic> propagates through the network to affect the activity of neuron <italic>i</italic>. <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> has been extensively studied in a number of frameworks [<xref ref-type="bibr" rid="pcbi.1006446.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref056">56</xref>].</p>
</sec>
<sec id="sec004">
<title>Measuring dimensionality</title>
<p>We aim to characterize the dimensionality of the distribution of population vector responses. Across many trials, these population vectors populate a cloud of points. The dimensionality is a weighted measure of the number of axes explored by that cloud:
<disp-formula id="pcbi.1006446.e005"><alternatives><graphic id="pcbi.1006446.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where λ<sub><italic>i</italic></sub> is the <italic>i</italic><sup><italic>th</italic></sup> eigenvalue of the covariance matrix <bold><italic>C</italic></bold>. The eigenvectors of the covariance matrix <bold>C</bold> are the axes of such cloud of points as in <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1c</xref>. If the components of <bold><italic>y</italic></bold> are independent and have equal variance, all the eigenvalues of the covariance matrix have the same value and <italic>Dim</italic>(<bold><italic>C</italic></bold>) = <italic>N</italic>. Alternatively, if the components are correlated so that the variance is evenly spread across M dimensions, only M eigenvalues would be nonzero and <italic>Dim</italic>(<bold><italic>C</italic></bold>) = <italic>M</italic> (<xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1d</xref>). For other correlation structures, this measure interpolates between these two regimes (<xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1e</xref>) and, as a rule of thumb, the dimensionality can be thought as corresponding to the number of dimensions required to explain about 80% of the total population variance in many settings [<xref ref-type="bibr" rid="pcbi.1006446.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref027">27</xref>].</p>
<p>Previous works have shown that the average correlation between neurons depends strongly on the motif structure of their connectivity [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref020">20</xref>]. We began by asking whether the same is true for the dimensionality. To do this, we generated random networks with a range of connection probabilities and, for each connection probability, a wide range of two-synapse motif frequencies (SONET networks; <xref ref-type="sec" rid="sec011">Methods</xref> c and e, and References [<xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref057">57</xref>]). In <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1d</xref> we plot the dimensionality of the network’s activity against the average probability of connection <italic>p</italic> (0 ≤ <italic>p</italic> ≤ 1) for an ensemble of SONET networks (cfr. <xref ref-type="sec" rid="sec011">Methods</xref> e for network details). The first notable observation from <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1f</xref> is that the dimensionality for such networks is strongly influenced by <italic>p</italic>: as <italic>p</italic> increases, the dimensionality decreases towards 1. Importantly, <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1f</xref> also shows a high range of variability in the dimension produced by networks with the <italic>same</italic> value of average connectivity <italic>p</italic>, indicating that the way that a given number of connections is arranged across the network also plays a strong role in determining the dimension of its activity <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1f</xref>. The dimensionality decreases in a narrow range of values of p around p = 0.08 (near the value p = 0.10 for which the spectral radius of the network approaches one). The wide span of dimensionality values produced in this narrow range points to the importance of predicting dimensionality and possibly of mechanisms that control it. Our next major goal is to describe how the statistics of connectivity motifs gives rise to this variability.</p>
</sec>
<sec id="sec005">
<title>Expressing the covariance in terms of network motifs</title>
<p>We review the main ideas of the theoretical framework that allows for an expansion of <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> in terms of connectivity motifs. For a more comprehensive description see Suppl. Mat. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> and [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>]. This framework aims to model the complexity of connectivity structures in real world networks, in terms of motif statistics. We first provide an intuitive idea and then a mathematical description of the framework.</p>
<p>A network with recurrent connections, as in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2a</xref>, can be characterized in terms of connectivity motifs. These building blocks quantify the amount of structure in the network by measuring the abundance of a specific connectivity features, or patterns. For example, in <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Fig 2a</xref> we highlighted in red a pattern made from two diverging branches of length 2 and 1, respectively, beginning from one neuron. This is called a (2,1) divergent motif, and its abundance, or probability of occurring in four randomly sampled cells in the network, is indicated by <inline-formula id="pcbi.1006446.e006"><alternatives><graphic id="pcbi.1006446.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>. The abundance of this pattern can also be measured relative to the probability of observing it given the abundance of its own building blocks: in this case, single connections, double connections (chains) and smaller divergent motifs (<inline-formula id="pcbi.1006446.e007"><alternatives><graphic id="pcbi.1006446.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>). Expressing the total probability of <inline-formula id="pcbi.1006446.e008"><alternatives><graphic id="pcbi.1006446.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> in terms of its building blocks is analogous to rewriting a moment <italic>μ</italic> in terms of its cumulants <italic>κ</italic> [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>]. This is illustrated in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2b</xref>. Importantly, this procedure of translating moments into cumulants turns out to be more than a simple “change of variables,” and allows one to resum the contributions of relatively small (or “low order”) motifs to abundance of motifs of any higher order. For example, it is possible to account for the contribution of chains of order 2 (two consecutive links) to any higher order motif. This is the key property identified in [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>] that we will use in developing predictions for global network activity based on local, low-order motifs. To provide intuition on how different motifs appear at different order we show in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2c</xref> examples of motifs and their cumulant description.</p>
<fig id="pcbi.1006446.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Description of connectivity motifs and cumulants.</title>
<p><bold>a</bold>) Motifs in the model of a recurrent neural network. In red is highlighted an example of a divergent motif. <bold>b</bold>) Example of the decomposition of a divergent motif into cumulants. <bold>c</bold>) Categorization of connectivity motifs into orders, with several example of the cumulants decomposition. Notably we highlight a novel motif: the trace motif <italic>μ</italic><sup>Tr</sup>, as an example of a 3rd order motif.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g002" xlink:type="simple"/>
</fig>
<p>This intuition translates into three main mathematical steps to highlight. We will introduce them in the case where the network does not receive any external input so that <bold><italic>C</italic></bold>(<italic>ω</italic>) = <bold>Δ</bold>(<italic>ω</italic>)<bold><italic>C</italic></bold><sub>0</sub>(<italic>ω</italic>)<bold>Δ</bold>(<italic>ω</italic>)* but they can be extended (cfr. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. S2) to the more general case where such an input is present.</p>
<p>The first step is to expand the propagator:
<disp-formula id="pcbi.1006446.e009"><alternatives><graphic id="pcbi.1006446.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo mathvariant="bold">Δ</mml:mo> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">G</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msup> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
By expressing <bold>Δ</bold> in this form we can then write <bold><italic>C</italic></bold> (dropping the dependency on <italic>w</italic>) via an expansion:
<disp-formula id="pcbi.1006446.e010"><alternatives><graphic id="pcbi.1006446.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mrow><mml:mo>*</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
where from now on we will consider the case where <bold><italic>C</italic></bold><sub>0</sub> is diagonal <inline-formula id="pcbi.1006446.e011"><alternatives><graphic id="pcbi.1006446.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>—as for the standard assumption and model of initially independent Poisson neurons that are then coupled together into a network. Then <xref ref-type="disp-formula" rid="pcbi.1006446.e010">Eq 5</xref> cum provides an intuitive description of the spike train cross-spectra in terms of paths through the network. This captures contributions to the cross-spectrum for paths that fork out of neuron <italic>k</italic> and end on one side in neuron <italic>i</italic> after <italic>m</italic> connections, and on the other side in neuron <italic>j</italic> after <italic>n</italic> connections. An example of such a path for <italic>m</italic> = 2 and <italic>n</italic> = 1, with corresponding <italic>i</italic>, <italic>j</italic>, <italic>k</italic> indices, is shown in red in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2a</xref>. The expression in <xref ref-type="disp-formula" rid="pcbi.1006446.e010">Eq 5</xref> cum has been studied extensively in previous works [<xref ref-type="bibr" rid="pcbi.1006446.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref059">59</xref>].</p>
<p>The framework in which we cast our theory relies on a second conceptual step, based on rewriting a function of the covariance <bold><italic>C</italic></bold>, <xref ref-type="disp-formula" rid="pcbi.1006446.e010">Eq 5</xref>, in terms of motifs. In the case of the where this function is the average covariance 〈<bold><italic>C</italic></bold>〉, this takes the form [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>]:
<disp-formula id="pcbi.1006446.e012"><alternatives><graphic id="pcbi.1006446.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>C</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>N</mml:mi> <mml:mn>3</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msup> <mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
Here, we assumed that cellular response properties are homogeneous <inline-formula id="pcbi.1006446.e013"><alternatives><graphic id="pcbi.1006446.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1006446.e014"><alternatives><graphic id="pcbi.1006446.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The motif moment <italic>μ</italic><sub><italic>m</italic>,<italic>n</italic></sub> measures the average strength of a (<italic>m</italic>, <italic>n</italic>)-motif composed of two paths (respectively of length <italic>m</italic> and <italic>n</italic>) connecting any neuron k with neuron i and j. The example of a (2,1)-motif is shown in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2a and 2b</xref>. Motifs of this kind, where paths originate from a common neuron, are called divergent motifs. We consider five kinds of motifs: convergent, divergent, chain, reciprocal and trace, depending on the direction of edges to the common node as illustrated in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2c</xref>. These motifs correspond to similar definitions to the one for <italic>μ</italic><sub><italic>m</italic>,<italic>n</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006446.e012">Eq 6</xref> (cfr. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. S2.1 for additional details). In networks where all synaptic weights have the same value, then <italic>μ</italic><sub><italic>m</italic>,<italic>n</italic></sub> is proportional to the frequency of the motif.</p>
<p>We can also define weighted motif statistics. For example:
<disp-formula id="pcbi.1006446.e015"><alternatives><graphic id="pcbi.1006446.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow> <mml:mi>u</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mi mathvariant="bold-italic">u</mml:mi> <mml:mo>/</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:msup> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <bold><italic>u</italic></bold> is a vector of norm 1 (||<italic>u</italic>|| = 1). For example, <bold><italic>u</italic></bold> could contain neuron’s firing rates, or be the eigenvectors of <bold><italic>W</italic></bold>. The case of <xref ref-type="disp-formula" rid="pcbi.1006446.e012">Eq 6</xref> corresponds to choosing the unit norm vector of constant entries, <inline-formula id="pcbi.1006446.e016"><alternatives><graphic id="pcbi.1006446.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. Ultimately the choice of <bold><italic>u</italic></bold> depends on the desired function of the covariance to compute (e.g. 〈<bold><italic>C</italic></bold>〉, Tr(<bold><italic>C</italic></bold>), <italic>Dim</italic>(<bold><italic>C</italic></bold>)…), on the structure of <bold><italic>G</italic></bold>, and on the presence or absence of inputs. In what follows this choice will be motivated in each case.</p>
<p>The last and crucial conceptual step of the theoretical framework is to re-sum the motif moments by rewriting them in terms of cumulants. The idea is to approximate the probability of finding a specific motif <italic>μ</italic><sub><italic>n</italic>,<italic>m</italic></sub> by iterative approximations built through the probabilities of finding the building blocks of that motif. For example, in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2b</xref> we see how the probability of motif <italic>μ</italic><sub>1,2</sub> to occur in the network can be subdivided in the probabilities of finding its building blocks: three synapses <inline-formula id="pcbi.1006446.e017"><alternatives><graphic id="pcbi.1006446.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, one synapse and one chain of length two <italic>κ</italic><sub>1</sub> <italic>κ</italic><sub>2</sub> and so on. The general relationship between moments and cumulants is [<xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>]:
<disp-formula id="pcbi.1006446.e018"><alternatives><graphic id="pcbi.1006446.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd> <mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:msub><mml:mi>κ</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>κ</mml:mi> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>κ</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:msub> <mml:msub><mml:mi>κ</mml:mi> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>s</mml:mi></mml:munderover> <mml:msub><mml:mi>κ</mml:mi> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where each <italic>κ</italic><sub><italic>n</italic></sub>, <italic>κ</italic><sub><italic>n</italic>,<italic>m</italic></sub> is a cumulant (respectively for chains and divergent motifs) and <inline-formula id="pcbi.1006446.e019"><alternatives><graphic id="pcbi.1006446.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi mathvariant="script">C</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the collection of ordered sets whose elements sum up to <italic>n</italic>. This step removes redundancies and improves the rate of convergence of the expansion, so that only relatively smaller motifs need to be measured and included. This is accomplished by “resumming,” via the identity:
<disp-formula id="pcbi.1006446.e020"><alternatives><graphic id="pcbi.1006446.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder> <mml:mo>[</mml:mo> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:munderover> <mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:msub><mml:mi>m</mml:mi> <mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>q</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>s</mml:mi></mml:munderover> <mml:msub><mml:mi>y</mml:mi> <mml:msub><mml:mi>m</mml:mi> <mml:mi>q</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mi>i</mml:mi></mml:msup> <mml:mo>]</mml:mo> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>[</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msub><mml:mi>y</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mi>j</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
that allows one to resum the contribution of each cumulant to any order in the expansion of <xref ref-type="disp-formula" rid="pcbi.1006446.e010">Eq 5</xref>. In this way the expression for a function of the covariance matrix assumes a closed form as a function of the cumulants (e.g. <xref ref-type="disp-formula" rid="pcbi.1006446.e012">Eq 6</xref> for the mean covariance).</p>
<p>Through the resumming procedure we are computing the contribution of any cumulant <italic>κ</italic> not to a specific term <bold><italic>G</italic></bold><sup><italic>m</italic></sup>(<bold><italic>G</italic></bold><sup><italic>T</italic></sup>)<sup><italic>n</italic></sup> but to the full sum <inline-formula id="pcbi.1006446.e021"><alternatives><graphic id="pcbi.1006446.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>m</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. In summary, this approach allows us to remove redundancies in motif statistics, and to isolate the impact solely due to higher order motif structures [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>].</p>
<p>The framework outlined above results in the ability to write any function of the covariance in terms of motif cumulants. Specifically, according to our interest here, the expressions for 〈<bold><italic>C</italic></bold>〉 and <italic>Dim</italic>(<bold><italic>C</italic></bold>) can be written in terms of a small subset of cumulants. In the following (cfr. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 2.4) we will explain how this framework can be deployed in computing <italic>Dim</italic>(<bold><italic>C</italic></bold>) for different networks, first in the absence of inputs, and then in their presence.</p>
</sec>
<sec id="sec006">
<title>Dimensionality of internally generated network activity</title>
<p>In our results we will include cumulants up to second order, although the expansion and theory can be taken to higher order. Second order cumulants correspond to chains <inline-formula id="pcbi.1006446.e022"><alternatives><graphic id="pcbi.1006446.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, convergent paths <inline-formula id="pcbi.1006446.e023"><alternatives><graphic id="pcbi.1006446.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, divergent paths <inline-formula id="pcbi.1006446.e024"><alternatives><graphic id="pcbi.1006446.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, reciprocal paths <inline-formula id="pcbi.1006446.e025"><alternatives><graphic id="pcbi.1006446.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and trace motifs <inline-formula id="pcbi.1006446.e026"><alternatives><graphic id="pcbi.1006446.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> as shown in <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2c</xref>. Mathematical definitions and more detailed explanations of the meaning of these cumulants can be found in the <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 2.1-2.4.</p>
<p>The expansion in terms of cumulants leads to the expression for the average covariance 〈<bold><italic>C</italic></bold>〉 ([<xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>]):
<disp-formula id="pcbi.1006446.e027"><alternatives><graphic id="pcbi.1006446.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
Notably, the contributions of chains and divergent motifs factor out in <xref ref-type="disp-formula" rid="pcbi.1006446.e027">Eq 10</xref>.</p>
<p>The expression for the dimensionality <italic>Dim</italic>(<bold><italic>C</italic></bold>) is the ratio between Tr(<bold><italic>C</italic></bold>)<sup>2</sup> and Tr(<bold><italic>C</italic></bold><sup>2</sup>), and these two quantities are general functions of the cumulants so that
<disp-formula id="pcbi.1006446.e028"><alternatives><graphic id="pcbi.1006446.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>k</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <inline-formula id="pcbi.1006446.e029"><alternatives><graphic id="pcbi.1006446.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="script">F</mml:mi></mml:math></alternatives></inline-formula> is a function whose full expression is shown in Methods a, in terms of its numerator Tr(<bold><italic>C</italic></bold>)<sup>2</sup> and denominator Tr(<bold><italic>C</italic></bold><sup>2</sup>). This full expression also shows that the dimensionality is directly related to the average covariance 〈<bold><italic>C</italic></bold>〉. Specifically, it turns out that the dependency of <italic>Dim</italic>(<bold><italic>C</italic></bold>) on <inline-formula id="pcbi.1006446.e030"><alternatives><graphic id="pcbi.1006446.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the same as that of 〈<bold><italic>C</italic></bold>〉, so that we can rewrite <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> as:
<disp-formula id="pcbi.1006446.e031"><alternatives><graphic id="pcbi.1006446.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="script">F</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>,</mml:mo> <mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>C</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
highlighting the role of convergent and trace motifs in regulating the relation between the average covariance and the dimensionality (a detailed expression of <xref ref-type="disp-formula" rid="pcbi.1006446.e031">Eq 12</xref> can be found in <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 2.3). The trace cumulants <inline-formula id="pcbi.1006446.e032"><alternatives><graphic id="pcbi.1006446.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> represent the statistics of motifs corresponding to patterns of connectivity that originate in one neuron and converge to a second neuron (<xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2c</xref>). We will show later how these statistics are highly correlated with reciprocal connections.</p>
<p>We next interpret and apply the formulas just described, which predict the dimension of network-wide activity in terms of localized connectivity motifs. We first use two classes of networks as examples: “purely random” Erdos-Reyni networks, and an exponential family of random graphs parameterized by second order motif statistics. While these are quite natural (but by no means automatic) cases for our theory, which is based on localized connectivity statistics, to succeed, we later apply it to different types of complex networks.</p>
<p>We begin by analyzing an interesting limit of <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref>: an Erdos-Reyni network. For a Erdos Reyni network all cumulants except for <inline-formula id="pcbi.1006446.e033"><alternatives><graphic id="pcbi.1006446.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (where <italic>p</italic> is the probability for each edge to be present in the graph) and the trace cumulants <inline-formula id="pcbi.1006446.e034"><alternatives><graphic id="pcbi.1006446.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are zero. In this limit <xref ref-type="disp-formula" rid="pcbi.1006446.e005">Eq 3</xref> becomes:
<disp-formula id="pcbi.1006446.e035"><alternatives><graphic id="pcbi.1006446.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>N</mml:mi> <mml:mi>p</mml:mi> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mspace width="4pt"/><mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>N</mml:mi> <mml:mi>p</mml:mi> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:msup> <mml:mspace width="4pt"/><mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
From this expression we see that when <inline-formula id="pcbi.1006446.e036"><alternatives><graphic id="pcbi.1006446.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>→</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> we obtain <italic>Dim</italic>(<bold><italic>C</italic></bold>) → <italic>N</italic> − 1.</p>
<p>This behavior can be interpreted in the following way: for <italic>p</italic> small enough that the structure of <bold><italic>C</italic></bold> is fully diagonal and all the elements are equal to <italic>c</italic><sub>0</sub>; in this regime all the neurons in the network act independently and contribute equally to <italic>Dim</italic>(<bold><italic>C</italic></bold>). As <italic>p</italic> increases more and more neurons start interacting and the dimensionality decreases until we obtain <italic>Dim</italic>(<bold><italic>C</italic></bold>) = 1. In <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3a</xref> we see how <xref ref-type="disp-formula" rid="pcbi.1006446.e035">Eq 13</xref> (red dashed line) is in agreement with the full expression for <italic>Dim</italic>(<bold><italic>C</italic></bold>) (green line) where <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> has been used for the internally generated covariance in spite of the cumulant approximation.</p>
<fig id="pcbi.1006446.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Theory of dimensionality in random excitatory recurrent networks through connectivity motifs.</title>
<p><bold>a</bold>) Dimensionality as a function of average connectivity in an Erdos-Renyi network. The full theory is in green while the theoretical approximation via the cumulant framework is shown in red. This color code is consistently used throughout the figure and paper. <bold>b</bold>) Dimensionality as a function of average connectivity in SONET networks. Highlighted in green (orange) is a point corresponding to a weakly (strongly) connected network. The inset shows the exact value of the dimensionality versus the approximated one. The gray line follows the ER case of panel a). <bold>c</bold>) Average correlation vs. average connectivity in SONET networks. <bold>d</bold>) Dimensionality vs. average connectivity in the ensemble of SONET networks used for the regression. Highlighted in orange is the point corresponding to the Erdos-Renyi network where the Taylor expansion is centered. <bold>e</bold>) Comparison between regression and Taylor coefficients. In green are the regressors of the multilinear regression of the dimensionality regressed against the cumulants, while in red are the Taylor coefficients of the expansion around the Erdos-Renyi network highlighted in panel e. <bold>f</bold>) Relation between trace cumulants for the ensemble of networks used in the regression. <bold>g</bold>) Relation between the trace cumulant and the reciprocal motifs in the ensemble of networks used in the regression. <bold>h</bold>) Dimensionality versus average correlation for the ensemble of networks used for panels b and c. The blue point highlights a point with relatively low dimensionality for a relatively low (and commonly observed) value of the average correlation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g003" xlink:type="simple"/>
</fig>
<p>To show the efficacy of <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> in capturing the dimensionality of network responses, we use this expression to compute <italic>Dim</italic>(<bold><italic>C</italic></bold>) in an ensemble of SONET networks [<xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>]. These (cfr. <xref ref-type="sec" rid="sec011">Methods</xref> f) are random networks where the probability of having a second order motif can be arbitrarily modified; such networks can therefore assume a wide range of values for second order motifs and cumulants. In <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b and 3c</xref> we show the dimensionality and average correlation values (given by 〈<bold><italic>C</italic></bold>〉/<italic>c</italic><sub>0</sub>) for a wide range of SONET networks, with a network’s dimensionality plotted against its connection probability <italic>p</italic>. Here, for each network we plot both the dimension computed via the full covariance formula <xref ref-type="disp-formula" rid="pcbi.1006446.e005">Eq 3</xref>, as well as via the cumulant truncation via <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> (red dots). Although the dimensionality varies strongly across networks with different motif statistics even at a fixed value of <italic>p</italic> (as was already pointed out in <xref ref-type="fig" rid="pcbi.1006446.g001">Fig 1f</xref>), the cumulant theory matches this variability closely across the range of SONET networks. This is shown in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b</xref> in two ways: for each network (every green dot) the corresponding theoretical approximation (corresponding red dot) lies right on top or closeby; the inset in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b</xref> confirms this by plotting dimension calculated via the cumulant approximation against the true values from the full covariance expression.</p>
<p>Together <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b</xref> shows that second order motifs contribute to the dimensionality of the response according to <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref>. However, from <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b</xref> it is not possible to single out the contribution of each motif. To address this question we consider an ensemble of SONET networks centered in their statistics around an Erdos-Renyi network with <italic>p</italic> = 0.08, corresponding to the orange dot in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b and 3d</xref> (see <xref ref-type="sec" rid="sec011">Methods</xref> f for details). The dimensionality for the response of each network in this ensemble is plotted against <italic>p</italic> in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d</xref>. Then we carry out a multilinear regression (see <xref ref-type="sec" rid="sec011">Methods</xref> f) of the dimensionality of this ensemble of networks against the values of each cumulant. The regression coefficients express how each cumulant influences the dimensionality (<italic>r</italic><sup>2</sup> = 0.994) (<xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref>) so that:
<disp-formula id="pcbi.1006446.e037"><alternatives><graphic id="pcbi.1006446.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mi>p</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mi>c</mml:mi></mml:msup> <mml:mi>h</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msup> <mml:mo>+</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where the <italic>α</italic>′<italic>s</italic> are the regression coefficients for each cumulant (green bars in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d</xref>). An increase of most cumulant, but not all, types of cumulants appears to lead to a decrease in dimensionality as most coefficients in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref> are negative. This is important as it suggests that adding most types of connectivity structure to a circuit generally lowers the dimensionality of the response.</p>
<p>In more detail, this analysis shows that, while increasing the average connectivity, chains, diverging and converging motifs leads to a decrease in dimensionality, terms contributing to the trace motifs may play a role in expanding the dimensionality. Complicating matters is that <inline-formula id="pcbi.1006446.e038"><alternatives><graphic id="pcbi.1006446.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006446.e039"><alternatives><graphic id="pcbi.1006446.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> are, in general, highly correlated in their values. This correlation is shown in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3f</xref> and it limits the applicability of the regression to the ensemble with respect to the trace cumulants, as can be seen in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref>. Regressing against two regressors which are highly correlated, like <inline-formula id="pcbi.1006446.e040"><alternatives><graphic id="pcbi.1006446.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006446.e041"><alternatives><graphic id="pcbi.1006446.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, is known to lead to opposite regression coefficients—as occurs in our case. We note that orthogonalizing the two regressors leads to qualitatively similar results (figure not shown), in line with the results to which we turn next. To get a theoretical handle on this, we analytically compute the Taylor coefficients of the expansion of the dimensionality formula <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> in terms of motifs, expanded around the Erdos-Renyi case (orange point in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d</xref>). The Taylor coefficients are the <italic>α</italic>′ <italic>s</italic> in the first order theoretical expansion of <xref ref-type="disp-formula" rid="pcbi.1006446.e037">Eq 14</xref> of the dimensionality formula. To ease reading of the resulting formulas we first define:
<disp-formula id="pcbi.1006446.e042"><alternatives><graphic id="pcbi.1006446.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
The expressions for Tr(<bold><italic>C</italic></bold>) and Tr(<bold><italic>C</italic></bold><sup>2</sup>) in the Erdos-Renyi case have then the form:
<disp-formula id="pcbi.1006446.e043"><alternatives><graphic id="pcbi.1006446.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
The expressions for the Taylor coefficients of second order motifs are:
<disp-formula id="pcbi.1006446.e044"><alternatives><graphic id="pcbi.1006446.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>∂</mml:mi> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:msub> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>4</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>5</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>∂</mml:mi> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:msub> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>∂</mml:mi> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:msub> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>∂</mml:mi> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="normal">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msub> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>C</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>∂</mml:mi> <mml:msup><mml:mi>k</mml:mi> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="normal">C</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msub> <mml:msub><mml:mo>|</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mi>N</mml:mi></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mn>4</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
A derivation with more details is available in the <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 2.5. These expressions represent the corresponding theoretical quantities for the regression coefficients of <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref> and are shown as red dots. As we see the Taylor coefficients provide a direct understanding of the effect of increasing different cumulants on the dimensionality. Moreover, as we show analytically in the <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref>, <italic>α</italic><sub><italic>ch</italic></sub> &lt; 0, and <italic>α</italic><sub><italic>div</italic></sub> = <italic>α</italic><sub><italic>conv</italic></sub> &lt; 0; thus, the effects of adding chain, diverging, or converging motifs to a given network is to drive down the dimension of the activity that it produces.</p>
<p>Although the regression fails to capture the right quantitative expressions for the trace motifs (see <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref>), it does suggest that these terms play a key role in regulating the dimensionality. The two corresponding Taylor coefficients are opposite in sign and their sum is positive pointing to Trace motifs as the only factor which enables the dimensionality to increase, this results are in line with the regression analysis. Trace motifs appear as critical features in regulating dimensionality, so we now elucidate more clearly their structure. The key contribution to trace motifs are reciprocal motifs. At second order the two are in tight one to one correspondence, as can be observed in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3g</xref> where the high correlation between the two is highlighted. At higher orders trace motifs may have more complicated forms (cfr. <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3g</xref> right side), but reciprocal connections maintain their key role as building blocks of such motifs. Thus a good intuition for trace motifs may simply be derived by thinking them at first as reciprocal connections. Our results point to such reciprocal connections as major players in determining the overall behavior of the dimensionality.</p>
<sec id="sec007">
<title>Dimensionality versus average covariance</title>
<p>Finally, in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3h</xref>, we show how the dimensionality is related to the average pairwise spike count correlation across the range of SONET networks. Importantly, we see that dimensionality attains very low values, even when the average correlation values are very weak. For example, when average correlations 〈<bold><italic>C</italic></bold>〉/<italic>c</italic><sub>0</sub> = 0.025, we see that <italic>Dim</italic>(<bold><italic>C</italic></bold>) = 0.5 (blue point in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3h</xref>). In other words, when cells appear almost uncorrelated on average (≈ 2 − 3%), the overall dimensionality of spiking activity can be cut by half compared with the uncoupled case.</p>
<p>While this phenomenon could be foreseen by looking closely at <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b and 3c</xref> we highlight it here as it helps to reconcile two observations often seen in the literature: relatively weak activity correlations [<xref ref-type="bibr" rid="pcbi.1006446.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref044">44</xref>] yet relatively low activity dimension [<xref ref-type="bibr" rid="pcbi.1006446.ref001">1</xref>]. We note that [<xref ref-type="bibr" rid="pcbi.1006446.ref044">44</xref>] has made closely related findings about highly restricted sets of firing patterns that can be implied by weak pairwise correlations. In our framework, the dimensionality can be tightly linked to the average covariance, see <xref ref-type="disp-formula" rid="pcbi.1006446.e031">Eq 12</xref>, but we also show that converging and trace motif cumulants influences the dimensionality but not the average covariance (see Eqs <xref ref-type="disp-formula" rid="pcbi.1006446.e031">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1006446.e059">22</xref> in <xref ref-type="sec" rid="sec011">Methods</xref> a). This points to dimensionality not only as a comprehensive measure of how coordinated network activity is, but also also as a more sensitive means to assess how coupling is coordinating that network activity (cfr. [<xref ref-type="bibr" rid="pcbi.1006446.ref044">44</xref>]). We will further expand on this important point in the Discussion.</p>
</sec>
</sec>
<sec id="sec008">
<title>Dimensionality of stimulus-driven responses</title>
<p>In <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> we highlighted two contributions to the total covariance of the network activity. The first is due to the internally generated activity (the reverberation of the stochastic Poissonian spiking through the network), and the second is due to the inputs to the network. While in the previous sections we have analyzed the dimensionality of the network response in the absence of inputs, here we generalize the results to include their contribution. The interplay between the connectivity of the network and the inputs can be captured by <xref ref-type="disp-formula" rid="pcbi.1006446.e005">Eq 3</xref> def where we expressed <bold><italic>C</italic></bold> as <bold><italic>C</italic></bold> = <bold><italic>C</italic></bold><sub><italic>int</italic></sub> + <bold><italic>C</italic></bold><sub><italic>ext</italic></sub>:
<disp-formula id="pcbi.1006446.e045"><alternatives><graphic id="pcbi.1006446.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
We decompose the input covariance <bold><italic>C</italic></bold><sub><italic>inp</italic></sub> into <italic>N</italic><sub><italic>inp</italic></sub> orthogonal unitary factors <bold><italic>ξ</italic></bold>, so that <inline-formula id="pcbi.1006446.e046"><alternatives><graphic id="pcbi.1006446.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. The external input to the network might arise from the spontaneous or evoked activity of other areas; regardless, it can be modeled as a sum of independent contributions where the number of factors <italic>N</italic><sub><italic>inp</italic></sub> and the individual strength of these factors <italic>c</italic><sub><italic>ξ</italic>,<italic>i</italic></sub> has to be determined.</p>
<p>The theory introduced in previous sections needs to be extended to reflect a crucial fact: the input may target different neurons in the network to a different degree. This is typically modeled with an input matrix to the network. In our model this is part of <bold><italic>C</italic></bold><sub><bold><italic>ext</italic></bold></sub>, because its effect can be included in the input by redefining <bold><italic>ξ</italic></bold> → <bold><italic>W</italic></bold><sub><italic>input</italic></sub> <bold><italic>ξ</italic></bold> in <xref ref-type="disp-formula" rid="pcbi.1006446.e002">Eq 1</xref>. In turn, connections from and to specific neurons will be more important than others in driving network-wide activity. In previous sections <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2</xref>, we introduced motif moments and cumulants by specifying that weights from different neurons were equally taking part to the computation of the dimensionality. This idea was rendered mathematically by using a uniform weight vector <inline-formula id="pcbi.1006446.e047"><alternatives><graphic id="pcbi.1006446.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> in defining and resumming motif cumulants. In the following we will also employ a set <italic>N</italic><sub><italic>inp</italic></sub> of vectors <bold><italic>u</italic></bold><sub><italic>ξ</italic>,<italic>i</italic></sub> = <bold><italic>ξ</italic></bold><sub><italic>i</italic></sub> to properly resum different contributions to the input structure and their reverberation through the network. In <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 3.2 we show how all these contributions can be dealt with and re-summed simultaneously via proper handling of <italic>weighted</italic> motifs and cumulants, building from [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>]. Here, each motif simply carries a weight corresponding to the product of input strengths for each of the neurons that compose it.</p>
<p>The resulting equations have function forms similar to the one of <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref>, but with weighted cumulants. Denoting with <italic>κ</italic><sub><italic>ext</italic></sub> the set of input weighted cumulants and with <bold><italic>κ</italic></bold><sub><bold><italic>int</italic></bold></sub> the set of internal cumulants employed in <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref>, we have:
<disp-formula id="pcbi.1006446.e048"><alternatives><graphic id="pcbi.1006446.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">κ</mml:mi> <mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi> <mml:mi mathvariant="bold-italic">n</mml:mi> <mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">κ</mml:mi> <mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
The full expression for this equation, in terms of its building blocks of <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref>, is given in Methods b. This equation formalizes the interplay between stimuli, connectivity, and internally generated activity in creating network activity with a particular dimension. This interplay is crucial to the understanding of network dynamics. Nevertheless, we emphasize that <xref ref-type="disp-formula" rid="pcbi.1006446.e048">Eq 19</xref> does not capture transient or dynamical features of the activity, as this equation (as our entire paper) focuses on the zero mode of the covariance matrix <bold><italic>C</italic></bold> thus pertains to the stationary features of the response. In what follows, we illustrate one aspect of this: how the strength and dimensionality of inputs to a network modify the “total” dimensionality of the network responses. While the limiting trends are exactly what one would expect—stronger inputs increasingly entrain the network response, and higher dimensional inputs lead to higher dimensional responses—both the limiting values of dimensionality and the approach to them depend on details of network connectivity.</p>
<p>To better illustrate this process, we study the response of two different networks: a weakly and a strongly connected network. These two cases correspond to the two points highlighted in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b</xref>: the green point (<italic>p</italic> = 0.03) to a weakly connected network, while the orange one (<italic>p</italic> = 0.08) to a more strongly connected one. In both cases the internally generated activity is uniformly weak or strong across all neurons. To gain more insight on how skewed distributions of intrinsic variances would affect our analysis we refer the reader to [<xref ref-type="bibr" rid="pcbi.1006446.ref026">26</xref>].</p>
<p>To begin, consider a weakly connected random network receiving <italic>N</italic><sub><italic>inp</italic></sub> input factors, each with the same strength <italic>c</italic><sub><italic>ξ</italic></sub>, so that <inline-formula id="pcbi.1006446.e049"><alternatives><graphic id="pcbi.1006446.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. We examine the dimensionality of the network response as a function of <italic>N</italic><sub><italic>inp</italic></sub> in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref>. Note that as <italic>N</italic><sub><italic>inp</italic></sub> grows, <italic>both</italic> the dimension of the input (<italic>N</italic><sub><italic>inp</italic></sub>) and its overall strength (variance <italic>N</italic><sub><italic>inp</italic></sub><italic>c</italic><sub><italic>ξ</italic></sub>) grow. The initial dimensionality in the absence of any input is close to 100%, then it decreases as more and more inputs are fed into the network, eventually growing with the number of inputs as these entrain the network activity. Both the extremes have dimensionality close to 100%, as shown in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref>, and in between there is a trade-off region where the low dimensionality of the input and the high dimensionality of the internal activity interact non-linearly as shown in <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref>. To better understand these trends we rewrite <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref> by using <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref> with <inline-formula id="pcbi.1006446.e050"><alternatives><graphic id="pcbi.1006446.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1006446.e051"><alternatives><graphic id="pcbi.1006446.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006446.e052"><alternatives><graphic id="pcbi.1006446.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> where we have highlighted the scaling factor of <bold><italic>C</italic></bold><sub><italic>inp</italic></sub>. The resulting expression is:
<disp-formula id="pcbi.1006446.e053"><alternatives><graphic id="pcbi.1006446.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mtext>Tr</mml:mtext> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mi>g</mml:mi> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mtext>Tr</mml:mtext> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mi>g</mml:mi> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>Δ</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>Δ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
In this formula we recognize that the limits highlighted above (absence of input regime and input dominated regime) correspond to the cases where either the terms in <inline-formula id="pcbi.1006446.e054"><alternatives><graphic id="pcbi.1006446.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> or in <inline-formula id="pcbi.1006446.e055"><alternatives><graphic id="pcbi.1006446.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> dominate, while intermediate cases are trading-off the contribution due to internal dynamics or external input. The distance of the full dimensionality from the diagonal (green region) measures the dimensionality expansion, where the input distribution is “inflated’ by the network’s noisy internal dynamics, <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4b</xref>.</p>
<fig id="pcbi.1006446.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Dimensionality of the network response in weakly and strongly connected excitatory recurrent networks.</title>
<p><bold>a</bold>) Dimensionality of stimulus driven responses as a function of the dimensionality of the stimulus in a weakly recurrent network (see text for important details on how the stimulus is defined). The line in green is the full theory while the line in red is the theoretical approximation in the cumulant framework. In light green is the area that marks the region of expansion of the dimensionality with respect to the input. <bold>b</bold>) Example of the expansion of the input to the network, schematized by the effect of the network in inflating the cloud of points. <bold>c</bold>) Dimensionality versus stimulus strength for a unidimensional input. <bold>d</bold>) Dimensionality versus stimulus dimensionality for a stimulus of fixed strength. The total strength is rather high so that the initial dimensionality for a unidimensional input is extremely low. <bold>e</bold>) Dimensionality of stimulus driven responses as a function of the dimensionality of the stimulus in a strongly recurrent network. The line in green is the full theory while the line in red is the theoretical approximation in the cumulant framework. In pink is a second approximation in the cumulant framework that accounts for a high dimensional input. The areas in orange and blue are mark respectively the cases of dimensionality expansion and reduction. <bold>f</bold>) Cartoons for examples of dimensionality reduction and expansion induced by the internal modes of a strongly recurrent network. These behaviors are induced by the strongly recurrent connectivity. <bold>g</bold>,<bold>h</bold>) Analogous panels to panels c and d for the strongly connected case.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g004" xlink:type="simple"/>
</fig>
<p>The non-monotonic behavior displayed in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref> can be explained as a trade-off of the two input properties introduced above: the input strength and dimensionality. The effect of the former can be understood in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4c</xref>, where we show how the dimensionality of the response decreases as a function of a gradually stronger unidimensional input (<italic>N</italic><sub><italic>inp</italic></sub> = 1 and increasing <italic>c</italic><sub><italic>ξ</italic></sub>). This behavior can be compared to established properties of stimulus driven dynamics in cortical circuits [<xref ref-type="bibr" rid="pcbi.1006446.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref061">61</xref>] where it has been observed that evoked activity suppresses the dimensionality of spontaneous activity. The influence of the latter factor, input dimensionality, is displayed in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4d</xref> where we provide the network an input of overall constant strength, of standard deviation <inline-formula id="pcbi.1006446.e056"><alternatives><graphic id="pcbi.1006446.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> (cfr. <xref ref-type="sec" rid="sec011">Methods</xref> g), with increasing number of factors (dimensions). In this case, as the inputs fully entrain the network response, the dimensionality constantly increases. The trend in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref> can be interpreted as a trade-off between these two trends, again recalling that stimulus dimension and strength increase together in that plot <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4c and 4d</xref>.</p>
<p>If we describe <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref> as passing stimuli into weakly coupled networks leading to an expansion of the input dimensionality, then <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e</xref> shows that strongly coupled networks lead to a more complex trend. At first the input dimensionality is expanded, but then it is compressed; overall, the network response never achieves the full dimensionality of the input. In other words, the response is always constrained by the network dynamics: a first phase of dimensionality expansion is followed by a second phase of dimensionality reduction (<xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4f</xref>).</p>
<p>These two phases can both be understood qualitatively in terms of the propagator Δ in <xref ref-type="disp-formula" rid="pcbi.1006446.e003">Eq 2</xref>, that restrains the total network dynamics. In <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e</xref> the theoretical prediction made by the second order cumulant framework (red line) agrees with the exact dimensionality from formula <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref> def inp (green line) only for a low dimensional input, but then departs. This can be attributed to the many ways with which the inputs can interact with the internal modes of the network: as the number of input factors increases, evidently, the term <bold><italic>C</italic></bold><sub><italic>int</italic></sub> ⋅ <bold><italic>C</italic></bold><sub><italic>ext</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006446.e053">Eq 20</xref> can no longer be captured by low order motif cumulants. In particular the motif cumulant approximation tends to overweight the importance of the input: the predictions for high <italic>N</italic><sub><italic>inp</italic></sub> in both <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a and 4e</xref> are similar. To weaken this limitation we show (pink line in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e</xref>) a second theoretical approximation, where the terms arising from the internal modes are disengaged from the input contribution in <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref>. See <xref ref-type="sec" rid="sec011">Methods</xref> g for more details. This approximation captures more closely the properties of the network when, in the case of high dimensional input <italic>N</italic><sub><italic>dim</italic></sub> the activity is mainly constrained by the internal modes. We denote the two approximations, red and pink lines in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e to 4h</xref>, respectively as the low and high dimensional input approximation. These two limits taken together show how low order cumulants are able to predict general trends in the dimensionality of driven responses.</p>
<p>Altogether we have shown in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4</xref> how the interaction between the input and the network dynamics gives rise to a number of scenarios where the input dimensionality can be expanded, reduced or somehow controlled through the internal recurrent dynamics. Specifically we point out three different scenarios:</p>
<list list-type="bullet">
<list-item>
<p>If the input has low dimensionality and the network has high dimensionality due to weakly recurrent connectivity, the network expands the dimensionality of the input. The dimensionality expansion is effectively an “inflation” of the input dimensionality into the high dimensional neural space of the recurrent network (see <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4b</xref>).</p>
</list-item>
<list-item>
<p>If the input has low dimensionality and the network has also a low dimensional internal response due to strongly recurrent connectivity, the network still expands the input dimensionality. This mechanism (see <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4f</xref> first case) is obtained as the input interacts with the internal activity of the network and their interaction adds up to create a new representation with higher dimensionality. This is mainly due to the constructive interaction between the internal and external covariance in the numerator of <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref>.</p>
</list-item>
<list-item>
<p>If the input has high dimensionality and the network has low dimensionality due to strongly recurrent connectivity, the network reduces the input dimensionality. This results from a “bottleneck” induced by the low dimensional recurrent dynamics of the network (see <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4f</xref> second case): the internal dynamics restrict high dimensional inputs to a lower dimensional subspace, as all they are projected onto the dominant eigenvectors of the network.</p>
</list-item>
</list>
<p specific-use="continuation">These points, as illustrated in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4</xref>, will be revisited in the Discussion.</p>
</sec>
<sec id="sec009">
<title>Complex and excitatory/inhibitory networks</title>
<p>Our results so far have shown a variety of phenomena in which the connectivity of a recurrent spiking network, and its resultant internal dynamics, shape its dimensionality. We have shown how this spectrum of behaviors can be interpreted in terms of the statistics of connectivity motifs: the theoretical framework introduced and illustrated in Figs <xref ref-type="fig" rid="pcbi.1006446.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006446.g004">4</xref> points to motif cumulants as the logical building blocks. Moreover, truncating motif cumulant expansions at second order, so that only very localized connectivity data enters, can lead to quantitatively accurate predictions of dimension of intrinsic network activity and qualitative predictions of trends in the presence of stimulus drive.</p>
<p>This said, above we have tested these results only for fully excitatory random networks, and for those that are either fully random (Erdos-Reyni) or are generated according to low order connectivity statistics (SONET networks). It is possible that either the theoretical framework proposed (cfr. <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3</xref>) or the dimensionality phenomena analyzed (cfr. <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4</xref>), may not generalize to more complex networks. To attest this, in this section we generalize the results to complex networks with other structures, and with both inhibitory and excitatory neurons.</p>
<p>We also introduced weighted cumulants to account for an input that was fed unevenly into different neurons within a networks. This is necessary as the cumulants originating in some neurons may have more impact on the network dynamics than others. The same argument holds true for the way internal activity in a recurrent network is generated intrinsically in a network, as some neurons, and their connectivity patterns, are known to have a stronger influence [<xref ref-type="bibr" rid="pcbi.1006446.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref063">63</xref>]. Thus, we make use of generalized motifs for the internal activity of the network to account for input effects, by using weight vectors <bold><italic>u</italic></bold> in <xref ref-type="disp-formula" rid="pcbi.1006446.e015">Eq 7</xref> that are chosen to be the eigenvectors of the connectivity matrix <bold><italic>G</italic></bold>. This choice is justified by the same logic as in the case of the input: the directions identified by the eigenvectors are the ones where the activity propagates, so that neurons which participate more to the dynamical mode of the network are weighted more in computing the cumulants. Weighting neurons and thus motifs in such a way therefore handles the relative importance of cumulants in propagating activity through the network in the directions of eigenmodes with eigenvalues near one (see <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. 3.2 and appendix of [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>] for further details).</p>
<p>To generalize our results we start by showing that the findings in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3</xref> hold true for a wider class of complex networks. Specifically we compare three different network topologies: the Erdos-Renyi case studied before, together with Small World and Free Scale (Albert-Barabasi model) networks. For each case, we vary a single common parameter (cfr. <xref ref-type="sec" rid="sec011">Methods</xref> h), the density (probability) of synapses in the network <italic>p</italic>. In <xref ref-type="fig" rid="pcbi.1006446.g005">Fig 5a to 5c</xref> we show three examples of the underlying weight matrices, one for each topology.</p>
<fig id="pcbi.1006446.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Dimensionality of networks with complex excitatory connectivity.</title>
<p><bold>a</bold>, <bold>b</bold>, <bold>c</bold>) Connectivity matrix <bold><italic>W</italic></bold> for the networks considered here: respectively Erdos-Renyi, Scale-Free, Small-World. <bold>d</bold>) Dimensionality as a function of average connectivity for the three classes of networks. Dashed red lines correspond to theoretical approximations based on second order motif cumulants, while continuous colored lines correspond to full (exact) values. <bold>e</bold>) Dimensionality versus average correlation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g005" xlink:type="simple"/>
</fig>
<p>We find that the dimensionality of the network response for the different connection topologies follows the same general trend: it decreases as a function of the average connectivity <italic>p</italic> (cfr. <xref ref-type="fig" rid="pcbi.1006446.g005">Fig 5b</xref>), until it reaches the boundary of instability for the dynamics. Interestingly, the relation between the average correlation and the dimensionality appears to be very tightly stereotyped as shown in <xref ref-type="fig" rid="pcbi.1006446.g005">Fig 5c</xref>. Such a tight relationship suggests that one may be able to interpret average correlations observed in a circuit in terms of their dimensionality, at least across some classes of network connectivity. Overall, these results suggest that the framework and results given so far do generalize to a more general class of excitatory networks.</p>
<p>In <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6</xref> we move beyond excitatory networks to consider the case of excitatory/inhibitory networks To do so we analyze a random Erdos-Renyi network where 10% of the neurons are randomly selected to be inhibitory and balance out, on average, the excitatory connection weight in the network (see <xref ref-type="sec" rid="sec011">Methods</xref> l for more details). The result of this process is a block Erdos-Renyi network with a non-trivial statistics of motifs and cumulants. The sign of the motifs reflects their excitatory, inhibitory or mixed nature. Importantly E-I networks tend to be more stable, which allows for stronger synapses overall. Taking advantage of this, we increase the average synaptic strength by changing its scaling from 1/<italic>N</italic> to <inline-formula id="pcbi.1006446.e057"><alternatives><graphic id="pcbi.1006446.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1006446.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref064">64</xref>].</p>
<fig id="pcbi.1006446.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006446.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Dimensionality in random balanced recurrent networks and the role of connectivity motifs.</title>
<p><bold>a</bold>) Dimensionality as a function of average connectivity in E-I balanced networks. Highlighted in orange is a network producing low dimensionality. <bold>b</bold>) Average correlation versus average connectivity. <bold>c</bold>) Dimensionality versus average correlation. <bold>e</bold>) Dimensionality as a function of stimulus dimension. The green line corresponds to the full theory, while the red and pink lines correspond to theoretical approximations in the second order cumulant framework, respectively for low and high stimulus dimensionality. The areas in orange and blue indicate, respectively, dimensionality expansion or reduction in the network (cfr. <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4f</xref>. <bold>f</bold>) Dimensionality versus average inhibitory connectivity for an ensemble of balanced networks with low dimensionality. The ensemble statistics are averaged around the network corresponding to the orange point. <bold>d</bold>) Regression of the dimensionality against the cumulants of the inhibitory population.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.g006" xlink:type="simple"/>
</fig>
<p>We see that the resulting relationship between the dimensionality of the network and the average synaptic connectivity <italic>p</italic> in <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6a</xref> is even stronger that in the fully excitatory case of <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3a</xref>. Specifically, <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6a</xref> shows that the dimensionality rapidly decreases as a function of the average connectivity, and—different from the purely excitatory case—does so with a very steep initial slope. Moreover, the dimensionality decreases very quickly as a function of average correlations <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6c</xref>, so that, once again, E-I networks whose activity might at first appear to be (at least on average) independent due to low values of average pairwise correlations actually show very tightly coordinated dynamics. We also find that the theoretical approximation (red dots), despite capturing the overall steeply decreasing trend, is in poor agreement with the full (exact) values of dimensionality. This is due to the fact that the theory shown is perturbative (we keep terms only up to second order cfr. <xref ref-type="fig" rid="pcbi.1006446.g002">Fig 2</xref>) and that excitatory/inhibitory networks require more resumming directions <bold><italic>u</italic></bold> due to their spectral properties. While these matters will be the focus of future work, at the price of increasing the complexity of our analysis [<xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>], we here wish to highlight the underlying limitations at second order, at least in our hands, while pointing out important trends in the relationship between the dimensionality of the response and other network properties. For example <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6b and 6c</xref> show that, in the balanced case, the theory approximates to a better extent the average correlations (<xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6b</xref>) than the dimensionality [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>].</p>
<p>To highlight the role of cumulants in controlling these effects we carry out a similar analysis to the one illustrated in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d to 3g</xref>. We compute the dimensionality for an ensemble of 500 SONETS networks of 1000 neurons each (see <xref ref-type="sec" rid="sec011">Methods</xref> m) with excitatory connectivity <italic>p</italic> = 0.03. The average connectivity between inhibitory neurons, together with the motif content, varies perturbatively around <italic>p</italic> = 0.03. How the dimensionality varies as a function of the probability of connection between inhibitory neurons is shown in <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6f</xref>, for each network in the ensemble. We then carry out a multilinear regression where the dimensionality of the network is regressed against all the values of the cumulants between neurons in the inhibitory population (<italic>r</italic><sup>2</sup> = 0.420). The result is shown in <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6d</xref>. This result is similar to the one shown in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e</xref> and shows how different motifs may lead to a dimensionality increase or decrease.</p>
<p>One of the main characteristics of E-I balanced networks is the cancellation between strong excitatory and inhibitory contributions. This, in turn, means that the network tends to be in a strongly coupled regime where the internal dynamics is strong and the inputs, rather than driving the network, are entrained to its dynamics. This is shown in <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6d</xref>, where the dimensionality of the network varies with the input dimensionality but the span over which the former is modulated by less than 30%, from a dimensionality of roughly 30% to a dimensionality of roughly 60%, over a wide range of input dimensions. If we imagine the input to itself vary in a reasonable range of, say, 30% then the network acts to equalize the dimensionality of its response across this range. Specifically, this seems to be achieved optimally at the minimum of the green line in <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6e</xref>, where the contribution of the input and internal network dynamics appears to be of similar strength. This may be an important working point for the network, as we will further cover in the Discussion.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>We have introduced a theory of dimensionality in linear, spiking recurrent networks, which predicts the dimensionality of a network’s response from basic features of its internal connectivity and the stimuli it receives. The theory builds on the existing framework of motif cumulants [<xref ref-type="bibr" rid="pcbi.1006446.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref043">43</xref>], which identified the significance of connectivity motifs in leading a number of other effects in the network dynamics. We single out three important results from our analysis for further discussion here.</p>
<p>First, we find that the statistics of highly local “second order” connectivity motifs—subnetworks of just two or three cells at a time—can be used to predict several (but not all) global aspects of the dimensionality of network-wide activity. These are as follows: for purely excitatory, autonomously spiking networks, the values of connection probability and the prevalance of second order connectivity motifs provides highly accurate quantitative predictions of dimension—and hence dimension appears to be regulated by these connection features alone. For excitatory-inhibitory networks, we can use these localized motifs to make qualitative predictions about trends in dimension with connectivity, but quantitative estimates have large errors. The same is true about the network response to strong inputs: trends can be captured from local motif cumulants, but quantitative accuracy demands a fuller description of network connectivity.</p>
<p>The ability, when it occurs, of local circuit features to regulate global activity patterns is important because local activity dependence appears as one of the major constraints in biological learning paradigms [<xref ref-type="bibr" rid="pcbi.1006446.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref067">67</xref>]. Thus, when it succeeds, expressing neural dynamics in terms of local connectivity motifs may reveal the function of learning rules, and how they target the dynamics of specific connectivity patterns [<xref ref-type="bibr" rid="pcbi.1006446.ref068">68</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref069">69</xref>].</p>
<p>Second, our results show that the dimensionality of the network activity has the tendency to assume low values, even when the average pairwise correlations in a network are themselves so low that it might be tempting to consider them as neglibible. In Figs <xref ref-type="fig" rid="pcbi.1006446.g003">3h</xref>, <xref ref-type="fig" rid="pcbi.1006446.g005">5e</xref> and <xref ref-type="fig" rid="pcbi.1006446.g006">6c</xref> we have shown that, across a number of different connectivity regimes, the network response has low dimensionality when the average correlation is lower than 0.025. This effect is important, it may point to the dimensionality, rather than the often reported statistic of average pairwise correlations (see review in [<xref ref-type="bibr" rid="pcbi.1006446.ref035">35</xref>]), as a better metric for describing how strongly network activity is coordinated [<xref ref-type="bibr" rid="pcbi.1006446.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref044">44</xref>]. Moreover, our theory suggests that specific connectivity motifs, i.e. reciprocal motifs, have a prominent role in influencing the activity dimensionality over and above its average correlation.</p>
<p>Third, depending on stimulus properties and network connectivity, the network response may have a higher or lower dimensionality than the stimulus; in this way, feeding a stimulus to a network results in either an expanded or contracted dimensionality in the response (cfr. Figs <xref ref-type="fig" rid="pcbi.1006446.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1006446.g006">6e</xref>). Which of these occurs depends strongly on the network connectivity. Here, stronger coupling leads to a more restricted range of dimensionalities with which the network operates. This restricted range—produced in response to a wide range of stimuli—may be interpreted as a type of “dimensionality equalization:” the network reduces or expands the stimulus dimensionality to lie in a relatively tight range Figs <xref ref-type="fig" rid="pcbi.1006446.g004">4e</xref> and <xref ref-type="fig" rid="pcbi.1006446.g006">6e</xref>. Moreover, when inputs assume a fixed strength in each dimension, there is a specific stimulus dimensionality where the network response assumes minimum value. This point is of interest as it marks the transition from a dynamical regime dominated by the internal network response to one governed by the stimulus: thus, near the minimum, the network is entrained by the stimulus but not dominated by it, with the internal dynamics serving as scaffold for the activity that is produced.</p>
<p>All these results are currently of high relevance to experimental efforts to quantify connectivity structures in neural networks. Because low-order connectivity motifs involve connections among only a few cells at a time, they can be measured with the techniques such as multi-patch recordings, with each new recording viewed as a sample from the network; in fact, [<xref ref-type="bibr" rid="pcbi.1006446.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref070">70</xref>] used exactly this approach and quantified synaptic motif motifs in cortical networks <italic>in vitro</italic>. While structural connectomics data [<xref ref-type="bibr" rid="pcbi.1006446.ref071">71</xref>] does not currently include precise estimates of synaptic strengths, it can be used to estimate the abundance of motif structures. Another approach begins by fitting “GLM” type models of functional connectivity directly to large-scale recordings of neural activity [<xref ref-type="bibr" rid="pcbi.1006446.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref073">73</xref>]. This results in an effective interaction matrix among all neurons that, while not a direct description of synaptic connections [<xref ref-type="bibr" rid="pcbi.1006446.ref074">74</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref075">75</xref>], still defines a network whose connection structures produce activity patterns in the same way that our theory describes. Altogether, we hope that our theory will be an important tool in interpreting the large scale data on neural activity and connectivity that is increasingly becoming available.</p>
<p>We close by considering three future research directions that our work here has helped to define.</p>
<p>The first is the question of finding efficient, readily measurable features of network connectivity that drive key aspects of neural network dynamics. Here, we demonstrated some substantial new successes, and failures, of local connectivity motifs in this regard. Further research across our field will be important to understand the relevance of specific connectivity patterns and their statistics, including how they vary across space and cell types [<xref ref-type="bibr" rid="pcbi.1006446.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref076">76</xref>]. This will be especially interesting in relation to next generation connectomics data, which may unlock new roles and new forms of connectivity structures.</p>
<p>The second is the extension to nonlinear network of this link between connectivity structure and activity dimension. While the theory in this study is for networks that are linearized around their working point, recent work [<xref ref-type="bibr" rid="pcbi.1006446.ref077">77</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref078">78</xref>] has developed an expansion that predicts correlations of arbitrary orders in similar Poisson-type networks, for increasing orders of nonlinearity. Further work to elucidate their influence in shaping the dimensionality of neural response would extend the scope of the present analysis beyond linear circuits, possibly bridging our framework with others that have been recently advanced [<xref ref-type="bibr" rid="pcbi.1006446.ref079">79</xref>].</p>
<p>The third and final direction for future study is analysis of the stimulus entrainment of network dynamics highlighted above. Specifically, neural representations, i.e. the encoding of stimulus-specific information by neural networks, may involve circuitry that either increases, decreases, or equalizes the dimensionality of neural responses, but further work is needed to understand the implications for neural coding [<xref ref-type="bibr" rid="pcbi.1006446.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref027">27</xref>].</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec012">
<title>a) Full expression of dimensionality as a function of cumulants</title>
<p>The expression for the dimensionality <italic>Dim</italic>(<bold><italic>C</italic></bold>) is the ratio between Tr(<bold><italic>C</italic></bold>)<sup>2</sup> and Tr(<bold><italic>C</italic></bold><sup>2</sup>). In terms of cumulants the functions of <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> can be written as:
<disp-formula id="pcbi.1006446.e058"><alternatives><graphic id="pcbi.1006446.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>N</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mspace width="2.em"/><mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
These two expressions are both tightly linked to the average covariance <xref ref-type="disp-formula" rid="pcbi.1006446.e027">Eq 10</xref>. In particular they can be written as follows to highlight this connection:
<disp-formula id="pcbi.1006446.e059"><alternatives><graphic id="pcbi.1006446.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi> <mml:mrow><mml:mo>⟨</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>N</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi> <mml:msup><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>·</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>k</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:msubsup><mml:mi>c</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msubsup> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula></p>
</sec>
<sec id="sec013">
<title>b) Full expression of dimensionality as a function of cumulants, in the presence of input stimuli</title>
<p>The full expression for <xref ref-type="disp-formula" rid="pcbi.1006446.e048">Eq 19</xref> is:
<disp-formula id="pcbi.1006446.e060"><alternatives><graphic id="pcbi.1006446.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msub><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mi>κ</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi>c</mml:mi> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mover accent="true"><mml:mi>κ</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mspace width="23em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>.</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>+</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mover accent="true"><mml:mi>κ</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>v</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
These formulas can be resumed with different choices of cumulants. In particular both <bold><italic>κ</italic></bold><sub><bold><italic>int</italic></bold></sub> and <bold><italic>κ</italic></bold><sub><bold><italic>ext</italic></bold></sub> can be employed simultaneously (cfr. <xref ref-type="supplementary-material" rid="pcbi.1006446.s001">S1 File</xref> Sec. S3.2). In <xref ref-type="disp-formula" rid="pcbi.1006446.e060">Eq 23</xref> we show the expression used to generate figures in the main text; this choice is best motivated in the case of low dimensional input.</p>
</sec>
<sec id="sec014">
<title>c) Description of SONET networks</title>
<p>The SONET model for random graphs can be seen as an extension of the Erdos-Renyi model. In an Erdos-Renyi graph two nodes are randomly connected with probability <italic>p</italic> (0 ≤ <italic>p</italic> ≤ 1). In SONET networks also second order connection motifs (convergent, divergent, etc.) appear with controlled statistics (see [<xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006446.ref057">57</xref>] for further details). The total number of parameters for the generation of a network is five: the average connectivity and the relative aboundance of convergent, divergent, reciprocal, chains motifs statistics. As an extension of the Erdos-Renyi model, the algorithm we use (provided by the authors of [<xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>]) generates a W with binary entries by maximizing a maximum likekelihood for the maximum entropy of the connectivity entries. To each binary connectivity state, the algorithm associates a probability under the 5 parameters of the model (average connectivity p and probability of convergent, divergent, reciprocal and chain motifs). For further details we point the reader to [<xref ref-type="bibr" rid="pcbi.1006446.ref080">80</xref>].</p>
</sec>
<sec id="sec015">
<title>d) Details for Figs <xref ref-type="fig" rid="pcbi.1006446.g001">1f</xref> and <xref ref-type="fig" rid="pcbi.1006446.g003">3b to 3h</xref></title>
<p>The ensemble of networks used for <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b, 3c and 3h</xref> consists of 500 networks with <italic>N</italic> = 1000 neurons each. All networks share the parameters <italic>c</italic><sub>0</sub> = 1, <italic>A</italic><sub><italic>ii</italic></sub> = 10 ∀<italic>i</italic> ∈ {1‥<italic>N</italic>} while the connectivity graph <italic>W</italic> is generated through the SONET algorithm with the same set of parameters (<italic>α</italic>’s) used in [<xref ref-type="bibr" rid="pcbi.1006446.ref041">41</xref>]. Such parameters regulate the statistics of convergent, divergent, reciprocal and chain motifs. They are uniformly sampled in the ranges <italic>p</italic> ∈ [0.01, 0.1], <italic>α</italic><sub><italic>recip</italic></sub> ∈ [−1, 4], <italic>α</italic><sub><italic>conv</italic></sub> ∈ [0, 1], <italic>α</italic><sub><italic>div</italic></sub> ∈ [0, 1] and <italic>α</italic><sub><italic>chain</italic></sub> ∈ [−1, 1].</p>
</sec>
<sec id="sec016">
<title>e) Details for the regression in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d to 3g</xref></title>
<p>The ensemble of networks used for <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3d to 3g</xref> has exactly the same parameters as the one above, except that the range for <italic>p</italic> is different: <italic>p</italic> ∈ [0.078, 0.082]. The dimensionality for each network is computed and the difference in dimensionality from an Erdos-Renyi network with <italic>p</italic> = <italic>p</italic><sub><italic>ER</italic></sub> = 0.08 is regressed against 6 different variables: <italic>p</italic> − <italic>p</italic><sub><italic>ER</italic></sub>, and the value of chain, convergent, divergent and the two trace cumulants. The coefficients of the regression are displayed in <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3e</xref>.</p>
</sec>
<sec id="sec017">
<title>f) Details for <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a, 4c and 4d</xref></title>
<p>These figures display the full covariance dimensionality expression <xref ref-type="disp-formula" rid="pcbi.1006446.e005">Eq 3</xref> and the motif reduction <xref ref-type="disp-formula" rid="pcbi.1006446.e028">Eq 11</xref> for a SONET network with p = 0.03 and a random choice of second order motifs. An input of varying strength and number of factors is fed onto the network. This is captured by <inline-formula id="pcbi.1006446.e061"><alternatives><graphic id="pcbi.1006446.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> where each <bold><italic>ξ</italic></bold> is a random vector of unit norm. In the case of <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a</xref> the number of factors <italic>N</italic><sub><italic>dim</italic></sub> is increased and <italic>c</italic><sub><italic>ξ</italic></sub> = 0.05. In <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4c</xref> the number of factors is one while <italic>c</italic><sub><italic>ξ</italic></sub> is increased. In <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4d</xref> the number of factors is increased but the total strength constrained to <inline-formula id="pcbi.1006446.e062"><alternatives><graphic id="pcbi.1006446.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:msubsup> <mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec018">
<title>g) Details for <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4e, 4g and 4h</xref> and theoretical approximation</title>
<p>The procedure for obtaining these figures is equal to the one used for <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a, 4c and 4d</xref> except that the initial network is a SONET network with <italic>p</italic> = 0.08 and random second order motifs.</p>
<p>The pink line in these figures corresponds to a theoretical approximation of the formula in <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref>. The term in the denominator Tr(<bold><italic>C</italic></bold><sub><italic>int</italic></sub> ⋅ <bold><italic>C</italic></bold><sub><italic>ext</italic></sub>) is the only term in the expression with the product <bold><italic>C</italic></bold><sub><italic>int</italic></sub> and <bold><italic>C</italic></bold><sub><italic>ext</italic></sub>. We used the following inequality to build an upper bound for this term:
<disp-formula id="pcbi.1006446.e063"><alternatives><graphic id="pcbi.1006446.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Tr</mml:mtext> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>≤</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>Tr</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
By substituting the rightmost side of this expression into <xref ref-type="disp-formula" rid="pcbi.1006446.e045">Eq 18</xref> we obtain the expression for the pink line displayed in <xref ref-type="fig" rid="pcbi.1006446.g004">Fig 4a, 4c and 4d</xref>.</p>
</sec>
<sec id="sec019">
<title>h) Details for <xref ref-type="fig" rid="pcbi.1006446.g005">Fig 5a to 5e</xref></title>
<p>The figures use the same values and techniques of <xref ref-type="fig" rid="pcbi.1006446.g003">Fig 3b and 3c</xref>. The different network architectures are all generated using the package NetworkX in Python 3.6. The Erdos-Renyi network is a randomly connected network, the small world network has a number of nodes denoted by <italic>p</italic> ⋅ <italic>N</italic> and probability of rewiring 0.3, the scale-free network is obtained through a Barabasi-Albert graph where the number of number of edges to attach from a new node to existing nodes (parameter <italic>m</italic>) is derived as a function of the final number of connections <italic>p</italic> ⋅ <italic>N</italic> and the number of nodes <italic>N</italic> (<inline-formula id="pcbi.1006446.e064"><alternatives><graphic id="pcbi.1006446.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:msup><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:msup><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>p</mml:mi></mml:mrow></mml:msqrt> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec020">
<title>i) Details for <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6a to 6e</xref></title>
<p>The networks displayed in these figures are 500 SONET networks with average synaptic strength <inline-formula id="pcbi.1006446.e065"><alternatives><graphic id="pcbi.1006446.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mn>25</mml:mn> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> that scales with <inline-formula id="pcbi.1006446.e066"><alternatives><graphic id="pcbi.1006446.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006446.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula> rather than <italic>N</italic>. For each network a random 10% of the neurons is selected to be inhibitory and their strength rescaled so that 〈<bold><italic>G</italic></bold><sub><italic>EE</italic></sub>〉 = 〈<bold><italic>G</italic></bold><sub><italic>II</italic></sub>〉 where <bold><italic>G</italic></bold><sub><italic>EE</italic></sub> and <bold><italic>G</italic></bold><sub><italic>II</italic></sub> denote respectively the part of the connectivity graph <bold><italic>G</italic></bold> in between the excitatory and the inhibitory population. We checked that the network so obtained respects the constraints for a balanced state determined in [<xref ref-type="bibr" rid="pcbi.1006446.ref081">81</xref>].</p>
</sec>
<sec id="sec021">
<title>l) Details for <xref ref-type="fig" rid="pcbi.1006446.g006">Fig 6d and 6f</xref></title>
<p>We generate 500 SONET networks with connectivity <italic>p</italic> = 0.03. Upon balancing the network 10% of the neurons are inhibitory. The dimensionality of this ensemble of networks is regressed against the values of the connectivity cumulants computed on the inhibitory part of the network.</p>
</sec>
</sec>
<sec id="sec022">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006446.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006446.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Supplementary material.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We wish to thank the Allen Institute for Brain Science founder, Paul G. Allen, for their vision, encouragement and support. We are grateful to Yu Hu and Kameron Harris for helpful insights and comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006446.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>. <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>11</issue>):<fpage>1500</fpage>–<lpage>1509</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3776" xlink:type="simple">10.1038/nn.3776</ext-link></comment> <object-id pub-id-type="pmid">25151264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cayco-Gajic</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>RA</given-names></name>. <article-title>Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks</article-title>. <source>Nature Communications</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1116</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-01109-y" xlink:type="simple">10.1038/s41467-017-01109-y</ext-link></comment> <object-id pub-id-type="pmid">29061964</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Axel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Optimal Degrees of Synaptic Connectivity</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>93</volume>(<issue>5</issue>):<fpage>1153</fpage>–<lpage>1164.e7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.01.030" xlink:type="simple">10.1016/j.neuron.2017.01.030</ext-link></comment> <object-id pub-id-type="pmid">28215558</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>WCA</given-names></name>, <name name-style="western"><surname>Bonin</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Reed</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Graham</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Hood</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Glattfelder</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Anatomy and function of an excitatory network in the visual cortex</article-title>. <source>Nature</source>. <year>2016</year>;<volume>532</volume>(<issue>7599</issue>):<fpage>370</fpage>–+. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature17192" xlink:type="simple">10.1038/nature17192</ext-link></comment> <object-id pub-id-type="pmid">27018655</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kasthuri</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hayworth</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Berger</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Schalek</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Conchello</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Knowles-Barley</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Saturated Reconstruction of a Volume of Neocortex</article-title>. <source>Cell</source>. <year>2015</year>;<volume>162</volume>(<issue>3</issue>):<fpage>648</fpage>–<lpage>661</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2015.06.054" xlink:type="simple">10.1016/j.cell.2015.06.054</ext-link></comment> <object-id pub-id-type="pmid">26232230</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bock</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>WCA</given-names></name>, <name name-style="western"><surname>Kerlin</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Andermann</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Hood</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wetzel</surname> <given-names>AW</given-names></name>, <etal>et al</etal>. <article-title>Network anatomy and in vivo physiology of visual cortical neurons</article-title>. <source>Nature</source>. <year>2011</year>;<volume>471</volume>(<issue>7337</issue>):<fpage>177</fpage>–<lpage>U59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09802" xlink:type="simple">10.1038/nature09802</ext-link></comment> <object-id pub-id-type="pmid">21390124</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kleinfeld</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bharioke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Blinder</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bock</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Briggman</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <etal>et al</etal>. <article-title>Large-Scale Automated Histology in the Pursuit of Connectomes</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>45</issue>):<fpage>16125</fpage>–<lpage>16138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4077-11.2011" xlink:type="simple">10.1523/JNEUROSCI.4077-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22072665</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Briggman</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Helmstaedter</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>. <article-title>Wiring specificity in the direction-selectivity circuit of the retina</article-title>. <source>Nature</source>. <year>2011</year>;<volume>471</volume>(<issue>7337</issue>):<fpage>183</fpage>–<lpage>U67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09818" xlink:type="simple">10.1038/nature09818</ext-link></comment> <object-id pub-id-type="pmid">21390125</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Helmstaedter</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Briggman</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Turaga</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Jain</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>. <article-title>Connectomic reconstruction of the inner plexiform layer in the mouse retina</article-title>. <source>Nature</source>. <year>2013</year>;<volume>500</volume>(<issue>7461</issue>):<fpage>168</fpage>–+. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12346" xlink:type="simple">10.1038/nature12346</ext-link></comment> <object-id pub-id-type="pmid">23925239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mishchenko</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Spacek</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mendenhall</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Ultrastructural Analysis of Hippocampal Neuropil from the Connectomics Perspective</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>67</volume>(<issue>6</issue>):<fpage>1009</fpage>–<lpage>1020</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.08.014" xlink:type="simple">10.1016/j.neuron.2010.08.014</ext-link></comment> <object-id pub-id-type="pmid">20869597</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>White</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Southgate</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Thomson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>S</given-names></name>. <article-title>The structure of the nervous system of the nematode Caenorhabditis elegans</article-title>. <source>Phil Trans R Soc Lond B</source>. <year>1986</year>;<volume>314</volume>(<issue>1165</issue>):<fpage>1</fpage>–<lpage>340</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.1986.0056" xlink:type="simple">10.1098/rstb.1986.0056</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref012">
<label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Ryan K, Lu Z, Meinertzhagen IA. The CNS connectome of a tadpole larva of Ciona intestinalis (L.) highlights sidedness in the brain of a chordate sibling; 2016. Available from: <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/16962" xlink:type="simple">https://elifesciences.org/articles/16962</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zheng</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lauritzen</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Perlman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Nichols</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Milkie</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>A Complete Electron Microscopy Volume of the Brain of Adult Drosophila melanogaster</article-title>. <source>Cell</source>. <year>2018</year>;<volume>174</volume>(<issue>3</issue>):<fpage>730</fpage>–<lpage>743.e22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2018.06.019" xlink:type="simple">10.1016/j.cell.2018.06.019</ext-link></comment> <object-id pub-id-type="pmid">30033368</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ohyama</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schneider-Mizell</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Fetter</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Aleman</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Franconville</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rivera-Alba</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A multilevel multimodal circuit enhances action selection in Drosophila</article-title>. <source>Nature</source>. <year>2015</year>;<volume>520</volume>(<issue>7549</issue>):<fpage>633</fpage>–<lpage>639</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14297" xlink:type="simple">10.1038/nature14297</ext-link></comment> <object-id pub-id-type="pmid">25896325</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Milo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shen-Orr</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Itzkovitz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kashtan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Alon</surname> <given-names>U</given-names></name>. <article-title>Network Motifs: Simple Building Blocks of Complex Networks</article-title>. <source>Science</source>. <year>2002</year>;<volume>298</volume>(<issue>5594</issue>):<fpage>824</fpage>–<lpage>827</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.298.5594.824" xlink:type="simple">10.1126/science.298.5594.824</ext-link></comment> <object-id pub-id-type="pmid">12399590</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjostrom</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>Plos Biology</source>. <year>2005</year>;<volume>3</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Trousdale</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>Motif statistics and spike correlations in neuronal networks</article-title>. <source>Journal of Statistical Mechanics-Theory and Experiment</source>. <year>2013</year>; p. <fpage>P03012</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1742-5468/2013/03/P03012" xlink:type="simple">10.1088/1742-5468/2013/03/P03012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Trousdale</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>Local paths to global coherence: Cutting networks down to size</article-title>. <source>Physical Review E</source>. <year>2014</year>;<volume>89</volume>(<issue>3</issue>):<fpage>032802</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.89.032802" xlink:type="simple">10.1103/PhysRevE.89.032802</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Trousdale</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>. <article-title>Impact of Network Structure and Cellular Response on Spike Time Correlations</article-title>. <source>Plos Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>3</issue>):<fpage>e1002408</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002408" xlink:type="simple">10.1371/journal.pcbi.1002408</ext-link></comment> <object-id pub-id-type="pmid">22457608</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pernice</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Staude</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Cardanobile</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rotter</surname> <given-names>S</given-names></name>. <article-title>How Structure Determines Correlations in Neuronal Networks</article-title>. <source>Plos Computational Biology</source>. <year>2011</year>;<volume>7</volume>(<issue>5</issue>):<fpage>e1002059</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002059" xlink:type="simple">10.1371/journal.pcbi.1002059</ext-link></comment> <object-id pub-id-type="pmid">21625580</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rosenbaum</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>The spatial structure of correlated neuronal variability</article-title>. <source>Nature Neuroscience</source>. <year>2017</year>;<volume>20</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4433" xlink:type="simple">10.1038/nn.4433</ext-link></comment> <object-id pub-id-type="pmid">27798630</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Mastrogiuseppe F, Ostojic S. Linking connectivity, dynamics and computations in recurrent neural networks. arXiv:171109672 [q-bio]. 2017.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rangan</surname> <given-names>AV</given-names></name>. <article-title>Diagrammatic Expansion of Pulse-Coupled Network Dynamics</article-title>. <source>Physical Review Letters</source>. <year>2009</year>;<volume>102</volume>(<issue>15</issue>):<fpage>158101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.102.158101" xlink:type="simple">10.1103/PhysRevLett.102.158101</ext-link></comment> <object-id pub-id-type="pmid">19518674</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rangan</surname> <given-names>AV</given-names></name>. <article-title>Diagrammatic expansion of pulse-coupled network dynamics in terms of subnetworks</article-title>. <source>Physical Review E</source>. <year>2009</year>;<volume>80</volume>(<issue>3</issue>):<fpage>036101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.80.036101" xlink:type="simple">10.1103/PhysRevE.80.036101</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Williamson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Cowley</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <etal>et al</etal>. <article-title>Scaling Properties of Dimensionality Reduction for Neural Populations and Network Models</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005141" xlink:type="simple">10.1371/journal.pcbi.1005141</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mazzucato</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fontanini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>La Camera</surname> <given-names>G</given-names></name>. <article-title>Stimuli Reduce the Dimensionality of Cortical Activity</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2016</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2016.00011" xlink:type="simple">10.3389/fnsys.2016.00011</ext-link></comment> <object-id pub-id-type="pmid">26924968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">A theory of multineuronal dimensionality, dynamics and measurement | bioRxiv;. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2017/11/05/214262" xlink:type="simple">https://www.biorxiv.org/content/early/2017/11/05/214262</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>The effect of correlated variability on the accuracy of a population code</article-title>. <source>Neural Computation</source>. <year>1999</year>;<volume>11</volume>(<issue>1</issue>):<fpage>91</fpage>–<lpage>101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976699300016827" xlink:type="simple">10.1162/089976699300016827</ext-link></comment> <object-id pub-id-type="pmid">9950724</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dettner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Muenzberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tchumatchenko</surname> <given-names>T</given-names></name>. <article-title>Temporal pairwise spike correlations fully capture single-neuron information</article-title>. <source>Nature Communications</source>. <year>2016</year>;<volume>7</volume>:<fpage>13805</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13805" xlink:type="simple">10.1038/ncomms13805</ext-link></comment> <object-id pub-id-type="pmid">27976717</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>The Sign Rule and Beyond: Boundary Effects, Flexibility, and Noise Correlations in Neural Population Codes</article-title>. <source>Plos Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>2</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003469" xlink:type="simple">10.1371/journal.pcbi.1003469</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kanitscheider</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Information-limiting correlations</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>10</issue>):<fpage>1410</fpage>–<lpage>1417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3807" xlink:type="simple">10.1038/nn.3807</ext-link></comment> <object-id pub-id-type="pmid">25195105</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cafaro</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Direction-Selective Circuits Shape Noise to Ensure a Precise Population Code</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>(<issue>2</issue>):<fpage>369</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.11.019" xlink:type="simple">10.1016/j.neuron.2015.11.019</ext-link></comment> <object-id pub-id-type="pmid">26796691</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Neural correlations, population coding and computation</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2006</year>;<volume>7</volume>(<issue>5</issue>):<fpage>358</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1888" xlink:type="simple">10.1038/nrn1888</ext-link></comment> <object-id pub-id-type="pmid">16760916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zohary</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>W</given-names></name>. <article-title>Correlated Neuronal Discharge Rate and Its Implications for Psychophysical Performance</article-title>. <source>Nature</source>. <year>1994</year>;<volume>370</volume>(<issue>6485</issue>):<fpage>140</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/370140a0" xlink:type="simple">10.1038/370140a0</ext-link></comment> <object-id pub-id-type="pmid">8022482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>. <article-title>Measuring and interpreting neuronal correlations</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>7</issue>):<fpage>811</fpage>–<lpage>819</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2842" xlink:type="simple">10.1038/nn.2842</ext-link></comment> <object-id pub-id-type="pmid">21709677</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McGinley</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Vinck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Reimer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Batista-Brito</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Zagha</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cadwell</surname> <given-names>CR</given-names></name>, <etal>et al</etal>. <article-title>Waking State: Rapid Variations Modulate Neural and Behavioral Responses</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>87</volume>(<issue>6</issue>):<fpage>1143</fpage>–<lpage>1161</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.09.012" xlink:type="simple">10.1016/j.neuron.2015.09.012</ext-link></comment> <object-id pub-id-type="pmid">26402600</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rosenbaum</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ocker</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>. <article-title>The mechanics of state-dependent neural correlations</article-title>. <source>Nature Neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>383</fpage>–<lpage>393</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4242" xlink:type="simple">10.1038/nn.4242</ext-link></comment> <object-id pub-id-type="pmid">26906505</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cossell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Iacaruso</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Muir</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Houlton</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sader</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>(<issue>7539</issue>):<fpage>399</fpage>–<lpage>403</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14182" xlink:type="simple">10.1038/nature14182</ext-link></comment> <object-id pub-id-type="pmid">25652823</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Subramaniyan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Denfield</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Cadwell</surname> <given-names>CR</given-names></name>, <etal>et al</etal>. <article-title>State Dependence of Noise Correlations in Macaque Primary Visual Cortex</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>(<issue>1</issue>):<fpage>235</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.02.006" xlink:type="simple">10.1016/j.neuron.2014.02.006</ext-link></comment> <object-id pub-id-type="pmid">24698278</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goris</surname> <given-names>RLT</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Partitioning neuronal variability</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>6</issue>):<fpage>858</fpage>–<lpage>865</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3711" xlink:type="simple">10.1038/nn.3711</ext-link></comment> <object-id pub-id-type="pmid">24777419</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhao</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Beverlin</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Netoff</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Nykamp</surname> <given-names>DQ</given-names></name>. <article-title>Synchronization from Second Order Network Connectivity Statistics</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2011</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2011.00028" xlink:type="simple">10.3389/fncom.2011.00028</ext-link></comment> <object-id pub-id-type="pmid">21779239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roxin</surname> <given-names>A</given-names></name>. <article-title>The role of degree distribution in shaping the dynamics in networks of sparsely connected spiking neurons</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2011</year>;<volume>5</volume>:<fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2011.00008" xlink:type="simple">10.3389/fncom.2011.00008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Hu Y, Brunton SL, Cain N, Mihalas S, Kutz JN, Shea-Brown E. Feedback through graph motifs relates structure and function in complex networks. arXiv:160509073 [cond-mat, physics:physics, q-bio]. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source>. <year>2006</year>;<volume>440</volume>(<issue>7087</issue>):<fpage>1007</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04701" xlink:type="simple">10.1038/nature04701</ext-link></comment> <object-id pub-id-type="pmid">16625187</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pernice</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Staude</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Cardanobile</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rotter</surname> <given-names>S</given-names></name>. <article-title>Recurrent interactions in spiking networks with arbitrary topology</article-title>. <source>Physical Review E</source>. <year>2012</year>;<volume>85</volume>(<issue>3</issue>):<fpage>031916</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.85.031916" xlink:type="simple">10.1103/PhysRevE.85.031916</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ferreira</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Koyama</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rad</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Vidne</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A new look at state-space models for neural data</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2010</year>;<volume>29</volume>(<issue>1-2</issue>):<fpage>107</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-009-0179-x" xlink:type="simple">10.1007/s10827-009-0179-x</ext-link></comment> <object-id pub-id-type="pmid">19649698</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref047">
<label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lewi</surname> <given-names>J</given-names></name>. <chapter-title>Statistical models for neural encoding, decoding, and optimal stimulus design</chapter-title>. In: <name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>JF</given-names></name>, editors. <source>Progress in Brain Research. vol. 165 of Computational Neuroscience: Theoretical Insights into Brain Function</source>. <publisher-name>Elsevier</publisher-name>; <year>2007</year>. p. <fpage>493</fpage>–<lpage>507</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0079612306650310" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/S0079612306650310</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>de la Rocha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Josić</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Reyes</surname> <given-names>A</given-names></name>. <article-title>Correlation between neural spike trains increases with firing rate</article-title>. <source>Nature</source>. <year>2007</year>;<volume>448</volume>(<issue>7155</issue>):<fpage>802</fpage>–<lpage>806</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06028" xlink:type="simple">10.1038/nature06028</ext-link></comment> <object-id pub-id-type="pmid">17700699</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>de la Rocha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Correlation and synchrony transfer in integrate-and-fire neurons: Basic properties and consequences for coding</article-title>. <source>Physical Review Letters</source>. <year>2008</year>;<volume>100</volume>(<issue>10</issue>):<fpage>108102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.100.108102" xlink:type="simple">10.1103/PhysRevLett.100.108102</ext-link></comment> <object-id pub-id-type="pmid">18352234</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bair</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Zohary</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Correlated firing in macaque visual area MT: Time scales and relationship to behavior</article-title>. <source>Journal of Neuroscience</source>. <year>2001</year>;<volume>21</volume>(<issue>5</issue>):<fpage>1676</fpage>–<lpage>1697</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.21-05-01676.2001" xlink:type="simple">10.1523/JNEUROSCI.21-05-01676.2001</ext-link></comment> <object-id pub-id-type="pmid">11222658</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yatsenko</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Froudarakis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Improved Estimation and Interpretation of Correlations in Neural Circuits</article-title>. <source>Plos Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>3</issue>):UNSP <fpage>e1004083</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004083" xlink:type="simple">10.1371/journal.pcbi.1004083</ext-link></comment> <object-id pub-id-type="pmid">25826696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sejnowski</surname> <given-names>T</given-names></name>. <article-title>Stochastic Dynamics of Neuronal Interaction</article-title>. <source>Biological Cybernetics</source>. <year>1976</year>;<volume>22</volume>(<issue>4</issue>):<fpage>203</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00365086" xlink:type="simple">10.1007/BF00365086</ext-link></comment> <object-id pub-id-type="pmid">953078</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lindner</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Longtin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Maler</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bastian</surname> <given-names>J</given-names></name>. <article-title>Oscillatory activity in electrosensory neurons increases with the spatial correlation of the stochastic input stimulus</article-title>. <source>Physical Review Letters</source>. <year>2004</year>;<volume>93</volume>(<issue>4</issue>):<fpage>048101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.93.048101" xlink:type="simple">10.1103/PhysRevLett.93.048101</ext-link></comment> <object-id pub-id-type="pmid">15323795</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lindner</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Longtin</surname> <given-names>A</given-names></name>. <article-title>Theory of oscillatory firing induced by spatially correlated noise and delayed inhibitory feedback</article-title>. <source>Physical Review E</source>. <year>2005</year>;<volume>72</volume>(<issue>6</issue>):<fpage>061919</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.72.061919" xlink:type="simple">10.1103/PhysRevE.72.061919</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hawkes</surname> <given-names>A</given-names></name>. S<article-title>pectra of Some Self-Exciting and Mutually Exciting Point Processes</article-title>. <source>Biometrika</source>. <year>1971</year>;<volume>58</volume>(<issue>1</issue>):<fpage>83</fpage>–&amp;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/58.1.83" xlink:type="simple">10.1093/biomet/58.1.83</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ocker</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Buice</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Josić</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rosenbaum</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>From the statistics of connectivity to the statistics of spike times in neuronal networks</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2017</year>;<volume>46</volume>:<fpage>109</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2017.07.011" xlink:type="simple">10.1016/j.conb.2017.07.011</ext-link></comment> <object-id pub-id-type="pmid">28863386</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nykamp</surname> <given-names>DQ</given-names></name>. <article-title>A mathematical framework for inferring connectivity in probabilistic neuronal networks</article-title>. <source>Mathematical Biosciences</source>. <year>2007</year>;<volume>205</volume>(<issue>2</issue>):<fpage>204</fpage>–<lpage>251</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.mbs.2006.08.020" xlink:type="simple">10.1016/j.mbs.2006.08.020</ext-link></comment> <object-id pub-id-type="pmid">17070863</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>IT</given-names></name>, <name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>Dendritic spikes enhance stimulus selectivity in cortical neurons <italic>in vivo</italic></article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>(<issue>7474</issue>):<fpage>115</fpage>–<lpage>120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12600" xlink:type="simple">10.1038/nature12600</ext-link></comment> <object-id pub-id-type="pmid">24162850</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ostojic</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname> <given-names>V</given-names></name>. <article-title>How Connectivity, Background Activity, and Synaptic Properties Shape the Cross-Correlation between Spike Trains</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>33</issue>):<fpage>10234</fpage>–<lpage>10253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1275-09.2009" xlink:type="simple">10.1523/JNEUROSCI.1275-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19692598</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Renart</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>de la Rocha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bartho</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hollender</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Reyes</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The Asynchronous State in Cortical Circuits</article-title>. <source>Science</source>. <year>2010</year>;<volume>327</volume>(<issue>5965</issue>):<fpage>587</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1179850" xlink:type="simple">10.1126/science.1179850</ext-link></comment> <object-id pub-id-type="pmid">20110507</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref061">
<label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Abbott LF, Rajan K, Sompolinsky H. Interactions between Intrinsic and Stimulus-Evoked Activity in Recurrent Neural Networks. arXiv:09123832 [cond-mat, physics:physics, q-bio]. 2009.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>YY</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Barabási</surname> <given-names>AL</given-names></name>. <article-title>Controllability of complex networks</article-title>. <source>Nature</source>. <year>2011</year>;<volume>473</volume>(<issue>7346</issue>):<fpage>167</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature10011" xlink:type="simple">10.1038/nature10011</ext-link></comment> <object-id pub-id-type="pmid">21562557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>YY</given-names></name>, <name name-style="western"><surname>D’Souza</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Barabási</surname> <given-names>AL</given-names></name>. <article-title>Target control of complex networks</article-title>. <source>Nature Communications</source>. <year>2014</year>;<volume>5</volume>:<fpage>5415</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms6415" xlink:type="simple">10.1038/ncomms6415</ext-link></comment> <object-id pub-id-type="pmid">25388503</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Vreeswijk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Chaotic balanced state in a model of cortical circuits</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>6</issue>):<fpage>1321</fpage>–<lpage>1371</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976698300017214" xlink:type="simple">10.1162/089976698300017214</ext-link></comment> <object-id pub-id-type="pmid">9698348</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guerguiev</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lillicrap</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Richards</surname> <given-names>BA</given-names></name>. <article-title>Towards deep learning with segregated dendrites</article-title>. <source>eLife</source>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.22901" xlink:type="simple">10.7554/eLife.22901</ext-link></comment> <object-id pub-id-type="pmid">29205151</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lillicrap</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Cownden</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tweed</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Akerman</surname> <given-names>CJ</given-names></name>. <article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title>. <source>Nature Communications</source>. <year>2016</year>;<volume>7</volume>:<fpage>13276</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13276" xlink:type="simple">10.1038/ncomms13276</ext-link></comment> <object-id pub-id-type="pmid">27824044</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hassabis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kumaran</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>. <article-title>Neuroscience-Inspired Artificial Intelligence</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>95</volume>(<issue>2</issue>):<fpage>245</fpage>–<lpage>258</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.06.011" xlink:type="simple">10.1016/j.neuron.2017.06.011</ext-link></comment> <object-id pub-id-type="pmid">28728020</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ocker</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Self-Organization of Microcircuits in Networks of Spiking Neurons with Plastic Synapses</article-title>. <source>Plos Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>8</issue>):<fpage>e1004458</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004458" xlink:type="simple">10.1371/journal.pcbi.1004458</ext-link></comment> <object-id pub-id-type="pmid">26291697</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ocker</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Training and Spontaneous Reinforcement of Neuronal Assemblies by Spike Timing Plasticity</article-title>. <source>Cerebral Cortex (New York, NY: 1991)</source>. <year>2018</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Berger</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>A synaptic organizing principle for cortical neuronal groups</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>; p. 201016051. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1016051108" xlink:type="simple">10.1073/pnas.1016051108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Knox JE, Harris KD, Graddis N, Whitesell JD, Zeng H, Harris JA, et al. High resolution data-driven model of the mouse connectome. bioRxiv. 2018.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Mitra</surname> <given-names>PP</given-names></name>. <article-title>Multiple neural spike train data analysis: state-of-the-art and future challenges</article-title>. <source>Nature neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>5</issue>):<fpage>456</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1228" xlink:type="simple">10.1038/nn1228</ext-link></comment> <object-id pub-id-type="pmid">15114358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kulkarni</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Common-input models for multiple neural spike-train data</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2007</year>;<volume>18</volume>(<issue>4</issue>):<fpage>375</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09548980701625173" xlink:type="simple">10.1080/09548980701625173</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brinkman</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Buice</surname> <given-names>MA</given-names></name>. <article-title>Predicting how and when hidden neurons skew measured synaptic interactions</article-title>. <source>PLoS computational biology</source>. <year>2018</year>;<volume>14</volume>(<issue>10</issue>):<fpage>e1006490</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1006490" xlink:type="simple">10.1371/journal.pcbi.1006490</ext-link></comment> <object-id pub-id-type="pmid">30346943</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Once upon a (slow) time in the land of recurrent neuronal networks…</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2017</year>;<volume>46</volume>:<fpage>31</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2017.07.003" xlink:type="simple">10.1016/j.conb.2017.07.003</ext-link></comment> <object-id pub-id-type="pmid">28756341</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ocker</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Buice</surname> <given-names>MA</given-names></name>. <article-title>Linking structure and activity in nonlinear spiking networks</article-title>. <source>Plos Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>6</issue>):<fpage>e1005583</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005583" xlink:type="simple">10.1371/journal.pcbi.1005583</ext-link></comment> <object-id pub-id-type="pmid">28644840</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref078">
<label>78</label>
<mixed-citation publication-type="other" xlink:type="simple">Path Integral Methods for Stochastic Differential Equations | SpringerLink;. Available from: <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1186/s13408-015-0018-5" xlink:type="simple">https://link.springer.com/article/10.1186/s13408-015-0018-5</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mastrogiuseppe</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ostojic</surname> <given-names>S</given-names></name>. <article-title>Intrinsically-generated fluctuating activity in excitatory-inhibitory networks</article-title>. <source>Plos Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>4</issue>):<fpage>e1005498</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005498" xlink:type="simple">10.1371/journal.pcbi.1005498</ext-link></comment> <object-id pub-id-type="pmid">28437436</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006446.ref080">
<label>80</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhao L. Synchronization on second order networks. PhD thesis. 2012.</mixed-citation>
</ref>
<ref id="pcbi.1006446.ref081">
<label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>vanVreeswijk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source>. <year>1996</year>;<volume>274</volume>(<issue>5293</issue>):<fpage>1724</fpage>–<lpage>1726</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.274.5293.1724" xlink:type="simple">10.1126/science.274.5293.1724</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>