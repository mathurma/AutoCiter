<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01170</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005743</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Ocular anatomy</subject><subj-group><subject>Fovea centralis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Ocular anatomy</subject><subj-group><subject>Fovea centralis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computer vision</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Animal management</subject><subj-group><subject>Animal performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Object detection through search with a foveated visual system</article-title>
<alt-title alt-title-type="running-head">Foveated object detector</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3760-6722</contrib-id>
<name name-style="western">
<surname>Akbas</surname> <given-names>Emre</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Eckstein</surname> <given-names>Miguel P.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Psychological and Brain Sciences, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Computer Engineering, Middle East Technical University, Ankara, Turkey</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Institute for Collaborative Biotechnologies, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname> <given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Technische Universitat Chemnitz, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">emre@ceng.metu.edu.tr</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>9</day>
<month>10</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>10</issue>
<elocation-id>e1005743</elocation-id>
<history>
<date date-type="received">
<day>5</day>
<month>8</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>8</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Akbas, Eckstein</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005743"/>
<abstract>
<p>Humans and many other species sense visual information with varying spatial resolution across the visual field (foveated vision) and deploy eye movements to actively sample regions of interests in scenes. The advantage of such varying resolution architecture is a reduced computational, hence metabolic cost. But what are the performance costs of such processing strategy relative to a scheme that processes the visual field at high spatial resolution? Here we first focus on visual search and combine object detectors from computer vision with a recent model of peripheral pooling regions found at the V1 layer of the human visual system. We develop a foveated object detector that processes the entire scene with varying resolution, uses retino-specific object detection classifiers to guide eye movements, aligns its fovea with regions of interest in the input image and integrates observations across multiple fixations. We compared the foveated object detector against a non-foveated version of the same object detector which processes the entire image at homogeneous high spatial resolution. We evaluated the accuracy of the foveated and non-foveated object detectors identifying 20 different objects classes in scenes from a standard computer vision data set (the PASCAL VOC 2007 dataset). We show that the foveated object detector can approximate the performance of the object detector with homogeneous high spatial resolution processing while bringing significant computational cost savings. Additionally, we assessed the impact of foveation on the computation of bottom-up saliency. An implementation of a simple foveated bottom-up saliency model with eye movements showed agreement in the selection of top salient regions of scenes with those selected by a non-foveated high resolution saliency model. Together, our results might help explain the evolution of foveated visual systems with eye movements as a solution that preserves perceptual performance in visual search while resulting in computational and metabolic savings to the brain.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>A large number of species from primates to shrimps do not see the visual world with uniform spatial detail. An area with heightened sensitivity to spatial detail, known as the fovea in mammals is oriented through eye and head movements to scrutinize regions of interest in the visual environment. But why did many species evolve such foveated architecture for vision? Seeing with high spatial detail everywhere requires greater neuronal machinery and energy consumption, thus the advantage of a foveated visual system is to reduce metabolic costs. But does having a foveated visual system incur a price to the performance of the organism in visual tasks? Here, we show using a computer vision object detection model, that the foveated version of the model can attain similar search performance to its non-foveated version that processes the entire visual field with high spatial detail. The results might help explain the evolution of foveated visual systems with eye movements as a solution that preserves perceptual performance while resulting in computational and metabolic savings to the brain.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Army Research Office (US)</institution>
</funding-source>
<award-id>W911NF-09-0001</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Eckstein</surname> <given-names>Miguel P.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Naval Air Warfare Center, Aircraft Division (US)</institution>
</funding-source>
<award-id>N68335-16-C-0028</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Eckstein</surname> <given-names>Miguel P.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Mayachitra Incorporated</institution>
</funding-source>
<award-id>N68335-16-C-0028</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Eckstein</surname> <given-names>Miguel P.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>Scientific and Techonological Research Council of Turkey</institution>
</funding-source>
<award-id>116C006</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3760-6722</contrib-id>
<name name-style="western">
<surname>Akbas</surname> <given-names>Emre</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>MPE and EA were supported by: Institute for Collaborative Biotechnologies through grant W911NF-09-0001 from the U.S. Army Research Office (<ext-link ext-link-type="uri" xlink:href="http://www.arl.army.mil/www/default.cfm?page=29" xlink:type="simple">http://www.arl.army.mil/www/default.cfm?page=29</ext-link>), the Naval Air Warfare Center AD under Prime Grant No N68335-16-C-0028 (<ext-link ext-link-type="uri" xlink:href="http://www.navair.navy.mil/nawcad/" xlink:type="simple">http://www.navair.navy.mil/nawcad/</ext-link>), and Mayachitra Incorporated (<ext-link ext-link-type="uri" xlink:href="http://www.mayachitra.com/" xlink:type="simple">http://www.mayachitra.com/</ext-link>); and EA was supported by the Scientific and Technological Research Council of Turkey (TUBITAK) through grant 116C006 (<ext-link ext-link-type="uri" xlink:href="http://www.tubitak.gov.tr/" xlink:type="simple">http://www.tubitak.gov.tr/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="11"/>
<table-count count="2"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-11-03</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Software code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/eakbas/FoveatedObjectDetector" xlink:type="simple">https://github.com/eakbas/FoveatedObjectDetector</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Many species from primates, birds and shrimps [<xref ref-type="bibr" rid="pcbi.1005743.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref002">2</xref>] have an area of their visual sensory system with heightened spatial fidelity and utilize eye and head movements to orient this area towards objects of interest in scenes. The pervasiveness of sensory systems with varying spatial resolution for species that heavily rely on vision to sense the world motivates the question about its advantages. The wide-accepted answer is that visual processing with varying spatial resolution reduces the brain’s computational cost. For example, for humans, the density of cones in the fovea is approximately 20 times larger than at 10 degrees into the periphery and 90 times at the far visual periphery [<xref ref-type="bibr" rid="pcbi.1005743.ref003">3</xref>]. The fovea occupies 0.01% of the retina but utilizes approximately 8-10% of the neuronal machinery in primary visual cortex [<xref ref-type="bibr" rid="pcbi.1005743.ref004">4</xref>]. A high spatial resolution processing system across the entire visual field matching the fovea’s ratio of primary cortex (V1) neurons per mm of retina would lead to roughly a one thousand increase in the size of the primary visual cortex. A full high spatial resolution visual system would likely drastically increase the brain’s computational expenditures and thus the metabolic cost. The ability of organisms with a heightened area of spatial fidelity (i.e., a fovea) to successfully support perceptual decision making relies critically on the guidance of saccadic eye movements to sample the visual world. Humans perform approximately three eye movements per second. The brain uses peripheral processing to extract critical information and guides the eyes across the scene. Eye movements can be directed to salient regions in a scene as potential locations of interest and for further processing [<xref ref-type="bibr" rid="pcbi.1005743.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref007">7</xref>]. During search, eye movements are also guided by information about the target including basic features including color, size, orientation and shape [<xref ref-type="bibr" rid="pcbi.1005743.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref011">11</xref>], probabilities of object occurrence, global scene statistics [<xref ref-type="bibr" rid="pcbi.1005743.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref013">13</xref>], object co-occurrence [<xref ref-type="bibr" rid="pcbi.1005743.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref016">16</xref>], and knowledge of the detectability of objects across the visual field [<xref ref-type="bibr" rid="pcbi.1005743.ref017">17</xref>] to successfully detect objects in cluttered scenes. The brain is also able to acquire information in the visual periphery to guide eye movements concurrent with analyses of information at the foveal region [<xref ref-type="bibr" rid="pcbi.1005743.ref018">18</xref>]. This foveated visual system with guided eye movements reduces the brain’s computational cost. What is not known are the decision accuracy costs of a foveated architecture relative to a non-foveated high spatial resolution system in ecologically relevant tasks. The current work aims at answering this question.</p>
<p>There have been many proposals for computational models of search and artificial systems with foveated visual systems [<xref ref-type="bibr" rid="pcbi.1005743.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref023">23</xref>]. A number of models use an ideal Bayesian observer that searches for a known target in noise-limited images [<xref ref-type="bibr" rid="pcbi.1005743.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref025">25</xref>]. Such frameworks typically do not model the degradation in retinal eccentricity with varying resolution feature extraction and the approach is limited to synthetic images for which the statistical distribution of the noise is known. There are other object detectors that can be applied to real world images but use the same high resolution representation of the target (template) across the whole visual field [<xref ref-type="bibr" rid="pcbi.1005743.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref027">27</xref>]. One group has implemented in hardware a visual sensing approach with two components: a pre-attentive component providing a fixed field of view (FOV) at low resolution, and a localized shiftable FOV at high resolution, designed to recognize and interpret events detected by the pre-attentive system [<xref ref-type="bibr" rid="pcbi.1005743.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref020">20</xref>]. However, no previous study has implemented a physiologically based foveated visual system and compared its performance for ecologically relevant search tasks against a homogeneous high resolution (non-foveated) system to specifically assess the decision accuracy costs of incorporating a varying resolution system.</p>
<p>The goal of the present work is to investigate the impact on object search performance of using a foveated visual field with physiologically plausible cortical peripheral pooling and saccade exploration, and compare it to a visual system with access to high spatial resolution at all points in the scene [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref030">30</xref>]. We evaluate the accuracy of the models in finding different objects in real scenes. To allow for a fair evaluation of the performance costs of foveation, both models (foveated and non-foveated) need to be implemented within a common computational framework.</p>
<p>Our reasoning is that if a foveated object detection model with eye movements can achieve similar object detection accuracy as a non-foveated approach, it might suggest a possible reason for the evolution of foveated systems in organisms: achieving successful object detection while minimizing computational and metabolic costs.</p>
<p>To achieve our goal we incorporate a visual field with varying spatial resolution [<xref ref-type="bibr" rid="pcbi.1005743.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref034">34</xref>] to contemporary object detection methods extended to use a latent linear discriminant formulation (§“The foveated object detector (FOD)”). There are many possible methods to implement a foveated visual field in an object detection system. In primates, foveation arises from various properties of the visual system including varying photoreceptor density across the retina [<xref ref-type="bibr" rid="pcbi.1005743.ref035">35</xref>], larger convergence onto ganglion cells with increasing retinal eccentricity [<xref ref-type="bibr" rid="pcbi.1005743.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref037">37</xref>], the higher number of neurons per mm<sup>2</sup> of retina at the visual cortex for foveal areas and spatial pooling possibly contributing to crowding effects (for a review of contributions, see Rosenholtz [<xref ref-type="bibr" rid="pcbi.1005743.ref038">38</xref>]). In this work, we opt to use a recent model [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>] which specifies how responses of elementary sensors are pooled at the layers (V1 and V2) of the human visual cortex. The model specifies the shapes and sizes of V1, V2 regions which pool responses from the visual field. This is clearly a simplification of the multi-stage processing in the human visual system (lower photoreceptor density and higher input convergence at the ganglion cells, lower number of V1 neurons with increasing eccentricity) accounting for the foveated properties of vision. However, such a simplified model seems to capture many aspects of peripheral processing including some crowding effects [<xref ref-type="bibr" rid="pcbi.1005743.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>]. We use a simplified version (only the V1 layer, see §“Foveated visual field” for details) of this model as the foveated visual field of our object detector (<xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref>). We call our detector the foveated object detector (FOD). The FOD computational savings arise from the fewer computations (dot products) related to the coarser spatial sampling of features (due to spatial pooling) in the visual periphery.</p>
<fig id="pcbi.1005743.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The foveated visual field of the proposed object detector.</title>
<p>Square blue boxes with white borders at the center are foveal pooling regions. Around them are peripheral pooling regions which are radially elongated. The sizes of peripheral regions increase with distance to the fixation point which is at the center of the fovea. The color within the peripheral regions represent pooling weights.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g001" xlink:type="simple"/>
</fig>
<p>Importantly, we used the same computer vision object detection framework to develop a foveated system, and a non-foveated system with homogeneous high spatial resolution across the visual field. The high spatial resolution system is the default object detection model which processes all information in the image with the same feature extraction stages and is known as a sliding window method (SW) in computer vision. The term sliding window refers to a re-current application of the feature extraction and classifier across the entire image by shifting spatial windows defining a region of interest.</p>
<p>We compared performances of the models on a standard computer vision image dataset (PASCAL VOC [<xref ref-type="bibr" rid="pcbi.1005743.ref040">40</xref>]) allowing for direct evaluation of the impact of a biologically based foveated visual system on the default non-foveated object detector across 20 different classes of objects and about 5000 test images.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Overview of the non-foveated object detector</title>
<p>The non-foveated object detector, or the sliding window (SW) object detector (<xref ref-type="fig" rid="pcbi.1005743.g002">Fig 2</xref>), starts by extracting from the input image a set of image features known as the histogram of oriented gradients (HoG) [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref041">41</xref>] (<xref ref-type="fig" rid="pcbi.1005743.g003">Fig 3</xref>) which is a feature descriptor utilized in object detection models in computer vision. The HoGs refer to a distribution of orientations within small neighborhoods akin to the responses of various cell receptive fields with different orientations. The first stage is to convolve the image with a 1-D point discrete derivative kernel (i.e., [-1, 0, 1]) in both of the horizontal and vertical directions. The second stage entails computing the cell histograms. Each pixel within the cell codes a linear response to the various oriented kernels (filters). The local histograms are normalized relative to the mean of a block (<xref ref-type="fig" rid="pcbi.1005743.g003">Fig 3</xref>). This process results in a <italic>M</italic>-by-<italic>N</italic>-by-<italic>D</italic> feature map where <italic>M</italic> is the number of rows, <italic>N</italic> is the number columns and <italic>D</italic> is the number of features which is determined by the number of different edge orientations considered. Next, the feature map is convolved with the object template which was learned from the training images. The object template is a model of object appearance in the form of a <italic>P</italic>-by-<italic>K</italic>-by-<italic>D</italic> matrix of feature weights (typically <italic>P</italic> &lt;&lt; <italic>M</italic> and <italic>K</italic> &lt;&lt; <italic>N</italic>). The template is evaluated at all <italic>M</italic> * <italic>N</italic> locations on the feature map. Each evaluation is a dot product between the template weights and the HoG features of the spatial <italic>P</italic>-by-<italic>K</italic> region (which corresponds to a bounding box on the image) that is covered by the template. The dot product result is recorded as the detection score for the corresponding bounding box.</p>
<fig id="pcbi.1005743.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Flowchart of the non-foveated sliding window (SW) model and the foveated object detector (FOD).</title>
<p>The feature extraction step is common to both models. First, the image is filtered with simple edge detection filters with different orientations, and gradient magnitude and orientation are estimated at each pixel. Then, the image is divided into small square boxes on a regular grid. Within each box, total gradient magnitude per orientation is computed, which results in a histogram. The output is a collection of feature maps for x, y locations and orientations. For simplicity, only one feature map (H) is shown as input to both models. <bold>Right side</bold>: Foveated Object Detector. The FOD has an initial fixation position that determines the pooling regions of the underlying histogram of gradient features. FOD’s templates are learned through training and are specific to each retinotopic location. The scores reflecting probability of target presence are used to guide saccades to the most likely target location. The object probability scores for each location are integrated across saccades and used for the final perceptual decision.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005743.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Histogram of oriented gradients (HoG) of a sample image.</title>
<p>Left: input image, right: HoG result. First, the input image is convolved with two 1-D filters, namely [+ 1 0 −1] and its transpose. The gradient magnitude and orientation at each pixel are estimated from the convolution results. Then, the image is divided into small, square bins. In each bin, an orientation histogram is computed, which shows the (relative) total gradient magnitude per orientation. Finally, the histogram in each bin is normalized by the total “energy” (e.g. <italic>ℓ</italic><sub>2</sub> norm) of a 2x2 block containing the bin akin to divisive local contrast normalization. This final step is known as block normalization. On the right, each HoG bin is represented with short, oriented line segments where brightness encodes the magnitude of the associated orientation. Due to the block normalization, in homogeneous areas (e.g. top-right) all orientations have high and similar magnitudes. (Image source statement: the original picture on the left was taken by the first author.)</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g003" xlink:type="simple"/>
</fig>
<p>The feature extraction and template evaluation process described above considers only a single template and a single scale of the input image, to simplify the explanation. However, in practice, there is uncertainty about the view-point and scale of the object appearing in the scene. Thus, the object detector had more than one template per object class, and each of these templates serves as an appearance model for a distinct view-point of the object (e.g. a bicycle viewed from the front and from the side). In all our experiments, we used two view-point templates per object class. In addition, since the scale of the object sought is not known apriori, a multiscale processing (we used 40 different scales as done in the Deformable Parts Model (DPM) model [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]) of the input image is required. Finally, all bounding boxes with detection scores higher than a given threshold are identified as object detections. However, if there are two bounding boxes that significantly overlap with each other, the one with the lower score is discarded (non-maxima suppression).</p>
</sec>
<sec id="sec004">
<title>Overview of the foveated object detector (FOD)</title>
<sec id="sec005">
<title>Feature extraction</title>
<p>The FOD (<xref ref-type="fig" rid="pcbi.1005743.g002">Fig 2</xref>) mimics the process by which humans search for objects in scenes utilizing eye movements to point the high resolution fovea to points of interest. The FOD gets assigned an initial fixation point on the input image and collects information by extracting image features through its foveated visual field. To allow for a fair comparison of models, we equated the feature extraction of the foveated model to that of the non-foveated. We used the histogram of oriented gradients (HoG [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref041">41</xref>]) as image features and a simplified version of the V1 model [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>] to compute pooled features within the visual field. The HoG features are extracted at full resolution over the whole image, however, after V1 pooling, the features around the fixation point are at fine spatial scale while features away from the fixation location are at coarser scale. This fine-to-coarse transition is dictated by the pooling region sizes of the visual field (<xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref>). Furthermore, because of the spatial pooling, a given region of interest has fewer features associated to it as the retinal eccentricity increases. Training such an object detector entails learning templates at all locations in the visual field. We refer to each of these templates as a retino-specific classifer. Because the visual field has varying resolution, the target related features vary depending on where it is located within the visual field. A mixture of linear templates is trained at selected locations in the visual field using a latent-support vector machine-like [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref042">42</xref>] framework. The section “<xref ref-type="sec" rid="sec021">Methods and models</xref>” specifies in detail all aspects of the model and its training. There were a total of around 500-700 different retino-specific classifiers trained to span the entire visual field. Each retino-specific classifier resulted in a object detection score reflecting the strength of evidence for the presence of the searched object at that location.</p>
</sec>
<sec id="sec006">
<title>Eye movement strategies</title>
<p>We assessed performance for two eye movement strategies, the maximum-a-posteriori (MAP) rule (§Eye movement strategy) and a random strategy (RAND) to demonstrate the importance of the guidance of eye movements. The MAP eye movement strategy moves the fovea to the location in the image with the highest posterior probability for the presence of the searched target. The MAP model has been shown to be consistent with human eye movements in a variety of visual search tasks [<xref ref-type="bibr" rid="pcbi.1005743.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref044">44</xref>]. Studies have demonstrated that in some circumstances human saccade statistics better match an ideal searcher [<xref ref-type="bibr" rid="pcbi.1005743.ref017">17</xref>] that makes eye movements to locations that maximize the accuracy of localizing targets, yet in many circumstances the MAP model approximates the ideal searcher [<xref ref-type="bibr" rid="pcbi.1005743.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref045">45</xref>] but is computationally more tractable for objects in real scenes.</p>
<p>New object detector scores are generated for each new fixation point. For each fixation, the FOD collects evidence through its foveal and peripheral detection templates and integrates the new evidence into an internal map, which keeps the evidence for target presence at all possible bounding box locations. Briefly, for a certain bounding box location, different fixations yield different detection scores arising from different retino-specific classifiers. The final detection score for that location is the summation of scores obtained through all fixations. The final scores are converted to posterior probabilities using a sigmoid transformation (see §Integrating observations across multiple fixations). The posterior probabilities are utilized to program the next eye movement using the MAP algorithm.</p>
<p>Object detector scores at fixated locations are reduced (inhibition of return) so that the foveated object detector is encouraged explore new locations and avoid revisits (see Part F in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref> for details and limitations on implementation of inhibition of return.)</p>
</sec>
<sec id="sec007">
<title>Perceptual decision</title>
<p>After multiple eye movements, the FOD integrates, for each spatial location, information collected at different fixations and computes object detection scores and associated bounding boxes. All bounding boxes with detection scores higher than a detection threshold are identified as final object detections. However, if there are two bounding boxes that significantly overlap (intersection over union greater than 0.5) with each other, the one with the lower score is discarded. Known as “non-maxima suppression,” this is a common post-processing step in computer vision object detection.</p>
</sec>
</sec>
<sec id="sec008">
<title>Evaluation of the effects of foveation on visual search for objects</title>
<p>We compared two models on the PASCAL VOC 2007 detection (<monospace>comp3</monospace>) challenge dataset and protocol [<xref ref-type="bibr" rid="pcbi.1005743.ref040">40</xref>]. The dataset contains 20 object classes, 5011 training images and 4952 test images. A training image might contain more than one instance of a specific object class. All results are obtained by training the classifiers on a different set of images than those utilized for testing.</p>
<sec id="sec009">
<title>Measures of performance</title>
<p>For a given object class, the performance of an object detector is computed as follows. First, the object detector is run over the testing images and generates a score for each evaluated location representing the evidence for the object being present at that location. Associated with each score is also a bounding box which encompasses the area of the image associated with the score. Scores are compared to a specific detection threshold <italic>T</italic>. The bounding boxes with scores above the threshold <italic>T</italic> are considered the object detector’s prediction about the presence and location of the objects. To evaluate whether the bounding boxes are considered correct, they are compared against the ground truth bounding boxes surrounding the actual objects in the images. The ground truth is obtained by annotation by multiple humans. A predicted box <italic>P</italic> is deemed a “true positive” if there is a ground truth box <italic>G</italic> such that the intersection area of <italic>P</italic> and <italic>G</italic> divided by the union area of <italic>P</italic> and <italic>G</italic> is larger than 0.5. Otherwise, <italic>P</italic> is deemed a “false positive.”</p>
<p>Next, <bold>recall</bold> and <bold>precision</bold> are computed for the specific threshold <italic>T</italic>. Recall is the hit rate, the number of true positives divided by the number of all ground truth boxes in the testing set. Precision is the number of true positives divided by the number of all predicted boxes retrieved by the detector (the sum of true positives and false positives). By varying the value of <italic>T</italic>, we obtain a recall-precision curve plot (an example is provided in Part A in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref>). The area under this curve is called “average precision”, or AP for short. Recall, precision and AP are the most common performance measures utilized in the computer vision community for the object detection problem. The AP can take values in the range [0, 1], however, to show more precision we use the “percent AP” (which is 100 times the original AP score) throughout the paper. To report the performance over many object classes, we average their AP scores which yields the “mean average precision” or “mAP” for short.</p>
</sec>
<sec id="sec010">
<title>Comparison of the FOD with non-foveated SW</title>
<p>As a first control, we compared the performance of our non-foveated (SW) implementation which corresponds to only using high-resolution foveal templates only, to three other object detection methods (DPM [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>], Examplar-SVM [<xref ref-type="bibr" rid="pcbi.1005743.ref029">29</xref>], LDA-based detection [<xref ref-type="bibr" rid="pcbi.1005743.ref042">42</xref>]) which also use sliding window for search, and whose image features (HoG [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref041">41</xref>]) and recognition models (mixture of linear templates) are similar to ours. We observed that our SW implementation is performing on par with the compared methods. This suggests that the main result in our paper (the influence of foveation on object detector performance) cannot be attributed to the implementation of a low-performing high resolution sliding window approach (SW). The reader is referred to the supplementary section Part B in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref> for details of the results of these comparisons.</p>
<p>We compared the performance of the foveated version of our object detector (FOD) with its non-foveated (SW) version. We also evaluated the importance of the eye movement strategy for the FOD by comparing the model with random eye movements vs. the inclusion of the MAP algorithm. <xref ref-type="table" rid="pcbi.1005743.t001">Table 1</xref> shows the percent average precision (AP) scores for FOD with different eye movement strategies and different number of fixations. The table also presents the performance of the non-foveated model (SW). The maximum-a-posteriori and random eye movement strategies are denoted with MAP and RAND, respectively. Because the model accuracy results will depend on the initial point of fixation, we ran the models with different initial points of fixation. The presence of a suffix on a model refers to the location of the initial fixation: “-C” stands for the center of the input image, i.e. (0.5, 0.5) in normalized image coordinates where the top-left corner is taken as (0, 0) and the bottom-right corner is (1, 1); and “-E” for the two locations at the left and right edges of the image, 10% of the image width away from the image border, that is (0.1, 0.5) and (0.9, 0.5). MAP-E and RAND-E results are the performance average of two different versions of the foveated models with initial fixations: one with initial fixation close to the left edge of the image, the other run close to the right edge of the image. For the random eye movement, we report the 95% confidence interval for AP over 10 different runs. We ran all systems for a total of 5 fixations. <xref ref-type="table" rid="pcbi.1005743.t001">Table 1</xref> shows results for after 1, 3 and 5 fixations. A condition with one fixation is a model that makes decisions based only on the initial fixation. A model with 3 fixations, executes two eye movements, integrates information across the initial fixation and two additional fixations to make a decision about locations of the searched object. The results show that the FOD using the MAP rule with 5 fixations (“MAP-C,5” for short) performs nearly as good as the SW detector (a difference of 0.2% in mean AP).</p>
<table-wrap id="pcbi.1005743.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.t001</object-id>
<label>Table 1</label>
<caption>
<title>Per class percent average precision (AP), mean average precision (mAP) over all 20 classes and relative computational costs of non-foveated SW and FOD on the PASCAL VOC 2007 dataset.</title>
<p>(Object class abbreviations are as follows. ap: aeroplane, bk: bike, bd: bird, bt:boat, bl: bottle, bs: bus, cr: car, ct: cat, ch: chair, cw: cow, dt: dining-table, dg: dog, hs: horse, mb: motorbike, pr: person, pt: potted-plant, sh: sheep, sf: sofa, tr: train, tv: tv-monitor).</p>
</caption>
<alternatives>
<graphic id="pcbi.1005743.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="2"/>
<th align="center">ap</th>
<th align="center">bk</th>
<th align="center">bd</th>
<th align="center">bt</th>
<th align="center">bl</th>
<th align="center">bs</th>
<th align="center">cr</th>
<th align="center">ct</th>
<th align="center">ch</th>
<th align="center">cw</th>
<th align="center">dt</th>
<th align="center">dg</th>
<th align="center">hs</th>
<th align="center">mb</th>
<th align="center">pr</th>
<th align="center">pt</th>
<th align="center">sh</th>
<th align="center">sf</th>
<th align="center">tr</th>
<th align="center">tv</th>
<th align="center">mAP</th>
<th align="center">Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" colspan="2" style="border-bottom:thick"><bold>SW</bold></td>
<td align="char" char="." style="border-bottom:thick">17.5</td>
<td align="char" char="." style="border-bottom:thick">28.6</td>
<td align="char" char="." style="border-bottom:thick">9.7</td>
<td align="char" char="." style="border-bottom:thick">10.4</td>
<td align="char" char="." style="border-bottom:thick">17.3</td>
<td align="char" char="." style="border-bottom:thick">29.8</td>
<td align="char" char="." style="border-bottom:thick">36.7</td>
<td align="char" char="." style="border-bottom:thick">7.9</td>
<td align="char" char="." style="border-bottom:thick">11.2</td>
<td align="char" char="." style="border-bottom:thick">21.0</td>
<td align="char" char="." style="border-bottom:thick">2.3</td>
<td align="char" char="." style="border-bottom:thick">2.7</td>
<td align="char" char="." style="border-bottom:thick">30.9</td>
<td align="char" char="." style="border-bottom:thick">21.1</td>
<td align="char" char="." style="border-bottom:thick">19.7</td>
<td align="char" char="." style="border-bottom:thick">3.0</td>
<td align="char" char="." style="border-bottom:thick">9.2</td>
<td align="char" char="." style="border-bottom:thick">13.7</td>
<td align="char" char="." style="border-bottom:thick">23.5</td>
<td align="char" char="." style="border-bottom:thick">25.2</td>
<td align="center" style="border-bottom:thick">17.1</td>
<td align="center" style="border-bottom:thick">100</td>
</tr>
<tr>
<td align="center" rowspan="3"><bold>MAP-C</bold></td>
<td align="center"><bold>1</bold></td>
<td align="char" char=".">17.0</td>
<td align="char" char=".">21.1</td>
<td align="char" char=".">4.9</td>
<td align="char" char=".">9.8</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">27.4</td>
<td align="char" char=".">27.9</td>
<td align="char" char=".">8.5</td>
<td align="char" char=".">3.7</td>
<td align="char" char=".">12.8</td>
<td align="char" char=".">2.0</td>
<td align="char" char=".">4.3</td>
<td align="char" char=".">29.7</td>
<td align="char" char=".">19.7</td>
<td align="char" char=".">18.2</td>
<td align="char" char=".">1.2</td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">14.0</td>
<td align="char" char=".">26.2</td>
<td align="char" char=".">21.8</td>
<td align="center">14.5</td>
<td align="center">11.5</td>
</tr>
<tr>
<td align="center"><bold>3</bold></td>
<td align="char" char=".">17.4</td>
<td align="char" char=".">27.7</td>
<td align="char" char=".">10.1</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">10.4</td>
<td align="char" char=".">30.8</td>
<td align="char" char=".">31.6</td>
<td align="char" char=".">8.4</td>
<td align="char" char=".">10.4</td>
<td align="char" char=".">17.2</td>
<td align="char" char=".">2.1</td>
<td align="char" char=".">3.4</td>
<td align="char" char=".">33.3</td>
<td align="char" char=".">21.1</td>
<td align="char" char=".">18.7</td>
<td align="char" char=".">3.4</td>
<td align="char" char=".">7.6</td>
<td align="char" char=".">15.4</td>
<td align="char" char=".">26.4</td>
<td align="char" char=".">23.5</td>
<td align="center">16.5</td>
<td align="center">31.2</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">17.0</td>
<td align="char" char=".">28.6</td>
<td align="char" char=".">10.0</td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">11.2</td>
<td align="char" char=".">31.0</td>
<td align="char" char=".">34.0</td>
<td align="char" char=".">8.3</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">18.2</td>
<td align="char" char=".">2.1</td>
<td align="char" char=".">3.4</td>
<td align="char" char=".">34.2</td>
<td align="char" char=".">21.8</td>
<td align="char" char=".">19.7</td>
<td align="char" char=".">2.8</td>
<td align="char" char=".">8.1</td>
<td align="char" char=".">15.1</td>
<td align="char" char=".">27.8</td>
<td align="char" char=".">24.0</td>
<td align="center">16.9</td>
<td align="center">49.6</td>
</tr>
<tr>
<td align="center" rowspan="3"><bold>MAP-E</bold></td>
<td align="center"><bold>1</bold></td>
<td align="char" char=".">1.6</td>
<td align="char" char=".">7.1</td>
<td align="char" char=".">4.1</td>
<td align="char" char=".">5.6</td>
<td align="char" char=".">9.1</td>
<td align="char" char=".">8.7</td>
<td align="char" char=".">11.7</td>
<td align="char" char=".">6.0</td>
<td align="char" char=".">3.6</td>
<td align="char" char=".">10.2</td>
<td align="char" char=".">2.0</td>
<td align="char" char=".">2.2</td>
<td align="char" char=".">8.5</td>
<td align="char" char=".">10.2</td>
<td align="char" char=".">13.5</td>
<td align="char" char=".">1.3</td>
<td align="char" char=".">6.8</td>
<td align="char" char=".">8.0</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">10.3</td>
<td align="center">7.1</td>
<td align="center">8.7</td>
</tr>
<tr>
<td align="center"><bold>3</bold></td>
<td align="char" char=".">13.0</td>
<td align="char" char=".">24.6</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">9.8</td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">27.2</td>
<td align="char" char=".">29.3</td>
<td align="char" char=".">7.4</td>
<td align="char" char=".">10.4</td>
<td align="char" char=".">16.4</td>
<td align="char" char=".">3.7</td>
<td align="char" char=".">2.2</td>
<td align="char" char=".">30.6</td>
<td align="char" char=".">20.8</td>
<td align="char" char=".">16.9</td>
<td align="char" char=".">3.3</td>
<td align="char" char=".">11.2</td>
<td align="char" char=".">13.8</td>
<td align="char" char=".">23.0</td>
<td align="char" char=".">24.1</td>
<td align="center">15.4</td>
<td align="center">28.1</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">15.1</td>
<td align="char" char=".">28.0</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">10.4</td>
<td align="char" char=".">11.6</td>
<td align="char" char=".">29.9</td>
<td align="char" char=".">33.0</td>
<td align="char" char=".">8.3</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">18.7</td>
<td align="char" char=".">2.7</td>
<td align="char" char=".">4.1</td>
<td align="char" char=".">33.7</td>
<td align="char" char=".">22.6</td>
<td align="char" char=".">18.9</td>
<td align="char" char=".">3.1</td>
<td align="char" char=".">7.1</td>
<td align="char" char=".">14.7</td>
<td align="char" char=".">25.5</td>
<td align="char" char=".">25.2</td>
<td align="center">16.7</td>
<td align="center">46.9</td>
</tr>
<tr>
<td align="center" rowspan="3"><bold>RAND</bold></td>
<td align="center"><bold>1</bold></td>
<td align="char" char=".">8.2</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">5.5</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">7.8</td>
<td align="char" char=".">12.2</td>
<td align="char" char=".">16.2</td>
<td align="char" char=".">6.1</td>
<td align="char" char=".">6.8</td>
<td align="char" char=".">7.5</td>
<td align="char" char=".">1.6</td>
<td align="char" char=".">2.5</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">9.1</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">1.9</td>
<td align="char" char=".">5.0</td>
<td align="char" char=".">6.7</td>
<td align="char" char=".">11.2</td>
<td align="char" char=".">10.0</td>
<td align="center">7.9±1.4</td>
<td align="center" rowspan="3">similar to above</td>
</tr>
<tr>
<td align="center"><bold>3</bold></td>
<td align="char" char=".">9.6</td>
<td align="char" char=".">13.0</td>
<td align="char" char=".">3.2</td>
<td align="char" char=".">9.6</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">16.9</td>
<td align="char" char=".">23.5</td>
<td align="char" char=".">8.8</td>
<td align="char" char=".">9.4</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">1.8</td>
<td align="char" char=".">3.2</td>
<td align="char" char=".">16.5</td>
<td align="char" char=".">12.3</td>
<td align="char" char=".">12.2</td>
<td align="char" char=".">2.7</td>
<td align="char" char=".">3.9</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">16.9</td>
<td align="char" char=".">11.7</td>
<td align="center">10.2±0.9</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">10.9</td>
<td align="char" char=".">15.3</td>
<td align="char" char=".">3.8</td>
<td align="char" char=".">9.7</td>
<td align="char" char=".">9.6</td>
<td align="char" char=".">20.5</td>
<td align="char" char=".">26.3</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">9.5</td>
<td align="char" char=".">10.6</td>
<td align="char" char=".">1.5</td>
<td align="char" char=".">3.1</td>
<td align="char" char=".">20.9</td>
<td align="char" char=".">13.7</td>
<td align="char" char=".">13.5</td>
<td align="char" char=".">2.7</td>
<td align="char" char=".">3.9</td>
<td align="char" char=".">12.0</td>
<td align="char" char=".">18.9</td>
<td align="char" char=".">12.4</td>
<td align="center">11.4±1.0</td>
</tr>
<tr>
<td align="center" rowspan="3"><bold>RAND-C</bold></td>
<td align="center"><bold>1</bold></td>
<td align="center" colspan="21">This row is the same with the “MAP-C, 1” above.</td>
<td align="center" rowspan="3">”</td>
</tr>
<tr>
<td align="center"><bold>3</bold></td>
<td align="char" char=".">17.5</td>
<td align="char" char=".">20.4</td>
<td align="char" char=".">3.7</td>
<td align="char" char=".">10.0</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">28.6</td>
<td align="char" char=".">27.4</td>
<td align="char" char=".">11.5</td>
<td align="char" char=".">6.7</td>
<td align="char" char=".">11.8</td>
<td align="char" char=".">1.7</td>
<td align="char" char=".">3.5</td>
<td align="char" char=".">31.7</td>
<td align="char" char=".">18.0</td>
<td align="char" char=".">15.4</td>
<td align="char" char=".">2.7</td>
<td align="char" char=".">5.4</td>
<td align="char" char=".">15.2</td>
<td align="char" char=".">26.1</td>
<td align="char" char=".">15.8</td>
<td align="center">14.1±0.5</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">17.6</td>
<td align="char" char=".">21.4</td>
<td align="char" char=".">5.2</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">9.7</td>
<td align="char" char=".">28.1</td>
<td align="char" char=".">28.6</td>
<td align="char" char=".">11.4</td>
<td align="char" char=".">9.6</td>
<td align="char" char=".">12.1</td>
<td align="char" char=".">1.6</td>
<td align="char" char=".">3.5</td>
<td align="char" char=".">30.0</td>
<td align="char" char=".">17.9</td>
<td align="char" char=".">15.3</td>
<td align="char" char=".">3.7</td>
<td align="char" char=".">6.7</td>
<td align="char" char=".">14.4</td>
<td align="char" char=".">25.4</td>
<td align="char" char=".">15.9</td>
<td align="center">14.4±0.7</td>
</tr>
<tr>
<td align="center" rowspan="3"><bold>RAND-E</bold></td>
<td align="center"><bold>1</bold></td>
<td align="center" colspan="21">This row is the same with the “MAP-E, 1” above.</td>
<td align="center" rowspan="3">”</td>
</tr>
<tr>
<td align="center"><bold>3</bold></td>
<td align="char" char=".">9.1</td>
<td align="char" char=".">13.1</td>
<td align="char" char=".">2.8</td>
<td align="char" char=".">9.7</td>
<td align="char" char=".">9.4</td>
<td align="char" char=".">17.8</td>
<td align="char" char=".">22.5</td>
<td align="char" char=".">9.0</td>
<td align="char" char=".">6.6</td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">2.3</td>
<td align="char" char=".">3.7</td>
<td align="char" char=".">14.9</td>
<td align="char" char=".">12.0</td>
<td align="char" char=".">14.9</td>
<td align="char" char=".">1.3</td>
<td align="char" char=".">3.9</td>
<td align="char" char=".">2.4</td>
<td align="char" char=".">13.6</td>
<td align="char" char=".">14.1</td>
<td align="center">9.7±0.7</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">15.9</td>
<td align="char" char=".">4.1</td>
<td align="char" char=".">8.7</td>
<td align="char" char=".">9.5</td>
<td align="char" char=".">21.9</td>
<td align="char" char=".">26.0</td>
<td align="char" char=".">8.2</td>
<td align="char" char=".">9.7</td>
<td align="char" char=".">11.6</td>
<td align="char" char=".">1.7</td>
<td align="char" char=".">4.3</td>
<td align="char" char=".">17.6</td>
<td align="char" char=".">13.7</td>
<td align="char" char=".">14.1</td>
<td align="char" char=".">1.9</td>
<td align="char" char=".">5.7</td>
<td align="char" char=".">4.8</td>
<td align="char" char=".">15.7</td>
<td align="char" char=".">15.8</td>
<td align="center">11.1±1.1</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p><xref ref-type="fig" rid="pcbi.1005743.g004">Fig 4</xref> shows the ratio of mean AP for the FOD with the various eye movement strategies to that of the non-foveated SW system (relative performance) as a function of fixation. The relative performance of the MAP-C to non-foveated SW (AP of MAP-C divided by AP of SW) is 98.8% for 5 fixations, 96.5% for 3 fixations and 84.8% for 1 fixation. The FOD with eye movement guidance towards the target (MAP-C,5) achieves or exceeds SW’s performance with only 1 fixation in 4 classes, with 3 fixations in 7 classes, with 5 fixations in 2 classes. For the remaining of 7 classes, FOD needs more than 5 fixations to achieve SW’s performance.</p>
<fig id="pcbi.1005743.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Ratio of mean average precision (AP) scores of FOD systems relative to that of the non-foveated SW system.</title>
<p>Graph shows two eye movement algorithms: maximum aposteriori probability (MAP) and random (RAND) and two starting points (C: center of the image; E: left or right edge of the image).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g004" xlink:type="simple"/>
</fig>
<p>MAP-C performs well (84.8% relative performance) even with 1 fixation. The reason behind this result is the fact that, on average, bounding boxes in the PASCAL dataset cover a large portion of the images (average bounding box area normalized by image area is 0.2) and are located at and around the center [<xref ref-type="bibr" rid="pcbi.1005743.ref046">46</xref>]. To reduce the effects of these biases about the location of object placement on the results, we assessed the models with an initial fixation close to the edge of the image (MAP-E). When the initial fixation is closer to the edge of the image, performance is initially worse than when the initial fixation is at the center of the image. The difference in performance diminishes achieving similar performance with five fixations (0.2 difference in mean AP). <xref ref-type="fig" rid="pcbi.1005743.g005">Fig 5</xref> shows how the distribution of AP scores for different object classes for MAP-E improves from 1 fixation to 5 fixations.</p>
<fig id="pcbi.1005743.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Area under the recall precision curve (AP scores) achieved by the non-foveated (SW) model and the foveated object detector with a Maximum a posteriori eye movement strategy and a starting fixation point to the side of the image (MAP-E).</title>
<p>Symbols represent each object class type. Identity (diagonal) line corresponds to equal performance across models.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Importance of the guidance algorithm</title>
<p>To assess the importance of guided saccades towards the target, we compared performance of the MAP strategy against FOD that guides eye movements based on a random eye movement generator. <xref ref-type="fig" rid="pcbi.1005743.g004">Fig 4</xref> allows comparisons of the relative performance of the MAP FOD and those with a random eye movement strategy. The performance gap between MAP-C, RAND-C pair and MAP-E, RAND-E pair highlights the performance costs of a foveated system without an algorithm to guide eye movements.</p>
</sec>
<sec id="sec012">
<title>Computational cost savings</title>
<p>In both non-foveated SW based methods and the FOD, linear template evaluations, i.e. taking dot-products, is the main computationally costly operation. We define the computational cost of a method based on the total number of template evaluations (dot products) it executes (as also done in [<xref ref-type="bibr" rid="pcbi.1005743.ref047">47</xref>]). A model may have several templates with different sizes, so instead of counting each template evaluation as 1 operation, we take into account the dimensionalities of the templates. For example, the cost of evaluating a (6-cell)x(8-cell) HoG template is counted as 48 operations.</p>
<p>In order to compute the computational cost of a model, we run it on a subset of the test image set and count the total number of operations (as described above) actually performed. Note that, in order to compute a detection score, the FOD first performs a feature pooling (based on the location of the component in the visual field) and then a linear template evaluation. Since these are both linear operations, we combine them into the evaluation of a single template. This means that the costs of feature pooling and template evaluation are included in the evaluation of this single template.</p>
<p>The last column of <xref ref-type="table" rid="pcbi.1005743.t001">Table 1</xref> gives the computational costs of the non-foveated SW method and the FOD. For the FOD the computational cost is reported as a function of different number of fixations. For ease of comparison, we normalized the costs so that the non-foveated SW method performs 100 operations in total. The results show that FOD is computationally more efficient. FOD achieves almost the same accuracy performance—98.8% of the non-foveated SW’s average-precision score—at 49.6% of the computational cost of the non-foveated SW model. Typically, in computer vision, complexity of algorithms are specified in terms of the input image size. The computational complexity of the non-foveated model, in this sense, can be expressed easily. However, this is not the case for the FOD whose computational complexity does not depend on image size but on a number of factors including the scaling factor of pooling regions and the number of required fixations. For this reason, we compare the computational costs of the FOD and the non-foveated SW models in terms of the total number of actual dot-product operations performed in template evaluations.</p>
</sec>
<sec id="sec013">
<title>Using richer object detection models at the fovea to increase performance</title>
<p>The FOD uses linear classifiers to detect objects. Here we evaluate the effects of using richer and more expensive classifiers but restricted only to the fovea. After each fixation, the FOD evaluated a full Deformable Parts Model (DPM) detector [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>] only at foveal locations that score above a threshold which is determined on the training set to achieve high recall rate (95%). The DPM is a computer vision object detector that models not only the overall appearance of the object (via what they call the root filter) but also its parts. We refer to the new foveated object detector that uses DPM at its fovea as the “FOD-DPM”.</p>
<p>
<xref ref-type="table" rid="pcbi.1005743.t002">Table 2</xref> and <xref ref-type="fig" rid="pcbi.1005743.g006">Fig 6</xref> present the performance results of this approach and compares it to the non-foveated (sliding window) DPM model which we call the SW-DPM, for short. FOD-DPM achieves a similar average performance to that of SW-DPM (98.2% relative performance, 0.6 AP gap) using 9 fixations and exceeds DPM’s performance starting from 11 fixations. On some classes (e.g. bus, car, horse), FOD-DPM exceeds SW-DPM’s performance probably due to lesser number of evaluations and reduced false positives; on other cases (e.g. bike, dog, tv) FOD-DPM underperforms probably due to low recall rate of the FOD detector for these classes. <xref ref-type="fig" rid="pcbi.1005743.g007">Fig 7</xref> shows AP scores of FOD-DPM and SW-DPM for each object class to demonstrate the improvement from 1 to 9 fixations.</p>
<table-wrap id="pcbi.1005743.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.t002</object-id>
<label>Table 2</label>
<caption>
<title>Per class percent average precision (AP), mean average precision (mAP) over all 20 classes and relative computational costs of FOD-DPM and DPM on the PASCAL VOC 2007 dataset.</title>
<p>(Object class abbreviations are as follows. ap: aeroplane, bk: bike, bd: bird, bt:boat, bl: bottle, bs: bus, cr: car, ct: cat, ch: chair, cw: cow, dt: dining-table, dg: dog, hs: horse, mb: motorbike, pr: person, pt: potted-plant, sh: sheep, sf: sofa, tr: train, tv: tv-monitor).</p>
</caption>
<alternatives>
<graphic id="pcbi.1005743.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="2"/>
<th align="center">ap</th>
<th align="center">bk</th>
<th align="center">bd</th>
<th align="center">bt</th>
<th align="center">bl</th>
<th align="center">bs</th>
<th align="center">cr</th>
<th align="center">ct</th>
<th align="center">ch</th>
<th align="center">cw</th>
<th align="center">dt</th>
<th align="center">dg</th>
<th align="center">hs</th>
<th align="center">mb</th>
<th align="center">pr</th>
<th align="center">pt</th>
<th align="center">sh</th>
<th align="center">sf</th>
<th align="center">tr</th>
<th align="center">tv</th>
<th align="center">mAP</th>
<th align="center">Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" colspan="2"><bold>SW-DPM</bold></td>
<td align="char" char=".">33.2</td>
<td align="char" char=".">60.3</td>
<td align="char" char=".">10.2</td>
<td align="char" char=".">16.1</td>
<td align="char" char=".">27.3</td>
<td align="char" char=".">54.3</td>
<td align="char" char=".">58.2</td>
<td align="char" char=".">23.0</td>
<td align="char" char=".">20.0</td>
<td align="char" char=".">24.1</td>
<td align="char" char=".">26.7</td>
<td align="char" char=".">12.7</td>
<td align="char" char=".">58.1</td>
<td align="char" char=".">48.2</td>
<td align="char" char=".">43.2</td>
<td align="char" char=".">12.0</td>
<td align="char" char=".">21.1</td>
<td align="char" char=".">36.1</td>
<td align="char" char=".">46.0</td>
<td align="char" char=".">43.5</td>
<td align="char" char=".">33.7</td>
<td align="center">100</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>FOD-DPM</bold></td>
<td align="center"><bold>1</bold></td>
<td align="char" char=".">31.0</td>
<td align="char" char=".">37.1</td>
<td align="char" char=".">10.0</td>
<td align="char" char=".">14.3</td>
<td align="char" char=".">12.9</td>
<td align="char" char=".">47.1</td>
<td align="char" char=".">46.7</td>
<td align="char" char=".">28.0</td>
<td align="char" char=".">9.3</td>
<td align="char" char=".">15.5</td>
<td align="char" char=".">26.2</td>
<td align="char" char=".">10.7</td>
<td align="char" char=".">56.0</td>
<td align="char" char=".">39.7</td>
<td align="char" char=".">29.4</td>
<td align="char" char=".">9.8</td>
<td align="char" char=".">15.5</td>
<td align="char" char=".">27.6</td>
<td align="char" char=".">43.4</td>
<td align="char" char=".">21.5</td>
<td align="char" char=".">26.6</td>
<td align="center">0.46</td>
</tr>
<tr>
<td align="center"><bold>5</bold></td>
<td align="char" char=".">32.3</td>
<td align="char" char=".">50.0</td>
<td align="char" char=".">9.8</td>
<td align="char" char=".">15.2</td>
<td align="char" char=".">21.8</td>
<td align="char" char=".">50.0</td>
<td align="char" char=".">63.0</td>
<td align="char" char=".">25.9</td>
<td align="char" char=".">17.1</td>
<td align="char" char=".">20.5</td>
<td align="char" char=".">25.4</td>
<td align="char" char=".">9.7</td>
<td align="char" char=".">61.4</td>
<td align="char" char=".">44.6</td>
<td align="char" char=".">38.0</td>
<td align="char" char=".">9.2</td>
<td align="char" char=".">19.7</td>
<td align="char" char=".">30.1</td>
<td align="char" char=".">43.1</td>
<td align="char" char=".">32.1</td>
<td align="char" char=".">31.0</td>
<td align="center">1.84</td>
</tr>
<tr>
<td align="center"><bold>9</bold></td>
<td align="char" char=".">33.2</td>
<td align="char" char=".">56.6</td>
<td align="char" char=".">9.9</td>
<td align="char" char=".">15.6</td>
<td align="char" char=".">25.3</td>
<td align="char" char=".">54.6</td>
<td align="char" char=".">65.3</td>
<td align="char" char=".">25.3</td>
<td align="char" char=".">19.8</td>
<td align="char" char=".">22.0</td>
<td align="char" char=".">24.9</td>
<td align="char" char=".">9.4</td>
<td align="char" char=".">60.9</td>
<td align="char" char=".">50.8</td>
<td align="char" char=".">41.7</td>
<td align="char" char=".">10.0</td>
<td align="char" char=".">20.4</td>
<td align="char" char=".">34.9</td>
<td align="char" char=".">44.3</td>
<td align="char" char=".">37.3</td>
<td align="char" char=".">33.1</td>
<td align="center">3.09</td>
</tr>
<tr>
<td align="center"><bold>13</bold></td>
<td align="char" char="."><bold>33.4</bold></td>
<td align="char" char=".">59.9</td>
<td align="char" char=".">10.0</td>
<td align="char" char=".">15.7</td>
<td align="char" char=".">27.2</td>
<td align="char" char="."><bold>54.8</bold></td>
<td align="char" char="."><bold>65.7</bold></td>
<td align="char" char="."><bold>25.0</bold></td>
<td align="char" char="."><bold>20.5</bold></td>
<td align="char" char=".">22.0</td>
<td align="char" char=".">24.8</td>
<td align="char" char=".">9.2</td>
<td align="char" char="."><bold>62.0</bold></td>
<td align="char" char="."><bold>51.9</bold></td>
<td align="char" char="."><bold>44.5</bold></td>
<td align="char" char=".">10.2</td>
<td align="char" char=".">20.9</td>
<td align="char" char="."><bold>36.8</bold></td>
<td align="char" char="."><bold>46.2</bold></td>
<td align="char" char=".">40.9</td>
<td align="char" char="."><bold>34.1</bold></td>
<td align="center">4.16</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1005743.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g006</object-id>
<label>Fig 6</label>
<caption>
<title>FOD-DPM’s performance (mean AP over all 20 classes) as a function of number of fixations.</title>
<p>FOD-DPM achieves SW-DPM’s performance at 11 fixations and exceeds it with more fixations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g006" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005743.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Per class AP scores achieved by FOD-DPM and non-foveated SW-DPM.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>Computational savings of FOD-DPM</title>
<p>We compare the computational complexities of FOD-DPM and SW-DPM by their total number of operations as defined above. For a given object class, DPM model has 3 root filters and 8 6x6 part filters. It is straightforward to calculate the number of operations performed by SW-DPM as it uses the sliding window method. For FOD-DPM, the total number of operations is calculated by adding: 1) FOD’s operations and 2) SW-DPM’s operations at each high-scoring foveal detection bounding box <bold><italic>b</italic></bold>, one DPM root filter (with the most similar shape as <bold><italic>b</italic></bold>) and 8 parts evaluated at all locations within the boundaries of this root filter. Cost of feature extraction is not included as the two methods use the same feature extraction code. We report the computational costs of FOD-DPM and SW-DPM in the last column of <xref ref-type="table" rid="pcbi.1005743.t002">Table 2</xref>. The costs are normalized so that SW-DPM’s cost is 100 operations. Results show that FOD-DPM drastically reduces the cost from 100 to 3.09 for 9 fixations. Assuming both methods are implemented equally efficiently, this would translate to an approximately <bold>32x</bold> speed-up. These results demonstrate the effectiveness of our foveated object detector in guiding the visual search. In the FOD-DPM implementation, the visual periphery has, in addition to the greater spatial pooling, much simpler processing relative to the fovea. The fovea has a subsequent parts processing that the periphery lacks. This is essential to account for much of the additional cost savings of the FOD-DPM vs. the simpler FOD model (compare the last columns of Tables <xref ref-type="table" rid="pcbi.1005743.t001">1</xref> and <xref ref-type="table" rid="pcbi.1005743.t002">2</xref>). A qualitative difference in computations at the fovea and periphery is consistent with recent findings utilizing brief dichoptic presentation of visual stimuli and proposing more top-down processing at the fovea [<xref ref-type="bibr" rid="pcbi.1005743.ref048">48</xref>].</p>
<p>Finally, <xref ref-type="fig" rid="pcbi.1005743.g008">Fig 8</xref> shows sample detections by the FOD. We illustrate the trained bicycle, person and car models on an image outside of the PASCAL dataset. The models were assigned the same initial fixation location and we ran them for 3 fixations. Results show that the each model fixates at different locations, and these locations are attracted towards instances of the target objects being searched.</p>
<fig id="pcbi.1005743.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Fixation locations and bounding box predictions of the FOD for three different object classes (person, car and bicycle) but for the same image and initial point of fixation.</title>
<p>Top-left: original image (source: <ext-link ext-link-type="uri" xlink:href="https://www.flickr.com/photos/kristoffer-trolle/27882648666/" xlink:type="simple">https://www.flickr.com/photos/kristoffer-trolle/27882648666/</ext-link> with Creative Commons license.), top-right: person detection, bottom-left: car detections, bottom-right: bicycle detection. Yellow dots show fixation points, numbers in yellow fonts indicate the sequence of fixations and the bounding boxes are the final detections.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g008" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec015">
<title>Evaluation of the effect of foveation on saliency</title>
<p>Our previous sections suggest that a computationally less costly foveated system can achieve similar performance accuracy finding an object in real scenes as a system with homogeneous high spatial resolution. Research has shown that visual areas in the brain also rapidly compute bottom-up information in terms of salient regions defined by contrast, edges and color [<xref ref-type="bibr" rid="pcbi.1005743.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref007">7</xref>]. These salient regions serve to identify potential locations in scenes for further computation. The impact of a foveated visual system in such saliency computations is not known. Here, we evaluate whether identifying the most salient region in an image, an important component of bottom-up attention useful to identify potential regions of a scene for further scrutiny, is affected by the process of foveation. Or in the contrary, can a foveated system with eye movements identify the same salient regions with less computation than a non-foveated system?</p>
<p>We implemented a simple model of saliency that followed conceptually the model proposed by Li [<xref ref-type="bibr" rid="pcbi.1005743.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref050">50</xref>]. Such saliency model involves two computational aspects of saliency, namely iso-orientation suppression and contour enhancement, and also their dynamics. Here we only implemented the iso-orientation suppression aspect by using a simple center-surround operation.</p>
<p>The current simplified implementation of the saliency model first extracts features by convolving Gabor receptive fields (4 scales and 8 orientations) with the input image. Each cell pools (sums) Gabor responses per orientation, within its receptive field. Then, a center surround computation is implemented by subtracting each cell’s response by the pooled response of the neighboring cells at that same orientation (iso-orientation suppression). The spatial distribution of responses after the suppressions were considered the saliency map and the highest value the top saliency score. We implemented two versions of this saliency model, a non-foveated version consisting of only foveal cells and a foveated version that uses the simplified Freeman-Simoncelli model as its visual field (The same visual field that is used by the FOD.) (§Foveated visual field). The non-foveated saliency model processes all locations of the input image with the same (high) resolution and each Gabor receptive field was suppressed by pooling the Gabors with the same orientation at eight neighboring locations. The foveated model processes the input image with varying resolution. The foveal center surround suppression consisted of subtracting from a cell’s receptive field response the pooled activity across eight (nearest) surrounding cells of the same orientation. However, the peripheral center surround was implemented by subtracting the pooled responses across four nearest neighboring cells. The foveated saliency model makes eye movements based on the saliency values computed at the peripheral cells. It executes a saccade to the location with the maximum saliency value. After saccade execution, the saliency values around foveation (a 2-degree radius area around each fixation point) is inhibited (inhibition of return). This prevents the model from getting caught at a maximally salient location and not executing additional eye movements. After <italic>n</italic> fixations, the model integrates saliency values for each location across fixations and selects the top saliency location.</p>
<p>We compared decisions of the non-foveated and foveated saliency models in terms of their agreement in selecting the top salient location within the image. We compared the computational costs of the two models in terms of their associated total number of center-surround operations. <xref ref-type="fig" rid="pcbi.1005743.g009">Fig 9</xref> shows the distance in degrees between the top salient location from the non-foveated saliency model (blue line) and that of the foveated saliency model. The comparison is plotted as a function of increasing number of fixations for the foveated saliency model. In red we show the fraction of the number of center surround operations of the foveated saliency model relative to the non-foveated model. This is shown as a function of number of fixations. The results show that the foveated model can generate a similar prediction for the most salient region as the non-foveated model but with a significantly less number of center-surround operations. Mathematical details of the foveated saliency model can be found in the supplementary section Part E in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref>.</p>
<fig id="pcbi.1005743.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Performance comparison of the foveated saliency model versus the non-foveated saliency model.</title>
<p>We ran both models for the simple task of identifying the topmost salient location, on 100 natural images randomly selected from the PASCAL VOC 2007 dataset. The blue curve plots the average distance (in degrees) between the topmost salient locations, S1 and S2, found by the foveated and the non-foveated model, respectively, on the same image. Note that this location is unique and fixed for the non-foveated model while it changes for the foveated model as the model explores the image, i.e. makes more and more fixations. The red curve plots the average number of iso-orientation suppression operations of the foveated model relative to that of the non-foveated model. Again, the number of such operations for the non-foveated model is fixed but it changes for the foveated model with the number of fixations. Foveated model finds the same topmost salient location as the non-foveated model, after 16 fixations. Notably, after 8 fixations, the distance between S1 and S2 becomes less than 1 degree. The foveated model achieves this level of accuracy by doing 42% less iso-orientation suppression operations than the non-foveated model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g009" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec016" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec017">
<title>The benefits of a foveated visual system</title>
<p>The objective of our current work was to evaluate within a common framework the accuracy costs and computational savings of a foveated visual system relative to a full high resolution system. We evaluated an object detection paradigm over 20 object classes from a standard object detection dataset. Our results show that with five exploratory fixations, the foveated method achieves nearly the same performance as the non-foveated (high resolution SW) method (<xref ref-type="fig" rid="pcbi.1005743.g005">Fig 5</xref>). The foveated achieved such accuracy with 49.6% of the sliding window method’s computational cost. Using a richer model (such as the Deformable Parts Model, DPM [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]) that selects potential locations for further object part processing, the foveated version of the model was able to match and even outperform the non-foveated SW-DPM while achieving computational savings (at less then <bold>4.16%</bold> of the SW-DPM’s computational cost). In addition, <xref ref-type="fig" rid="pcbi.1005743.g004">Fig 4</xref> highlights the importance of guided eye movements in re-orienting the fovea to regions of interest in the scene. Eliminating the guidance diminishes the model’s ability to correctly detect the object with additional fixations. Together the results suggest that a foveated visual system with guided eye movements provide computational savings while preserving an organism’s ability to successfully detect objects in scenes. Our conclusions are limited by the utilized data set which although large does not represent all set of tasks that an organism or a human might face. For example, the PASCAL dataset does not contain a large number of images with small objects in the scenes. Such scenes might represent a more challenging test set for the foveated object detector and show potential accuracy losses beyond those quantified in the current investigation. On the other hand, our model only guides its eye movements based on peripheral information about the target while humans are known to utilize information about global statistics [<xref ref-type="bibr" rid="pcbi.1005743.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref051">51</xref>] and object co-occurrence [<xref ref-type="bibr" rid="pcbi.1005743.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref052">52</xref>] and multiple object configuration [<xref ref-type="bibr" rid="pcbi.1005743.ref053">53</xref>] to guide eye movements and aids perceptual decisions [<xref ref-type="bibr" rid="pcbi.1005743.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref055">55</xref>]. Inclusion of such sources of scene information might improve the FOD’s accuracy.</p>
<p>In addition to evaluating performance of a foveated visual system for object search, we assessed the impact of a foveated visual system on the computation of saliency which is a fundamental bottom-up component that guides attention [<xref ref-type="bibr" rid="pcbi.1005743.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref007">7</xref>]. We compared a simplified implementation of a non-foveated saliency model and its foveated counterpart. The results showed that the foveated visual system with about 8-10 eye movements could approximate the same selection of the top salient location by a non-foveated high resolution saliency model. Future work should evaluate the generality of the results to more complex and different models of saliency [<xref ref-type="bibr" rid="pcbi.1005743.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref057">57</xref>]. In addition, our implementation of a foveated visual system concentrated on the spatial pooling losses occurring at cortical areas and possibly responsible for a great portion of the bottleneck of visual processing at the periphery [<xref ref-type="bibr" rid="pcbi.1005743.ref038">38</xref>]. A more detailed model could implement the peripheral loss starting with the reduction in photoreceptor density [<xref ref-type="bibr" rid="pcbi.1005743.ref035">35</xref>] and spatial sampling of retinal ganglion cells [<xref ref-type="bibr" rid="pcbi.1005743.ref058">58</xref>]. We believe that such detailed implementation would not qualitatively change the fundamental result of our paper, but future efforts need to test such prediction.</p>
<p>Our work assessed the impact of foveation on perceptual performance but does not address the mechanism by which the foveated visual system is evolved or shaped during development. An interesting theory contends that cone density is shaped by the probability distribution of objects across the retina and is thus influenced by the frequency and accuracy of eye movements [<xref ref-type="bibr" rid="pcbi.1005743.ref059">59</xref>].</p>
</sec>
<sec id="sec018">
<title>Comparison to other biologically inspired methods</title>
<p>There have been previous efforts, (e.g. [<xref ref-type="bibr" rid="pcbi.1005743.ref060">60</xref>]), on biologically inspired object recognition. However, most of such models do not have a foveated visual field and thus do not execute eye movements. More recent work has implemented biologically inspired search methods. In [<xref ref-type="bibr" rid="pcbi.1005743.ref019">19</xref>], a fixed, pre-attentive, low-resolution wide-field camera is combined with a shiftable, attentive, high-resolution narrow-field camera, where the pre-attentive camera generates saccadic targets for the attentive, high-resolution camera. The difference between this and our method is that while their pre-attentive system has the same coarse resolution everywhere in the visual field, our method, which is a model of the V1 layer of the visual cortex, has a varying resolution that depends on the radial distance to the center of the fovea. There have been previous efforts to create foveated search models with eye movements [<xref ref-type="bibr" rid="pcbi.1005743.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref061">61</xref>]. Such models have been applied mostly to detect simple signals in computer generated noise [<xref ref-type="bibr" rid="pcbi.1005743.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref062">62</xref>] and used as benchmarks to compare against human eye movements and performance.</p>
<p>Other biologically inspired methods include the target acquisition model (TAM) [<xref ref-type="bibr" rid="pcbi.1005743.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref063">63</xref>], the Infomax model [<xref ref-type="bibr" rid="pcbi.1005743.ref027">27</xref>] and artificial neural network based models [<xref ref-type="bibr" rid="pcbi.1005743.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref065">65</xref>]. TAM is a foveated model and it uses the Scale Invariant Feature Transform (SIFT) features [<xref ref-type="bibr" rid="pcbi.1005743.ref066">66</xref>] for representation and utilizes a training set of images to learn the appearance of the target object. However, their evaluation did not include variability in object appearance due to scale and viewpoint, i.e. the object instances always appeared at the same size and viewpoints both in training and testing sets. Simply using SIFT features does not guarantee successful detection of objects appearing at different sizes. Furthermore, their evaluation involves placing the objects on a uniform background rather than real scenes such as in the current work. The Infomax model, on the other hand, can use any previously trained object detector and works on natural images, although they report results for face detection only and not for generic object detection. Critically, the Infomax’s foveated architecture is not based on physiology and uses non-biological rectangular pooling regions. We emphasize that any evaluation of the performance cost of a foveated visual system will depend critically on the parameters of the pooling regions with retinal eccentricity. Thus, an accurate assessment of the performance of a human/primate foveated visual system requires implementing a model which pooling regions are bio-inspired and based on physiological measurements.</p>
<p>Larochelle and Hinton [<xref ref-type="bibr" rid="pcbi.1005743.ref064">64</xref>], and Bazzani et al. [<xref ref-type="bibr" rid="pcbi.1005743.ref065">65</xref>] developed artificial neural network based models that have some sort of foveation. However, their application areas were different. Larochelle and Hinton [<xref ref-type="bibr" rid="pcbi.1005743.ref064">64</xref>] applied their model to image categorization and Bazzani et al. [<xref ref-type="bibr" rid="pcbi.1005743.ref065">65</xref>] applied their model to object tracking in videos.</p>
<p>Most importantly, none of these models have evaluated a biologically plausible foveated architecture relative to a high resolution scheme within a common theoretical framework to assess the potential performance loss of a system with a human foveated visual system and guided eye movements.</p>
</sec>
<sec id="sec019">
<title>Relation to current state of the art approaches in object detection</title>
<p>There has been substantial progress (e.g. [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref067">67</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref073">73</xref>]) in object detection research in recent years. However, humans, for now, are still unsurpassed in their ability to search for objects in visual scenes. The human brain relies on a variety of strategies. Object detection approaches have increasingly included some of the human strategies [<xref ref-type="bibr" rid="pcbi.1005743.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref074">74</xref>].</p>
<p>One remaining crucial difference between the human visual system and a modern object detector is that while humans process the visual field with decreasing resolution away [<xref ref-type="bibr" rid="pcbi.1005743.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref034">34</xref>] from the fixation point and make saccades to collect information, typical object detectors [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>] scan all locations at the same resolution and repeats this at multiple scales.</p>
<p>The sliding window (SW) method is the dominant model of search in object detection. Efficient alternatives to sliding windows can be categorized in two groups: 1. methods aimed at reducing the number of locations (<italic>m</italic>), 2. methods aimed at reducing the number of object categories (<italic>n</italic>). Since typically <italic>m</italic> &gt;&gt; <italic>n</italic>, there are a larger number efforts in trying to reduce <italic>m</italic>, however, reducing the contribution of the number of object classes has recently been receiving increasing interest as search for hundreds of thousands of object classes has started to be tackled [<xref ref-type="bibr" rid="pcbi.1005743.ref069">69</xref>]. According to this categorization, our proposed FOD method falls into the first group as it is designed to locate object instances by making a set of sequential fixations where in each fixation only a sparse set of locations are evaluated. Thus our proposed FOD scheme might provide an alternative bio-inspired method to other proposed methods to reduce the number of evaluated locations. There are number of previously proposed methods to reduce the number of locations to be evaluated. One line of research is the branch-and-bound methods [<xref ref-type="bibr" rid="pcbi.1005743.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref076">76</xref>] where an upper bound on the quality function of the detection model is used in a global branch and bound optimization scheme. Although the authors provide efficiently computable upper bounds for popular quality functions (e.g. linear template, bag-of-words, spatial pyramid), it might not be trivial to derive suitable upper bounds for a custom quality function. Our method, on the other hand, uses binary classification detection model and is agnostic to the quality function used.</p>
<p>Another line of research is the cascaded detection framework [<xref ref-type="bibr" rid="pcbi.1005743.ref077">77</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref079">79</xref>] where a series of cheap to expensive tests are done to locate the object. Cascaded detection is similar to our method in the sense that simple, coarse and cheap evaluations are used together with complex, fine and expensive evaluations. However, we differ with it in that it is essentially a sliding window method with a coarse-to-fine heuristic used to reduce the number of total evaluations. Another coarse-to-fine search scheme is presented in [<xref ref-type="bibr" rid="pcbi.1005743.ref080">80</xref>] where a set of low to high resolution templates are used. The method starts by evaluating the lowest resolution template—which is essentially a sliding window operation—and selecting the high responding locations for further processing with higher resolution templates. Our FOD method, too, uses a set of varying resolution templates; however, these templates are evaluated at every fixation instead of serializing their evaluations with respect to resolution.</p>
<p>In [<xref ref-type="bibr" rid="pcbi.1005743.ref047">47</xref>], a segmentation based method is proposed to yield a small set of locations that are likely to correspond to objects, which are subsequently used to guide the search in a selective manner. The locations are identified in an object class-independent way using an unsupervised multiscale segmentation approach. Thus, the method evaluates the same set of locations regardless of which object class is being searched for. In contrast, in our method, selection of locations to be foveated is guided by learned object class templates.</p>
<p>The method in [<xref ref-type="bibr" rid="pcbi.1005743.ref074">74</xref>], similar to ours, works like a fixational system: at a given time step, the location to be evaluated next is decided based on previous observations. However, there are important differences. In [<xref ref-type="bibr" rid="pcbi.1005743.ref074">74</xref>], only a single location is evaluated at a time step whereas we evaluate all template locations within the visual field at each fixation. Their method returns only one box as the result whereas our method is able to output many predictions.</p>
<p>Mathe et al. [<xref ref-type="bibr" rid="pcbi.1005743.ref081">81</xref>] proposed a search model that has foveation. However, this model does not have peripheral processing. The next fixation location is decided based on the history of foveal observations. A foveal observation corresponds to evaluating several regions in high (original) resolution (produced by a third-party segmentation algorithm) around the current fixation location.</p>
<p>Finally, in recent years, we have witnessed a surge of research in convolutional neural network (CNN) based object detection [<xref ref-type="bibr" rid="pcbi.1005743.ref070">70</xref>–<xref ref-type="bibr" rid="pcbi.1005743.ref073">73</xref>]. The new models have almost doubled the detection performance (34.1 mAP our result versus 59.9 mAP for Faster RCNN [<xref ref-type="bibr" rid="pcbi.1005743.ref070">70</xref>] on the same dataset). It is clear that the type of features we extract from images (i.e. HOG) limit FOD’s performance as indicated by the higher performance of neural network based models (e.g. Faster RCNN). The FOD’s performance would improve if CNN-features were used instead of HoGs (but note that it would not be neurobiologically consistent to pool CNN features which have been shown to compute features beyond V1 [<xref ref-type="bibr" rid="pcbi.1005743.ref082">82</xref>], using a V1 model [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>]. Nevertheless, it is important that future work evaluate the cost of a foveated system within the context of CNN framework to assess whether the findings in the current paper generalize to that approach.</p>
<p>Also, the computational cost of the new CNN models is much higher compared to the DPM-like (e.g. HOG+SVM) motivating even further the development of alternatives to sliding window method. Thus, a number of region proposal methods including Selective Search [<xref ref-type="bibr" rid="pcbi.1005743.ref047">47</xref>], edge boxes [<xref ref-type="bibr" rid="pcbi.1005743.ref083">83</xref>], region proposal networks [<xref ref-type="bibr" rid="pcbi.1005743.ref070">70</xref>] have been proposed (see Hosang et al.’s work [<xref ref-type="bibr" rid="pcbi.1005743.ref084">84</xref>] for a review). While it is true that region proposal methods greatly reduce the number of evaluation candidates, whether they are better than sliding window classifiers (in terms of accuracy and computational savings) is not a settled debate. State-of-the-art object detection (RCNN [<xref ref-type="bibr" rid="pcbi.1005743.ref085">85</xref>], Fast RCNN [<xref ref-type="bibr" rid="pcbi.1005743.ref086">86</xref>], Faster RCNN [<xref ref-type="bibr" rid="pcbi.1005743.ref070">70</xref>]) has abandoned region proposal methods. Faster RCNN, the best available object detector known to us, is not using a region proposal method to generate evaluation candidates. Instead, it uses a sliding window classifier which they call the “Region Proposal Network (RPN).” RPN slides a 3x3 window on the output of the topmost convolutional layer, evaluates 9 different hypothesis (3 scales, 3 aspect ratios) at each location, and outputs the best scoring 300 hypothesis as the object candidates. Theoretically, RPN itself could potentially be made faster by using a foveated method such as ours.</p>
</sec>
<sec id="sec020">
<title>Conclusion</title>
<p>To summarize, the findings show that a foveated architecture with guided eye movements can preserve both bottom-up saliency and top-down search for objects of a homogeneous high resolution system while incurring important computational cost savings. The findings might suggest a possible explanation for the evolution of a foveated visual system with eye movements as a possible solution that gives the organism similar ability to that of a non-foveated high resolution system but with decreased metabolic costs for the brain as well as reduced neural resource allocation.</p>
</sec>
</sec>
<sec id="sec021">
<title>Methods and models</title>
<sec id="sec022">
<title>Foveated visual field</title>
<p>The Freeman-Simoncelli (FS) model [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>] is a neuronal population model of V1 and V2 layers of the visual cortex. The model specifies how responses are pooled (averaged together) hierarchically beginning from the lateral geniculate nucleus to V1 and then the V2 layer. V1 cells encode information about local orientation and spatial frequency whereas the cells in V2 pools V1 responses non-linearly to achieve selectivity for compound features such as corners and junctions. The model is based on findings and physiological measurements of the primate visual cortex and specifies the shapes and sizes of the receptive fields of the cells in V1 and V2. According to the model, the sizes of receptive fields increase linearly as a function of the distance from the fovea and this rate of increase in V2 is larger than that of V1, which means V2 pools larger areas of the visual field in the periphery. The reader is referred to [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>] for further details.</p>
<p>We simplify the FS model in two ways. First, the model uses a Gabor filter bank to compute image features and we replace these with the HOG features [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref041">41</xref>]. Second, we only use the V1 layer and leave the non-linear pooling at V2 as future work. We use this simplified FS model as the foveated visual field of our object detector which is shown in <xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref>. The fovea subtends a radius of 2 degrees. We also only simulate a visual field with a radius of 10 degrees which is sufficient to cover the test images presented at a typical viewing distance of 40 cm. The square boxes with white borders (<xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref> represent the pooling regions within the fovea. The surrounding colored regions are the peripheral pooling regions. While the foveal regions have equal sizes, the peripheral regions grow in size as a function—which is specified by the FS model—of their distance to the center of the fovea. The color represents the weights that are used in pooling, i.e. weighted summation of, the underlying responses. A pooling region partly overlaps with its neighboring pooling regions (see the supplementary material of [<xref ref-type="bibr" rid="pcbi.1005743.ref039">39</xref>] for details). Specifically, (i) spatial weights of the pooling regions, (ii) locations of pooling regions, and (iii) the number of angle and eccentricity bins, and (iv) the scaling factor of the pooling regions with eccentricity in our FOD model are all directly based on the FS model’s V1 layer. Assuming a viewing distance of 40cm, the whole visual field covers about a 500x500 pixel area (a pixel subtends 0.08°). The foveal radius is 52 pixels subtending a visual angle of 4 degrees.</p>
<sec id="sec023">
<title>Feature pooling</title>
<p>First, HoG features are extract from the input image (see <xref ref-type="fig" rid="pcbi.1005743.g002">Fig 2</xref>). Then, we center the visual field around the current fixation point. At the fovea, where the pooling regions are 8x8 pixels, we directly use the HoG features, and in the periphery, each pooling region takes a weighted sum of HoG features of the 8x8 regions that are covered by that pooling region.</p>
</sec>
</sec>
<sec id="sec024">
<title>The foveated object detector (FOD)</title>
<p>The model <italic>M</italic> consists of the application of <italic>n</italic> retino-specific, linear templates (i.e. classifiers) corresponding to different object viewpoints and resolutions. Thus, the model has <italic>n</italic> components, each of which consists of a linear template and its specific location vector:
<disp-formula id="pcbi.1005743.e001"><alternatives><graphic id="pcbi.1005743.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Model</mml:mtext> <mml:mspace width="0.277778em"/><mml:mspace width="0.166667em"/><mml:msup><mml:mi mathvariant="italic">M</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi mathvariant="normal">s</mml:mi> <mml:mspace width="0.277778em"/><mml:mspace width="0.166667em"/><mml:mtext>parameters</mml:mtext> <mml:mo>:</mml:mo> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">ℓ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>:</mml:mo> <mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <bold>w</bold><italic><sub>i</sub></italic> is a linear template and <bold><italic>ℓ</italic></bold><sub><italic>i</italic></sub> is the location of the template with respect to the center of the visual field. Among these parameters, <bold>w</bold><italic><sub>i</sub></italic> are learnable (given a dataset) but <bold><italic>ℓ</italic></bold><sub><italic>i</italic></sub> are fixed (more on this in §“Initialization”) given the visual field parameters. The output of <italic>M</italic> given a fixation point <italic>f</italic> is an array of detection scores produced by the <italic>n</italic> retino-specific classifiers, corresponding to different locations within the image.</p>
<p>The location variable <bold><italic>ℓ</italic></bold><sub><italic>i</italic></sub> defines a unique bounding box within the visual field for the <italic>i</italic><sup><italic>th</italic></sup> template. Specifically, <bold><italic>ℓ</italic></bold><sub><italic>i</italic></sub> = (<italic>ω</italic><sub><italic>i</italic></sub>, <italic>h</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) is a vector whose variables respectively denote width, height and <italic>x</italic>, <italic>y</italic> coordinates of the <italic>i</italic><sup><italic>th</italic></sup> template within the visual field. The template, <bold>w</bold><italic><sub>i</sub></italic>, is a matrix of weights on the features extracted from the pooling regions underlying the bounding box <bold><italic>ℓ</italic></bold><sub><italic>i</italic></sub>. The dimensionality of <bold>w</bold><italic><sub>i</sub></italic>, i.e. the total number of weights, depends both on the width and height of its bounding box and its location in the visual field. A component within the fovea covers a larger number of pooling regions compared to a peripheral component with the same width and height, hence the dimensionality of a foveal template is larger. Three example components are illustrated in <xref ref-type="fig" rid="pcbi.1005743.g010">Fig 10</xref> where the foveal component (red) covers 7x5 = 35 pooling regions while the (blue and green) peripheral components cover 15 and 2 regions, respectively. Since a fixed number of features is extracted from each pooling region (regardless of its size), foveal components have higher-resolution templates associated with them. We use the feature extraction implementation of DPM (rel5) [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005743.ref087">87</xref>].</p>
<fig id="pcbi.1005743.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Illustration of the visual field of the model.</title>
<p>(a) The model is fixating at the red cross mark on the image (see <xref ref-type="fig" rid="pcbi.1005743.g008">Fig 8</xref>’s caption for the source of the image). (b) Visual field (<xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref>) overlaid on the image, centered at the fixation location. White line delineate the borders of pooling regions. Nearby pooling regions do overlap. The weights (<xref ref-type="fig" rid="pcbi.1005743.g001">Fig 1</xref>) of a pooling region sharply decrease outside of its shown borders. White borders are actually iso-weight contours for neighboring regions. Colored bounding boxes show the templates of three components on the visual field: red, a template within the fovea; blue and green, two peripheral templates at 2.8 and 7 degree periphery, respectively. (c, d, e) Zoomed in versions of the red (foveal), blue (peripheral) and green (peripheral) templates. The weights of a template, <bold>w</bold><italic><sub>i</sub></italic>, are defined on the gray shaded pooling regions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g010" xlink:type="simple"/>
</fig>
<sec id="sec025">
<title>Detection model</title>
<p>Suppose that we are given a model <italic>M</italic> that is already trained for a certain object class. The model is presented with an image <italic>I</italic> and assigned an initial fixation location <bold><italic>f</italic></bold>. We are interested in searching for an object instance in <italic>I</italic>. Because the size of a searched object is not known apriori, the model has to analyze the input image at various scales (the image is scaled up and down at several levels, and the whole feature extraction and template evaluation process are repeated per scale). It would be more desirable to utilize different size templates rather than scaling the input image. The reason we chose to scale the image to calculate the various templates is to equate this aspect of the FOD to the available high resolution SW model, i.e. the Deformable Parts Model (DPM) [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]. If we had chosen to train the FOD by using different template size, then the performance comparison against the existing SW model would have been a function of both foveation and any difference in object detection classifiers. This would have hindered the assessment of the isolated effect of foveation. While not perfect, it was the best solution to achieve the goals of the paper.</p>
<p>We use the same set of image scales given in [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>] and use <italic>σ</italic> to denote a scale from that set. When used as a subscript to an image, e.g. <italic>I</italic><sub><italic>σ</italic></sub>, it denotes the scaled version of that image, i.e. width (and height) of <italic>I</italic><sub><italic>σ</italic></sub> is <italic>σ</italic> times the width (and height) of <italic>I</italic>. <italic>σ</italic> also applies to fixation locations and bounding boxes: if <bold><italic>f</italic></bold> denotes a fixation location (<italic>f</italic><sub><italic>x</italic></sub>, <italic>f</italic><sub><italic>y</italic></sub>), then <bold><italic>f</italic></bold><sub><italic>σ</italic></sub> = (<italic>σf</italic><sub><italic>x</italic></sub>, <italic>σf</italic><sub><italic>y</italic></sub>) (i.e. <bold><italic>f</italic></bold><sub><italic>σ</italic></sub> is a vector containing two elements: <italic>σ</italic> times <italic>f</italic><sub><italic>x</italic></sub> and <italic>σ</italic> times <italic>f</italic><sub><italic>y</italic></sub>); for a bounding box <bold><italic>b</italic></bold> = (<italic>w</italic>, <italic>h</italic>, <italic>x</italic>, <italic>y</italic>), <bold><italic>b</italic></bold><sub><italic>σ</italic></sub> = (<italic>σw</italic>, <italic>σh</italic>, <italic>σx</italic>, <italic>σy</italic>).</p>
<p>To check whether an arbitrary bounding box <bold><italic>b</italic></bold> within <italic>I</italic> contains an object instance, while the model is fixating at location f, we compute a detection score as
<disp-formula id="pcbi.1005743.e002"><alternatives><graphic id="pcbi.1005743.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mtext>max</mml:mtext> <mml:mi>σ</mml:mi></mml:munder> <mml:munder><mml:mtext>max</mml:mtext> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>G</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:mspace width="0.277778em"/><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where Ψ(<italic>I</italic><sub><italic>σ</italic></sub>, <bold><italic>f</italic></bold><sub><italic>σ</italic></sub>, <italic>c</italic>) is a feature extraction function which returns the features of <italic>I</italic><sub><italic>σ</italic></sub> for component <italic>c</italic> (see <xref ref-type="disp-formula" rid="pcbi.1005743.e001">Eq (1)</xref>) when the model is fixating at <bold><italic>f</italic></bold><sub><italic>σ</italic></sub>. The vector <bold>w</bold> is the blockwise concatenation of the templates of all components. Ψ(⋅) effectively chooses which component to use, that is <inline-formula id="pcbi.1005743.e003"><alternatives><graphic id="pcbi.1005743.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Ψ</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mi>c</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>Ψ</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The fixation location, <bold><italic>f</italic></bold><sub><italic>σ</italic></sub>, together with the component <italic>c</italic> define a unique location, i.e. a bounding box, on <italic>I</italic><sub><italic>σ</italic></sub>. <italic>G</italic>(<bold><italic>b</italic></bold><sub><italic>σ</italic></sub>, <bold><italic>f</italic></bold><sub><italic>σ</italic></sub>) returns the set of all components whose templates have a predetermined overlap (intersection over union should be at least 0.7 as in [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]) with <bold><italic>b</italic></bold><sub><italic>σ</italic></sub> when the model is fixating at <bold><italic>f</italic></bold><sub><italic>σ</italic></sub>. During both training and testing, <italic>σ</italic> and <italic>c</italic> are latent variables for example (<italic>I</italic>, <bold><italic>b</italic></bold>).</p>
<p>Ideally, <italic>s</italic>(<italic>I</italic>, <bold><italic>b</italic></bold>, <bold><italic>f</italic></bold>) &gt; 0 should hold for an appropriate <bold><italic>f</italic></bold> when <italic>I</italic> contains an object instance within <bold><italic>b</italic></bold>. For an image that does not contain an object instance, <italic>s</italic>(<italic>I</italic>, <bold><italic>b</italic></bold> = ∅, <bold><italic>f</italic></bold>)&lt;0 should hold for any <bold><italic>f</italic></bold>. For this to work, a subtlety in <italic>G</italic>(⋅)’s definition is needed: <italic>G</italic>(∅, <bold><italic>f</italic></bold>) returns all components of the model (<xref ref-type="disp-formula" rid="pcbi.1005743.e001">Eq (1)</xref>). During training, this will enforce the responses of all components for a negative image to be suppressed down.</p>
</sec>
<sec id="sec026">
<title>Integrating observations across multiple fixations</title>
<p>So far, we have looked at the situation where the model has made only one fixation. We describe in Section Eye movement strategy how the model chooses the next fixation location. For now, suppose that the model has made <italic>m</italic> fixations, <bold><italic>f</italic></bold><sub>1</sub>, <bold><italic>f</italic></bold><sub>2</sub>, …, <bold><italic>f</italic></bold><sub><italic>m</italic></sub>, and we want to find out whether an arbitrary bounding box <bold><italic>b</italic></bold> contains an object instance. This computation involves integrating observations across multiple fixations, which is a considerably more complicated problem than the single fixation case. The Bayesian decision on whether <bold><italic>b</italic></bold> contains an object instance is based on the comparison of posterior probabilities:
<disp-formula id="pcbi.1005743.e004"><alternatives><graphic id="pcbi.1005743.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msubsup><mml:mspace width="4pt"/><mml:mo>&gt;</mml:mo><mml:mo>&lt;</mml:mo></mml:msubsup><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>y</italic><sub><bold><italic>b</italic></bold></sub> = 1 denotes the event that there is an object instance at location <bold><italic>b</italic></bold>. We use the posteriors’ ratio as a detection score, the higher it is the more likely <bold><italic>b</italic></bold> contains an instance. Computing the probabilities in (<xref ref-type="disp-formula" rid="pcbi.1005743.e004">3</xref>) requires training a classifier per combination of fixation locations for each different value of <bold><italic>m</italic></bold>, which is intractable. We approximate it using a conditional independence assumption (for the derivation, see Part C in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref>): 
<disp-formula id="pcbi.1005743.e005"><alternatives><graphic id="pcbi.1005743.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>≈</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>We model the probability <italic>P</italic>(<italic>y</italic><sub><bold><italic>b</italic></bold></sub> = 1|<italic><bold>f</bold></italic>, <italic>I</italic>) using a classifier and use the sigmoid transfer function to convert raw classification scores to probabilities:
<disp-formula id="pcbi.1005743.e006"><alternatives><graphic id="pcbi.1005743.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>s</mml:mi> <mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>We simplify the computation in (<xref ref-type="disp-formula" rid="pcbi.1005743.e005">4</xref>) by taking the log (for the derivation, see Part D in <xref ref-type="supplementary-material" rid="pcbi.1005743.s001">S1 Text</xref>):
<disp-formula id="pcbi.1005743.e007"><alternatives><graphic id="pcbi.1005743.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>log</mml:mtext> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>Taking the logarithm of posterior ratios does not alter the ranking of detection scores for different locations, i.e. <bold><italic>b</italic></bold>’s, because logarithm is a monotonic function. In short, the detection score computed by the FOD for a certain location <bold><italic>b</italic></bold>, is the sum of the individual scores for <bold><italic>b</italic></bold> computed at each fixation.</p>
<p>After evaluating (<xref ref-type="disp-formula" rid="pcbi.1005743.e007">6</xref>) for a set of candidate locations, final bounding box predictions are obtained by non-maxima suppression [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>], i.e. given multiple predictions for a certain location, all predictions except the one with the maximal score are discarded.</p>
</sec>
</sec>
<sec id="sec027">
<title>Eye movement strategy</title>
<p>We use the maximum-a-posteriori (MAP) model [<xref ref-type="bibr" rid="pcbi.1005743.ref043">43</xref>] with inhibition of return (see next subsection) as the basic eye movement strategy of the FOD. The MAP model selects the location with the highest posterior probability of containing the target object as the next fixation location, that is <bold><italic>f</italic></bold><sub><italic>i</italic>+1</sub> = center of <bold><italic>ℓ</italic></bold>* where
<disp-formula id="pcbi.1005743.e008"><alternatives><graphic id="pcbi.1005743.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">ℓ</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:mspace width="0.166667em"/><mml:mo form="prefix">max</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:munder> <mml:mspace width="0.277778em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>This search is done over uninhibited locations only. Finding the maximum of the posterior above is equivalent to finding the maximum of the posterior ratios, 
<disp-formula id="pcbi.1005743.e009"><alternatives><graphic id="pcbi.1005743.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:mspace width="0.166667em"/><mml:mo form="prefix">max</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:munder> <mml:mspace width="0.277778em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:mspace width="0.166667em"/><mml:mo form="prefix">max</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:munder> <mml:mspace width="0.277778em"/><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
since for two arbitrary locations <bold><italic>ℓ</italic></bold><sub>1</sub>, <bold><italic>ℓ</italic></bold><sub>2</sub>; let <italic>p</italic><sub>1</sub> = <italic>P</italic>(<italic>y</italic><sub><bold><italic>ℓ</italic></bold><sub>1</sub></sub> = 1|⋅) and <italic>p</italic><sub>2</sub> = <italic>P</italic>(<italic>y</italic><sub><bold><italic>ℓ</italic></bold><sub>2</sub></sub> = 1|⋅), then we have
<disp-formula id="pcbi.1005743.e010"><alternatives><graphic id="pcbi.1005743.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>&gt;</mml:mo> <mml:mfrac><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>⇒</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula></p>
</sec>
<sec id="sec028">
<title>Inhibition of return</title>
<p>After each fixation, a circular area with approximately 2 degree radius around the fixation location is inhibited. The model is not allowed to fixate to a previously inhibited location.</p>
</sec>
<sec id="sec029">
<title>Training the model</title>
<sec id="sec030">
<title>Initialization</title>
<p>A set of dimensions (width and height) is determined from the bounding box statistics of the examples in the training set as done in the initialization of the DPM model [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]. Then, for each width and height, new components with these dimensions are created to tile the entire visual field. However, the density of components in the visual field is not uniform. Locations, i.e. bounding boxes, that do not overlap well with the underlying pooling regions are discarded. To define goodness of overlap, a bounding box is said to intersect with an underlying pooling region if more than one fifth of that region is covered by the bounding box. Overlap is the average coverage across the intersected regions. If the overlap is more than 75%, then a component for that location is created, otherwise the location is discarded (see <xref ref-type="fig" rid="pcbi.1005743.g011">Fig 11</xref> for an example). In addition, no components are created for locations that are outside of the visual field. Weights of the component templates (<bold>w</bold><italic><sub>i</sub></italic>) are initialized to arbitrary values. Training the model is essentially optimizing these weights on a given dataset.</p>
<fig id="pcbi.1005743.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005743.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Two bounding boxes (A, B) are shown on the visual field.</title>
<p>While box A covers a large portion of the pooling regions that it intersects with, box B’s coverage is not as good. Box B is discarded as it does not meet the overlap criteria (see text), therefore a component for B in the model is not created.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec031">
<title>Training</title>
<p>Consider a training set <inline-formula id="pcbi.1005743.e011"><alternatives><graphic id="pcbi.1005743.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> where <italic>I</italic><sub><italic>i</italic></sub> is an image and <bold><italic>b</italic></bold><sub><italic>i</italic></sub> a bounding box and <italic>K</italic> is the total number of examples. If <italic>I</italic><sub><italic>i</italic></sub> does not contain any positive examples, i.e. object instances, then <bold><italic>b</italic></bold><sub><italic>i</italic></sub> = ∅. Following the DPM model [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>], we train model templates using a latent-SVM formulation:
<disp-formula id="pcbi.1005743.e012"><alternatives><graphic id="pcbi.1005743.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:mo form="prefix" movablelimits="true">min</mml:mo></mml:mrow> <mml:mi mathvariant="bold">w</mml:mi></mml:munder> <mml:mspace width="0.277778em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msubsup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>C</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:mtext>max</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>y</italic><sub><italic>i</italic></sub> = 1 if <bold><italic>b</italic></bold><sub><italic>i</italic></sub> ≠ ∅ and <italic>y</italic><sub><italic>i</italic></sub> = −1, otherwise. The set <bold><italic>f</italic></bold>(<italic>I</italic><sub><italic>i</italic></sub>, <bold><italic>b</italic></bold><sub><italic>i</italic></sub>) denotes the set of all <italic>feasible</italic> fixation locations for example (<italic>I</italic><sub><italic>i</italic></sub>, <bold><italic>b</italic></bold><sub><italic>i</italic></sub>). For <bold><italic>b</italic></bold><sub><italic>i</italic></sub> ≠ ∅, a fixation location is considered feasible if there exists a model component whose bounding box overlaps with <bold><italic>b</italic></bold><sub><italic>i</italic></sub>. For <bold><italic>b</italic></bold><sub><italic>i</italic></sub> = ∅, all possible fixation locations on <italic>I</italic><sub><italic>i</italic></sub> are considered feasible.</p>
<p>Optimizing the cost function in (<xref ref-type="disp-formula" rid="pcbi.1005743.e012">10</xref>) is manageable for mixtures with few components, however, the FOD has a large number of components in its visual field (typically, for an object class in the PASCAL VOC 2007 dataset [<xref ref-type="bibr" rid="pcbi.1005743.ref040">40</xref>], there are around 500–700) and optimizing this cost function becomes prohibitive in terms of computational cost. As an alternative, cheaper linear classifiers can be used. Recently, linear discriminant analysis (LDA) has been used in object detection ([<xref ref-type="bibr" rid="pcbi.1005743.ref042">42</xref>]) producing surprisingly good results with much faster training time. Training a LDA classifier amounts to computing Σ<sup>−1</sup>(<italic>μ</italic><sub>1</sub>−<italic>μ</italic><sub>0</sub>) where <italic>μ</italic><sub>1</sub> is the mean of the feature vectors of the positive examples, <italic>μ</italic><sub>0</sub> is the same for the negative examples and Σ is the covariance matrix of these features. Here, the most expensive computation is the estimation of Σ, which is required for each template with different dimensions. However, it is possible to estimate a global Σ from which covariance matrices for templates of different dimensions can be obtained [<xref ref-type="bibr" rid="pcbi.1005743.ref042">42</xref>]. For the FOD, we estimate the covariance matrices for the foveal templates and estimate the covariance matrices for peripheral templates by applying the feature pooling transformations to the foveal covariance matrices.</p>
<p>We propose to use LDA in a latent-SVM-like framework as an alternative to the method in [<xref ref-type="bibr" rid="pcbi.1005743.ref042">42</xref>] where positive examples are clustered first and then a LDA classifier is trained per cluster. Consider the <italic>t</italic><sup><italic>th</italic></sup> template, <bold>w</bold><italic><sub>t</sub></italic>. LDA gives us that LDA gives us that <inline-formula id="pcbi.1005743.e013"><alternatives><graphic id="pcbi.1005743.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>LDA</mml:mtext></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>pos</mml:mtext></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>neg</mml:mtext></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where Σ<sub><italic>t</italic></sub> is the covariance matrix for template <italic>t</italic>, <inline-formula id="pcbi.1005743.e014"><alternatives><graphic id="pcbi.1005743.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>pos</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005743.e015"><alternatives><graphic id="pcbi.1005743.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>neg</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> are the mean of positive and negative feature vectors, respectively, assigned to template <italic>t</italic>. We propose to apply an affine transformation to the LDA classifier:
<disp-formula id="pcbi.1005743.e016"><alternatives><graphic id="pcbi.1005743.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>LDA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>LDA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
and modify the cost function as 
<disp-formula id="pcbi.1005743.e017"><alternatives><graphic id="pcbi.1005743.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005743.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="4pt"/><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle> <mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mo>∅</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where the first summation pushes the score of the mean of the negative examples to under zero and the second summation, taken over positive examples only, pushes the scores to above 0. <bold><italic>α</italic></bold> and <bold><italic>β</italic></bold> are appropriate blockwise concatenation of <italic>α</italic><sub><italic>t</italic></sub> and <italic>β</italic><sub><italic>t</italic></sub>s. <italic>C</italic> is the regularization constant. Overall, this optimization effectively calibrates the dynamic ranges of different templates’ responses in the model so that the scores of positive examples and negative means are pushed away from each other while the norm of <bold>w</bold> is constraint to prevent overfitting. This formulation does not require the costly mining of hard-negative examples of latent-SVM. We call this formulation (<xref ref-type="disp-formula" rid="pcbi.1005743.e017">Eq 12</xref>) as latent-LDA.</p>
<p>To optimize (<xref ref-type="disp-formula" rid="pcbi.1005743.e017">12</xref>), we use the classical coordinate-descent procedure. We start by initializing <bold>w</bold> by training on warped-positive examples as in [<xref ref-type="bibr" rid="pcbi.1005743.ref028">28</xref>]. Then, we alternate between choosing the best values for the latent variables while keeping <bold>w</bold> fixed, and optimizing for <bold>w</bold> while keeping the latent variables of positive examples fixed.</p>
</sec>
</sec>
</sec>
<sec id="sec032">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005743.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005743.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supporting information for various sections.</title>
<p>S1 Text contains a sample recall-precision curve, comparison of sliding-window based methods, derivations for Eqs (<xref ref-type="disp-formula" rid="pcbi.1005743.e005">4</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005743.e007">6</xref>), details of the foveated saliency model and comments on the effects of inhibition-of-return on performance.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005743.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Land</surname> <given-names>MF</given-names></name>. <chapter-title>Oculomotor behaviour in vertebrates and invertebrates</chapter-title>. In: <name name-style="western"><surname>Liversedge</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Gilchrist</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Everling</surname> <given-names>S</given-names></name>, editors. <source>The Oxford Handbook of Eye Movements</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2011</year>. p. <fpage>3</fpage>–<lpage>16</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marshall</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Land</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Cronin</surname> <given-names>TW</given-names></name>. <article-title>Shrimps that pay attention: saccadic eye movements in stomatopod crustaceans</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2014</year>;<volume>369</volume> (<issue>1636</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2013.0042" xlink:type="simple">10.1098/rstb.2013.0042</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Curcio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sloan</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Kalina</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Hendrickson</surname> <given-names>AE</given-names></name>. <article-title>Human photoreceptor topography</article-title>. <source>The Journal of Comparative Neurology</source>. <year>1990</year>;<volume>292</volume>(<issue>4</issue>):<fpage>497</fpage>–<lpage>523</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.902920402" xlink:type="simple">10.1002/cne.902920402</ext-link></comment> <object-id pub-id-type="pmid">2324310</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Azzopardi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cowey</surname> <given-names>A</given-names></name>. <article-title>Preferential representation of the fovea in the primary visual cortex</article-title>. <source>Nature</source>. <year>1993</year>;<volume>361</volume>:<fpage>719</fpage>–<lpage>721</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/361719a0" xlink:type="simple">10.1038/361719a0</ext-link></comment> <object-id pub-id-type="pmid">7680108</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Computational modelling of visual attention</article-title>. <source>Nature reviews neuroscience</source>. <year>2001</year>;<volume>2</volume>(<issue>3</issue>):<fpage>194</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35058500" xlink:type="simple">10.1038/35058500</ext-link></comment> <object-id pub-id-type="pmid">11256080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>ZP</given-names></name>. <article-title>A saliency map in primary visual cortex</article-title>. <source>Trends in cognitive sciences</source>. <year>2002</year>;<volume>6</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(00)01817-9" xlink:type="simple">10.1016/S1364-6613(00)01817-9</ext-link></comment> <object-id pub-id-type="pmid">11849610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bruce</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name>. <article-title>Saliency, attention, and visual search: An information theoretic approach</article-title>. <source>Journal of vision</source>. <year>2009</year>;<volume>9</volume>(<issue>3</issue>):<fpage>5</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/9.3.5" xlink:type="simple">10.1167/9.3.5</ext-link></comment> <object-id pub-id-type="pmid">19757944</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Beutter</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Pham</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Shimozaki</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Stone</surname> <given-names>LS</given-names></name>. <article-title>Similar Neural Representations of the Target for Saccades and Perception during Search</article-title>. <source>The Journal of Neuroscience</source>. <year>2007</year>;<volume>27</volume>(<issue>6</issue>):<fpage>1266</fpage>–<lpage>1270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3975-06.2007" xlink:type="simple">10.1523/JNEUROSCI.3975-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17287501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Findlay</surname> <given-names>JM</given-names></name>. <article-title>Saccade Target Selection During Visual Search</article-title>. <source>Vision Research</source>. <year>1997</year>;<volume>37</volume>(<issue>5</issue>):<fpage>617</fpage>–<lpage>631</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(96)00218-0" xlink:type="simple">10.1016/S0042-6989(96)00218-0</ext-link></comment> <object-id pub-id-type="pmid">9156206</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref010">
<label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Findlay</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Gilchrist</surname> <given-names>ID</given-names></name>. <source>Active Vision: The Psychology of Looking and Seeing</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2003</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198524793.001.0001/acprof-9780198524793" xlink:type="simple">http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198524793.001.0001/acprof-9780198524793</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Malcolm</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Henderson</surname> <given-names>JM</given-names></name>. <article-title>The effects of target template specificity on visual search in real-world scenes: Evidence from eye movements</article-title>. <source>Journal of Vision</source>. <year>2009</year>;<volume>9</volume>(<issue>11</issue>):<fpage>8</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/9.11.8" xlink:type="simple">10.1167/9.11.8</ext-link></comment> <object-id pub-id-type="pmid">20053071</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Castelhano</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Henderson</surname> <given-names>JM</given-names></name>. <article-title>Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search</article-title>. <source>Psychological Review</source>. <year>2006</year>;<volume>113</volume>(<issue>4</issue>):<fpage>766</fpage>–<lpage>786</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.113.4.766" xlink:type="simple">10.1037/0033-295X.113.4.766</ext-link></comment> <object-id pub-id-type="pmid">17014302</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neider</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Zelinsky</surname> <given-names>GJ</given-names></name>. <article-title>Scene context guides eye movements during visual search</article-title>. <source>Vision research</source>. <year>2006</year>;<volume>46</volume>(<issue>5</issue>):<fpage>614</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2005.08.025" xlink:type="simple">10.1016/j.visres.2005.08.025</ext-link></comment> <object-id pub-id-type="pmid">16236336</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Drescher</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Shimozaki</surname> <given-names>SS</given-names></name>. <article-title>Attentional cues in real scenes, saccadic targeting, and Bayesian priors</article-title>. <source>Psychological science</source>. <year>2006</year>;<volume>17</volume>(<issue>11</issue>):<fpage>973</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9280.2006.01815.x" xlink:type="simple">10.1111/j.1467-9280.2006.01815.x</ext-link></comment> <object-id pub-id-type="pmid">17176430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mack</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Object co-occurrence serves as a contextual cue to guide and facilitate visual search in a natural viewing environment</article-title>. <source>Journal of vision</source>. <year>2011</year>;<volume>11</volume>(<issue>9</issue>):<fpage>1</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/11.9.9" xlink:type="simple">10.1167/11.9.9</ext-link></comment> <object-id pub-id-type="pmid">21856869</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Preston</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Das</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Giesbrecht</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Neural representations of contextual guidance in visual search of real-world scenes</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>18</issue>):<fpage>7846</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5840-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5840-12.2013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Najemnik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Optimal eye movement strategies in visual search</article-title>. <source>Nature</source>. <year>2005</year>;<volume>434</volume>:<fpage>387</fpage>–<lpage>391</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03390" xlink:type="simple">10.1038/nature03390</ext-link></comment> <object-id pub-id-type="pmid">15772663</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ludwig</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Davies</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Foveal analysis and peripheral selection during active visual sampling</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>2</issue>):<fpage>E291</fpage>–<lpage>E299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1313553111" xlink:type="simple">10.1073/pnas.1313553111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Prince</surname> <given-names>SJD</given-names></name>, <name name-style="western"><surname>Hou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sizintsev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Olevskiy</surname> <given-names>E</given-names></name>. <article-title>Pre-Attentive and Attentive Detection of Humans in Wide-Field Scenes</article-title>. <source>International Journal of Computer Vision</source>. <year>2007</year>;<volume>72</volume>(<issue>1</issue>):<fpage>47</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11263-006-8892-7" xlink:type="simple">10.1007/s11263-006-8892-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref020">
<label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Elder</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dornaika</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Goldstein</surname> <given-names>R</given-names></name>. <chapter-title>Attentive wide-field sensing for visual telepresence and surveillance</chapter-title>. In: <source>Neurobiology of Attention</source>. <publisher-name>Academic Press</publisher-name>; <year>2005</year>. p. <fpage>624</fpage>–<lpage>633</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Prince SJD, Elder JH, Hou Y, Sizintsev M, Olevskiy Y. Statistical cue integration for foveated wide-field surveillance. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05). vol. 2; 2005. p. 603–610 vol. 2.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Prince SJD, Elder JH, Hou Y, Sizinstev M. Pre-Attentive Face Detection for Foveated Wide-Field Surveillance. In: Application of Computer Vision, 2005. WACV/MOTIONS’05 Volume 1. Seventh IEEE Workshops on. vol. 1; 2005. p. 439–446.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamamoto</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yeshurun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Levine</surname> <given-names>MD</given-names></name>. <article-title>An Active Foveated Vision System: Attentional Mechanisms and Scan Path Covergence Measures</article-title>. <source>Computer Vision and Image Understanding</source>. <year>1996</year>;<volume>63</volume>(<issue>1</issue>):<fpage>50</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/cviu.1996.0004" xlink:type="simple">10.1006/cviu.1996.0004</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Evolution and Optimality of Similar Neural Mechanisms for Perception and Action during Search</article-title>. <source>PLoS Computational Biology</source>. <year>2010</year>;<volume>6</volume>(<issue>9</issue>):<fpage>e1000930</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000930" xlink:type="simple">10.1371/journal.pcbi.1000930</ext-link></comment> <object-id pub-id-type="pmid">20838589</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morvan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Human visual search does not maximize the post-saccadic probability of identifying targets</article-title>. <source>PLoS computational biology</source>. <year>2012</year>;<volume>8</volume>(<issue>2</issue>):<fpage>e1002342</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002342" xlink:type="simple">10.1371/journal.pcbi.1002342</ext-link></comment> <object-id pub-id-type="pmid">22319428</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zelinsky</surname> <given-names>GJ</given-names></name>. <article-title>A theory of eye movements during target acquisition</article-title>. <source>Psychological Review</source>. <year>2008</year>;<volume>115</volume>:<fpage>787</fpage>–<lpage>835</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0013118" xlink:type="simple">10.1037/a0013118</ext-link></comment> <object-id pub-id-type="pmid">18954205</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Butko</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Movellan</surname> <given-names>JR</given-names></name>. <article-title>Infomax Control of Eye Movements</article-title>. <source>IEEE Trans on Auton Ment Dev</source>. <year>2010</year>;<volume>2</volume>(<issue>2</issue>):<fpage>91</fpage>–<lpage>107</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TAMD.2010.2051029" xlink:type="simple">10.1109/TAMD.2010.2051029</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Felzenszwalb</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Girshick</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>McAllester</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ramanan</surname> <given-names>D</given-names></name>. <article-title>Object Detection with Discriminatively Trained Part Based Models</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2010</year>;<volume>32</volume>(<issue>9</issue>):<fpage>1627</fpage>–<lpage>1645</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2009.167" xlink:type="simple">10.1109/TPAMI.2009.167</ext-link></comment> <object-id pub-id-type="pmid">20634557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref029">
<label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Malisiewicz</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Efros</surname> <given-names>AA</given-names></name>. <source>Ensemble of Exemplar-SVMs for Object Detection and Beyond</source>. In: <publisher-name>ICCV</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhu L, Chen Y, Yuille A, Freeman W. Latent Hierarchical Structural Learning for Object Detection. In: Conference on Computer Vision and Pattern Recognition; 2010.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wertheim</surname> <given-names>T</given-names></name>. <article-title>Über die indirekte Sehschärfe</article-title>. <source>Zeitschrift für Psychologie und Physiologie der Sinnesorgane</source>. <year>1894</year>;<volume>7</volume>:<fpage>172</fpage>–<lpage>187</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Aitsebaomo</surname> <given-names>AP</given-names></name>. <article-title>Vernier acuity, crowding and cortical magnification</article-title>. <source>Vision Research</source>. <year>1985</year>;<volume>25</volume>(<issue>7</issue>):<fpage>963</fpage>–<lpage>977</lpage>. <object-id pub-id-type="pmid">4049746</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rovamo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Leinonen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Laurinen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Virsu</surname> <given-names>V</given-names></name>. <article-title>Temporal integration and contrast sensitivity in foveal and peripheral vision</article-title>. <source>Perception</source>. <year>1984</year>;<volume>13</volume>(<issue>6</issue>):<fpage>665</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1068/p130665" xlink:type="simple">10.1068/p130665</ext-link></comment> <object-id pub-id-type="pmid">6543946</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Strasburger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rentschler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Jüttner</surname> <given-names>M</given-names></name>. <article-title>Peripheral vision and pattern recognition: a review</article-title>. <source>Journal of vision</source>. <year>2011</year>;<volume>11</volume>(<issue>5</issue>):<fpage>13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/11.5.13" xlink:type="simple">10.1167/11.5.13</ext-link></comment> <object-id pub-id-type="pmid">22207654</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Curcio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sloan</surname> <given-names>KR</given-names></name>. <article-title>Packing geometry of human cone photoreceptors: variation with eccentricity and evidence for local anisotropy</article-title>. <source>Visual neuroscience</source>. <year>1992</year>;<volume>9</volume>(<issue>02</issue>):<fpage>169</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0952523800009639" xlink:type="simple">10.1017/S0952523800009639</ext-link></comment> <object-id pub-id-type="pmid">1504026</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Curcio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Allen</surname> <given-names>KA</given-names></name>. <article-title>Topography of ganglion cells in human retina</article-title>. <source>Journal of comparative Neurology</source>. <year>1990</year>;<volume>300</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.903000103" xlink:type="simple">10.1002/cne.903000103</ext-link></comment> <object-id pub-id-type="pmid">2229487</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref037">
<label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dacey</surname> <given-names>DM</given-names></name>. <chapter-title>Physiology, morphology and spatial densities of identified ganglion cell types in primate retina</chapter-title>. In: <source>Ciba Foundation Symposium 184-Higher-Order Processing in the Visual System</source>. <publisher-name>Wiley Online Library</publisher-name>; <year>1994</year>. p. <fpage>12</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rosenholtz</surname> <given-names>R</given-names></name>. <article-title>Capabilities and limitations of peripheral vision</article-title>. <source>Annual Review of Vision Science</source>. <year>2016</year>;<volume>2</volume>:<fpage>437</fpage>–<lpage>457</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-vision-082114-035733" xlink:type="simple">10.1146/annurev-vision-082114-035733</ext-link></comment> <object-id pub-id-type="pmid">28532349</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>9</issue>):<fpage>1195</fpage>–<lpage>1201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2889" xlink:type="simple">10.1038/nn.2889</ext-link></comment> <object-id pub-id-type="pmid">21841776</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Everingham M, Van Gool L, Williams CKI, Winn J, Zisserman A. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results; 2007. <ext-link ext-link-type="uri" xlink:href="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" xlink:type="simple">http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Dalal N, Triggs B. Histograms of Oriented Gradients for Human Detection. In: Conference on Computer Vision and Pattern Recognition; 2005. p. 886–893. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/CVPR.2005.177" xlink:type="simple">http://dx.doi.org/10.1109/CVPR.2005.177</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Hariharan B, Malik J, Ramanan D. Discriminative Decorrelation for Clustering and Classification. In: European Conference on Computer Vision; 2012.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beutter</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Stone</surname> <given-names>LS</given-names></name>. <article-title>Saccadic and perceptual performance in visual search tasks. I. Contrast detection and discrimination</article-title>. <source>Journal of Optical Society of America</source>. <year>2003</year>;<volume>20</volume>:<fpage>1341</fpage>–<lpage>1355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1364/JOSAA.20.001341" xlink:type="simple">10.1364/JOSAA.20.001341</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Verghese</surname> <given-names>P</given-names></name>. <article-title>Active search for multiple targets is inefficient</article-title>. <source>Vision Research</source>. <year>2012</year>;<volume>74</volume>:<fpage>61</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2012.08.008" xlink:type="simple">10.1016/j.visres.2012.08.008</ext-link></comment> <object-id pub-id-type="pmid">22929812</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Najemnik</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Simple summation rule for optimal fixation selection in visual search</article-title>. <source>Vision research</source>. <year>2009</year>;<volume>49</volume>(<issue>10</issue>):<fpage>1286</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2008.12.005" xlink:type="simple">10.1016/j.visres.2008.12.005</ext-link></comment> <object-id pub-id-type="pmid">19138697</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tatler</surname> <given-names>BW</given-names></name>. <article-title>The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor biases and image feature distributions</article-title>. <source>Journal of vision</source>. <year>2007</year>;<volume>7</volume>(<issue>14</issue>):<fpage>4.1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/7.14.4" xlink:type="simple">10.1167/7.14.4</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">van de Sande KEA, Uijlings JRR, Gevers T, Smeulders AWM. Segmentation As Selective Search for Object Recognition. In: International Conference on Computer Vision; 2011. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.science.uva.nl/research/publications/2011/vandeSandeICCV2011" xlink:type="simple">http://www.science.uva.nl/research/publications/2011/vandeSandeICCV2011</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhaoping</surname> <given-names>L</given-names></name>. <article-title>Feedback from higher to lower visual areas for visual recognition may be weaker in the periphery: Glimpses from the perception of brief dichoptic stimuli</article-title>. <source>Vision Research</source>. <year>2017</year>;<volume>136</volume>:<fpage>32</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2017.05.002" xlink:type="simple">10.1016/j.visres.2017.05.002</ext-link></comment> <object-id pub-id-type="pmid">28545983</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name>. <article-title>A neural model of contour integration in the primary visual cortex</article-title>. <source>Neural computation</source>. <year>1998</year>;<volume>10</volume>(<issue>4</issue>):<fpage>903</fpage>–<lpage>940</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976698300017557" xlink:type="simple">10.1162/089976698300017557</ext-link></comment> <object-id pub-id-type="pmid">9573412</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref050">
<label>50</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zhaoping</surname> <given-names>L</given-names></name>. <source>Understanding vision: theory, models, and data</source>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>; <year>2014</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Choi</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Willsky</surname> <given-names>AS</given-names></name>. <article-title>A tree-based context model for object recognition</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2012</year>;<volume>34</volume>(<issue>2</issue>):<fpage>240</fpage>–<lpage>252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2011.119" xlink:type="simple">10.1109/TPAMI.2011.119</ext-link></comment> <object-id pub-id-type="pmid">21670482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zelinsky</surname> <given-names>GJ</given-names></name>. <article-title>Real-world visual search is dominated by top-down guidance</article-title>. <source>Vision Research</source>. <year>2006</year>;<volume>46</volume>(<issue>24</issue>):<fpage>4118</fpage>–<lpage>4133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2006.08.008" xlink:type="simple">10.1016/j.visres.2006.08.008</ext-link></comment> <object-id pub-id-type="pmid">17005231</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koehler</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Beyond scene gist: Objects guide search more than scene background</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2017</year>;<volume>43</volume>(<issue>6</issue>):<fpage>1177</fpage>. <object-id pub-id-type="pmid">28287759</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>. <article-title>Probabilistic Computations for Attention, Eye Movements, and Search</article-title>. <source>Annual Review of Vision Science</source>. <year>2017</year>;<volume>3</volume>(<issue>1</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-vision-102016-061220" xlink:type="simple">10.1146/annurev-vision-102016-061220</ext-link></comment> <object-id pub-id-type="pmid">28746814</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Koehler</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Welbourne</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Akbas</surname> <given-names>E</given-names></name>. <article-title>Humans but not deep neural networks miss giant targets in scenes</article-title>. <source>Current Biology</source>. <year>2017</year>;<volume>27</volume>(<issue>18</issue>):<fpage>R1002</fpage>–<lpage>R1003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2017.07.068" xlink:type="simple">10.1016/j.cub.2017.07.068</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Niebur</surname> <given-names>E</given-names></name>. <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>1998</year>;<volume>20</volume>(<issue>11</issue>):<fpage>1254</fpage>–<lpage>1259</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.730558" xlink:type="simple">10.1109/34.730558</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borji</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>. <article-title>State-of-the-art in visual attention modeling</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2013</year>;<volume>35</volume>(<issue>1</issue>):<fpage>185</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2012.89" xlink:type="simple">10.1109/TPAMI.2012.89</ext-link></comment> <object-id pub-id-type="pmid">22487985</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bradley</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Abrams</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>. <article-title>Retina-V1 model of detectability across the visual field</article-title>. <source>Journal of vision</source>. <year>2014</year>;<volume>14</volume>(<issue>12</issue>):<fpage>22</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/14.12.22" xlink:type="simple">10.1167/14.12.22</ext-link></comment> <object-id pub-id-type="pmid">25336179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lewis</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Garcia</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Zhaoping</surname> <given-names>L</given-names></name>. <article-title>The distribution of visual objects on the retina: connecting eye movements and cone distributions</article-title>. <source>Journal of vision</source>. <year>2003</year>;<volume>3</volume>(<issue>11</issue>):<fpage>21</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/3.11.21" xlink:type="simple">10.1167/3.11.21</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Serre T, Wolf L, Poggio T. Object Recognition with Features Inspired by Visual Cortex. In: Conference on Computer Vision and Pattern Recognition; 2005.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref061">
<label>61</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Renninger</surname> <given-names>LW</given-names></name>, <name name-style="western"><surname>Coughlan</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Verghese</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name>. <chapter-title>An information maximization model of eye movements</chapter-title>. In: <source>Advances in Neural Information Processing</source>; <year>2004</year>. p. <fpage>1121</fpage>–<lpage>1128</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Schoonveld</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mack</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Akbas</surname> <given-names>E</given-names></name>. <article-title>Optimal and human eye movements to clustered low value cues to increase decision rewards during search</article-title>. <source>Vision Research</source>. <year>2015</year>;<volume>113</volume>, <issue>Part B</issue>:<fpage>137</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2015.05.016" xlink:type="simple">10.1016/j.visres.2015.05.016</ext-link></comment> <object-id pub-id-type="pmid">26093154</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Samaras</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zelinsky</surname> <given-names>GJ</given-names></name>. <article-title>A Computational Model of Eye Movements during Object Class Detection</article-title>. In: <source>Advances in Neural Information Processing</source>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Larochelle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Learning to combine foveal glimpses with a third-order Boltzmann machine</article-title>. In: <source>Advances in Neural Information Processing</source>; <year>2010</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref065">
<label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Bazzani L, de Freitas N, Larochelle H, Murino V, Ting JA. Learning attentional policies for tracking and recognition in video with deep networks. In: International Conference on Machine Learning; 2011.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lowe</surname> <given-names>DG</given-names></name>. <article-title>Distinctive Image Features from Scale-Invariant Keypoints</article-title>. <source>International Journal of Computer Vision</source>. <year>2004</year>;<volume>60</volume>(<issue>2</issue>):<fpage>91</fpage>–<lpage>110</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94" xlink:type="simple">10.1023/B:VISI.0000029664.99615.94</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref067">
<label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Ren X, Ramanan D. Histograms of Sparse Codes for Object Detection. In: Conference on Computer Vision and Pattern Recognition; 2013.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kontschieder</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bulò</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Criminisi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kohli</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pelillo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bischof</surname> <given-names>H</given-names></name>. <article-title>Context-Sensitive Decision Forests for Object Detection</article-title>. In: <source>Advances in Neural Information Processing</source>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Dean T, Ruzon MA, Segal M, Shlens J, Vijayanarasimhan S, Yagnik J. Fast, Accurate Detection of 100,000 Object Classes on a Single Machine. In: Conference on Computer Vision and Pattern Recognition; 2013.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ren</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Girshick</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>J</given-names></name>. <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>. In: <source>Advances in neural information processing systems</source>; <year>2015</year>. p. <fpage>91</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Ren S, He K, Girshick R, Sun J. Faster R-CNN: Towards real-time object detection with region proposal networks. arXiv e-print 150601497v3. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref072">
<label>72</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Spatial pyramid pooling in deep convolutional networks for visual recognition. In: European Conference on Computer Vision. Springer; 2014. p. 346–361.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref073">
<label>73</label>
<mixed-citation publication-type="other" xlink:type="simple">Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, LeCun Y. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv e-print arXiv:13126229. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alexe</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Heess</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>, <name name-style="western"><surname>Ferrari</surname> <given-names>V</given-names></name>. <article-title>Searching for objects driven by context</article-title>. In: <source>Advances in Neural Information Processing</source>; <year>2012</year>. p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lampert</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Blaschko</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Hofmann</surname> <given-names>T</given-names></name>. <article-title>Efficient Subwindow Search: A Branch and Bound Framework for Object Localization</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2009</year>;<volume>31</volume>(<issue>12</issue>):<fpage>2129</fpage>–<lpage>2142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2009.144" xlink:type="simple">10.1109/TPAMI.2009.144</ext-link></comment> <object-id pub-id-type="pmid">19834136</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kokkinos</surname> <given-names>I</given-names></name>. <article-title>Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</article-title>. In: <source>Advances in Neural Information Processing</source>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Viola</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MJ</given-names></name>. <article-title>Robust Real-Time Face Detection</article-title>. <source>International Journal of Computer Vision</source>. <year>2004</year>;<volume>57</volume>(<issue>2</issue>):<fpage>137</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fb" xlink:type="simple">10.1023/B:VISI.0000013087.49260.fb</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref078">
<label>78</label>
<mixed-citation publication-type="other" xlink:type="simple">Felzenszwalb P, Girshick R, McAllester D. Cascade object detection with deformable part models. In: Conference on Computer Vision and Pattern Recognition; 2010.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref079">
<label>79</label>
<mixed-citation publication-type="other" xlink:type="simple">Lampert CH. An Efficient Divide-and-Conquer Cascade for Nonlinear Object Detection. In: Conference on Computer Vision and Pattern Recognition; 2010.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref080">
<label>80</label>
<mixed-citation publication-type="other" xlink:type="simple">Pedersoli M, Vedaldi A, Gonzalez J. A coarse-to-fine approach for fast deformable object detection. In: Conference on Computer Vision and Pattern Recognition; 2011. p. 1353–1360.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref081">
<label>81</label>
<mixed-citation publication-type="other" xlink:type="simple">Mathe S, Pirinen A, Sminchisescu C. Reinforcement learning for visual object detection. In: Conference on Computer Vision and Pattern Recognition; 2016. p. 2894–2902.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref082">
<label>82</label>
<mixed-citation publication-type="other" xlink:type="simple">Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European Conference on Computer Vision. Springer; 2014. p. 818–833.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref083">
<label>83</label>
<mixed-citation publication-type="other" xlink:type="simple">Zitnick CL, Dollár P. Edge boxes: Locating object proposals from edges. In: European Conference on Computer Vision. Springer; 2014. p. 391–405.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref084">
<label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hosang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Benenson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dollár</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schiele</surname> <given-names>B</given-names></name>. <article-title>What makes for effective detection proposals?</article-title> <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2016</year>;<volume>38</volume>(<issue>4</issue>):<fpage>814</fpage>–<lpage>830</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2015.2465908" xlink:type="simple">10.1109/TPAMI.2015.2465908</ext-link></comment> <object-id pub-id-type="pmid">26959679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005743.ref085">
<label>85</label>
<mixed-citation publication-type="other" xlink:type="simple">Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for accurate object detection and semantic segmentation. In: Conference on Computer Vision and Pattern Recognition; 2014. p. 580–587.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref086">
<label>86</label>
<mixed-citation publication-type="other" xlink:type="simple">Girshick R. Fast R-CNN. In: Conference on Computer Vision and Pattern Recognition; 2015. p. 1440–<lpage>1448</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005743.ref087">
<label>87</label>
<mixed-citation publication-type="other" xlink:type="simple">Girshick RB, Felzenszwalb PF, McAllester D. Discriminatively Trained Deformable Part Models, Release 5; 2012. <ext-link ext-link-type="uri" xlink:href="http://people.cs.uchicago.edu/rbg/latent-release5/" xlink:type="simple">http://people.cs.uchicago.edu/rbg/latent-release5/</ext-link>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>