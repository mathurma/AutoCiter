<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-00013</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007290</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Electronics</subject><subj-group><subject>Signal decoders</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Approximation methods</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Signaling networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information theory</subject><subj-group><subject>Information entropy</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Estimating information in time-varying signals</article-title>
<alt-title alt-title-type="running-head">Estimating information in time-varying signals</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cepeda-Humerez</surname> <given-names>Sarah Anhala</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1615-3282</contrib-id>
<name name-style="western">
<surname>Ruess</surname> <given-names>Jakob</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6699-1455</contrib-id>
<name name-style="western">
<surname>Tkačik</surname> <given-names>Gašper</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Institute of Science and Technology Austria, A-3400 Klosterneuburg, Austria</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Inria Saclay – Ile-de-France, F-91120 Palaiseau, France</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Institut Pasteur, F-75015 Paris, France</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Morozov</surname> <given-names>Alexandre V.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Rutgers University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">gtkacik@ist.ac.at</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>3</day>
<month>9</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>9</issue>
<elocation-id>e1007290</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>1</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>7</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Cepeda-Humerez et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007290"/>
<abstract>
<p>Across diverse biological systems—ranging from neural networks to intracellular signaling and genetic regulatory networks—the information about changes in the environment is frequently encoded in the full temporal dynamics of the network nodes. A pressing data-analysis challenge has thus been to efficiently estimate the amount of information that these dynamics convey from experimental data. Here we develop and evaluate decoding-based estimation methods to lower bound the mutual information about a finite set of inputs, encoded in single-cell high-dimensional time series data. For biological reaction networks governed by the chemical Master equation, we derive model-based information approximations and analytical upper bounds, against which we benchmark our proposed model-free decoding estimators. In contrast to the frequently-used k-nearest-neighbor estimator, decoding-based estimators robustly extract a large fraction of the available information from high-dimensional trajectories with a realistic number of data samples. We apply these estimators to previously published data on Erk and Ca<sup>2+</sup> signaling in mammalian cells and to yeast stress-response, and find that substantial amount of information about environmental state can be encoded by non-trivial response statistics even in stationary signals. We argue that these single-cell, decoding-based information estimates, rather than the commonly-used tests for significant differences between selected population response statistics, provide a proper and unbiased measure for the performance of biological signaling networks.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Cells represent changes in their own state or in the state of their environment by temporally varying the concentrations of intracellular signaling molecules, mimicking in a simple chemical context the way we humans represent our thoughts and observations through temporally varying patterns of sounds that constitute speech. These time-varying concentrations are used as signals to regulate downstream molecular processes, to mount appropriate cellular responses for the environmental challenges, or to communicate with nearby cells. But how precise and unambiguous is such chemical communication, in theory and in data? On the one hand, intuition tells us that many possible environmental changes could be represented by variation in concentration patterns of multiple signaling chemicals; on the other, we know that chemical signals are inherently noisy at the molecular scale. Here we develop data analysis methodology that allows us to pose and answer these questions rigorously. Our decoding-based information estimators, which we test on simulated and real data from yeast and mammalian cells, measure how precisely individual cells can detect and report environmental changes, without making assumptions about the structure of the chemical communication and using only the amounts of data that is typically available in today’s experiments.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002428</institution-id>
<institution>Austrian Science Fund</institution>
</institution-wrap>
</funding-source>
<award-id>P28844</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6699-1455</contrib-id>
<name name-style="western">
<surname>Tkačik</surname> <given-names>Gašper</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>GT and SACH acknowledge the support of the Austrian Science Fund (<ext-link ext-link-type="uri" xlink:href="https://fwf.ac.at/en/" xlink:type="simple">https://fwf.ac.at/en/</ext-link>) grant FWF P28844. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="0"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-13</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data used in this paper has been previously published in two other publications (Granados AA et al, PNAS 115: 6088, 2018; and Selimkhanov J et al, Science 346: 1370, 2014; both cited in the paper), and have been obtained by contacting the corresponding authors of those publications. We have produced no new data ourselves in this publication.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>For their survival, reproduction, and differentiation, cells depend on their ability to respond and adapt to continually changing environmental conditions. Environmental information must be sensed and often transduced to the nucleus, where an appropriate response is initiated, usually by selectively up- or down-regulating the expression levels of target genes. This information flow is mediated by biochemical reaction networks, in which concentrations of various signaling molecules code for different environmental states or different response programs. This map between environmental input or response output and the internal chemical state is, however, highly stochastic, because typical networks operate with small absolute copy numbers of signaling molecules [<xref ref-type="bibr" rid="pcbi.1007290.ref001">1</xref>]; moreover, different environments can be encoded by the same signaling molecule, by differentially regulating the dynamics of its concentration [<xref ref-type="bibr" rid="pcbi.1007290.ref002">2</xref>]. This raises two fundamental questions: first, how much information the cells could, even in principle, encode in the combinatorial and possibly time-varying concentrations of multiple signaling molecules and how such information could be plausibly read out during “downstream” processing; and second, how can we quantify, in an unbiased and model-free fashion, the amount of information available to the cells from limited experimental data.</p>
<p>Information theory provides a framework within which the theoretical study of limits to communication as well as the empirical study of actual information flows can be addressed [<xref ref-type="bibr" rid="pcbi.1007290.ref003">3</xref>]. Applications of information theory to questions in biology and, in particular, neuroscience started already in the 1950s and continue to this day, with the main focus to understand how—and with what accuracy—neural activity encodes information about the environment [<xref ref-type="bibr" rid="pcbi.1007290.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1007290.ref006">6</xref>]. Applications of analogous techniques to biochemical signaling only started recently and represent an active area of research at the interface of physics, biology, statistics, and engineering [<xref ref-type="bibr" rid="pcbi.1007290.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1007290.ref010">10</xref>].</p>
<p>Recent theoretical work analyzed the reliability of information transmission through specific reaction systems in the presence of molecular noise, e.g., during ligand binding [<xref ref-type="bibr" rid="pcbi.1007290.ref011">11</xref>], in chemotaxis [<xref ref-type="bibr" rid="pcbi.1007290.ref012">12</xref>], gene regulation [<xref ref-type="bibr" rid="pcbi.1007290.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1007290.ref019">19</xref>], biochemical signaling networks [<xref ref-type="bibr" rid="pcbi.1007290.ref020">20</xref>], etc., and asked how such transmission can be maximized by tuning the reaction rates. Generally, these studies focused on steady state, by considering the information encoded in a single temporal snapshot of the reaction network at equilibrium given the input signals. Rigorous extensions to dynamical signals have been either rare and only possible for simple cases, like the BIND channel [<xref ref-type="bibr" rid="pcbi.1007290.ref011">11</xref>], or required specific operating regimes that permitted linearization and Gaussianity assumptions [<xref ref-type="bibr" rid="pcbi.1007290.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref022">22</xref>]. At its core, the analysis of signal transduction through nonlinear noisy chemical systems requires one to have control over the distribution of concentration trajectories given the (possibly) time-varying inputs; even if it were possible to calculate this distribution in principle, the curse of dimensionality puts strong limits to the manipulations required to compute the information transmission. Consequently, problems of this kind are currently considered intractable in their full generality.</p>
<p>Empirical estimates of information transmission in biochemical networks similarly focused on the steady state [<xref ref-type="bibr" rid="pcbi.1007290.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref024">24</xref>], or considered only specific, hand-picked dynamical features, such as the amplitude or the frequency of the response, as information carriers [<xref ref-type="bibr" rid="pcbi.1007290.ref025">25</xref>]. Recent developments of fluorescent reporters and microfluidics have enabled the characterization of dynamical responses at a single cell resolution using large (&gt; 10<sup>4</sup>) numbers of sampled response trajectories, thereby permitting direct information estimates using generic estimators like the k-nearest-neighbors (knn) [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>]. Existing approaches, however, suffer from severe limitations: they still require a prohibitive number of samples, especially when the response is distributed over multiple chemical species; or they necessitate uncontrolled assumptions about trajectory features that are supposed to be “relevant”. We recently proposed and applied decoding-based information estimators [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>] as an alternative that draws on the past experiences in neuroscience [<xref ref-type="bibr" rid="pcbi.1007290.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1007290.ref030">30</xref>] to dissect the yeast stress-response network. In this paper we provide a detailed account of the new methodology, show that it alleviates the most pressing problems of existing approaches, and benchmark it against synthetic and real data.</p>
</sec>
<sec id="sec002">
<title>Models and methods</title>
<sec id="sec003">
<title>Biochemical reaction networks</title>
<p>At their core, cellular processes consist of networks of chemical reactions. A chemical reaction network consists of a set of <italic>m</italic> molecular species <inline-formula id="pcbi.1007290.e001"><alternatives><graphic id="pcbi.1007290.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that interact through <italic>K</italic> coupled reactions of the form:
<disp-formula id="pcbi.1007290.e002"><alternatives><graphic id="pcbi.1007290.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mover><mml:mo>→</mml:mo><mml:mrow><mml:mspace width="1.em"/><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="1.em"/></mml:mrow></mml:mover><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>

where <inline-formula id="pcbi.1007290.e003"><alternatives><graphic id="pcbi.1007290.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007290.e004"><alternatives><graphic id="pcbi.1007290.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are coefficients that determine how many molecules of each species are consumed and produced in the <italic>k</italic>-th reaction. <inline-formula id="pcbi.1007290.e005"><alternatives><graphic id="pcbi.1007290.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> determine the rates at which the reactions occur and depend on binding affinities of chemical species, temperature and possibly the external conditions.</p>
<p>If we assume that the system is well-stirred, in thermal equilibrium and the reaction volume is constant, it can be shown that the probability that a reaction of type <italic>k</italic> takes place in an infinitesimal time interval [<italic>t</italic>, <italic>t</italic> + <italic>dt</italic>] can be written as <inline-formula id="pcbi.1007290.e006"><alternatives><graphic id="pcbi.1007290.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1007290.e007"><alternatives><graphic id="pcbi.1007290.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> contains the amounts of molecules of the <italic>m</italic> species that are present in the system at time <italic>t</italic>, and <inline-formula id="pcbi.1007290.e008"><alternatives><graphic id="pcbi.1007290.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> counts all possibilities of choosing the required reaction molecules out of all available molecules [<xref ref-type="bibr" rid="pcbi.1007290.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref032">32</xref>]. <italic>θ</italic><sub><italic>k</italic></sub> is a constant that depends on the physical characteristics of the cell but also on the environmental conditions.</p>
<p>Let us denote the probability that <inline-formula id="pcbi.1007290.e009"><alternatives><graphic id="pcbi.1007290.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> molecules of the <italic>m</italic> species are present in the system at time <inline-formula id="pcbi.1007290.e010"><alternatives><graphic id="pcbi.1007290.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> by <inline-formula id="pcbi.1007290.e011"><alternatives><graphic id="pcbi.1007290.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and define the stoichiometric change vectors <inline-formula id="pcbi.1007290.e012"><alternatives><graphic id="pcbi.1007290.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">Z</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, as the net changes in the amount of molecules in the reactions, i.e. <inline-formula id="pcbi.1007290.e013"><alternatives><graphic id="pcbi.1007290.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Then it can be shown [<xref ref-type="bibr" rid="pcbi.1007290.ref032">32</xref>] that the chemical master equation (CME) can be written as:
<disp-formula id="pcbi.1007290.e014"><alternatives><graphic id="pcbi.1007290.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
or in a more compact form [<xref ref-type="bibr" rid="pcbi.1007290.ref032">32</xref>]
<disp-formula id="pcbi.1007290.e015"><alternatives><graphic id="pcbi.1007290.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">p</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">M</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <bold>p</bold>(<italic>t</italic>) is a vector with components <inline-formula id="pcbi.1007290.e016"><alternatives><graphic id="pcbi.1007290.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which is, in principle, infinite dimensional, and <bold>M</bold> contains the transition rates between all possible states, e.g. the transition rate from state <inline-formula id="pcbi.1007290.e017"><alternatives><graphic id="pcbi.1007290.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> to state <inline-formula id="pcbi.1007290.e018"><alternatives><graphic id="pcbi.1007290.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is given by
<disp-formula id="pcbi.1007290.e019"><alternatives><graphic id="pcbi.1007290.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>q</mml:mi></mml:munder><mml:msub><mml:mi>a</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>δ</italic> is the Kronecker delta, which is 1 when <inline-formula id="pcbi.1007290.e020"><alternatives><graphic id="pcbi.1007290.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and 0 otherwise.</p>
<p>The CME given in <xref ref-type="disp-formula" rid="pcbi.1007290.e015">Eq (3)</xref> is an instance of a continuous-time discrete-state-space Markov Chain for a random process <italic>X</italic> that can be solved exactly only for a few simple cases. It is nevertheless possible to efficiently generate samples <italic>x</italic> of the random process <italic>X</italic>, which we will refer to as “trajectories” or “paths”, for a selected time interval, <italic>t</italic> ∈ [0, <italic>T</italic>], according to the correct probability distribution <italic>p</italic>, by the Stochastic Simulation Algorithm (SSA, or the Gillespie algorithm) [<xref ref-type="bibr" rid="pcbi.1007290.ref033">33</xref>].</p>
<p>To study information transmission through the biochemical networks described by the CME, we need to define the input and output signals. In the simplest setup considered here, the input <italic>U</italic> is a discrete random variable that can take on one of the <italic>q</italic> ≥ 2 possible values, <italic>U</italic> ∈ {<italic>u</italic><sup>(1)</sup>, <italic>u</italic><sup>(2)</sup>, …, <italic>u</italic><sup>(<italic>q</italic>)</sup>}. Each input in general corresponds to a distinct set of reaction rate constants <italic>θ</italic>, but in models of real biological networks, changing input often modulates only one or a few rates in the system, e.g., by representing the change in a key external ligand concentration, receptor activity, etc. Changes in the input are reflected in changes in the resulting trajectories of chemical species amounts, <italic>x</italic>. Typically, only a subset of chemical species could be considered as biologically-relevant “outputs” that encode the information about the environmental change: this would correspond to marginalizing <italic>p</italic> in <xref ref-type="disp-formula" rid="pcbi.1007290.e015">Eq (3)</xref> over the unobserved (non-output) chemical species for the purposes of information transmission. While this is an interesting theoretical problem in its own right, here we work with simple toy examples where the output will be the trajectory, <italic>x</italic>, over the complete state space, i.e., we assume that all chemical species in the reaction network can be fully and perfectly observed. As we explain below, this allows us to define and compute the mutual information between a discrete input, <italic>U</italic>, and the output random process <italic>X</italic> given by the CME in a straightforward fashion. We later show that this computation can be carried out also when the continuous-time process <italic>X</italic> is sampled at uniform discrete times, as would be the case with experimental measurements.</p>
</sec>
<sec id="sec004">
<title>Mutual information between discrete inputs and response trajectories</title>
<p>Information theory introduces the mutual information as the measure of fidelity by which changes in one random variable, e.g., the input <italic>U</italic>, can effect changes in another random variable, e.g., <italic>X</italic>. In this sense, mutual information is simply a measure of statistical dependency (i.e., any correlation, be it linear or not) between <italic>U</italic> and <italic>X</italic>, and can thus be written as a functional of the joint probability density function <italic>p</italic>(<italic>x</italic>, <italic>u</italic>):
<disp-formula id="pcbi.1007290.e021"><alternatives><graphic id="pcbi.1007290.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>X</mml:mi></mml:msub><mml:msub><mml:mo>∫</mml:mo><mml:mi>U</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mspace width="0.277778em"/><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>p</italic><sub><italic>U</italic></sub> and <italic>p</italic><sub><italic>X</italic></sub> are the marginal density functions for <italic>U</italic> and <italic>X</italic>, respectively, and we have generically written <italic>u</italic> and <italic>x</italic> as continuous variables; if they are discrete, integral signs are replaced by summations over the support for the corresponding probability distributions, as appropriate.</p>
<p>Mutual information is a non-negative symmetric quantity that is measured in bits, and is zero only if <italic>X</italic> and <italic>U</italic> are statistically independent. When studying information transmission through a channel <italic>U</italic> → <italic>X</italic> specified by <italic>p</italic>(<italic>x</italic>|<italic>u</italic>), for which <italic>U</italic> serve as inputs drawn from an input distribution <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic>), it is common to rewrite <xref ref-type="disp-formula" rid="pcbi.1007290.e021">Eq (5)</xref> as
<disp-formula id="pcbi.1007290.e022"><alternatives><graphic id="pcbi.1007290.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>H</italic>(<italic>X</italic>) is the differential entropy of <italic>X</italic> (and analogously for <italic>H</italic>(<italic>U</italic>)), defined as
<disp-formula id="pcbi.1007290.e023"><alternatives><graphic id="pcbi.1007290.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>X</mml:mi></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
and the conditional entropy, <italic>H</italic>(<italic>X</italic>|<italic>U</italic>), is
<disp-formula id="pcbi.1007290.e024"><alternatives><graphic id="pcbi.1007290.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>U</mml:mi></mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>U</mml:mi></mml:msub><mml:msub><mml:mo>∫</mml:mo><mml:mi>X</mml:mi></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p><xref ref-type="disp-formula" rid="pcbi.1007290.e022">Eq (6)</xref> can be interpreted in two ways: information is either the difference between the total variability in the repertoire of responses <italic>X</italic> that the biochemical network can generate (measured by the <italic>response entropy</italic>, <italic>H</italic>(<italic>X</italic>)) and the average variability at fixed input that is due to noise in the network (measured by the <italic>noise entropy</italic>, <italic>H</italic>(<italic>X</italic>|<italic>U</italic>)); alternatively, information is also the entropy of the inputs, <italic>H</italic>(<italic>U</italic>), minus equivocation <italic>H</italic>(<italic>U</italic>|<italic>X</italic>), or the average uncertainty in what input was sent given that a particular response was observed. These interpretations make explicit the dependence of information both on the properties of the channel (the biochemical reaction network), as well as on the distribution of signals <italic>p</italic><sub><italic>U</italic></sub> that the network receives. In this work, we will consider discrete inputs and will assume uniform <italic>p</italic><sub><italic>U</italic></sub>. It is, however, also possible to compute the <italic>channel capacity</italic> <italic>C</italic> by maximizing the information flow at given <italic>p</italic>(<italic>x</italic>|<italic>u</italic>) over all possible input distributions,
<disp-formula id="pcbi.1007290.e025"><alternatives><graphic id="pcbi.1007290.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mtext>max</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub></mml:munder><mml:mspace width="0.277778em"/><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
Shannon’s classic work then proves that error-free transmission at rates higher than those given by capacity is impossible, while error-free transmission at rates below capacity can be achieved with the optimal use of the channel. Contrary to engineering, where the focus is on finding encoding and decoding schemes that best utilize a given channel, in biophysics and systems biology mutual information is used as a tool to quantify the limits to biological signal processing due to noise without needing to make assumptions about possible biochemical encoding and decoding mechanisms.</p>
<p>The setup we consider here is one in which inputs <italic>U</italic> are drawn independently from a uniform distribution and change rarely, i.e., at a rate that is much lower than the (inverse) timescale on which the reaction network in <xref ref-type="disp-formula" rid="pcbi.1007290.e002">Eq (1)</xref> relaxes to its steady state. We assume that after an input change, we observe a fixed-time segment of the complete network dynamics, <italic>x</italic>, which is a sample path in <italic>m</italic>-dimensional discrete space, making direct calculation of information, <italic>I</italic>(<italic>X</italic>; <italic>U</italic>), by integrating / summing over all possible trajectories as implied by <xref ref-type="disp-formula" rid="pcbi.1007290.e021">Eq (5)</xref> intractable. We will nevertheless show that estimates of exact information are possible if the reaction network is known, by explicitly using the transition matrix <bold>M</bold> of the Markov Chain from <xref ref-type="disp-formula" rid="pcbi.1007290.e015">Eq (3)</xref> and generating exact sample paths, that is, realizations of <italic>X</italic>, using SSA. We call this model-based approach <italic>exact Monte Carlo approximation</italic> and contrast it to uncontrolled model-free estimations such as those obtained by using Gaussian approximations or k-nearest-neighbors methodology. We then introduce various decoding estimators and establish a hierarchy through which these estimates upper and lower-bound the true information, as shown in <xref ref-type="fig" rid="pcbi.1007290.g001">Fig 1</xref>.</p>
<fig id="pcbi.1007290.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Information transmission between discrete inputs and response trajectories in biochemical networks.</title>
<p>For fully-observed reaction networks whose dynamics are governed by a known chemical Master equation, information can be approximated to an arbitrary accuracy via Monte Carlo integration for either continuous-time or discrete-time response trajectories (model-based <italic>exact Monte Carlo</italic>, Section <italic>Exact information calculations for fully observed reaction networks</italic>). Since full knowledge of the reaction system is used, these approximations are tractable deep in the regimes where model-free estimations break down with uncontrolled errors (Section <italic>Model–free information estimators</italic>). True information estimates are lower-bounded by model-based maximum a posteriori (MAP) or Bayes optimal decoding (Section <italic>Decoding–based information bounds</italic>). This decoding gives the lowest average probability of error and the corresponding information lower bound can be used as a benchmark for information estimates derived from other model-free decoding approaches (that have at least the error probability of the MAP decoder); in Section <italic>Decoding–based information estimators</italic> we compare Support Vector Machine (SVM), Gaussian Decoding (GD) and Neural Network (NN) decoding approaches. Upper bounds like the Feder-Merhav bound [<xref ref-type="bibr" rid="pcbi.1007290.ref034">34</xref>] and our improvement on it [<xref ref-type="bibr" rid="pcbi.1007290.ref035">35</xref>] complete the picture by estimating the gap between optimal decoding-derived and exact information values (Section <italic>Decoding–based information bounds</italic>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Exact information calculations for fully observed reaction networks</title>
<sec id="sec006">
<title>Responses in continuous time</title>
<p>Given the specification of the biochemical reaction network in <xref ref-type="disp-formula" rid="pcbi.1007290.e002">Eq (1)</xref>, we sample <italic>N</italic> trajectories, <italic>x</italic>, using the Gillespie (SSA) algorithm. Each trajectory <italic>x</italic> can be represented as the sequence of consecutive states representing molecular species counts, <bold>s</bold> = [<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>, …, <italic>s</italic><sub><italic>r</italic></sub>], where <inline-formula id="pcbi.1007290.e026"><alternatives><graphic id="pcbi.1007290.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, etc., and <inline-formula id="pcbi.1007290.e027"><alternatives><graphic id="pcbi.1007290.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the sequence of time intervals spent in each state, <bold>t</bold> = [<italic>t</italic><sub>1</sub>, …, <italic>t</italic><sub><italic>r</italic></sub>], 0 &lt; <italic>t</italic><sub><italic>i</italic></sub> &lt; <italic>T</italic>, <italic>i</italic> = 1, …, <italic>r</italic> and <inline-formula id="pcbi.1007290.e028"><alternatives><graphic id="pcbi.1007290.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. We recall that SSA generates sample trajectories from the correct conditional distribution, <italic>p</italic>(<italic>x</italic>|<italic>u</italic>). Given a particular trajectory <italic>x</italic> drawn from this distribution, we can compute the likelihood of <italic>x</italic> for a given input <italic>u</italic> exactly, by explicitly multiplying through all reaction events indexed by <italic>r</italic>:
<disp-formula id="pcbi.1007290.e029"><alternatives><graphic id="pcbi.1007290.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>p</italic>(<italic>s</italic><sub>1</sub>) is given by the initial conditions of the process, and the transition matrix <bold>M</bold> depends on the input <italic>u</italic>. To get the marginal distribution, <italic>p</italic><sub><italic>X</italic></sub>(<italic>x</italic>), we sum over all possible input values (since we are considering cases where the number of distinct inputs is finite, this summation can be done explicitly for each <italic>x</italic>):
<disp-formula id="pcbi.1007290.e030"><alternatives><graphic id="pcbi.1007290.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Since we are able to compute the exact likelihoods (Eqs (<xref ref-type="disp-formula" rid="pcbi.1007290.e029">10</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1007290.e030">11</xref>)) for each path generated by the stochastic process, entropic quantities can be approximated without significant biases using Monte Carlo integration, where the integral over states in <xref ref-type="disp-formula" rid="pcbi.1007290.e023">Eq (7)</xref> is replaced by an average over <italic>N</italic> sampled trajectories:
<disp-formula id="pcbi.1007290.e031"><alternatives><graphic id="pcbi.1007290.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
Similarly, we can approximate <italic>H</italic>(<italic>X</italic>|<italic>U</italic>):
<disp-formula id="pcbi.1007290.e032"><alternatives><graphic id="pcbi.1007290.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
The exact Monte Carlo information approximation is finally obtained using <xref ref-type="disp-formula" rid="pcbi.1007290.e022">Eq (6)</xref>:
<disp-formula id="pcbi.1007290.e033"><alternatives><graphic id="pcbi.1007290.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where * reminds us that the paths are represented in continuous time. Taken together, this procedure relies on three key facts: first, that trajectories from the biochemical stochastic process determined by the Master equation can be drawn exactly using SSA; second, that the computation of the log likelihood for these trajectories is straightforward to evaluate from the known Master equation; and third, that a Monte Carlo approximation to the integrals for entropic terms is just an empirical average of the log likelihoods over the sampled trajectories.</p>
</sec>
<sec id="sec007">
<title>Responses in discrete time</title>
<p>We can resample the continuous trajectories <italic>X</italic> on a grid of uniformly spaced time points to obtain a new discrete random variable, <inline-formula id="pcbi.1007290.e034"><alternatives><graphic id="pcbi.1007290.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, where Δ<italic>t</italic> is the discretization step, <italic>d</italic> = <italic>T</italic>/Δ<italic>t</italic> is the length of <bold>X</bold>. For convenient notation we denote this random variable as <bold>X</bold> = [<italic>X</italic><sup>0</sup>, …, <italic>X</italic><sup><italic>d</italic></sup>], and its realizations, the discrete trajectories, as <bold>x</bold>.</p>
<p>In the discrete case, the likelihood of <bold>x</bold> for a given input <italic>u</italic> can be computed using the general solution to <xref ref-type="disp-formula" rid="pcbi.1007290.e015">Eq (3)</xref>:
<disp-formula id="pcbi.1007290.e035"><alternatives><graphic id="pcbi.1007290.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where <bold>p</bold>(<italic>t</italic>) is the probability distribution of states after time <italic>t</italic>, with the initial probability distribution <bold>p</bold>(<italic>t</italic> = 0) = <bold>p</bold>(0). Using this formal solution we compute the transition matrix between discrete timesteps separated by Δ<italic>t</italic> to get:
<disp-formula id="pcbi.1007290.e036"><alternatives><graphic id="pcbi.1007290.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
where <bold>M</bold> and thus <bold>W</bold> again depend on <italic>u</italic>. The likelihood of any discrete path can then be obtained by multiplying the transition probabilities between all the <italic>d</italic> consecutive states in the path for a given input <italic>u</italic>:
<disp-formula id="pcbi.1007290.e037"><alternatives><graphic id="pcbi.1007290.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
We can now approximate the information between input <italic>U</italic> and a discretely sampled trajectory <bold>X</bold>, <italic>I</italic><sub>exact</sub>, as in the continuous case: we get the marginal <italic>p</italic><sub><italic>X</italic></sub>(<bold>x</bold>) with <xref ref-type="disp-formula" rid="pcbi.1007290.e030">Eq (11)</xref> and use Eqs (<xref ref-type="disp-formula" rid="pcbi.1007290.e031">12</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1007290.e032">13</xref>) in <xref ref-type="disp-formula" rid="pcbi.1007290.e033">Eq (14)</xref>. In general, temporal discretization loses information relative to the full (continuous-time) trajectory, where reaction events in the trajectory <italic>x</italic> are recorded with infinite temporal precision, so the information in discretely-sampled trajectories, <italic>I</italic>, must be bounded from above by the information in continuous-time trajectories, <italic>I</italic>*:
<disp-formula id="pcbi.1007290.e038"><alternatives><graphic id="pcbi.1007290.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where equality is approached in the limit of ever finer temporal discretization, Δ<italic>t</italic> → 0.</p>
</sec>
</sec>
<sec id="sec008">
<title>Model–free information estimators</title>
<p>In the absence of a full stochastic model for the biochemical reaction network, mutual information estimation is tractable only if we make assumptions about the distribution of response trajectories given the input. We briefly summarize two approaches below: in the first, k-nearest-neighbor procedure, the space in which the response trajectories are embedded is assumed to have a particular metric; in the second, Gaussian approximation, we assume a particularly tractable functional form for the channel, <italic>p</italic>(<bold>x</bold>|<italic>u</italic>).</p>
<sec id="sec009">
<title>K-nearest-neighbors (knn) estimator</title>
<p>The idea of using the nearest neighbour statistics to estimates entropies is at least 70 years old [<xref ref-type="bibr" rid="pcbi.1007290.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref037">37</xref>], while estimators for mutual information have been developed during the early 2000s [<xref ref-type="bibr" rid="pcbi.1007290.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref039">39</xref>]. The cornerstone of the approach is to compute the estimate from the distances of <italic>d</italic>-dimensional real valued data points to their <italic>k</italic>-th nearest neighbour. Hence, the estimator depends on the metric chosen to define this distance. Furthermore, its performance is known to depend on the value of <italic>k</italic> (number of nearest neighbours), where small <italic>k</italic> increase the variance and decrease the bias [<xref ref-type="bibr" rid="pcbi.1007290.ref040">40</xref>]. This method has been used in several studies that estimated mutual information from single cell time series [<xref ref-type="bibr" rid="pcbi.1007290.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref041">41</xref>]. These studies used large numbers of response trajectories to provide the first evidence that the information available from the full timeseries of the response could be substantially higher than the information available from any response snapshot.</p>
</sec>
<sec id="sec010">
<title>Gaussian approximation</title>
<p>A simplifying assumption in the Gaussian approximation is that the distribution of trajectories sampled at discrete times given input is approximately Gaussian, with the mean <inline-formula id="pcbi.1007290.e039"><alternatives><graphic id="pcbi.1007290.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and covariance matrix <inline-formula id="pcbi.1007290.e040"><alternatives><graphic id="pcbi.1007290.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:mo>Σ</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> that may both depend on the input, <italic>u</italic>:
<disp-formula id="pcbi.1007290.e041"><alternatives><graphic id="pcbi.1007290.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo mathvariant="bold">Σ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mtext>det</mml:mtext><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo mathvariant="bold">Σ</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mtext>exp</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo mathvariant="bold">Σ</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
The entropy of the multivariate distribution in <xref ref-type="disp-formula" rid="pcbi.1007290.e041">Eq (19)</xref> has an analytical expression that only depends on <bold>Σ</bold>:
<disp-formula id="pcbi.1007290.e042"><alternatives><graphic id="pcbi.1007290.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>det</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo mathvariant="bold">Σ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
which can be averaged over <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic>) to get the conditional entropy, <italic>H</italic><sub><italic>G</italic></sub>(<bold>X</bold>|<italic>U</italic>). To estimate the information, we further need <italic>H</italic>(<bold>X</bold>) from <xref ref-type="disp-formula" rid="pcbi.1007290.e022">Eq (6)</xref>. This entropy of a Gaussian mixture has no closed form solution, but can be computed by Monte Carlo integration as in the previous section, following discrete analogs of Eqs (<xref ref-type="disp-formula" rid="pcbi.1007290.e030">11</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1007290.e031">12</xref>): we draw random samples from each of the <italic>q</italic> different multivariate Gaussian distributions, <xref ref-type="disp-formula" rid="pcbi.1007290.e041">Eq (19)</xref>, one for each possible input <italic>u</italic>, and assign the marginal probabilities to each sample <bold>x</bold> as
<disp-formula id="pcbi.1007290.e043"><alternatives><graphic id="pcbi.1007290.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo mathvariant="bold">Σ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
permitting us to use <xref ref-type="disp-formula" rid="pcbi.1007290.e031">Eq (12)</xref> to approximate the total entropy of output trajectories in the Gaussian approximation, <italic>H</italic><sub><italic>G</italic></sub>(<bold>X</bold>), and thus to obtain the Gaussian estimate for the information, <italic>I</italic><sub>G</sub>(<bold>X</bold>; <italic>U</italic>) = <italic>H</italic><sub>G</sub>(<bold>X</bold>) − <italic>H</italic><sub>G</sub>(<bold>X</bold>|<italic>U</italic>).</p>
<p>To apply this estimator, one must use real (or simulated) data to estimate the conditional mean, <italic>μ</italic>(<italic>u</italic>), and conditional covariance, <bold>Σ</bold>(<italic>u</italic>) for every possible <italic>u</italic>, from a limited number of samples. While general caveats for such estimations have been detailed in many textbooks [<xref ref-type="bibr" rid="pcbi.1007290.ref042">42</xref>], we emphasize that information estimation is particularly sensitive due to the computation of the determinant in <xref ref-type="disp-formula" rid="pcbi.1007290.e042">Eq (20)</xref> which can easily lead to ill-posed numerics when the number of samples is small. This can be mitigated by various regularization methods (one of which, the diagonal regularization [<xref ref-type="bibr" rid="pcbi.1007290.ref064">64</xref>], we demonstrate later) that impose a prior structure on the estimated covariance. Yet even in the case of significant oversampling that we can explore using simulated data, the Gaussian approximation introduced here—in contrast to Gaussian decoding estimator introduced in the next section—can provide information values that deviate significantly from the true value and are not guaranteed to bound the true value from either above or below. This is because the true solutions of the CME live in the positive quadrant of the discrete space, and are thus essentially different from the Gaussian distributions assumed here. We nevertheless present this estimator because (i) it forms the basis for the Gaussian decoding estimator, introduced below, and (ii) real data itself often deviates from stochastic trajectories sampled from the CME in that it is continuous (since we measure, e.g., fluorescence proxy for a concentration of a protein of interest) and contains extra noise, making Gaussian approximation potentially applicable.</p>
</sec>
</sec>
<sec id="sec011">
<title>Decoding-based information bounds</title>
<p>Here and in the next section we introduce a class of decoding-based calculations that lower-bound the exact information, <italic>I</italic>(<bold>X</bold>; <italic>U</italic>), and can tractably be used as information estimators over realistically-sized data sets. Let <inline-formula id="pcbi.1007290.e044"><alternatives><graphic id="pcbi.1007290.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> consist of a set of <italic>N</italic> labeled paths, typically represented in discretely sampled time, <inline-formula id="pcbi.1007290.e045"><alternatives><graphic id="pcbi.1007290.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>u</italic><sub><italic>i</italic></sub> and <bold>x</bold><sub><bold>i</bold></sub>, for <italic>i</italic> = 1, …, <italic>N</italic>, are realizations of the random variables <italic>U</italic> ∈ {<italic>u</italic><sup>(1)</sup>, …, <italic>u</italic><sup>(<italic>q</italic>)</sup>} and <inline-formula id="pcbi.1007290.e046"><alternatives><graphic id="pcbi.1007290.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, respectively. Here, <inline-formula id="pcbi.1007290.e047"><alternatives><graphic id="pcbi.1007290.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> can represent either real data (typically containing <italic>N</italic> ∼ 10<sup>2</sup> − 10<sup>3</sup> trajectories) in case of model-free information estimates, or trajectories generated by exact simulation algorithms (in which case the sample size, <italic>N</italic>, is not limiting) from the full specification of the biochemical reaction network in case of model-based approximations.</p>
<p>The procedure of estimating the input <inline-formula id="pcbi.1007290.e048"><alternatives><graphic id="pcbi.1007290.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> from <bold>x</bold>, such that the estimated <inline-formula id="pcbi.1007290.e049"><alternatives><graphic id="pcbi.1007290.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is “as close as possible” to true <italic>u</italic> for a given trajectory <bold>x</bold>, is known as decoding in information theory and neuroscience, and can equivalently be viewed as a classification task in machine learning or as an inference task in statistics. This procedure is implemented by a decoding function,
<disp-formula id="pcbi.1007290.e050"><alternatives><graphic id="pcbi.1007290.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
<italic>F</italic> is typically parametrized by parameters <italic>ω</italic> that need to be learned from data for model-free approaches, or derived from biochemical reaction network specification in case of model-based approaches. <italic>F</italic> assigns to every <bold>x</bold><sub><italic>i</italic></sub> in the dataset a corresponding “decode” <inline-formula id="pcbi.1007290.e051"><alternatives><graphic id="pcbi.1007290.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> from the same space over which the random variable <italic>U</italic> is defined; formally, these decodes are instances of a new random variable <inline-formula id="pcbi.1007290.e052"><alternatives><graphic id="pcbi.1007290.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. The key idea of using decoding for information estimation starts with the observation that random variables
<disp-formula id="pcbi.1007290.e053"><alternatives><graphic id="pcbi.1007290.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>U</mml:mi><mml:mo>→</mml:mo><mml:mi>X</mml:mi><mml:mover><mml:mo>→</mml:mo><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mover><mml:mi mathvariant="bold">X</mml:mi><mml:mover><mml:mo>→</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:mover><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
where <italic>T</italic><sub><italic>d</italic></sub> represents time discretization, form a Markov chain. In other words, the distribution of <inline-formula id="pcbi.1007290.e054"><alternatives><graphic id="pcbi.1007290.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is conditionally independent of <italic>U</italic> and only depends on <bold>X</bold>, <inline-formula id="pcbi.1007290.e055"><alternatives><graphic id="pcbi.1007290.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and so
<disp-formula id="pcbi.1007290.e056"><alternatives><graphic id="pcbi.1007290.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
The data processing inequality [<xref ref-type="bibr" rid="pcbi.1007290.ref043">43</xref>] can be used to further extend the bounds in <xref ref-type="disp-formula" rid="pcbi.1007290.e038">Eq (18)</xref>:
<disp-formula id="pcbi.1007290.e057"><alternatives><graphic id="pcbi.1007290.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula>
where equality between the first two terms holds only if <inline-formula id="pcbi.1007290.e058"><alternatives><graphic id="pcbi.1007290.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Consequently, <inline-formula id="pcbi.1007290.e059"><alternatives><graphic id="pcbi.1007290.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is a lower bound to the information between trajectories <italic>X</italic> and the input <italic>U</italic> [<xref ref-type="bibr" rid="pcbi.1007290.ref044">44</xref>]. Note that analogous reasoning holds for decoding directly from continuous-time trajectories <italic>X</italic>. Better decoders which increase the correspondence between the true inputs and the corresponding decoded inputs will typically provide a tighter lower bound on the information.</p>
<p>To compute the information lower bound, we apply the decoding function to each trajectory in <inline-formula id="pcbi.1007290.e060"><alternatives><graphic id="pcbi.1007290.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> in model-based approximations or to each trajectory in the testing dataset for model-free estimators that need to be learned over training data first. We subsequently construct a <italic>q</italic> × <italic>q</italic> confusion matrix, also known as an error matrix, where each element <italic>ϵ</italic><sub><italic>ij</italic></sub> counts the fraction of realizations of <bold>x</bold> generated by an input <italic>u</italic> = <italic>u</italic><sup>(<italic>i</italic>)</sup> that decode into <inline-formula id="pcbi.1007290.e061"><alternatives><graphic id="pcbi.1007290.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. This matrix provides an empirical estimate of the probability distribution <inline-formula id="pcbi.1007290.e062"><alternatives><graphic id="pcbi.1007290.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which can thus be used to compute the information estimate:
<disp-formula id="pcbi.1007290.e063"><alternatives><graphic id="pcbi.1007290.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula>
Crucially, in this estimation <italic>O</italic>(<italic>N</italic>) data points are used to empirically estimate the elements of a <italic>q</italic> × <italic>q</italic> matrix <italic>ϵ</italic>, and information estimation involves a tractable summation over these matrix elements; in contrast, direct estimates of <italic>I</italic>(<italic>U</italic>; <bold>X</bold>) would involve an intractable summation over (vastly undersampled) space for <bold>X</bold>. For typical applications where <italic>q</italic> is small, decoding thus provides an essential dimensionality reduction prior to information estimation: in a simple but biologically relevant case of two distinct stimuli (<italic>q</italic> = 2), information estimation only requires us to empirically construct a 2 × 2 confusion matrix. If required, one can apply well-known debiasing techniques for larger <italic>q</italic> [<xref ref-type="bibr" rid="pcbi.1007290.ref005">5</xref>].</p>
<sec id="sec012">
<title>Maximum a posteriori (MAP) lower bound</title>
<p>In MAP lower bound, the decoding function <italic>F</italic><sub><italic>ω</italic></sub> is given by Bayesian inference of the most likely input <italic>u</italic> given that a response trajectory <bold>x</bold> was observed, under the exact probabilistic model for the biochemical reaction network. MAP decoder is optimal in that it provides the lowest average probability of error, <inline-formula id="pcbi.1007290.e064"><alternatives><graphic id="pcbi.1007290.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>≠</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, among all decoders. Typically, this will lead to a high mutual information value <inline-formula id="pcbi.1007290.e065"><alternatives><graphic id="pcbi.1007290.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> compared to other (sub-optimal) decoders whose probability of error will likely be higher, making the information lower bound from MAP decoder a good benchmark for other decoder-based information estimates. We remind the reader, however, that even though MAP decoder achieves minimal error and typically high <inline-formula id="pcbi.1007290.e066"><alternatives><graphic id="pcbi.1007290.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> values, this does not mathematically guarantee that its information will <italic>always</italic> be higher or equal to the information of any other possible decoder, a fact that can be demonstrated explicitly using toy examples.</p>
<p>The MAP inference consists of finding the input that maximizes the posterior distribution [<xref ref-type="bibr" rid="pcbi.1007290.ref045">45</xref>]
<disp-formula id="pcbi.1007290.e067"><alternatives><graphic id="pcbi.1007290.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e067" xlink:type="simple"/><mml:math display="block" id="M67"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">X</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula>
This corresponds to the following decoding function:
<disp-formula id="pcbi.1007290.e068"><alternatives><graphic id="pcbi.1007290.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mi>u</mml:mi></mml:msub><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula>
where <italic>ω</italic> represents the specification of the biochemical reaction network which determines <italic>p</italic>(<bold>x</bold>|<italic>u</italic>). Here, <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic>) is assumed to be known, and the likelihood <italic>p</italic>(<bold>x</bold>|<italic>u</italic>) can be calculated using Eqs (<xref ref-type="disp-formula" rid="pcbi.1007290.e029">10</xref>) or (<xref ref-type="disp-formula" rid="pcbi.1007290.e037">17</xref>), for the continuous or discrete time representations, respectively.</p>
<p>One can apply the MAP-decoding based calculation of <inline-formula id="pcbi.1007290.e069"><alternatives><graphic id="pcbi.1007290.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> in two ways. First, when applied over real data <inline-formula id="pcbi.1007290.e070"><alternatives><graphic id="pcbi.1007290.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula>, one can think of the procedure as a proper statistical estimation assuming that the biochemical network model is the correct generative model of the data (with estimation bias arising if it is not). Second, when applied, as we will do in the Results section, over trajectories <inline-formula id="pcbi.1007290.e071"><alternatives><graphic id="pcbi.1007290.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> generated using exact stochastic simulation from the biochemical network model in the large <italic>N</italic> limit, this procedure is a Monte Carlo approximation to the information lower bound.</p>
<p>Note that even though the MAP decoder is optimal, it does not follow that <inline-formula id="pcbi.1007290.e072"><alternatives><graphic id="pcbi.1007290.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. This is because optimal channel use that realizes <italic>I</italic>(<bold>X</bold>; <italic>U</italic>) may need to employ block codes, where a <italic>sequence</italic> of inputs is encoded jointly into a <italic>sequence</italic> of trajectories, which is later also jointly decoded. In contrast, the decoding bound <inline-formula id="pcbi.1007290.e073"><alternatives><graphic id="pcbi.1007290.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> relies on one-shot use of the channel: a single input <italic>u</italic> is transduced into <bold>x</bold> which can immediately be decoded back into the estimate of the input, <inline-formula id="pcbi.1007290.e074"><alternatives><graphic id="pcbi.1007290.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, on the basis of which the cell might make a decision. For many biological situations, this decoding setup should be more appropriate than the exact information calculation, as cells often need to react to stimuli as rapidly as possible in order to gain a selective advantage. Furthermore, it is difficult to conceive of biologically realistic encoders that would transform inputs into a block code in order to use the biochemical network channels optimally.</p>
</sec>
<sec id="sec013">
<title>Maximum a posteriori upper bound (UB)</title>
<p>Given that the optimal MAP decoding does not necessarily reach the exact mutual information, it is reasonable to ask how large the gap is between these two quantities. For discrete inputs, classic work in information theory proved a number of upper bounds on this gap when the channel is known [<xref ref-type="bibr" rid="pcbi.1007290.ref046">46</xref>], with the Feder-Merhav bound perhaps being the most well known [<xref ref-type="bibr" rid="pcbi.1007290.ref034">34</xref>]; Feder-Merhav provides an upper bound on the channel capacity given the overall probability of error in MAP decoding. In a separate work [<xref ref-type="bibr" rid="pcbi.1007290.ref035">35</xref>], we computed a new upper bound on information <italic>I</italic><sub>UB</sub>(<italic>U</italic>; <bold>X</bold>) that is consistent with not just the overall probability of error as in Feder-Merhav bound, but with the full confusion matrix <italic>ϵ</italic> obtained from optimal MAP decoding, and showed that the new bound is tight.</p>
<p>Our self-contained derivation [<xref ref-type="bibr" rid="pcbi.1007290.ref035">35</xref>] gives the following result
<disp-formula id="pcbi.1007290.e075"><alternatives><graphic id="pcbi.1007290.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e075" xlink:type="simple"/><mml:math display="block" id="M75"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>UB</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
where <inline-formula id="pcbi.1007290.e076"><alternatives><graphic id="pcbi.1007290.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>≠</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and functions <italic>ϕ</italic> and <italic>α</italic> can be expressed with the help of the floor and ceiling functions as:
<disp-formula id="pcbi.1007290.e077"><alternatives><graphic id="pcbi.1007290.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:mo>⌊</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⌋</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mspace width="2pt"/><mml:mo>⌈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⌉</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula>
<disp-formula id="pcbi.1007290.e078"><alternatives><graphic id="pcbi.1007290.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e078" xlink:type="simple"/><mml:math display="block" id="M78"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mo>⌊</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⌋</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⌈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⌉</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula>
This bound applies irrespectively of how the response trajectory space is represented (continuous or discrete, possibly of dimensionality much larger than that of the random variable <italic>U</italic>), since it is stated solely in terms of the input variable <italic>U</italic> and its MAP decode, <inline-formula id="pcbi.1007290.e079"><alternatives><graphic id="pcbi.1007290.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="sec014">
<title>Decoding–based information estimators</title>
<sec id="sec015">
<title>Support Vector Machine (SVM) lower bound estimator</title>
<p>The first model-free decoding approach we consider is based on classifiers called Support Vector Machines (SVMs). To begin we consider two possible inputs, <italic>q</italic> = 2. We define a decoding function <italic>F</italic><sub><italic>ω</italic></sub> by means of a helper function <italic>f</italic><sub><italic>ω</italic></sub>(<bold>x</bold>), such that <italic>F</italic><sub><italic>ω</italic></sub>(<bold>x</bold>) = <italic>u</italic><sup>(1)</sup> if sign <italic>f</italic><sub><italic>ω</italic></sub>(<bold>x</bold>) = −1 and <italic>F</italic><sub><italic>ω</italic></sub>(<bold>x</bold>) = <italic>u</italic><sup>(2)</sup> otherwise. Here,
<disp-formula id="pcbi.1007290.e080"><alternatives><graphic id="pcbi.1007290.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e080" xlink:type="simple"/><mml:math display="block" id="M80"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula>
where <inline-formula id="pcbi.1007290.e081"><alternatives><graphic id="pcbi.1007290.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the so-called “kernel function” to be defined below, <italic>b</italic> is the bias constant, <italic>N</italic><sub>t</sub> is the number of samples in <inline-formula id="pcbi.1007290.e082"><alternatives><graphic id="pcbi.1007290.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>train</mml:mtext></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007290.e083"><alternatives><graphic id="pcbi.1007290.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e083" xlink:type="simple"/><mml:math display="inline" id="M83"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are obtained by solving standard SVM equations:
<disp-formula id="pcbi.1007290.e084"><alternatives><graphic id="pcbi.1007290.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e084" xlink:type="simple"/><mml:math display="block" id="M84"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mtext>t</mml:mtext></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ξ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mtext>t</mml:mtext></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>t</mml:mtext></mml:msub></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>C</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mtext>t</mml:mtext></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula>
subject to
<disp-formula id="pcbi.1007290.e085"><alternatives><graphic id="pcbi.1007290.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e085" xlink:type="simple"/><mml:math display="block" id="M85"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2.em"/><mml:mtext>for</mml:mtext><mml:mspace width="5.0pt"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula>
<italic>y</italic><sub><italic>i</italic></sub> = −1 whenever the input corresponding to the <italic>i</italic>-th trajectory in the training set, <bold>x</bold><sub><italic>i</italic></sub>, is <italic>u</italic><sup>(1)</sup>, i.e., <italic>u</italic><sub><italic>i</italic></sub> = <italic>u</italic><sup>(1)</sup>; similarly <italic>y</italic><sub><italic>i</italic></sub> = +1 whenever the corresponding input is <italic>u</italic><sup>(2)</sup>, i.e., <italic>u</italic><sub><italic>i</italic></sub> = <italic>u</italic><sup>(2)</sup>. <italic>C</italic> is a positive regularization constant. Together, the parameters of the decoding function are <italic>ω</italic> = {<italic>b</italic>, <italic>α</italic>, <italic>ξ</italic>, <italic>C</italic>}.</p>
<p>To prevent overfitting and set the regularization parameter <italic>C</italic> using cross-validation, we split the full dataset <inline-formula id="pcbi.1007290.e086"><alternatives><graphic id="pcbi.1007290.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e086" xlink:type="simple"/><mml:math display="inline" id="M86"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> into training data, <inline-formula id="pcbi.1007290.e087"><alternatives><graphic id="pcbi.1007290.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>train</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>, that consists of <italic>N</italic><sub><italic>t</italic></sub> (here ~ 70% of the total, <italic>N</italic>) of labeled sample trajectories, chosen randomly but balanced across different inputs <italic>u</italic>; the remaining 30% of the data constitutes the testing data, <inline-formula id="pcbi.1007290.e088"><alternatives><graphic id="pcbi.1007290.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>. Parameters <italic>ω</italic> are estimated only over <inline-formula id="pcbi.1007290.e089"><alternatives><graphic id="pcbi.1007290.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e089" xlink:type="simple"/><mml:math display="inline" id="M89"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>train</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>, after which the error matrix <italic>ϵ</italic> and the corresponding information estimate <inline-formula id="pcbi.1007290.e090"><alternatives><graphic id="pcbi.1007290.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>SVM</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> of <xref ref-type="disp-formula" rid="pcbi.1007290.e063">Eq (26)</xref> are evaluated solely over <inline-formula id="pcbi.1007290.e091"><alternatives><graphic id="pcbi.1007290.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>. The test/train split procedure can be repeated multiple times to compute the mean and the bootstrapped error bar estimate for the information estimator, <italic>I</italic><sub>SVM</sub> [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>].</p>
<p>When we apply SVM decoding, we are still free to choose the kernel function. Here, we focus on two possibilities:
<list list-type="bullet">
<list-item><p><bold>Linear kernel</bold>, <italic>k</italic>(<italic>x</italic>, <italic>x</italic>′) = <italic>x</italic><sup><italic>T</italic></sup> <italic>x</italic>′. The information estimate is based on a linear classifier that can learn to distinguish responses that differ in their conditional means, <italic>μ</italic>(<italic>u</italic>), but will result in close to chance performance if they don’t. This is the simplest model-free decoding estimator and is thus a useful benchmark for more complex, non-linear decoders.</p></list-item>
<list-item><p><bold>Radial basis functions kernel</bold>, <inline-formula id="pcbi.1007290.e092"><alternatives><graphic id="pcbi.1007290.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. This model-free decoder can be sensitive both to difference in the conditional means as well as higher-order statistics, e.g., the covariance matrix. Parameter <italic>σ</italic> is set via cross-validation to maximize the performance.</p></list-item>
</list></p>
<p>For multiclass classification we use a decision-tree SVM classification method [<xref ref-type="bibr" rid="pcbi.1007290.ref047">47</xref>], also called Dendrogram-SVM (DSVM) [<xref ref-type="bibr" rid="pcbi.1007290.ref048">48</xref>]. To translate the multi-class classification into the canonical binary classification problem, this method uses hierarchical bottom-up clustering to define the structure of the graph, on which a binary classification is performed using SVMs at each graph node.</p>
</sec>
<sec id="sec016">
<title>Gaussian decoder (GD) lower bound estimator</title>
<p>In this model-free estimation, we revisit the assumption that the (discretely sampled) output trajectories <bold>x</bold> given input <italic>u</italic> can be approximated with a multivariate Gaussian distribution, <xref ref-type="disp-formula" rid="pcbi.1007290.e041">Eq (19)</xref>. The decoding function is then
<disp-formula id="pcbi.1007290.e093"><alternatives><graphic id="pcbi.1007290.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e093" xlink:type="simple"/><mml:math display="block" id="M93"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mi>u</mml:mi></mml:msub><mml:mspace width="2pt"/><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo mathvariant="bold">Σ</mml:mo><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mi>p</mml:mi><mml:mi>U</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula>
Here, parameters <italic>ω</italic> consist of conditional means and (possibly regularized) covariance matrices of the Gaussian distributions that need to be estimated from data, following the test/train procedure analogous to SVM decoding.</p>
<p>This method can be used with different parametric multivariate probability density functions replacing the multivariate Gaussian in <xref ref-type="disp-formula" rid="pcbi.1007290.e093">Eq (35)</xref>, with choices that approximate the statistics of the data (and thus the CME-derived distribution) better providing tighter lower bound estimates, <inline-formula id="pcbi.1007290.e094"><alternatives><graphic id="pcbi.1007290.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>GD</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, of the exact information. By analogy with the exact MAP decoding using CME-derived response distribution, this method can also be understood as maximum a posteriori decoder but using approximate response distributions that need to be estimated from data. Here we decided to use the Gaussian distributions because they are the most unstructured (random) distributions based on measured first- and second-order statistics of the data. GD decoder thus should be able to discriminate various inputs if their responses differs either in the response mean or response covariance.</p>
</sec>
<sec id="sec017">
<title>Neural network (NN) lower bound estimator</title>
<p>Artificial neural networks, first introduced by the neurophysiologist Waren McCulloch and the mathematician Walter Pitts in 1943 [<xref ref-type="bibr" rid="pcbi.1007290.ref049">49</xref>], are nowadays the method of choice for classification that generally outperforms alternative machine learning techniques on very large and complex problems. Here we use one of the simplest neural networks, called the multi-layer perceptron (MLP). MLP is composed of layers of linear-threshold units (or LTUs), where each LTU computes a weighted sum of its inputs <italic>z</italic> = <italic>ω</italic><sup><italic>T</italic></sup> <bold>x</bold>, then applies an activation function to that sum and outputs the result <italic>y</italic> = <italic>h</italic>(<italic>z</italic>) = <italic>h</italic>(<italic>ω</italic><sup><italic>T</italic></sup> <bold>x</bold> + <italic>ω</italic><sub>0</sub>). Using a single LTU amounts to training a binary linear classifier by learning the weights <italic>ω</italic>. As with linear SVM, such classifier only has a limited expressive power [<xref ref-type="bibr" rid="pcbi.1007290.ref050">50</xref>], which can, however, be extended by stacking layers of LTUs so that outputs of the first layer are inputs to the second layer etc.</p>
<p>For illustrative purposes we choose for our decoding function <italic>F</italic><sub><italic>ω</italic></sub>(<bold>x</bold>) a fully connected neural network with two hidden layers (with 300 and 200 LTUs, respectively) that uses the exponential activation function with <italic>α</italic> = 1:
<disp-formula id="pcbi.1007290.e095"><alternatives><graphic id="pcbi.1007290.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e095" xlink:type="simple"/><mml:math display="block" id="M95"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mi>z</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>z</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mi>z</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>For training, we used He-initialization, which initializes the weights with a random number from a normal distribution with zero mean and standard deviation <inline-formula id="pcbi.1007290.e096"><alternatives><graphic id="pcbi.1007290.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>n</italic><sub><italic>in</italic></sub> is the number of inputs to units in a particular layer [<xref ref-type="bibr" rid="pcbi.1007290.ref051">51</xref>], and Adam optimization with batch normalization and drop-out regularization [<xref ref-type="bibr" rid="pcbi.1007290.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref052">52</xref>]. As before, we trained the neural network on <inline-formula id="pcbi.1007290.e097"><alternatives><graphic id="pcbi.1007290.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>train</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>, followed by the evaluation of the error matrix <italic>ϵ</italic> and of the corresponding information estimate, <inline-formula id="pcbi.1007290.e098"><alternatives><graphic id="pcbi.1007290.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, from <xref ref-type="disp-formula" rid="pcbi.1007290.e063">Eq (26)</xref>, over <inline-formula id="pcbi.1007290.e099"><alternatives><graphic id="pcbi.1007290.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>. We emphasize that the detailed architecture of the neural network we selected here is not relevant for other estimation cases; in general, the architecture is completely adjustable to the problem at hand and should be selected depending on the size of the training dataset. The only selection criterion is the network performance on test data, with better performing networks for a given dataset typically providing tighter information estimates.</p>
</sec>
</sec>
</sec>
<sec id="sec018" sec-type="results">
<title>Results</title>
<sec id="sec019">
<title>Information estimation on simulated data</title>
<p>We start by considering three simple chemical reaction networks for which we can obtain exact information values using the model-based approach outlined in Section <italic>Exact information calculations for fully observed reaction networks</italic>. This will allow us to precisely assess the performance of decoding-based model-free estimates, and systematically study the effects of time discretization, the number of sample trajectories, and the number of distinct discrete inputs, <italic>q</italic>.</p>
<p>The three examples are all instances of a simple molecular birth-death process, where molecules of <inline-formula id="pcbi.1007290.e100"><alternatives><graphic id="pcbi.1007290.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are created and destroyed with rates <italic>α</italic> and <italic>β</italic>, respectively:
<disp-formula id="pcbi.1007290.e101"><alternatives><graphic id="pcbi.1007290.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e101" xlink:type="simple"/><mml:math display="block" id="M101"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover><mml:mo>→</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mover><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mover><mml:mo>→</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mover><mml:mi>⌀</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula>
The reaction rates, <italic>α</italic> and <italic>β</italic>, will depend in various ways on the input, <italic>U</italic>, and possibly time, as specified below. Given an initial condition, <italic>x</italic>(<italic>t</italic> = 0), the production and degradation reactions generate continuous-time stochastic trajectories, <italic>x</italic>(<italic>t</italic>), recording the number of molecules of <inline-formula id="pcbi.1007290.e102"><alternatives><graphic id="pcbi.1007290.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> at every time <italic>t</italic> ∈ [0, <italic>T</italic>], according to the Chemical Master <xref ref-type="disp-formula" rid="pcbi.1007290.e015">Eq (3)</xref>. These trajectories, or their discretized representations, are considered as the “outputs” of the example reaction networks, defining the mutual information <italic>I</italic>(<italic>X</italic>; <italic>U</italic>) that we wish to compute. In all three examples we start with the simplest case, where the random variable <italic>U</italic> can only take on two possible values, <italic>u</italic><sup>(1)</sup> and <italic>u</italic><sup>(2)</sup>, with equal probability, <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic><sup>(1)</sup>) = <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic><sup>(2)</sup>) = 0.5.
<list list-type="bullet">
<list-item><p><bold>Example 1</bold>. In this case, <italic>x</italic>(<italic>t</italic> = 0) = 0, <italic>β</italic> = 0.01, independent of the input <italic>U</italic>, and the production rate depends on the input as <italic>α</italic>(<italic>u</italic><sup>(1)</sup>) = 0.1, <italic>α</italic>(<italic>u</italic><sup>(2)</sup>) = 0.07. Here, the steady state is given by Poisson distribution with mean number of molecules 〈<italic>x</italic>(<italic>t</italic> → ∞)〉 = <italic>α</italic>/<italic>β</italic>. Steady-state is approached exponentially with the timescale that is the inverse of the degradation rate, <italic>β</italic><sup>−1</sup>. These dynamics stylize a class of frequently observed biochemical responses where the steady-state mean expression level encodes the relevant input value. Even if the stochastic trajectories for the two possible inputs are noisy as shown in <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2A</xref>, we expect that the mutual information will climb quickly with the duration of the trajectory, <italic>T</italic>, since (especially in steady state) more samples provide direct evidence about the relevant input already at the level of the mean trajectories.</p></list-item>
<list-item><p><bold>Example 2</bold>. In this case, <italic>x</italic>(<italic>t</italic> = 0) = 0, <italic>β</italic> = 0.01, independent of the input <italic>U</italic>, and the production rate depends on the input as <italic>α</italic>(<italic>u</italic><sup>(1)</sup>, <italic>t</italic>) = 0.1, <italic>α</italic>(<italic>u</italic><sup>(2)</sup>, <italic>t</italic>) = 0.05 for all <italic>t</italic> &lt; 1000, while for <italic>t</italic> ≥ 1000 the production rate is very small and independent of input, <italic>α</italic>(<italic>u</italic>, <italic>t</italic>) = 5 ⋅ 10<sup>−4</sup>. In the early period, this network approaches input-dependent steady state with means whose differences are larger than in Example 1, but the difference decays away for <italic>t</italic> &gt; 1000 as the network settles towards vanishingly small activity for both inputs, as shown in <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2B</xref>. These dynamics stylize a class of transient biochemical responses that are adapted away even if the input state persists. In this case, lengthening the observation window <italic>T</italic> will not provide significant increases in information.</p></list-item>
<list-item><p><bold>Example 3</bold>. In this case, <italic>x</italic>(<italic>t</italic> = 0) = 10. All reaction rates depend on the input, <italic>α</italic>(<italic>u</italic><sup>(1)</sup>) = 0.1, <italic>α</italic>(<italic>u</italic><sup>(2)</sup>) = 0.05, <italic>β</italic>(<italic>u</italic><sup>(1)</sup>) = 0.01, <italic>β</italic>(<italic>u</italic><sup>(2)</sup>) = 0.005, and are chosen so that the mean 〈<italic>x</italic>(<italic>t</italic>)〉 = 10 is constant across time and equal for both conditions, as shown in <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2C</xref>. In this difficult case, inputs cannot be decoded at the level of mean responses but require sensitivity to at least second-order statistics of the trajectories. Specifically, signatures of the input are present in the autocorrelation function for <italic>x</italic>: the timescale of fluctuations and mean-reversion is two-fold faster for <italic>u</italic><sup>1</sup> than <italic>u</italic><sup>2</sup>. While this case is not frequently observed in biological systems, it represents a scenario where, by construction, no information about the input is present at the level of single concentration values and having access to the trajectories is essential. Because there is no difference in the mean response, we expect linear decoding methods to provide zero bits of information about the input. This case is also interesting because of the recent focus on pulsatile stationary-state dynamics in biochemical networks [<xref ref-type="bibr" rid="pcbi.1007290.ref053">53</xref>]. These pulses, reported for transcription factors such as Msn2, NF-<italic>κ</italic>B, p53, etc., occur stochastically and, when averaged over a population of desynchronized cells, can yield a flat and featureless mean response. Information about the stimulus could, nevertheless, be encoded in either the frequency, amplitude, or other shape parameters of the pulses. While a generative description of such pulsatile dynamics goes beyond a birth-death process considered here, from the viewpoint of decoding, both pulsatile signaling and our example present an analogous problem, where the mean response is not informative about the applied input.</p></list-item>
</list></p>
<fig id="pcbi.1007290.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Example biochemical reaction networks and their behavior.</title>
<p>Three example birth-death processes, specified by the reactions in the top row for each of the two possible inputs (<italic>u</italic><sup>(1)</sup> in blue, <italic>u</italic><sup>(2)</sup> in red), stylize simple behaviors of biochemical signaling networks. <bold>(A)</bold> Input is encoded in both the transient approach to steady state and the steady state value. <bold>(B)</bold> Input is encoded in the magnitude of the transient response which is subsequently adapted away. <bold>(C)</bold> Input is encoded only at the level of temporal correlations of the response trajectory. Bottom row shows example trajectories generated using the Stochastic Simulation Algorithm for the copy number of <inline-formula id="pcbi.1007290.e103"><alternatives><graphic id="pcbi.1007290.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> molecules, <italic>t</italic> ∈ [0, 2000], for each network and the two possible inputs (light blue, light red); while plotted as a connected line for clarity, each trajectory represents molecular counts and is thus a step-wise function taking on only integer or zero values. Dark blue, red lines show the conditional means over <italic>N</italic> = 1000 trajectory realizations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g002" xlink:type="simple"/>
</fig>
<p>Before proceeding, we note that our examples are not intended to be realistic models of intracellular biochemical networks, but are chosen here for their simplicity and analytical tractability, in order to benchmark model-free estimators against known “gold truth” standard. In particular, while our examples include intrinsic noise due to the stochasticity of biochemical reactions at low concentration, they do not include extrinsic noise or cell-to-cell variability which, in some systems, is known to importantly or even dominantly contribute to the total variability in the response [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref054">54</xref>]. The presence of such additional sources of variation by no means makes the model-free estimators inapplicable, as we show in <xref ref-type="supplementary-material" rid="pcbi.1007290.s001">S1 Fig</xref> where we study estimator performance in the simplest Example 1 model that includes cell-to-cell variability; it solely prevents us from comparing their performance to a tractably-computable MAP decoder result.</p>
<sec id="sec020">
<title>Exact information approximations and bounds for continuous and discrete trajectories</title>
<p>Armed with the full stochastic model for the three example reaction networks, we can compute the mutual information, <inline-formula id="pcbi.1007290.e104"><alternatives><graphic id="pcbi.1007290.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, between the continuous-time stochastic trajectories and the (binary) input variable <italic>U</italic>, following <xref ref-type="disp-formula" rid="pcbi.1007290.e033">Eq (14)</xref>. This result depends essentially on the length of the observed trajectory, <italic>t</italic> ∈ [0, <italic>T</italic>], since <italic>T</italic> controls the number of observed reaction events and thus the accumulation of evidence for one or the other alternative input. As the approximation is implemented by Monte-Carlo averaging of exact log probabilities for the response trajectories, its variance will depend on the number of sample trajectories generated by the SSA. Because these information values will represent the “gold truth” against which to evaluate subsequent estimators, we choose a large number of <italic>N</italic> = 1000 trajectory realizations per input condition, and verify the tightness of the exact Monte Carlo approximation by computing the standard deviation over 20 independent re-runs of the approximation procedure.</p>
<p><xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> shows how the exact Monte Carlo information computation depends on the trajectory duration, <italic>T</italic>, for each of the three example cases. As expected, the information increases monotonically with <italic>T</italic> towards the theoretically maximal value of 1 bit, corresponding to perfect information about two <italic>a priori</italic> equally likely input conditions. The exact shape of the information curve depends on the shape of the mean trajectory, as well as on its variance and higher-order statistics: for example, even though the two inputs for Example 1 are most distinct at the level of mean responses for later <italic>T</italic> values, the noise is higher compared to Example 2, such that at <italic>T</italic> = 2000 there is more total information in trajectories of Example 2 than Example 1. Conversely, even though the trajectories in Example 3 do not differ at the level of the mean at all, they still carry all information about the relevant input once sufficiently long trajectories can be observed (and assuming full knowledge of the reaction network is available).</p>
<fig id="pcbi.1007290.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Information about inputs encoded by complete response trajectories of the example biochemical reaction networks.</title>
<p>Exact Monte Carlo approximation for the information, <inline-formula id="pcbi.1007290.e105"><alternatives><graphic id="pcbi.1007290.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, is shown for Example 1 <bold>(A)</bold>, Example 2 <bold>(B)</bold>, and Example 3 <bold>(C)</bold> from <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2</xref> in dashed dark gray line; error bars are standard deviations across 20 replicate estimations, each computed over <italic>N</italic> = 1000 independently generated sample trajectories per input condition. Information is plotted as a function of the trajectory duration, <italic>T</italic>; yellow vertical line indicates <italic>T</italic> = 2000 as a representative duration used in further analyses below, at which most of the information about input is in principle available from the response trajectories of our systems. <inline-formula id="pcbi.1007290.e106"><alternatives><graphic id="pcbi.1007290.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>MAP</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (dashed black line) is the optimal decoding lower bound, and <inline-formula id="pcbi.1007290.e107"><alternatives><graphic id="pcbi.1007290.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:msubsup><mml:mi>I</mml:mi><mml:mtext>UB</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> (dashed light gray) is the upper bound on the information, computed by applying <xref ref-type="disp-formula" rid="pcbi.1007290.e075">Eq (29)</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g003" xlink:type="simple"/>
</fig>
<p>One can similarly compute the Bayes-optimal or MAP decoding bound using <xref ref-type="disp-formula" rid="pcbi.1007290.e068">Eq (28)</xref> for continuous trajectories. This quantifies the ultimate accuracy limit with which each single observed trajectory can be decoded into the input that gave rise to it. As demonstrated in <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> in dashed black line and consistent with the Data Processing Inequality requirements outlined in the Methods, <inline-formula id="pcbi.1007290.e108"><alternatives><graphic id="pcbi.1007290.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e108" xlink:type="simple"/><mml:math display="inline" id="M108"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>MAP</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mtext>exact</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Equality is not reached because the optimal use of the channel requires block coding schemes, in contrast to our setting where different inputs are sequentially sent through the biochemical network and immediately decoded. The observed gap between the MAP optimal decoding estimate and the true information appears to be small in each of the three cases; one can upper-bound the gap itself by an improvement over the standard Feder-Merhav calculation following Section <italic>Maximum a posteriori upper bound</italic>. While the resulting upper bound on information, <italic>I</italic><sub>UB</sub>, is not tight in this case, it nevertheless provides a control of how far optimal decoding could be from the true information estimate, a question that has repeatedly worried the neuroscience community facing similar problems [<xref ref-type="bibr" rid="pcbi.1007290.ref028">28</xref>]. It is worth noting that if MAP decoder can tractably be computed, so can the upper bound, irrespective of the dimensionality of the space of responses, <italic>X</italic>.</p>
<p><xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> summarizes the absolute limits on information transmission and optimally decodable information, for each of our three example networks. These values are limits inasmuch as they assume that every reaction event can be observed and recorded with infinite temporal precision, and that the encoding stochastic process is perfectly known. Qualitatively, the curves in <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> show the same sigmoidal behavior: as the time horizon <italic>T</italic> is increased, the trajectories contain signatures of more and more reaction events that are input-specific, leading to a monotonic increase in the information, which must saturate at 1 bit (the total entropy of a two-state equally likely stimulus). The fact that we near saturation as the time horizon <italic>T</italic> is increased suggests that later time points add zero (conditional) mutual information. When faced with real data, this could be used as a criterion to determine the relevant response timescale <italic>T</italic>, by testing when the transfer entropy (i.e., mutual information between the subsequent trajectory piece and the input conditioned on the prior segment of the trajectory of duration <italic>T</italic>) becomes zero [<xref ref-type="bibr" rid="pcbi.1007290.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref056">56</xref>].</p>
<p>Examining the information increase specific to each example, we see 1 bit is reached more quickly in Example 2 than in Example 1 because of a larger difference in reaction rate parameters for both inputs in Example 2 relative to Example 1 in the period <italic>T</italic> &lt; 1000. In contrast, in the period from <italic>T</italic> &gt; 1000 until the <italic>T</italic> = 2000 at vertical yellow line, reaction rates for both inputs in Example 2 are the same, leading to essentially no accumulation of new information and a flat information curve, in contrast to Example 1. Finally, Example 3 shows that a model-based decoder can also make optimal use of the higher-order statistics of response trajectories even when the means under two conditions are the same, to extract the full bit of information before the <italic>T</italic> = 2000 cutoff.</p>
<p>While it is interesting to contemplate whether biological systems themselves could compute with or act on singular, precisely-timed reaction events and thus make optimal use of the resulting channel capacity (mimicking the debate between spike timing code and spike rate code in neuroscience), our primary focus here is to estimate information flows from experimental data. Typically, experiments record the state of the system—e.g., concentration of signaling molecules—in discretely sampled time. To explore the effects of time discretization, we first fix the observation length for our trajectories to <italic>T</italic> = 2000, sufficiently long that the trajectories in principle contain more than 90% of the theoretically maximal information for each of the three example cases. We then resample the trajectories on a grid of <italic>d</italic> equally spaced time points, as illustrated in <xref ref-type="fig" rid="pcbi.1007290.g004">Fig 4A</xref>.</p>
<fig id="pcbi.1007290.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Information loss due to temporal sampling.</title>
<p><bold>(A)</bold> Schematic representation of the resampling of a continuous-time response trajectory (left) at <italic>d</italic> = 14 (middle) or <italic>d</italic> = 41 (right) equally spaced time points. Resampled response trajectories are represented as <italic>d</italic>-dimensional real vectors, <inline-formula id="pcbi.1007290.e109"><alternatives><graphic id="pcbi.1007290.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, for the case of a single output chemical species. <bold>(B–D)</bold> Exact Monte Carlo information approximations for discrete trajectories, <italic>I</italic><sub>exact</sub>(<bold>X</bold>; <italic>U</italic>) (dark solid gray), optimal decoding lower bound, <inline-formula id="pcbi.1007290.e110"><alternatives><graphic id="pcbi.1007290.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e110" xlink:type="simple"/><mml:math display="inline" id="M110"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (dark solid black), and the upper bound, <italic>I</italic><sub>UB</sub> (light solid gray) are plotted as a function of <italic>d</italic>. Continuous-time limits from <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> are shown as horizontal lines: <inline-formula id="pcbi.1007290.e111"><alternatives><graphic id="pcbi.1007290.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (dashed dark gray), <inline-formula id="pcbi.1007290.e112"><alternatives><graphic id="pcbi.1007290.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>MAP</mml:mtext></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (dashed black). Error bars as in <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g004" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1007290.g004">Fig 4B–4D</xref> compare the exact Monte Carlo information approximation for discrete trajectories, <italic>I</italic><sub>exact</sub>(<bold>X</bold>; <italic>U</italic>), MAP lower bound for discrete trajectories, <inline-formula id="pcbi.1007290.e113"><alternatives><graphic id="pcbi.1007290.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e113" xlink:type="simple"/><mml:math display="inline" id="M113"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the corresponding upper bound, <italic>I</italic><sub>UB</sub>, to the theoretical limits from <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref> obtained using continuous trajectories. In line with the chain of inequalities in <xref ref-type="disp-formula" rid="pcbi.1007290.e057">Eq (25)</xref>, information in discretely resampled trajectories is lower than the true information in continuous trajectories, but converges to the true value as <italic>d</italic> → ∞. In particular, once the discretization timestep <italic>T</italic>/<italic>d</italic> is much lower than the inverse of the fastest reaction rate in the system, discretization should incur no significant loss of information. In practice, however, high sampling rate (large <italic>d</italic>) limit has significant drawbacks: first, it is technically difficult to take snapshots of the system at such high rates (e.g., due to fluorophore bleaching); second, the fast dynamics of the reaction network may be low-pass filtered by the readout process (e.g., due to fluorophore maturation time, or slower downstream reaction kinetics); and third, for model-free approaches high <italic>d</italic> implies that decoders need to be learned over input spaces of high dimensionality, which could be infeasible given a limited number of experimentally recorded response trajectories. In previous work [<xref ref-type="bibr" rid="pcbi.1007290.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref057">57</xref>], trajectories were typically represented as <italic>d</italic> ≈ 1 ∼ 100 dimensional vectors, which in our examples would capture ∼ 80% or more of the theoretically available information. It is likely that this can be improved further with smart positioning of the sampling points and that not all theoretically available information could actually be accessed by the organism itself, suggesting that typically used discretization approaches have the potential to capture most of the relevant information in the responses. What is important for the analysis at hand is that given the dimensionality <italic>d</italic> of the discretized response trajectories, MAP decoder is guaranteed to reach the minimal decoding error among all possible decoders, and will turn out to be a relevant benchmark, by yielding the highest information, <inline-formula id="pcbi.1007290.e114"><alternatives><graphic id="pcbi.1007290.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, in <xref ref-type="fig" rid="pcbi.1007290.g004">Fig 4B–4D</xref> among all decoders considered. In what follows, we will examine how various model-free decoding estimators approach this limit, as a function of <italic>d</italic> and the number of sample trajectories, <italic>N</italic>.</p>
</sec>
<sec id="sec021">
<title>Performance of decoding-based estimators</title>
<p>After establishing our model-based “gold standard” for decoding-based estimators acting on trajectories represented in discretized time, <inline-formula id="pcbi.1007290.e115"><alternatives><graphic id="pcbi.1007290.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e115" xlink:type="simple"/><mml:math display="inline" id="M115"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we turn our attention to the performance comparison between various model-free algorithms. The results are summarized in <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5</xref>, which shows how estimator accuracy depends on the dimensionality of the problem, <italic>d</italic>, and the number <italic>N</italic>, of sample trajectories per input condition.</p>
<fig id="pcbi.1007290.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Performance of decoding-based estimators depends on the dimensionality of the response trajectories and on the number of response trajectory samples.</title>
<p>Performance of various model-free decoding estimators (colored lines) for Examples 1 <bold>(A, D)</bold>, 2 <bold>(B, E)</bold>, 3 <bold>(C, F)</bold>, respectively, compared to the MAP bound, <italic>I</italic><sub>MAP</sub> (black line), as a function of input trajectory dimension, <italic>d</italic> (at fixed <italic>N</italic> = 1000) in A, B, C; or as a function of the number of samples, <italic>N</italic>, per input condition (at fixed <italic>d</italic> = 100) in D, E, F. Error bars are std over 20 replicate estimations. Decoding estimators: linear SVM, <italic>I</italic><sub>SVM(lin)</sub> (orange); radial basis functions SVM, <italic>I</italic><sub>SVM(rbf)</sub> (blue); the Gaussian decoder with diagonal regularization (see <xref ref-type="supplementary-material" rid="pcbi.1007290.s002">S2 Fig</xref> for the effects of covariance matrix regularization and signal filtering on Gaussian decoder estimates), <italic>I</italic><sub>GD</sub> (yellow); multi-layer perceptron neural network, <italic>I</italic><sub>NN</sub> (green). Dashed vertical orange line marks the <italic>d</italic> ≤ 100 regime typical of current experiments. Note that while the amount of information must in principle increase monotonically with <italic>d</italic>, the amount that decoders can actually extract given a limited number of samples, <italic>N</italic>, has no such guarantee. The decrease, at high <italic>d</italic>, in Gaussian decoder information estimate in A, B, C and neural network information estimate in C, happens because the number of parameters of the decoder grows with <italic>d</italic> albeit at fixed number of samples, leading to overfitting that regularization cannot fully compensate, and thus to the consequent loss of performance on the test data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g005" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5</xref> leads us to the following conclusions:
<list list-type="bullet">
<list-item><p><bold>Nonlinear SVM</bold> using the radial basis functions (rbf) kernel performs best for Examples 1 and 2. Regardless of the number of samples, <italic>N</italic>, or the number of time bins, <italic>d</italic>, its estimates are very close to <italic>I</italic><sub>MAP</sub>, especially for the relevant regime <italic>d</italic> ∼ 10 − 100. Even for higher <italic>d</italic>, the estimator shows hardly any overfitting and thus stable performance, a feature we have observed commonly in our numerical explorations. The estimator is sample efficient, typically providing estimates with smallest error bars.</p></list-item>
<list-item><p><bold>Linear SVM</bold> slightly underperforms kernelized SVM on Examples 1 and 2, and—as expected—completely fails on the linearly inseparable Example 3. Interestingly, even though more expressive, kernelized SVM seems to incur no generalization cost relative to linear SVM even at low number of samples. For all examples we tested, kernelized SVM thus appears to be a method of choice; linear SVM, however, is still useful as a benchmark to measure what fraction of the information is linearly decodable from the signal.</p></list-item>
<list-item><p><bold>The Gaussian decoder</bold> has the best performance on Example 3, is competitive for low <italic>d</italic> for Example 1, and does not perform satisfactory for Example 2. As shown in <xref ref-type="supplementary-material" rid="pcbi.1007290.s002">S2 Fig</xref>, regularized estimation of covariance matrix appears crucial for good performance, but smoothing of the originally discrete trajectory does not help. Even with regularization, this estimator is not sample efficient for Example 1: the trajectories are linearly separable without a full estimate of the covariance (as evidenced by the success of the linear SVM), yet the Gaussian decoder requires one to two orders of magnitude more samples to match the linear decoder performance. This drawback turns into a benefit for Example 3: the Gaussian assumption can be viewed as a prior that second-order statistics are important for decoding (which is correct in this case). Kernelized SVM and the neural network, while more general, need to learn from many more training samples to zero in on these features, and fail to reach the Gaussian decoder performance even for <italic>N</italic> = 10<sup>4</sup>. We hypothesized that the failure of the Gaussian decoder on Example 2 is due to the difficulty of the Gaussian approximation to capture the period <italic>T</italic> &gt; 1000 when the mean number of <inline-formula id="pcbi.1007290.e116"><alternatives><graphic id="pcbi.1007290.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is close to zero: here, first, the Gaussian assumption must be strongly violated, and, second, the estimation of (co)variance from finite number of samples is close to singular due to the small number of reaction events in this period. Even though the <italic>T</italic> &gt; 1000 epoch is not informative about the input, a badly conditioned decoder for this epoch can actually adversely affect performance. We confirmed this hypothesis by building the Gaussian decoder restricted to <italic>T</italic> &lt; 1000 that reliably extracted ≥ 0.8 bits of information in Example 2, close to the MAP decoding bound and the performance of SVM-based estimators. We also examined the performance of the Gaussian decoder when the mean steady state number of molecules in Example 2 in <italic>T</italic> &lt; 1000 period is increased from 10 to 20, 50, or 100 by scaling up the production rates, to see consistent increases in decoder performance (which can approach 1 bit, the MAP decoder limit, for <italic>d</italic> ≤ 100 with the same parameters as shown in the main figure). At higher production rates, the signal-to-noise ratio is higher and trajectories in the <italic>T</italic> &lt; 1000 period become more distinguishable, increasing the information; simultaneously, in the <italic>T</italic> &gt; 1000 period, the mean number, although decaying, is also higher, making Gaussian approximation more applicable and preventing the covariance matrix from becoming poorly-conditioned. In sum, Gaussian decoder can provide competitive performance if the mean signal is sufficiently high so that the discretness and positivity of molecular counts is not an issue, and when the covariance matrix is not close to ill-conditioned.</p></list-item>
<list-item><p><bold>Neural network decoder</bold> reaches a comparable performance on Examples 2 and 3 to the SVMs, but fails to be competitive for the simple Example 1. This is most likely because this estimator is sample inefficient, as implied by its continual increase in performance with <italic>N</italic> that did not saturate at highest <italic>N</italic> shown in the figure; we confirmed further growth in performance of neural network decoder reaching (but not saturating) <italic>I</italic><sub>NN</sub> ≈ 0.65 bits at <italic>N</italic> = 10<sup>5</sup>. Given their expressive power, neural network decoders should be viewed as the opposite benchmark to the linear decoders: they have the ability to pick up complex statistical structures but only with a sufficient number of samples. Indeed, as we will see subsequently for applications to real data, neural networks can match and exceed the performance of SVMs. We emphasize that we used a neural network with a fixed architecture for all three examples on purpose, to make results comparable across examples; the performance can likely be improved by optimizing the architecture separately for each estimation problem. We did examine the issue of network architecture in greater detail in <xref ref-type="supplementary-material" rid="pcbi.1007290.s003">S3 Fig</xref>, where we compared 14 architectures (1 one-layer, 12 two-layer, and 1 three-layer) on the difficult Example 3. We found that the simple one-layer network cannot extract any information; the three-layer network and several different two-layer networks can reach, but not exceed, the performance of the architecture used in the main figure given only <italic>N</italic> = 1000 samples, confirming our conclusion that higher performance requires significant increases in training set size independent of the neural network architecture.</p></list-item>
</list></p>
</sec>
<sec id="sec022">
<title>Multilevel information estimation</title>
<p>We next asked whether our conclusions hold also when the space of possible inputs is expanded beyond binary, assuming that <italic>U</italic> can take on <italic>q</italic> distinct values with equal probability, i.e., <italic>p</italic><sub><italic>U</italic></sub>(<italic>u</italic>) = 1/<italic>q</italic>. We focused on Example 2, and constructed cases for <italic>q</italic> = 2, …, 5 such that the production rate <italic>α</italic> for 0 &lt; <italic>T</italic> &lt; 1000 takes on <italic>q</italic> uniformly spaced values between 0 and the maximal rate equal to <italic>α</italic> = 0.1 used in <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2B</xref>. In effect, this “tiles” the original, two-state-input dynamic range uniformly with <italic>q</italic> input states, as illustrated in <xref ref-type="fig" rid="pcbi.1007290.g006">Fig 6A</xref>.</p>
<fig id="pcbi.1007290.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Information estimation for multilevel inputs.</title>
<p><bold>(A)</bold> Extension of Example 2 from <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2B</xref> to <italic>q</italic> = 2, …, 5 discrete inputs. We chose the inputs such that the response for the system at <italic>T</italic> &lt; 1000 converges towards <italic>q</italic> equally spaced levels with the same dynamic range as the original example; dynamics at <italic>T</italic> ≥ 1000 remain unchanged from the original Example 2. <bold>(B)</bold> Model-based information bounds as a function of the number of input levels for trajectories represented as <italic>d</italic> = 100 dimensional vectors: exact Monte Carlo calculation (dark gray), MAP decoding bound (black), upper bound (light gray). <bold>(C)</bold> Performance of model-free estimators, as indicated in the panel, compared to the MAP bound (black). Dashed lines show estimations using <italic>N</italic> = 10<sup>3</sup> sample trajectories per condition, solid lines using <italic>N</italic> = 10<sup>4</sup> samples per condition; in both cases, we show an average over 20 independent replicates, error bars are suppressed for readability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g006" xlink:type="simple"/>
</fig>
<p>Our expectation is that with increasing <italic>q</italic>, the information should increase, but slowly saturate as reliable distinctions between nearby input levels can no longer be made due to the intrinsic biochemical stochasticity. This is indeed what we see in <xref ref-type="fig" rid="pcbi.1007290.g006">Fig 6B</xref>, which shows the exact information, the MAP lower bound and the upper bound. Consistent with our findings for two-input case, SVM using radial basis functions remains the estimator of choice for all values of <italic>q</italic>, followed by the linear SVM and then the neural network decoder, as shown in <xref ref-type="fig" rid="pcbi.1007290.g006">Fig 6C</xref>.</p>
</sec>
<sec id="sec023">
<title>Performance comparisons with model-free information approximations</title>
<p>There exist many algorithms for estimating information directly, without making use of the decoding lower bound. The best known estimator for continuous signals is perhaps the k-nearest-neighbor (knn) estimator [<xref ref-type="bibr" rid="pcbi.1007290.ref039">39</xref>]. We have also introduced estimators based on parametric assumptions about the response distribution, such as the Gaussian approximation (Section <italic>Model–free information estimators</italic>); both belong in the family of binless approximations, which act directly on real-valued response vectors. In contrast, binning approximations first discretize the responses <bold>X</bold>. The simplest such approach is perhaps the direct estimator of information or entropy [<xref ref-type="bibr" rid="pcbi.1007290.ref005">5</xref>], and a good review is provided in Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref004">4</xref>]. We evaluated the performance of the Gaussian approximation to find that it can systematically overshoot the true information with a bias that is difficult to assess (<xref ref-type="supplementary-material" rid="pcbi.1007290.s004">S4 Fig</xref>); this appears to happen also in the regime where the biochemical noise should be small (relative to the mean), and the stochastic dynamics should be describable in terms of Langevin approximations with the resulting Gaussian response distributions. These approximations converge to the true solution in terms of their first and second moments, yet do not seem to lead to unbiased estimate for the entropies and thus the mutual information. In contrast to the Gaussian decoder, Gaussian approximation should not be used without a better understanding of its bias and applicability.</p>
<p>We therefore decided to focus on the comparison of decoding estimators with knn, which has been used previously on data from biochemical signaling networks [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>]. The results are shown in <xref ref-type="fig" rid="pcbi.1007290.g007">Fig 7</xref>. K-nearest-neighbors performs well on the easy Example 1, and suffers drastic performance drop for Example 2, while crashing catastrophically by reporting negative values in Example 3. We reasoned that part of the difficulty may be the fact that synthetic trajectories for our Examples are defined over non-negative whole numbers only, whereas the knn assumes real valued vectors. This is confirmed by <xref ref-type="supplementary-material" rid="pcbi.1007290.s005">S5 Fig</xref> which shows that the knn performance can be substantially improved by adding a small amount of gaussian iid noise to every component of the response trajectory vectors, <bold>X</bold>. This restores the knn performance in Example 2 close to that of the SVM-based estimators, but still produces close-to-zero bits of information for Example 3.</p>
<fig id="pcbi.1007290.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of decoding-based and knn information estimators.</title>
<p>Information estimates for decoding-based (color bars) and knn (gray bar) algorithms (here we set <italic>k</italic> = 1, for further details on knn estimation, including varying <italic>k</italic>, see <xref ref-type="supplementary-material" rid="pcbi.1007290.s005">S5 Fig</xref>). Note that knn is not a decoding estimator and thus could exceed <inline-formula id="pcbi.1007290.e117"><alternatives><graphic id="pcbi.1007290.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>MAP</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>U</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (shown as a horizontal black line for each of the three example cases) to approach the exact <italic>I</italic><sub>exact</sub>(<bold>X</bold>; <italic>U</italic>). Here we use trajectories discretized over <italic>d</italic> = 100 time bins, and <italic>N</italic> = 10<sup>4</sup> trajectory samples per input. The performance of knn can be substantially improved by adding a small amount of gaussian noise to the trajectory samples; its resulting performance as a function of <italic>N</italic> and <italic>d</italic> is shown in <xref ref-type="supplementary-material" rid="pcbi.1007290.s005">S5 Fig</xref>. Red star denotes the failure of knn on Example 3 where substantially negative information values are returned (exact value not plotted).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec024">
<title>Applications to real data</title>
<p>To illustrate the use of our estimators in a realistic context, we analyzed data from two previously published papers. The first paper focused on the representation of environmental stress in the nuclear localization dynamics of several transcription factors (here we focus on data for Msn2, Dot6, and Sfp1) in budding yeast [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>]. The second paper studied information transmission in biochemical signaling networks in mammalian cells (here we focus on data for ERK and Ca<sup>2+</sup>) [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>]. In both cases, single-cell trajectory data were collected in hundreds or thousands of single cells sampled at sufficient resolution to represent the trajectories discretized at tens to hundreds of timepoints. Similarly, both papers estimate the information transmission in trajectories about a discrete number of environmental conditions: Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>] uses the linear SVM approach presented here, while Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>] uses the knn estimator. This makes the two datasets perfectly suited for estimator comparisons. We further note that in both datasets the trajectories can be divided into two response periods: the early “transient” response period when the external condition changes, and the late “near steady-state” response period. Typically, the transient dynamics exhibit clear differences in the trajectory means between various conditions, reminiscent of our Example 1 or early Example 2; in contrast, in the late period the response may have been adapted away, or the stimulus could be encoded only in higher-order statistics of the traces, reminiscent of the late period in Example 2 or Example 3.</p>
<p><xref ref-type="fig" rid="pcbi.1007290.g008">Fig 8</xref> shows the raw data and summarizes our estimation results for the early and late response periods for the three translocating factors in yeast that report on the change from 2% glucose rich medium to 0.1% glucose poor stress medium. <xref ref-type="fig" rid="pcbi.1007290.g009">Fig 9</xref> similarly shows the raw data and estimation results for the early and late response periods for the signaling molecules in mammalian cells responding to multilevel inputs.</p>
<fig id="pcbi.1007290.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Two-level mutual information estimates from single-cell time-series data for nuclear translocation of yeast transcription factors.</title>
<p><bold>(A, B)</bold> Data replotted from Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>] for Msn2 (top row), Dot6 (middle row), and Sfp1 (bottom row); early transient responses (A) after nutrient shift at <italic>t</italic> = 0 min from glucose rich (2%, blue traces) to glucose poor (0.1%, red traces) medium are shown in the left column, stationary responses (B) are collected after cells are fully adapted to the new medium. Sampling frequency is 2.5 min, <italic>d</italic> = 45, and the number of sample trajectories per nutrient condition is <italic>N</italic> = 100. Thin lines are individual single cell traces, solid lines are population averages. <bold>(C, D)</bold> Information estimates for the transient (left, C) and stationary (right, D) response periods. Colored bars use model-free decoding-based estimators as indicated in the legend, gray bar is the knn estimate; error bars computed from estimation bootstraps by randomly splitting the data into testing and training sets.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g008" xlink:type="simple"/>
</fig>
<fig id="pcbi.1007290.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007290.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Multilevel mutual information estimates from single-cell time-series data for mammalian intracellular signaling.</title>
<p>Data replotted from Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref026">26</xref>] for ERK (top row) and Ca<sup>2+</sup> (bottom row). <bold>(A)</bold> Early transient responses after addition of 5 different levels of EGF for ERK (or 4 different levels of ATP for Ca<sup>2+</sup>, respectively) at <italic>t</italic> = 0 min, as indicated in the legend. <bold>(B)</bold> In the late response most, but not all, of the transients have decayed. <bold>(C,D)</bold> Information estimation using different methods (legend) in the early (C) and late (D) period, for ERK (left half of the panels) and Ca<sup>2+</sup> (right half). Data for ERK: <italic>N</italic> = 1678 per condition, <italic>T</italic> = 30 min (<italic>d</italic> = 30) for early response and <italic>T</italic> = 30 min (<italic>d</italic> = 30) for late response. Data for Ca<sup>2+</sup>: <italic>N</italic> = 2995 per condition, <italic>T</italic> = 10 min (<italic>d</italic> = 200) for early response and <italic>T</italic> = 5 min (<italic>d</italic> = 100) for late response. Plotting conventions as in <xref ref-type="fig" rid="pcbi.1007290.g008">Fig 8</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.g009" xlink:type="simple"/>
</fig>
<p>Consistent with the published report [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>], transient response in yeast nuclear localization signal can be decoded well with the linear SVM estimator that yields about 0.6 bits of information per gene about the external condition. Kernelized SVM outperforms the linear method slightly by extracting an extra 0.1-0.2 bits of information, while knn underperforms the linear method significantly for Msn2 and Dot6 (but not for Sfp1). The Gaussian decoder estimate shows a mixed performance and the neural network estimate is the worst performer, most likely because the number of samples here is only <italic>N</italic> = 100 per input condition and neural network training is significantly impacted.</p>
<p>It is interesting to look at the stationary responses in yeast which have not previously been analyzed in detail. First, low estimates provided by linear SVM for Msn2 and Dot6 imply that information in the stationary regime, if present, cannot be extracted by the linear classifier. Second, the Gaussian decoder also performs poorly in the stationary regime, potentially indicating that the relevant features are encoded in higher-than-pairwise order statistics of the response (e.g., pulses could be “sparse” features as in sparse coding [<xref ref-type="bibr" rid="pcbi.1007290.ref058">58</xref>]); it is, however, hard to exclude small number of training samples as the explanation for the poor performance of the Gaussian decoder. Third, K-nearest-neighbor estimator also yields low estimates, either due to small sample number or low signal-to-noise ratio, the regime for which knn method has been observed to show reduced performance [<xref ref-type="bibr" rid="pcbi.1007290.ref040">40</xref>]. A particularly worrying feature of the knn estimates is their non-robust dependence on the length of the trajectory <italic>T</italic>. As <xref ref-type="supplementary-material" rid="pcbi.1007290.s006">S6 Fig</xref> shows, the performance of knn peaks at <italic>T</italic> ≈ 50 min and then drops, even well into unrealistic negative estimates for <italic>T</italic> ≈ 400 min (corresponding to the highest dimensionality <italic>d</italic> = 170 of discrete trajectories). While it is possible to make an <italic>ad hoc</italic> choice to always select trajectory duration at which the estimate peaks, the performance of kernelized SVM is, in comparison, extremely well behaved and increases monotonically with <italic>T</italic>, as theoretically expected. Finally, nonlinear SVM estimator extracts up to 0.4 bits of information about condition per gene, more than half of the information in the early transient period. This is even though on average the response trajectories for the two conditions, 2% glucose and 0.1% glucose, for Msn2 and Dot6 are nearly identical. For Sfp1 there is a notable difference in the mean response, which the linear estimator can use to provide a ∼ 0.15 bits of information, yet still significantly below ∼ 0.4 bits extracted by the nonlinear SVM. For both transient and stationary responses in yeast, our results are qualitatively in line with the expectations from the synthetic example cases—given the small number of trajectories, tightest and most robust estimates are provided by the decoding information estimator based on nonlinear (kernelized) SVM. Regardless of the decoding methodology and even without small sample corrections at <italic>N</italic> = 100 trajectories per input, our estimates are not significantly impacted by the well-known information estimation biases thanks to the dimensionality reduction that decoding provides by mapping high dimensional trajectories <bold>X</bold> back into the space for inputs <italic>U</italic> which is low dimensional; this is verified in <xref ref-type="supplementary-material" rid="pcbi.1007290.s007">S7 Fig</xref> by estimating the (zero) information in trajectories whose input labels have been randomly assigned.</p>
<p>Random pulses that encode stationary environmental signals have been observed for at least 10 transcription factors in yeast [<xref ref-type="bibr" rid="pcbi.1007290.ref053">53</xref>] and for tens of transcription factors in mammalian cells [<xref ref-type="bibr" rid="pcbi.1007290.ref059">59</xref>]. Recent studies investigated the role of the pulsatile dynamics in cellular decision-making [<xref ref-type="bibr" rid="pcbi.1007290.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref060">60</xref>]. Nevertheless, methods for quantifying the information encoded in stochastic pulses are still in their infancy. Our nonlinear SVM decoding estimates convincingly show that there is information to be learned at the single cell level from the stationary stochastic pulsing. An interesting direction for future work is to ask whether hand-crafted features of the response trajectories (pulse frequency, amplitude, shape, etc) can extract as much information from the trajectories as the generic SVM classifier: for that, one would construct for each response trajectory a “feature vector” by hand, compute the linear SVM decoding bound information estimate from the feature vectors, and compare that to the kernelized SVM estimate over the original trajectories. This approach is a generic and operationally-defined path for finding “sufficient statistics” of the response trajectories—or a compression of the original signal to the relevant set of features—in the information-theoretic sense.</p>
<p>A different picture emerges from the mammalian signaling network data shown in <xref ref-type="fig" rid="pcbi.1007290.g009">Fig 9</xref>. The key difference here is the order of magnitude larger number of sample trajectories per condition compared to yeast data. Most of the information seems linearly separable in both the early and late response periods, as evidenced by the success of the linear SVM based estimator whose performance is not improved upon by the kernelized SVM (indeed, for early ERK response period linear SVM gives a slightly higher estimate than the nonlinear version). The big winner on this dataset is the neural-network-based estimator that yields the best performance in all conditions among the decoding-based estimators, likely owning to sufficient training data. As before, the Gaussian decoder shows mixed performance which can get competitive with the best estimators under some conditions. Lastly, knn appears to do well except on the late Ca<sup>2+</sup> data (perhaps due to low signal-to-noise ratio). It also shows counter-intuitive non-monotonic behavior with trajectory duration <italic>T</italic> in <xref ref-type="supplementary-material" rid="pcbi.1007290.s008">S8 Fig</xref> (cf. with Fig 2C of Ref [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>], where the analysis of information conveyed in dynamical signals as a function of trajectory duration was also very revealing about signaling in yeast). Once again it is worth keeping in mind that knn is estimating the full mutual information which could be higher than the information decodable from single responses.</p>
</sec>
</sec>
<sec id="sec025" sec-type="conclusions">
<title>Discussion</title>
<p>Increasing availability of single-cell time-resolved data should allow us to address open questions regarding the amount of information about the external world that is available in the time-varying concentrations, activation or localization patterns, and modification state of various biochemical molecules. Do full response trajectories provide more information than single temporal snapshots, as early studies suggest? Is this information gain purely due to noise averaging enabled by observing multiple snapshots, or—more interestingly—due to the ability of these intrinsically high-dimensional signals to provide a richer representation of the cellular environment? Can we isolate biologically relevant features of the response trajectories, e.g., amplitude, frequency, pulse shape, relative phase or timing, without <italic>a priori</italic> assuming what these features are? How can cells read out the environmental state from these response trajectories and how close to the information-theoretic bounds is this readout process? More broadly, a framework for analyzing complete response trajectories in signaling or genetic regulatory networks at the single cell level could lead to architectural and functional constraints on the biological network [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>], and allow us to further pursue the ideas of optimal information representation in biological systems [<xref ref-type="bibr" rid="pcbi.1007290.ref009">9</xref>].</p>
<p>Here, we made methodological steps towards answering these questions by focusing on two related problems: first, if we are given a full stochastic description of a biochemical reaction network, under what conditions can we theoretically compute information transmission through this network and various related bounds; second, if we are given real data with no description of the network, what are tractable schemes to estimate the information transmission. We show that when the complete state of the reaction network is observed and the inputs are discrete sets of reaction rates, there exist tractable Monte Carlo approximation schemes for the information transmission. These exact results that we compute for three simple biological network examples then serve to benchmark a family of decoding-based model-free estimators and compare their performance to the commonly-used knn estimator. We show that decoding-based estimators can closely approach the optimal decoder performance and in many cases perform better than knn, especially with typical problem dimensions (<italic>d</italic> ∼ 1 − 100) and typical number of sample trajectories (<italic>N</italic> ∼ 10<sup>2</sup> − 10<sup>3</sup>). This is especially true when we ask about the combinatorial representation of the environmental state in the time trajectories of several jointly observed chemical species, as in our previous work [<xref ref-type="bibr" rid="pcbi.1007290.ref027">27</xref>], where alternative information estimation methods usually completely fail due to the high dimensionality of the input space.</p>
<p>It is necessary to emphasize the flexibility of the decoding approach: decoding-based information estimation is based directly on the statistical problems of classification (for discrete input variable, <italic>U</italic>) or regression (for continuous input variable, <italic>U</italic>), so any classification / regression algorithm with good performance can provide the basis for information estimation. Concretely, for problems in the low data regime (small <italic>N</italic>), linear or kernelized SVM approaches appear powerful, while at larger <italic>N</italic> neural-network-based schemes can provide a better performance and thus typically a tighter information lower bound. In contrast to information approximations for which it is often impossible to assess their precision or bias (or even its sign) when the dimension, <italic>d</italic>, of the problem is large, the decoding approach yields a conservative estimate of the true information. Statistical algorithms underlying decoding-based estimations have the extra advantage that, (i), we may be able to gain biological insight by inspecting which features of the response carry the relevant stimulus information (e.g., by looking at the linear kernels or features that neural networks extract in their various layers); (ii), pick a decoding algorithm based on features previously reported as relevant (e.g., the Gaussian decoder for second-order statistics as in Example 3); (iii), estimate the information as a function of trajectory duration; and (iv), gain confidence in our estimates by testing their performance on withheld data. While we tested these estimators on a very restricted set of toy examples in order to be able to compare to analytically computed results, model-free decoding-based approaches are applicable more generally, e.g., to complex, partially-observed reaction systems, or networks with significant contribution of cell-to-cell variability or extrinsic noise.</p>
<p>By construction, decoding-based estimators only provide a lower bound to the true information. This, however, could turn out to be a smaller problem in practice than it appears in theory, especially for biochemical reaction networks. First, our extension to the Feder-Merhav bound provides us with an estimate of how large the gap between the true information and the decoded estimation can be. The bound is not tight on our examples, and can only be applied when the optimal MAP decoder can be constructed [<xref ref-type="bibr" rid="pcbi.1007290.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1007290.ref062">62</xref>]. Second, and perhaps more importantly, information that can be decoded after single input presentations is the quantity that is likely more biologically relevant than the true channel capacity, if the organisms are under constraint to respond to the environmental changes quickly. Typically, organisms across the complexity scale operate under speed-accuracy tradeoffs [<xref ref-type="bibr" rid="pcbi.1007290.ref063">63</xref>]: faster decisions based on noisy information lead to more errors and, conversely, with enough time to integrate sensory information errors can be reduced. When speed is at a premium or relevant inputs are sparse, decisions need to be taken after single input presentations. In this case, decoding-based estimation should not be viewed as an approximate but rather as the correct methodology for the biological problem at hand. Of course, there is still the question of whether the model-free decoders that we use on real data can achieve a performance that is close to the optimal MAP decoder that represents the absolute performance limit. While there is no general way to answer this question, it appears that simple SVM decoding schemes work well when the response trajectories differ in their conditional mean, and neural networks as general approximators can be used to check for more complicated encoding features when data is plentiful. Unlike in neuroscience, there is much less clarity about what kind of read-out or decoding operations biochemical networks can mechanistically realize to mimic the functioning of our <italic>in silico</italic> decoders, and it may be challenging to biochemically implement even arbitrary linear classification of response trajectories. Until experimentally shown otherwise, it thus appears reasonable to proceed with the assumption that environmental signals can be read out from the time-dependent internal chemical state with a simple repertoire of computations.</p>
<p>We also mention a caveat when using decoding-based estimators that rely on classification or regression methods with large expressive power, such as neural networks. While it is possible to successfully guard against overfitting within the same dataset using cross-validation, scientific insights into biological function often require generalization beyond one particular dataset. Typically, we ask for generalization at least over independent experimental replicates, but sometimes even over similar (but not same) external conditions, strains, or experimental setups. This can present a serious issue if e.g., neural networks overfit to such systematic variations between replicates or conditions even when such variations are not biologically relevant. Regularization alone will not necessarily guard against this, unless the networks are actually trained over a subset of all data on which they will be tested. A pertinent recommendation here is to evaluate the difference in performance of expressive decoding-based estimators when trained over a subset or over all replicates, and to compare that to the generalization of less-expressive methods for which the sufficient statistics are known (e.g., linear or Gaussian decoders).</p>
<p>We conclude by emphasizing a simple yet important point. The decoding-based approach that we introduced here should also motivate us to look beyond methodological problems of significance and estimation, to truly biological problems of cellular decision making. Currently, data on biological regulatory processes is often analyzed by looking for “statistically significant differences” in the network response for, say, two possible network inputs. For example, one may report that the steady-state mean expression level of a certain gene is significantly larger in the stimulated vs unstimulated condition, with the statistical significance of the mean difference established through an appropriate statistical test that takes into account the number of collected population samples. While statistical significance is a necessary condition to validly report <italic>any</italic> difference in the response, it is very different from the question of whether <italic>a single cell</italic> could discriminate the two conditions given access only to its own expression levels. In caricature, population-level statistics tell us with what confidence we, as scientists having access to <italic>N</italic> samples, can discriminate between conditions given some biological readout; decoding based information estimates, on the other hand, are relevant to the <italic>N</italic> = 1 case of individual cells. We hope that further work along the latter path can clarify and quantify better the difficult constraints and conditions under which real cells need to act based on individual noisy readouts of their stochastic biochemistry.</p>
</sec>
<sec id="sec026">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007290.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Decoding-based estimators applied to Example 1 trajectories with extrinsic noise.</title>
<p><bold>(A)</bold> Sample trajectories as in Example 1 of <xref ref-type="fig" rid="pcbi.1007290.g002">Fig 2A</xref>, using the same parameters and plotting conventions. Extrinsic noise was introduced by perturbing the degradation rate (fixed at 0.01 for all cells in the main paper), by adding a Gaussian IID distributed random variable with <italic>σ</italic> = 0.001 (shown in A and B) or <italic>σ</italic> = 0.0002 (shown in C); the random variable is drawn separately for each cell at the beginning of the simulation and is held fixed through time. <bold>(B,C)</bold> Estimator performance on test data (where extrinsic noise is also resampled for each cell in the test set) for both extrinsic noise levels. SVM and knn estimators perform best, but unlike in the main paper, here we do not have a reference comparison of the MAP decoder. While decoding-based estimators are giving a conservative lower-bound, we have no guarantees of whether knn extracts more information (and thus has better performance), or actually overestimates decodable information.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Effects of covariance matrix regularization and signal smoothing on Gaussian-decoder-based estimation.</title>
<p><bold>(A)</bold> At left. Diagonal covariance regularization following Ref 64 of the main paper. Briefly, λ times the identity matrix is added to the empirical covariance matrix with the hyperparameter λ set so that the likelihood on test data is maximized. Shown is the empirical (left) and regularized (right) covariance matrix for Example 3, using <italic>d</italic> = 20 and <italic>N</italic> = 30 sample trajectories. At right. Information estimates for Example 3: <italic>I</italic><sub>MAP</sub> decoding bound (black), Gaussian decoder estimate, <italic>I</italic><sub>GD(reg)</sub>, with optimal diagonal regularization for each <italic>d</italic> (yellow, as in <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5C</xref>), Gaussian decoder estimate, <italic>I</italic><sub>GD(noreg)</sub> (brown). Without regularization, the estimate suffers an abrupt drop as <italic>d</italic> increases and the empirically estimated covariance matrix becomes close to singular. <italic>N</italic> and plotting conventions are as in <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5</xref>. <bold>(B)</bold> The effects of trajectory filtering on information estimates. At left. A raw integer-valued stochastic trajectory for <inline-formula id="pcbi.1007290.e118"><alternatives><graphic id="pcbi.1007290.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007290.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (blue) can be filtered by a low-pass exponential decay filter with adjustable timescale, <italic>τ</italic> = 1 − 10<sup>3</sup>, here <italic>τ</italic> = 50 (red) to yield real-valued trajectory. At right. Regularized Gaussian-decoder information estimates with (brown) and without (yellow) filtering. Filtering does not improve but can decrease the estimation performance, even when the filtering timescale is adjusted.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>The effect of neural network architecture on the performance of information estimators for Example 3.</title>
<p>Shown are <italic>I</italic><sub>NN</sub> estimates on Example 3, analogous to <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5C and 5F</xref> of the main paper. Decoders are trained on <italic>N</italic> = 1000 sample trajectories per input condition, and trajectories are represented as <italic>d</italic> = 50 dimensional vectors. The performance of the network architecture used in the main paper is shown as the horizontal red line. Alternative architectures are denoted on the x-axis label. “1L, 2L, 3L” stands for 1, 2 or 3 layers, respectively. The number in front of letter “N” represents the total number of neurons on the hidden layers. Other technical details of the networks as reported in the Methods.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Gaussian approximation to the information can lead to an uncontrolled overestimation of the true information.</title>
<p>Gaussian approximation is evaluated for Example 3 in <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5C</xref>, using <italic>N</italic> = 1000 per condition. Exact Monte Carlo approximation of the information, <italic>I</italic><sub>exact</sub>(<bold>X</bold>; <italic>U</italic>), is shown in dark gray. Information estimates following Section <italic>Model-free information estimators</italic> are shown in violet (Gaussian approximation for raw, integer-valued response trajectories) or in cyan (Gaussian approximation for filtered trajectories), as in <xref ref-type="supplementary-material" rid="pcbi.1007290.s002">S2 Fig</xref>. In both cases the Gaussian approximation overshoots the true information value. Further numerical analyses indicated that the difference is hard to predict and that it persists even when the reaction rates are chosen such that the mean expression level is ten-fold higher (and the intrinsic stochasticity correspondingly lower). This makes direct Gaussian approximation risky to use, in contrast to the Gaussian-decoder based estimate, which is guaranteed to stay below <italic>I</italic><sub>exact</sub>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Behavior of the knn information estimator.</title>
<p>Compared to knn results in <xref ref-type="fig" rid="pcbi.1007290.g007">Fig 7</xref>, the results in A, B and D are estimated following the same procedure, while adding a small amount of IID zero-mean Gaussian noise to each response trajectory at every time bin; the noise variance must be ≪1 but otherwise does not affect the results much. This results in good estimates even at low sample number, <italic>N</italic>, and provides nearly stable estimation as a function of the trajectory dimension, <italic>d</italic>, for Example 1 and Example 2. It, however, does not resolve the estimator failure for Example 3. <bold>(A)</bold> Dependence of the knn estimator performance on the number of samples. Yellow plot symbols indicate the number of samples per condition, <italic>N</italic> = 10<sup>3</sup>, used in <xref ref-type="fig" rid="pcbi.1007290.g007">Fig 7</xref>. <bold>(B)</bold> Dependence of the knn estimator performance on the trajectory dimension. Yellow plot symbols indicate the dimension, <italic>d</italic> = 10<sup>2</sup>, used in <xref ref-type="fig" rid="pcbi.1007290.g007">Fig 7</xref>. <bold>(C, D)</bold> Dependence of the knn estimates on the number of nearest neighbors, <italic>k</italic>, at <italic>N</italic> = 10<sup>3</sup> and <italic>d</italic> = 10<sup>2</sup>, without the addition of noise (C) or with the addition of noise (D).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Estimator behavior for longer trajectory data for Dot6.</title>
<p>When the samples are limited, here to <italic>N</italic> = 100 samples per input glucose level condition as in <xref ref-type="fig" rid="pcbi.1007290.g008">Fig 8A</xref> (middle), radial-basis-function SVM estimate (blue) is well-behaved with no observable overfitting and consequent drop in information estimate as the trajectory duration, <italic>T</italic>, is increased (maximal <italic>T</italic> corresponds to <italic>d</italic> = 170 dimensional trajectory vectors). In contrast, knn estimate (brown) shows a collapse in the estimation performance, even yielding strongly negative numbers, as the dimensionality of input vectors is increased at fixed number of trajectory samples.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Assessing information estimation bias due to small sample size.</title>
<p>By randomly shuffling the binary labels assigned to different response trajectories, we break all response-input correlations leading to zero information. Here we test whether our estimators correctly report zero information within error bars given a finite number of samples, or are subject to positive information estimation bias. Decoding-based estimates (linear SVM, red; kernelized SVM, blue; Gaussian decoder, yellow) and knn (gray). First three sets of bars correspond to synthetic examples of <xref ref-type="fig" rid="pcbi.1007290.g003">Fig 3</xref>; estimations are done with <italic>d</italic> = 100 and <italic>N</italic> = 1000 per input condition as in <xref ref-type="fig" rid="pcbi.1007290.g005">Fig 5</xref>, following the same plotting conventions. Last two sets of bars are estimated with <italic>N</italic> = 100 per input condition using real data for Sfp1 yeast TF from <xref ref-type="fig" rid="pcbi.1007290.g008">Fig 8A</xref>. In all cases, even without explicit small-sample debiasing for <xref ref-type="disp-formula" rid="pcbi.1007290.e063">Eq (26)</xref> (which may be required for multilevel estimation), the estimates are consistent with zero.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007290.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007290.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Information estimates for mammalian signaling networks as a function of the trajectory duration.</title>
<p>Shown are information estimates as a function of the total trajectory duration, <italic>T</italic>, for the early response period for ERK (A) and Ca<sup>2+</sup> (B). Plotting conventions, procedures, and data set sizes same as in <xref ref-type="fig" rid="pcbi.1007290.g009">Fig 9</xref>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Alejandro Granados, Mihal Hledik, Julian Pietsch, and Christoph Zechner for stimulating discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1007290.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eldar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Elowitz</surname> <given-names>MB</given-names></name>. <article-title>Functional roles for noise in genetic circuits</article-title>. <source>Nature</source>. <year>2010</year>;<volume>467</volume>(<issue>7312</issue>):<fpage>167</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09326" xlink:type="simple">10.1038/nature09326</ext-link></comment> <object-id pub-id-type="pmid">20829787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Purvis</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Lahav</surname> <given-names>G</given-names></name>. <article-title>Encoding and Decoding Cellular Information through Signaling Dynamics</article-title>. <source>Cell</source>. <year>2013</year>;<volume>152</volume>(<issue>5</issue>):<fpage>945</fpage>–<lpage>956</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2013.02.005" xlink:type="simple">10.1016/j.cell.2013.02.005</ext-link></comment> <object-id pub-id-type="pmid">23452846</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Shannon</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Weaver</surname> <given-names>W</given-names></name>. <source>The Mathematical Theory of Communication</source>. <volume>vol. 27</volume>. <publisher-loc>Urbana</publisher-loc>: <publisher-name>University of Illinois Press</publisher-name>; <year>1949</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Estimation of Entropy and Mutual Information</article-title>. <source>Neural Computation</source>. <year>2003</year>;<volume>15</volume>:<fpage>1191</fpage>–<lpage>1253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603321780272" xlink:type="simple">10.1162/089976603321780272</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Strong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Koberle</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Entropy and Information in Neural Spike Trains</article-title>. <source>Physical Review Letters</source>. <year>1998</year>;<volume>80</volume>:<fpage>197</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.80.197" xlink:type="simple">10.1103/PhysRevLett.80.197</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>. <article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2009</year>;<volume>10</volume>(<issue>3</issue>):<fpage>173</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2578" xlink:type="simple">10.1038/nrn2578</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bowsher</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Swain</surname> <given-names>PS</given-names></name>. <article-title>Environmental sensing, information transfer, and cellular decision-making</article-title>. <source>Current Opinion in Biotechnology</source>. <year>2014</year>;<volume>28</volume>:<fpage>149</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.copbio.2014.04.010" xlink:type="simple">10.1016/j.copbio.2014.04.010</ext-link></comment> <object-id pub-id-type="pmid">24846821</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref008">
<label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <source>Biophysics: Searching for Principles</source>. <publisher-name>Princeton University Press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Information Processing in Living Systems</article-title>. <source>Annual Review of Condensed Matter Physics</source>. <year>2016</year>;<volume>7</volume>(<issue>1</issue>):<fpage>89</fpage>–<lpage>117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-conmatphys-031214-014803" xlink:type="simple">10.1146/annurev-conmatphys-031214-014803</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkacik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Aleksandra</surname> <given-names>W</given-names></name>. <article-title>Information transmission in genetic regulatory networks: a review</article-title>. <source>Journal of Physics: Condensed Matter</source>. <year>2011</year>;<volume>23</volume>. <object-id pub-id-type="pmid">21460423</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thomas</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Eckford</surname> <given-names>AW</given-names></name>. <article-title>Capacity of a Simple Intercellular Signal Transduction Channel</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>2016</year>;<volume>62</volume>(<issue>12</issue>):<fpage>7358</fpage>–<lpage>7382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIT.2016.2599178" xlink:type="simple">10.1109/TIT.2016.2599178</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tostevin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>ten Wolde</surname> <given-names>PR</given-names></name>. <article-title>Mutual Information between Input and Output Trajectories of Biochemical Networks</article-title>. <source>Phys Rev Lett</source>. <year>2009</year>;<volume>102</volume>:<fpage>218101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.102.218101" xlink:type="simple">10.1103/PhysRevLett.102.218101</ext-link></comment> <object-id pub-id-type="pmid">19519137</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Callan</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Information capacity of genetic regulatory elements</article-title>. <source>Phys Rev E</source>. <year>2008</year>;<volume>78</volume>:<fpage>011910</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.78.011910" xlink:type="simple">10.1103/PhysRevE.78.011910</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sokolowski</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>. <article-title>Optimizing information flow in small genetic networks. IV. Spatial coupling</article-title>. <source>Physical Review E</source>. <year>2015</year>;<volume>91</volume>(<issue>6</issue>):<fpage>062710</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.91.062710" xlink:type="simple">10.1103/PhysRevE.91.062710</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sokolowski</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>. <article-title>Extending the dynamic range of transcription factor action by translational regulation</article-title>. <source>Physical Review E</source>. <year>2016</year>;<volume>93</volume>(<issue>2</issue>):<fpage>022404</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.93.022404" xlink:type="simple">10.1103/PhysRevE.93.022404</ext-link></comment> <object-id pub-id-type="pmid">26986359</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkacik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Optimizing information flow in small genetic networks. III. A self-interacting gene</article-title>. <source>Phys Rev E</source>. <year>2012</year>;<volume>85</volume>(<issue>4</issue>):<fpage>041903</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.85.041903" xlink:type="simple">10.1103/PhysRevE.85.041903</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Optimizing information flow in small genetic networks. II. Feed-forward interactions</article-title>. <source>Physical Review E</source>. <year>2010</year>;<volume>81</volume>(<issue>4</issue>):<fpage>041905</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.81.041905" xlink:type="simple">10.1103/PhysRevE.81.041905</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Optimizing information flow in small genetic networks</article-title>. <source>Physical Review E</source>. <year>2009</year>;<volume>80</volume>(<issue>3</issue>):<fpage>031920</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.80.031920" xlink:type="simple">10.1103/PhysRevE.80.031920</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rieckh</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>. <article-title>Noise and Information Transmission in Promoters with Multiple Internal States</article-title>. <source>Biophysical Journal</source>. <year>2014</year>;<volume>106</volume>(<issue>5</issue>):<fpage>1194</fpage>–<lpage>1204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bpj.2014.01.014" xlink:type="simple">10.1016/j.bpj.2014.01.014</ext-link></comment> <object-id pub-id-type="pmid">24606943</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cheong</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rhee</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Nemenman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Levchenko</surname> <given-names>A</given-names></name>. <article-title>Information Transduction Capacity of Noisy Biochemical Signaling Networks</article-title>. <source>Science</source>. <year>2011</year>;<volume>334</volume>(<issue>6054</issue>):<fpage>354</fpage>–<lpage>358</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1204553" xlink:type="simple">10.1126/science.1204553</ext-link></comment> <object-id pub-id-type="pmid">21921160</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tostevin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ten Wolde</surname> <given-names>PR</given-names></name>. <article-title>Mutual information in time-varying biochemical systems</article-title>. <source>Physical Review E</source>. <year>2010</year>;<volume>81</volume>(<issue>6</issue>):<fpage>061917</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.81.061917" xlink:type="simple">10.1103/PhysRevE.81.061917</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>de Ronde</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tostevin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>ten Wolde</surname> <given-names>PR</given-names></name>. <article-title>Multiplexing Biochemical Signals</article-title>. <source>Phys Rev Lett</source>. <year>2011</year>;<volume>107</volume>:<fpage>048101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.107.048101" xlink:type="simple">10.1103/PhysRevLett.107.048101</ext-link></comment> <object-id pub-id-type="pmid">21867046</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dubuis</surname> <given-names>JO</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wieschaus</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Gregor</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Positional information, in bits</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year>;<volume>110</volume>(<issue>41</issue>):<fpage>16301</fpage>–<lpage>16308</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1315642110" xlink:type="simple">10.1073/pnas.1315642110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Voliotis</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perrett</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>McWilliams</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>McArdle</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Bowsher</surname> <given-names>CG</given-names></name>. <article-title>Information transfer by leaky, heterogeneous, protein kinase signaling systems</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>3</issue>):<fpage>E326</fpage>–<lpage>E333</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1314446111" xlink:type="simple">10.1073/pnas.1314446111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hansen</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Shea</surname> <given-names>EKO</given-names></name>. <article-title>Limits on information transduction through amplitude and frequency regulation of transcription factor activity</article-title>. <source>eLife</source>. <year>2015</year>; p. <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Selimkhanov</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pilko</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Albeck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hoffmann</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Accurate information transmission through dynamic biochemical signaling networks</article-title>. <source>Science (New York, NY)</source>. <year>2014</year>;<volume>346</volume>(<issue>6215</issue>):<fpage>1370</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1254933" xlink:type="simple">10.1126/science.1254933</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Granados</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Pietsch</surname> <given-names>JMJ</given-names></name>, <name name-style="western"><surname>Cepeda-Humerez</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Farquhar</surname> <given-names>IL</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Swain</surname> <given-names>PS</given-names></name>. <article-title>Distributed and dynamic intracellular organization of extracellular information</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2018</year>;<volume>115</volume>(<issue>23</issue>):<fpage>6088</fpage>–<lpage>6093</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1716659115" xlink:type="simple">10.1073/pnas.1716659115</ext-link></comment> <object-id pub-id-type="pmid">29784812</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Information theory and neural coding</article-title>. <source>Nature neuroscience</source>. <year>1999</year>;<volume>2</volume>:<fpage>947</fpage>–<lpage>957</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14731" xlink:type="simple">10.1038/14731</ext-link></comment> <object-id pub-id-type="pmid">10526332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marre</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Botella-Soler</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Mora</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names> <suffix>II</suffix></name>. <article-title>High accuracy decoding of dynamical motion from a large retinal population</article-title>. <source>PLoS computational biology</source>. <year>2015</year>;<volume>11</volume>(<issue>7</issue>):<fpage>e1004304</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004304" xlink:type="simple">10.1371/journal.pcbi.1004304</ext-link></comment> <object-id pub-id-type="pmid">26132103</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Warland</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Coding Efficiency and Information Rates in Sensory Neurons</article-title>. <source>EPL (Europhysics Letters)</source>. <year>1993</year>;<volume>22</volume>(<issue>2</issue>):<fpage>151</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gillespie</surname> <given-names>DT</given-names></name>. <article-title>A rigorous derivation of the chemical master equation</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>1992</year>;<volume>188</volume>(<issue>1-3</issue>):<fpage>404</fpage>–<lpage>425</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0378-4371(92)90283-V" xlink:type="simple">10.1016/0378-4371(92)90283-V</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref032">
<label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Van Kampen</surname> <given-names>NG</given-names></name>. <source>Stochastic Processes in Physics and Chemistry</source>; <year>2007</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gillespie</surname> <given-names>DT</given-names></name>. <article-title>Exact stochastic simulation of coupled chemical reactions</article-title>. <source>The Journal of Physical Chemistry</source>. <year>1977</year>;<volume>81</volume>(<issue>25</issue>):<fpage>2340</fpage>–<lpage>2361</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1021/j100540a008" xlink:type="simple">10.1021/j100540a008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Feder</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Merhav</surname> <given-names>N</given-names></name>. <article-title>Relations between entropy and error probability</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>1994</year>;<volume>40</volume>(<issue>1</issue>):<fpage>259</fpage>–<lpage>266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/18.272494" xlink:type="simple">10.1109/18.272494</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Hledík M, Sokolowski T, Tkačik G. A tight upper bound on mutual information. arxivorg. 2018; p. 1812.01475.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dobrushin</surname> <given-names>RL</given-names></name>. <article-title>A simplified method of experimental estimation of the entropy of a stationary distribution</article-title>. <source>Tear Veroyatnost i Primenen; English transl Theory Probab Appl</source>. <year>1958</year>;<volume>3</volume>:<fpage>462</fpage>–<lpage>464</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vasicek</surname> <given-names>OA</given-names></name>. <article-title>A note on using cross-sectional information in bayesian estimation of security betas</article-title>. <source>The Journal of Finance</source>. <year>1973</year>;<volume>28</volume>(<issue>5</issue>):<fpage>1233</fpage>–<lpage>1239</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1540-6261.1973.tb01452.x" xlink:type="simple">10.1111/j.1540-6261.1973.tb01452.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaiser</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>. <article-title>Information transfer in continuous processes</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <year>2002</year>;<volume>166</volume>(<issue>1-2</issue>):<fpage>43</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0167-2789(02)00432-3" xlink:type="simple">10.1016/S0167-2789(02)00432-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kraskov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stögbauer</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Grassberger</surname> <given-names>P</given-names></name>. <article-title>Estimating mutual information</article-title>. <source>Physical Review E—Statistical, Nonlinear, and Soft Matter Physics</source>. <year>2004</year>;<volume>69</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bandyopadhyay</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ganguly</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Saigal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Erickson</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Protopopescu</surname> <given-names>V</given-names></name>, <etal>et al</etal>. <article-title>Relative performance of mutual information estimation methods for quantifying the dependence among short and noisy data</article-title>. <source>Physical Review E</source>. <year>2007</year>;<volume>76</volume>(<issue>2</issue>):<fpage>026209</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.76.026209" xlink:type="simple">10.1103/PhysRevE.76.026209</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Potter</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Byrd</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Mugler</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>B</given-names></name>. <article-title>Dynamic Sampling and Information Encoding in Biochemical Networks</article-title>. <source>Biophysical Journal</source>. <year>2017</year>;<volume>112</volume>(<issue>4</issue>):<fpage>795</fpage>–<lpage>804</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bpj.2016.12.045" xlink:type="simple">10.1016/j.bpj.2016.12.045</ext-link></comment> <object-id pub-id-type="pmid">28256238</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref042">
<label>42</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Anderson</surname> <given-names>TW</given-names></name>. <source>An introduction to multivariate statistical analysis</source>. <volume>vol. 2</volume>. <publisher-name>Wiley</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1958</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref043">
<label>43</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cover</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JA</given-names></name>. <source>Elements of Information Theory</source>. <publisher-name>John Wiley and Sons</publisher-name>; <year>2005</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>Mutual information, Fisher information, and population coding</article-title>. <source>Neural computation</source>. <year>1998</year>;<volume>10</volume>(<issue>7</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976698300017115" xlink:type="simple">10.1162/089976698300017115</ext-link></comment> <object-id pub-id-type="pmid">9744895</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref045">
<label>45</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Murphy</surname> <given-names>KP</given-names></name>. <source>Machine Learning: A Probabilistic Perspective</source>. <publisher-name>The MIT Press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Samengo</surname> <given-names>I</given-names></name>. <article-title>Information loss in an optimal maximum likelihood decoding</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>4</issue>):<fpage>771</fpage>–<lpage>779</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602317318947" xlink:type="simple">10.1162/089976602317318947</ext-link></comment> <object-id pub-id-type="pmid">11936960</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Benabdeslem</surname> <given-names>Khalid</given-names></name> and <name name-style="western"><surname>Bennani</surname> <given-names>Y.</given-names></name> <article-title>Dendogram-based SVM for Multi-Class Classification</article-title>. <source>Journal of Computing and Information Technology</source>. <year>2006</year>;<volume>14</volume>(<issue>4</issue>):<fpage>283</fpage>–<lpage>289</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2498/cit.2006.04.03" xlink:type="simple">10.2498/cit.2006.04.03</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lajnef</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Chaibi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ruby</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Aguera</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Eichenlaub</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Samet</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Learning machines and sleeping brains: Automatic sleep stage classification using decision-tree multi-class support vector machines</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2015</year>;<volume>250</volume>:<fpage>94</fpage>–<lpage>105</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2015.01.022" xlink:type="simple">10.1016/j.jneumeth.2015.01.022</ext-link></comment> <object-id pub-id-type="pmid">25629798</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McCulloch</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Pitts</surname> <given-names>W</given-names></name>. <article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>The Bulletin of Mathematical Biophysics</source>. <year>1943</year>;<volume>5</volume>(<issue>4</issue>):<fpage>115</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02478259" xlink:type="simple">10.1007/BF02478259</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Rosenblatt F. The Perceptron—A Perceiving and Recognizing Automaton. Cornell Aeronautical Laboratory; 1957.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref051">
<label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Géron</surname> <given-names>A</given-names></name>. <source>Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</source>. <publisher-name>O’Reilly</publisher-name>; <year>2017</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref052">
<label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1007290.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dalal</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rahbar</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Elowitz</surname> <given-names>MB</given-names></name>. <article-title>Pulsatile Dynamics in the Yeast Proteome</article-title>. <source>Current Biology</source>. <year>2014</year>;<volume>24</volume>(<issue>18</issue>):<fpage>2189</fpage>–<lpage>2194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.07.076" xlink:type="simple">10.1016/j.cub.2014.07.076</ext-link></comment> <object-id pub-id-type="pmid">25220054</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taniguchi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Gene-Wei</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Babu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hearn</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Quantifying E. coli proteome and transcriptome with single-molecule sensitivity in single cells</article-title>. <source>Science</source>. <year>2010</year>;<volume>30</volume>:<fpage>533</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1188308" xlink:type="simple">10.1126/science.1188308</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>. <article-title>Measuring information transfer</article-title>. <source>Phys Rev Lett</source>. <year>2000</year>;<volume>85</volume>:<fpage>461</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.85.461" xlink:type="simple">10.1103/PhysRevLett.85.461</ext-link></comment> <object-id pub-id-type="pmid">10991308</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lindner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vicente</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Priesemann</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Wibral</surname> <given-names>M</given-names></name>. <article-title>TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy</article-title>. <source>BMC Neuroscience</source>. <year>2011</year>;<volume>12</volume>:<fpage>119</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1471-2202-12-119" xlink:type="simple">10.1186/1471-2202-12-119</ext-link></comment> <object-id pub-id-type="pmid">22098775</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hafner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stewart-Ornstein</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Purvis</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Forrester</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Bulyk</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Lahav</surname> <given-names>G</given-names></name>. <article-title>p53 pulses lead to distinct patterns of gene expression albeit similar DNA-binding dynamics</article-title>. <source>Nature Structural &amp; Molecular Biology</source>. <year>2017</year>;<volume>24</volume>(<issue>10</issue>):<fpage>840</fpage>–<lpage>847</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nsmb.3452" xlink:type="simple">10.1038/nsmb.3452</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding of sensory inputs</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2004</year>;<volume>14</volume>(<issue>4</issue>):<fpage>481</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2004.07.007" xlink:type="simple">10.1016/j.conb.2004.07.007</ext-link></comment> <object-id pub-id-type="pmid">15321069</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Levine</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Elowitz</surname> <given-names>MB</given-names></name>. <article-title>Functional Roles of Pulsing in Genetic Circuits</article-title>. <source>Science (New York, NY)</source>. <year>2013</year>;<volume>342</volume>(<issue>December</issue>):<fpage>1193</fpage>–<lpage>1200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1239999" xlink:type="simple">10.1126/science.1239999</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Albeck</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Mills</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Brugge</surname> <given-names>JS</given-names></name>. <article-title>Frequency-modulated pulses of ERK activity transmit quantitative proliferation signals</article-title>. <source>Molecular cell</source>. <year>2013</year>;<volume>49</volume>(<issue>2</issue>):<fpage>249</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.molcel.2012.11.002" xlink:type="simple">10.1016/j.molcel.2012.11.002</ext-link></comment> <object-id pub-id-type="pmid">23219535</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Dubuis</surname> <given-names>JO</given-names></name>, <name name-style="western"><surname>Petkova</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Gregor</surname> <given-names>T</given-names></name>. <article-title>Positional Information, Positional Error, and Readout Precision in Morphogenesis: A Mathematical Framework</article-title>. <source>Genetics</source>. <year>2015</year>;<volume>199</volume>(<issue>1</issue>):<fpage>39</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1534/genetics.114.171850" xlink:type="simple">10.1534/genetics.114.171850</ext-link></comment> <object-id pub-id-type="pmid">25361898</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Petkova</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tkacik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wieschaus</surname> <given-names>EF</given-names></name>, <name name-style="western"><surname>Gregor</surname> <given-names>T</given-names></name>. <article-title>Optimal decoding of cellular identities in a genetic network</article-title>. <source>Cell</source>. <year>2019</year>;<volume>176</volume>:<fpage>844</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2019.01.007" xlink:type="simple">10.1016/j.cell.2019.01.007</ext-link></comment> <object-id pub-id-type="pmid">30712870</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heitz</surname> <given-names>RP</given-names></name>. <article-title>The speed-accuracy tradeoff: history, physiology, methodology, and behavior</article-title>. <source>Frontiers in neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>150</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2014.00150" xlink:type="simple">10.3389/fnins.2014.00150</ext-link></comment> <object-id pub-id-type="pmid">24966810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007290.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yatsenko</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Josić</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Froudarakis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Improved Estimation and Interpretation of Correlations in Neural Circuits</article-title>. <source>PLOS Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>3</issue>):<fpage>e1004083</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004083" xlink:type="simple">10.1371/journal.pcbi.1004083</ext-link></comment> <object-id pub-id-type="pmid">25826696</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>