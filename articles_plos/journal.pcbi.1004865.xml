<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004865</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01092</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Age groups</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Education</subject><subj-group><subject>Schools</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Age groups</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Families</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The Development of Audio-Visual Integration for Temporal Judgements</article-title>
<alt-title alt-title-type="running-head">Development of Audio-Visual Integration</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Adams</surname>
<given-names>Wendy J.</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Psychology, University of Southampton, Southampton, Hampshire, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Beierholm</surname>
<given-names>Ulrik R.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Birmingham, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The author has declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: WJA. Performed the experiments: WJA. Analyzed the data: WJA. Wrote the paper: WJA.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">w.adams@soton.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>14</day>
<month>4</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>4</issue>
<elocation-id>e1004865</elocation-id>
<history>
<date date-type="received">
<day>5</day>
<month>7</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>3</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Wendy J. Adams</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004865"/>
<abstract>
<p>Adults combine information from different sensory modalities to estimate object properties such as size or location. This process is optimal in that (i) sensory information is weighted according to relative reliability: more reliable estimates have more influence on the combined estimate and (ii) the combined estimate is more reliable than the component uni-modal estimates. Previous studies suggest that optimal sensory integration does not emerge until around 10 years of age. Younger children rely on a single modality or combine information using inappropriate sensory weights. Children aged 4–11 and adults completed a simple audio-visual task in which they reported either the number of beeps or the number of flashes in uni-modal and bi-modal conditions. In bi-modal trials, beeps and flashes differed in number by 0, 1 or 2. Mutual interactions between the sensory signals were evident at all ages: the reported number of flashes was influenced by the number of simultaneously presented beeps and vice versa. Furthermore, for all ages, the relative strength of these interactions was predicted by the relative reliabilities of the two modalities, in other words, all observers weighted the signals appropriately. The degree of cross-modal interaction decreased with age: the youngest observers could not ignore the task-irrelevant modality—they fully combined vision and audition such that they perceived equal numbers of flashes and beeps for bi-modal stimuli. Older observers showed much smaller effects of the task-irrelevant modality. Do these interactions reflect optimal integration? Full or partial cross-modal integration predicts improved reliability in bi-modal conditions. In contrast, switching between modalities reduces reliability. Model comparison suggests that older observers employed partial integration, whereas younger observers (up to around 8 years) did not integrate, but followed a sub-optimal switching strategy, responding according to either visual or auditory information on each trial.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>To complete everyday activities, such as judging where or when something occurred, we combine information from multiple senses such as vision and audition. In adults, this merging of information is optimal: more reliable sensory estimates have more influence (higher weight) in the combined, multisensory estimate. Multisensory integration can result in illusions: if a single visual flash (e.g. a bright disk appearing briefly on a screen) occurs at the same time as two beeps, we sometimes perceive two flashes. This is because auditory information is generally more reliable than vision for judging <italic>when</italic> things happen; it dominates our audio-visual percept for temporal tasks. Previous work suggests that children don’t combine information from different senses in this adult-like way until around 10 years. To investigate this further, we asked children and adults to report the number of visual flashes or auditory beeps when these were presented simultaneously. Surprisingly, all children used appropriate sensory weights: audition—the more reliable signal—tended to dominate perception, with less weight given to vision. However, children didn’t show the adult-like reduction in uncertainty until around 8–10 years. Before that age, they switched between using only auditory or only visual information on each trial.</p>
</abstract>
<funding-group>
<funding-statement>The author received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="1"/>
<page-count count="17"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data are from human subjects. All data are available on request from the University of Southampton: DOI:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5258/SOTON/386590" xlink:type="simple">10.5258/SOTON/386590</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Imagine you are at an academic conference. A heated debate turns nasty and one scientist is repeatedly hit before falling to the floor. You are later asked how many punches were thrown. You confidently answer ‘3’; you were able to combine information from audition and vision, having both seen and heard the incident. We often receive information about the same object or event from multiple sensory modalities that we can integrate to improve the precision of our perceptual estimates. As adults, we integrate multisensory information for a variety of spatial and temporal tasks, such as judging the size, location, number or duration of objects or events [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004865.ref005">5</xref>]. A key benefit of this integration is that uncertainty, or variance (random noise) in the combined, multisensory estimate is reduced, relative to either of the component uni-sensory estimates, see e.g. [<xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>].</p>
<p>Under standard models of integration, sensory estimates are combined via weighted averaging, according to the estimates’ relative reliabilities, see e.g. [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>]. For example, consider the case in which an aspect of the environment is estimated from vision and audition. The visual and auditory estimates, <inline-formula id="pcbi.1004865.e001"><alternatives><graphic id="pcbi.1004865.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004865.e002"><alternatives><graphic id="pcbi.1004865.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are not perfectly precise, but contain noise with variance <inline-formula id="pcbi.1004865.e003"><alternatives><graphic id="pcbi.1004865.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004865.e004"><alternatives><graphic id="pcbi.1004865.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. It is commonly assumed that these noise distributions are Gaussian and independent. Under these assumptions, and given that the prior probability distribution over the estimated variable is uniform, then the optimal audio-visual estimate (i.e. that with the lowest possible variance), for a continuous variable is given by:
<disp-formula id="pcbi.1004865.e005">
<alternatives>
<graphic id="pcbi.1004865.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
with the visual and auditory weights, <italic>w</italic><sub><italic>V</italic></sub> and <italic>w</italic><sub><italic>A</italic></sub> defined as:
<disp-formula id="pcbi.1004865.e006">
<alternatives>
<graphic id="pcbi.1004865.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mtext> and </mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>As can be seen from the equations above, sensory weights give the relative influence of each uni-modal sensory estimate in determining responses to bi-modal stimuli. These weights can be estimated from behavioural data corresponding to bi-modal and uni-modal stimulus conditions. For example, in a size estimation task such as [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>], subjects might be required to estimate an object’s size from vision alone, from haptics (touch) alone, or from both vision and hatpics. If the visual size is 9cm and the haptic size is 12cm, then given unbiased uni-modal estimates, a mean bi-modal response of 10cm would correspond to visual and haptic weights of 2/3 and 1/3, respectively, i.e. vision has double the influence of haptics. These observed weights would be optimal if the uni-modal visual estimates were twice as reliable as the uni-modal haptic estimates, i.e. <inline-formula id="pcbi.1004865.e007"><alternatives><graphic id="pcbi.1004865.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Observing optimal sensory weights is consistent with optimal integration, i.e. the integration behaviour that minimises variance in the multimodal estimates. However, optimal sensory weights might be observed in the absence of integration: as an alternative to integration, an observer may select one of the uni-modal estimates on each trial, rather than computing a weighted average [<xref ref-type="bibr" rid="pcbi.1004865.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref008">8</xref>]. In the example above, the observer may select the visual estimate on 2/3 of trials, and the haptic estimate on 1/3 of trials. This ‘switching’ behaviour would produce the same mean response in bi-modal conditions as optimal integration, but with higher variance. Standard models predict that variance will be reduced in bi-modal, relative to uni-modal conditions under optimal integration, see, e.g. [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref009">9</xref>]. For example in the visual-haptic size example, under optimal integration the predicted variance of the visual-haptic estimates, <inline-formula id="pcbi.1004865.e008"><alternatives><graphic id="pcbi.1004865.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, is given by <inline-formula id="pcbi.1004865.e009"><alternatives><graphic id="pcbi.1004865.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. In contrast, switching behaviour will result in variance that is at least as large as the more reliable cue. For this reason, studies of multimodal integration generally determine (i) whether the sensory weights are optimal, given uni-sensory variability, and (ii) whether variability in the bi-modal estimates is reduced, relative to uni-modal estimates.</p>
<p>Recently, a number of studies have asked whether children show optimal integration of sensory cues, as indexed by (i) appropriate cue weighting and (ii) a reduction in variance, relative to single cue conditions. Gori and Burr [<xref ref-type="bibr" rid="pcbi.1004865.ref010">10</xref>] reported that optimal integration of multisensory information doesn’t appear until surprisingly late—at the age of around 10 years. In two visual-haptic tasks, younger children who were asked to judge object size or orientation relied on only one modality, and not necessarily the most reliable one. Other work has confirmed that children as old as 8 years fail to optimally integrate visual cues with movement-based information (proprioceptive and vestibular) for navigation [<xref ref-type="bibr" rid="pcbi.1004865.ref007">7</xref>], and another study suggests that optimal integration of auditory and haptic information does not occur until after age 11 [<xref ref-type="bibr" rid="pcbi.1004865.ref011">11</xref>]. Interestingly, this developmentally late integration is not limited to situations in which information must be combined from different sensory modalities: Nardini and colleagues reported similarly late integration for cues within a modality—optimal integration of two visual depth cues did not emerge until around age 12 [<xref ref-type="bibr" rid="pcbi.1004865.ref012">12</xref>].</p>
<p>The current study focuses on the developmental trajectory of audio-visual integration, using a straightforward counting task. The age at which optimal integration emerges for vision and audition is not yet clear. One previous audio-visual study with children aged 5–14 years and adults failed to find optimal integration at any age [<xref ref-type="bibr" rid="pcbi.1004865.ref013">13</xref>]. We employed a simple audio-visual task in which, on each trial, observers were presented with a number of beeps and / or flashes [<xref ref-type="bibr" rid="pcbi.1004865.ref014">14</xref>]. In separate blocks, they either reported the number of flashes, or the number of beeps. The task had the benefit of reduced memory and decisional demands, relative to previous studies that have used two-alternative forced choice designs. By comparing observers’ responses to different integration models we ask:</p>
<list list-type="order">
<list-item><p>Do children show optimal integration of auditory and visual information? If so, from what age?</p></list-item>
<list-item><p>Is integration mandatory? In our task, observers are asked to report only one modality or the other, i.e. either beeps, or flashes, rather than the number of audio-visual events. We ask whether children do ignore the irrelevant (non-reported) modality, and we determine whether the strength of cross-modal interactions changes as function of age.</p></list-item>
</list>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Data from 76 observers, split into 5 age groups, are summarised in <xref ref-type="fig" rid="pcbi.1004865.g001">Fig 1</xref>. Observers reported either the number of flashes (upper panels, green) or the number of beeps (lower panels, red). On some (uni-modal) trials, only flashes or only beeps were presented (<xref ref-type="fig" rid="pcbi.1004865.g001">Fig 1</xref>: horizontal dotted and dashed lines). These were intermingled with bi-modal trials in which both flashes and beeps were presented; the number of beeps and flashes could be the same or different.</p>
<fig id="pcbi.1004865.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Summary of observers’ data.</title>
<p>The reported number of flashes (upper row) and reported number of beeps (lower row). Each age group is shown by a separate column. Symbols give data from bi-modal trials. Horizontal dotted and dashed lines give responses on uni-modal trials, with the error bars / shaded regions giving ±1 SE across observers for bi- and uni-modal data, respectively. The influence of audition on vision (top row) or of vision on audition (lower row) is characterised by the slope of the best fitting regression lines (black lines). Regressions were performed for individual observers and subsequently averaged (for illustration only).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g001" xlink:type="simple"/>
</fig>
<p>On each trial, observers were explicitly asked to report either the number of flashes or the number of beeps, whilst ignoring the other modality. In the absence of any cross-modal interactions, data for the bi-modal conditions would fall along horizontal lines: increasing or decreasing the number of events in the task-irrelevant modality would have no effect on subjects’ responses. However, for all age groups, observers were unable to ignore the irrelevant, non-focal stimulus. The influence, or weight of the task irrelevant cue can be quantified by the slopes of the regression lines shown in <xref ref-type="fig" rid="pcbi.1004865.g001">Fig 1</xref>. These regression lines were fit to the bi-modal data separately for each observer and modality: one slope parameter quantifies the influence of audition on vision (upper plots) and another quantifies the influence of vision on audition (lower plots).</p>
<p>For all groups, the reported number of flashes was significantly modulated by the number of simultaneously presented beeps (one-sample <italic>t</italic>-tests against 0, all <italic>p</italic>&lt;0.01). Likewise, the reported number of beeps was significantly affected by the number of flashes (all <italic>p</italic>&lt;0.05). However, it is clear from <xref ref-type="fig" rid="pcbi.1004865.g001">Fig 1</xref> that the size of this cross-modal interaction depended on which modality was being reported: task-irrelevant beeps had a much larger effect on the number of reported flashes than vice versa. The mean weight for audition, when reporting flashes, <italic>w</italic><sub><italic>VA</italic></sub> (as defined by the regression coefficients) was 0.38. The mean weight given to vision when reporting the number of beeps (<italic>w</italic><sub><italic>AV</italic></sub>) was significantly smaller: 0.14 (main effect of modality: F<sub>1, 71</sub> = 57.6, <italic>p</italic>&lt;0.001).</p>
<p>We can ask whether the relative influence of the two modalities is predicted by their relative reliability, in line with standard models of optimal integration. In general, audition is more reliable than vision for temporal tasks such as the one employed here [<xref ref-type="bibr" rid="pcbi.1004865.ref015">15</xref>], and is therefore given more weight when integrated with vision, when observers are required to make temporal judgements. We can estimate the reliability of visual and auditory signals from the variance of observers’ responses on uni-modal trials. Across observers, audition was indeed more reliable than vision (mean variance for vision, <inline-formula id="pcbi.1004865.e010"><alternatives><graphic id="pcbi.1004865.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.36</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, for audition, <inline-formula id="pcbi.1004865.e011"><alternatives><graphic id="pcbi.1004865.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>).</p>
<p><xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2a</xref> summarises the relationship between the relative reliability of vision (<italic>rr</italic><sub><italic>V</italic></sub>), as estimated from the uni-modal responses, and the relative weight of vision (<italic>rw</italic><sub><italic>V</italic></sub>), as estimated from the bi-modal responses, where:
<disp-formula id="pcbi.1004865.e012">
<alternatives>
<graphic id="pcbi.1004865.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:mi>r</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mtext> and </mml:mtext><mml:mi>r</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>w</italic><sub><italic>AV</italic></sub> is the weight given to vision when reporting the auditory stimulus, and <italic>w</italic><sub><italic>VA</italic></sub> is the weight given to audition when reporting the visual stimulus. The relative influence of the two modalities is well predicted by their relative reliability for all groups (<xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2a</xref>) and this relationship is significant across all individual observers (<italic>r</italic> = 0.54, <italic>p</italic>&lt;0.001). It also reaches significance within the youngest (<italic>r</italic> = 0.77), middle (<italic>r</italic> = 0.67) and 4<sup>th</sup> (<italic>r</italic> = 0.57) age groups (all <italic>p</italic>&lt;0.05).</p>
<fig id="pcbi.1004865.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Sensory weights.</title>
<p>(a) The relationship between the relative reliability of vision and the relative influence of vision, for each age group. It is clear that the relative reliability of vision predicts its relative influence for all groups. Covariance ellipses give 1SE around the mean. Note that the relative reliability of audition, <italic>rr</italic><sub><italic>A</italic></sub>, and the relative weight for audition, <italic>rw</italic><sub><italic>A</italic></sub> can be calculated in an analogous way such that <italic>rr</italic><sub><italic>A</italic></sub> = 1 − <italic>rr</italic><sub><italic>V</italic></sub> and <italic>rw</italic><sub><italic>A</italic></sub> = 1 − <italic>rw</italic><sub><italic>V</italic></sub>. Thus, the relative reliability of audition predicts the relative weight for audition in exactly the same way as for vision. (b) Sensory weights for non-focal modalities. Red bars give the weight given to (task-irrelevant) auditory information when reporting the number of flashes, while green bars give the visual weight when estimating the number of beeps. Black bars show the amount of integration, as quantified by the sum of the weights given to non-focal cues. Asterisks show the groups for which this sum is significantly less than 1 (one-sample <italic>t</italic>-tests).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g002" xlink:type="simple"/>
</fig>
<p>Interestingly, the strength of cross-modal interactions, as indexed by the sum of the non-focal cue weights, decreased substantially with age (main effect of age group: F<sub>4, 71</sub> = 16.0, <italic>p</italic>&lt;0.001; see the black bars in <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). If participants used the same weights for visual and auditory information, irrespective of the task (report beeps vs. report flashes), then the sum of the non-focal weights would be 1 (dashed line, <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). This is the prediction under standard models of full integration, e.g. [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref002">2</xref>]. Conversely, if participants gave more weight to the focal, task relevant modality whilst down-weighting the task irrelevant one, the average weight would be less than 1, and would be 0 if observers were able ignore the task-irrelevant modality completely.</p>
<p>For the youngest observers only, the weights given to visual (red) and auditory (green) information did not vary according to whether observers were reporting beeps or flashes; the sum of the non-focal weights did not differ from 1 (see asterisks in <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). In other words, 4–5 year olds did not show any selectivity in reporting the focal, rather than non-focal modality. All other groups, however, showed partial cross-modal interactions: the reported number of flashes was dominated by visual information, whilst the reported number of beeps was dominated by auditory information, i.e. more weight was given to the focal modality, and the task-irrelevant modality was increasingly ignored as a function of age. Note that, in order to avoid floor or ceiling effects, the inter-stimulus interval (ISI) decreased with age (see <xref ref-type="sec" rid="sec009">Methods</xref>). A decrease in ISI (with all other factors constant) would be expected to increase uncertainty about the number of events, and thus increase the interaction between the sensory signals. For example, in the limiting case, the double flash illusion [<xref ref-type="bibr" rid="pcbi.1004865.ref016">16</xref>] will be eliminated with a large enough ISI. Note that the opposite pattern is seen here as a function of age—the influence of the non-focal cue <italic>decreases</italic> with age, despite the reduction in ISI. In other words, if the ISI had been more similar across age groups we would expect this age-related decrease in cross-modal interactions to be even larger.</p>
<p>Ernst [<xref ref-type="bibr" rid="pcbi.1004865.ref017">17</xref>] and Ernst &amp; Di Luca [<xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>] have described a variant of the standard Bayesian optimal integration model that allows partial integration, similarly to the behaviour described above. The model incorporates a ‘coupling prior’ that determines the strength of integration. This coupling prior represents the observer’s prior knowledge about the joint distribution of the two signals, i.e. the extent to which flashes and beeps tend to be correlated in the world and thus the probability that the visual and auditory signals contain redundant information. Under partial integration, variance in sensory estimates is no longer minimised, and in this sense, the integration strategy is no longer optimal. However, partial integration considers both precision (inverse variance) and accuracy (mean error). Under partial integration with potentially biased sensory signals, expected bias (inaccuracy) in the final estimates is reduced relative to full integration, whereas expected variance will be greater than under full integration. Partial integration can thus be described as optimal in the sense that it represents a balanced compromise between precision and accuracy [<xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>]. Note that the standard, full integration model represents a special case of the partial integration model, in which signals are assumed to be accurate and the coupling prior is infinitely narrow (see Model 1: Partial Integration).</p>
<p>The partial integration model has previously provided a good account of adults’ cross-modal integration in a similar, discretized task [<xref ref-type="bibr" rid="pcbi.1004865.ref005">5</xref>]. Similarly to standard models of integration, the partial integration model predicts a reliability benefit (i.e. a reduction in variability) when information is combined across modalities. However, the magnitude of this benefit is proportional to the strength of integration, and the reliability of the bi-modal estimates will not necessarily exceed the reliability of <italic>both</italic> of the component uni-modal estimates. However, bi-modal reliability should always improve relative to estimates from the focal modality alone. In other words, it predicts that our observers will be more reliable in reporting the number of flashes when both visual and auditory information is available, than from vision alone. We should expect a similar reliability improvement for bi-modal, relative to uni-modal auditory estimates. Furthermore, if all our observers were integrating optimally (i.e. following the Bayesian partial integration model) then the youngest group would show the largest bi-modal improvement in reliability, given that they show the strongest cross-modal interactions (black bars, <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). <xref ref-type="fig" rid="pcbi.1004865.g003">Fig 3</xref> compares response variance for uni- and bi-modal estimates for the 5 age groups.</p>
<fig id="pcbi.1004865.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Response variance as a function of age.</title>
<p>Lighter bars give response variance, averaged across uni-modal conditions for vision (V; green) and audition (A; red). Variance for bi-modal conditions is shown by darker bars for visual responses (VA; green) and auditory responses (AV; red). Error bars give ±1SE across observers.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g003" xlink:type="simple"/>
</fig>
<p>The youngest group shows no evidence of improved reliability in bi-modal, relative to uni-modal conditions for either vision or audition; their responses do not appear to reflect optimal sensory integration. All other groups show reduced variance in bi-modal, relative to uni-modal conditions for visual responses: compare dark and light green bars in <xref ref-type="fig" rid="pcbi.1004865.g003">Fig 3</xref>, this approached significance for 6–7 year olds and 8–9 year olds (<italic>p</italic> = 0.08, <italic>p</italic> = 0.07 for groups 6–7 and 8–9 years, all other age groups <italic>p</italic>&gt;0.1, from paired <italic>t</italic>-tests). However, there is little difference between uni- and bi-modal variance for auditory responses (red bars). Many older observers had little response variance in the uni-modal and bi-modal auditory conditions, and, given the discretized nature of the task, we must be somewhat cautious in using our observers’ response variance as an accurate estimator of their underlying sensory noise. Furthermore, older observers gave very little weight to vision in bi-modal auditory conditions (as indicated by the small slopes in the lower plots of <xref ref-type="fig" rid="pcbi.1004865.g001">Fig 1</xref>) and thus the predicted improvement under optimal partial integration is very small. For these reasons, we evaluated whether the Bayesian partial integration model provides a good account of observers’ behaviour by calculating the likelihood of each observer’s data given this model. We compared it with two other candidate models in which observers do not integrate auditory and visual information, but instead (sub-optimally) switch between them, responding on each trial according to only visual or only auditory information.</p>
<sec id="sec003">
<title>Modelling</title>
<p>Three models were compared: (i) Partial Integration, (ii) Focal Switching, and (iii) Modality Switching. Note that these were evaluated separately for each observer; averaged fits are shown in Figs <xref ref-type="fig" rid="pcbi.1004865.g004">4</xref>–<xref ref-type="fig" rid="pcbi.1004865.g007">7</xref> for illustration only. For all three models, because the number of events can take integer values only, noise distributions, and the resultant uni-sensory likelihoods were approximated by discretised Gaussians, i.e. the probability of a sensory estimate equal to <italic>x</italic>, is given by <inline-formula id="pcbi.1004865.e013"><alternatives><graphic id="pcbi.1004865.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, {<italic>x</italic> ∈ℤ | ≥ 0} where <italic>a</italic> is a normalising constant. Noise distributions were centred on the true stimulus value, <italic>μ</italic>, but differed in variance, <italic>σ</italic><sup>2</sup>, for vision and audition (see <xref ref-type="fig" rid="pcbi.1004865.g004">Fig 4</xref>).</p>
<fig id="pcbi.1004865.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Uni-modal likelihoods.</title>
<p>The best-fitting uni-modal likelihoods for vision (green) and audition (red), averaged (for illustration only) across all observers; they have been slightly horizontally offset for visibility. The spread of the likelihood (i.e. the inverse reliability) is fixed as a function of the number of events, but differs between vision and audition. On average, vision was less reliable than audition (<italic>σ</italic><sub><italic>V</italic></sub> = 0.772, SE = 0.052; <italic>σ</italic><sub><italic>A</italic></sub> = 0.488, SE = 0.058).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004865.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The partial integration model.</title>
<p>(a) An example bi-modal likelihood, centred on 1 flash and 3 beeps. The uni-modal marginals are shown alongside. (b) The coupling prior, and (c) the bi-modal likelihood after combination with the coupling prior; the peak of the distribution has shifted towards V = A. (d) The visual marginal (dashed green) is multiplied by the prior over the number of events (black) to give the posterior probability distribution of the number of visual events (solid green). (e) The posterior distribution for audition (solid red), given the prior over the number of events (black). Note that to allow easy comparison across the three models, the prior over the number of events is shown as a sequential step after the coupling prior is applied and the subsequent marginals are estimated. The two priors could equivalently be combined and applied in a single step. All plots show the averaged model fit across the set of observers (N = 36) who were best characterised by the PI model, as determined by comparing the likelihood of the data, given each of the three models.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g005" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004865.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Focal switching model.</title>
<p>Example (a) Uni-modal visual (green) and auditory (red) likelihoods. (b) On visual trials, the observer samples from the visual estimator with probability <italic>p</italic><sub><italic>F</italic></sub>, and from the auditory estimator with probability 1 − <italic>p</italic><sub><italic>F</italic></sub>. On auditory trials, these probabilities, or weights are reversed. The resultant likelihoods are shown in (c). Similarly to the PI model, posterior distributions (d, e, solid lines) are created by combining these likelihoods with a prior (black) over the number of events. All plots show the averaged model fit, averaged across the set of observers (N = 25) who were best characterised by the Focal Switching model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g006" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004865.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Modality switching model.</title>
<p>(a) Unimodal visual (green) and auditory (red) likelihoods. (b) On both visual and auditory trials, the observer samples the visual estimator with probability <italic>p</italic><sub><italic>V</italic></sub>, and the auditory estimator with probability 1 − <italic>p</italic><sub><italic>V</italic></sub>. The resultant likelihoods (slightly offset for visibility) are shown in (c). Posterior distributions (d, e, solid lines) are created by combining these likelihoods with a prior (black) over the number of events. All plots show the averaged model fit across the set of observers (N = 15) who were best characterised by the Modality Switching model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g007" xlink:type="simple"/>
</fig>
<p>In addition, alternative models were evaluated including the Causal Inference model [<xref ref-type="bibr" rid="pcbi.1004865.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref019">19</xref>], models with logarithmic coding of number (corresponding to skewed likelihoods in linear space), and those that allowed likelihoods to be biased and / or to vary in reliability as a function of the number of events (beeps or flashes). These other models provided an inferior account of the data, as described in the supporting information file: <xref ref-type="supplementary-material" rid="pcbi.1004865.s001">S1 Text</xref>.</p>
<p>The three models (Partial Integration, Focal Switching, Modality Switching) differ in the way that sensory information from vision and audition interact:</p>
<sec id="sec004">
<title>Model 1: Partial integration</title>
<p>The partial integration (PI) model [<xref ref-type="bibr" rid="pcbi.1004865.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref017">17</xref>] is a variant of the widely used, standard Bayesian model in which sensory estimates are always fully integrated, e.g. [<xref ref-type="bibr" rid="pcbi.1004865.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref020">20</xref>]. A coupling prior determines the strength of integration: a flat prior results in no integration—visual flashes and auditory beeps are processed independently. Conversely, a 2D prior given by the unity line V = A is equivalent to the standard full integration model, which would result in observers always perceiving an equal number of flashes and beeps. <xref ref-type="fig" rid="pcbi.1004865.g005">Fig 5</xref> depicts an intermediate case—the best-fitting coupling prior, averaged across all observers whose responses followed the PI model.</p>
<p>The coupling prior is given by <inline-formula id="pcbi.1004865.e014"><alternatives><graphic id="pcbi.1004865.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, {(<italic>x</italic><sub><italic>V</italic></sub>,<italic>x</italic><sub><italic>A</italic></sub>)∈ ℤ(<italic>x</italic><sub><italic>V</italic></sub>,<italic>x</italic><sub><italic>A</italic></sub>)≥0} where <italic>x</italic><sub><italic>V</italic></sub> and <italic>x</italic><sub><italic>A</italic></sub> are visual and auditory sensory estimates, <inline-formula id="pcbi.1004865.e015"><alternatives><graphic id="pcbi.1004865.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the variance of the coupling prior and <italic>a</italic> is a normalising constant. All three models include a prior over the number of events—models without this prior were inferior (see supporting information: <xref ref-type="supplementary-material" rid="pcbi.1004865.s001">S1 Text</xref>). The prior distribution over the number of events, <italic>s</italic>, is given by <inline-formula id="pcbi.1004865.e016"><alternatives><graphic id="pcbi.1004865.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, {<italic>s</italic>∈ℤ| <italic>s</italic> ≥ 0} where <italic>μ</italic><sub><italic>P</italic></sub> and <inline-formula id="pcbi.1004865.e017"><alternatives><graphic id="pcbi.1004865.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are the mean and variance of the prior and <italic>a</italic> is a normalising constant. For the PI model, one consequence of this prior is that responses from uni-modal trials are more biased than responses from congruent bi-modal trials; the prior has more influence on uni-modal trials when the available sensory information is less reliable.</p>
<p>Following the standard Bayes’ formulation, the posterior probability of a particular pair of visual and auditory estimates, <inline-formula id="pcbi.1004865.e018"><alternatives><graphic id="pcbi.1004865.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, is given by multiplying likelihoods and priors:
<disp-formula id="pcbi.1004865.e019">
<alternatives>
<graphic id="pcbi.1004865.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>μ</italic><sub><italic>V</italic></sub> and <italic>μ</italic><sub><italic>A</italic></sub> give the true number of flashes and beeps, respectively. On any single trial, the observer reports only one estimate: either the number of flashes, or the number of beeps. The posterior probability of a particular response is given by summing over all non-focal response estimates, i.e. finding the marginal probability distributions.</p>
<p>The PI model has 5 free parameters: (i) visual reliability, (ii) auditory reliability, (iii) width of coupling prior, <italic>σ</italic><sub><italic>C</italic></sub> (iv) mean <italic>μ</italic><sub><italic>P</italic></sub> and (v) spread <italic>σ</italic><sub><italic>P</italic></sub> of the prior over the number of events (beeps or flashes). Fitted values of these parameters are summarised in <xref ref-type="table" rid="pcbi.1004865.t001">Table 1</xref>.</p>
<table-wrap id="pcbi.1004865.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.t001</object-id>
<label>Table 1</label> <caption><title>Fitted parameters for the three models.</title></caption>
<alternatives>
<graphic id="pcbi.1004865.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2">Parameter</th>
<th align="left" colspan="2">Partial Integration (N = 36)</th>
<th align="left" colspan="2">Focal Switching (N = 25)</th>
<th align="left" colspan="2">Modality Switching (N = 15)</th>
</tr>
<tr>
<th align="left" colspan="2"/>
<th align="left"><italic>μ</italic></th>
<th align="left"><italic>σ</italic></th>
<th align="left"><italic>μ</italic></th>
<th align="left"><italic>σ</italic></th>
<th align="left"><italic>μ</italic></th>
<th align="left"><italic>σ</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">1</td>
<td align="left">Visual variability, <italic>σ</italic><sub><italic>V</italic></sub></td>
<td align="left">0.5433</td>
<td align="left">0.1749</td>
<td align="char" char=".">0.9943</td>
<td align="char" char=".">0.5329</td>
<td align="char" char=".">0.949</td>
<td align="char" char=".">0.5322</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">Auditory variability, <italic>σ</italic><sub><italic>A</italic></sub></td>
<td align="left">0.3441</td>
<td align="left">0.2473</td>
<td align="char" char=".">0.729</td>
<td align="char" char=".">0.7767</td>
<td align="char" char=".">0.4328</td>
<td align="char" char=".">0.1832</td>
</tr>
<tr>
<td align="left" rowspan="3">3</td>
<td align="left">Coupling prior spread, <italic>σ</italic><sub><italic>C</italic></sub></td>
<td align="left">5.31E+05<xref ref-type="table-fn" rid="t001fn002"><sup>a</sup></xref></td>
<td align="left">1.97E+06</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">—</td>
</tr>
<tr>
<td align="left">Focal weight, <italic>p</italic><sub><italic>F</italic></sub></td>
<td align="left">—</td>
<td align="left">—</td>
<td align="char" char=".">0.7579</td>
<td align="char" char=".">0.2151</td>
<td align="left">—</td>
<td align="left">—</td>
</tr>
<tr>
<td align="left">Visual weight, <italic>p</italic><sub><italic>V</italic></sub></td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="char" char=".">0.2146</td>
<td align="char" char=".">0.1534</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">Prior over n events, mean, <italic>μ</italic><sub><italic>P</italic></sub></td>
<td align="left">2.6168</td>
<td align="left">1.3401</td>
<td align="char" char=".">2.7884</td>
<td align="char" char=".">1.023</td>
<td align="char" char=".">2.6467</td>
<td align="char" char=".">0.8162</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">Prior over n events, std, <italic>σ</italic><sub><italic>P</italic></sub></td>
<td align="left">1.3705</td>
<td align="left">1.1015</td>
<td align="char" char=".">1.3723</td>
<td align="char" char=".">0.6105</td>
<td align="char" char=".">1.2695</td>
<td align="char" char=".">0.8499</td>
</tr>
<tr>
<td align="left" colspan="2">-Log likelihood</td>
<td align="left">70.15</td>
<td align="left">45.17</td>
<td align="char" char=".">143.67</td>
<td align="char" char=".">71.42</td>
<td align="char" char=".">97.41</td>
<td align="char" char=".">35.80</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>The values of the 5 parameters for each model that maximise the likelihood of observers’ data. For each of the three models, the mean and standard deviation of the parameter values are shown, across all observers whose data were best fit by that model.</p></fn>
<fn id="t001fn002"><p><sup>a</sup>The distribution of fitted values for this parameter was very skewed across observers; the median value was 0.79.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec005">
<title>Model 2: Focal switching</title>
<p>Rather than integrating visual and auditory information, observers might stochastically switch between the two—sometimes responding according to the visual information, and sometimes according to audition [<xref ref-type="bibr" rid="pcbi.1004865.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref008">8</xref>]. In the focal switching model, the distribution of responses depends on whether the observer is reporting the perceived number of flashes or beeps: observers select their estimate from the focal modality with probability <italic>p</italic><sub><italic>F</italic></sub> and the non-focal cue with probability (1-<italic>p</italic><sub><italic>F</italic></sub>). Equivalently, this strategy produces a combined, bi-modal likelihood that is a weighted sum of the two uni-modal likelihoods: if the observer is reporting flashes, the likelihood of a particular estimate, <italic>i</italic>, is given by a weighted average of the probabilities of that estimate given the visual and auditory likelihoods: <inline-formula id="pcbi.1004865.e020"><alternatives><graphic id="pcbi.1004865.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004865.e021"><alternatives><graphic id="pcbi.1004865.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the estimate from the combined, bi-modal likelihood, and <inline-formula id="pcbi.1004865.e022"><alternatives><graphic id="pcbi.1004865.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004865.e023"><alternatives><graphic id="pcbi.1004865.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are estimates from the visual (focal) and auditory (non-focal) likelihoods.</p>
<p>The model has 5 free parameters: (i) visual reliability, (ii) auditory reliability, (iii) focal probability and (iv) the mean and (v) variance of the prior over the number of events (beeps or flashes). Note that on conflict trials (in which n flashes ≠ n beeps), such as the example shown in <xref ref-type="fig" rid="pcbi.1004865.g006">Fig 6</xref>, cross-modal interactions produce an <italic>increase</italic> in variance, relative to the uni-modal likelihoods.</p>
</sec>
<sec id="sec006">
<title>Model 3: Modality switching</title>
<p>In the Modality Switching model, observers again stochastically sample from auditory and visual information. However, in this model observers sample visual information with probability <italic>p</italic><sub><italic>V</italic></sub>, and auditory information with probability <italic>p</italic><sub><italic>A</italic></sub> (where <italic>p</italic><sub><italic>V</italic></sub> + <italic>p</italic><sub><italic>A</italic></sub> = 1), irrespective of the focal modality. Under Modality Switching, <inline-formula id="pcbi.1004865.e024"><alternatives><graphic id="pcbi.1004865.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004865.e025"><alternatives><graphic id="pcbi.1004865.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is an estimate from the combined, bimodal likelihood, and <inline-formula id="pcbi.1004865.e026"><alternatives><graphic id="pcbi.1004865.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004865.e027"><alternatives><graphic id="pcbi.1004865.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004865.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are estimates from the visual and auditory likelihoods. In other words, for bi-modal conditions with a given number of flashes and beeps (e.g. 1 flash and 3 beeps, as shown in <xref ref-type="fig" rid="pcbi.1004865.g007">Fig 7</xref>), the model predicts the same pattern of responses, irrespective of whether the observer is reporting beeps or flashes. However, similarly to the Focal Switching model, when the visual and auditory estimates differ, bi-modal response variance will be increased, relative to variance in uni-modal conditions.</p>
<p>For older observers, responses were strongly modulated by the response modality, with more weight given to the focal cue (see <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). However, this was not the case for the youngest observers, who gave similar weight to vision and audition, irrespective of which was focal (compare the leftmost pair of bars in <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2b</xref>). The Modality Switching model could, therefore, provide a good fit to younger observers’ behaviour.</p>
</sec>
</sec>
<sec id="sec007">
<title>Modelling results</title>
<p>For each observer and each model, the values of the 5 free parameters were found (Matlab: fminsearch) that maximised the joint likelihood of the observer’s data across all uni-modal and bi-modal conditions. To avoid the problem of local minima, 288 iterations of the search were performed, making use of the University of Southampton’s IRIDIS High Performance Computing facility, with initial values uniformly sampled from the multidimensional space of plausible parameters. <xref ref-type="fig" rid="pcbi.1004865.g008">Fig 8</xref> shows how multisensory interactions change as a function of age. Observers in the two youngest groups were best described by the switching models. Children aged 8–9 years were evenly split, and by 10 years the majority of observers followed the partial integration model.</p>
<fig id="pcbi.1004865.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g008</object-id>
<label>Fig 8</label>
<caption>
<title>The best-fitting model of audio-visual interactions, as a function of age.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g008" xlink:type="simple"/>
</fig>
<p>As the three different models have common parameters, (visual and auditory noise, and the mean and variance of the prior over the number of events) we can consider how the best fitting values of these change as a function of age. Recent work [<xref ref-type="bibr" rid="pcbi.1004865.ref021">21</xref>] suggests that children as young as 7 quickly learn the statistics of a stimulus set and bias their estimates towards the mean. In the current study, knowledge of the stimulus statistics would be represented within the prior over the number of events. As participants learn these statistics we might expect both the mean and standard deviation of the prior to decrease, as participants learn that only a small numbers of beeps and / or flashes are presented. The youngest group had the weakest prior (largest standard deviation) of all age groups; this parameter varied significantly as a function of age (F<sub>4, 71</sub> = 3.03, <italic>p</italic>&lt;0.05). Post hoc comparisons showed that the youngest group had a significantly weaker prior than the 6–7 and 8–9 year olds (<italic>p</italic>&lt;0.05 from independent <italic>t</italic>-tests, after correction for multiple comparisons), no other comparisons were significant. Whilst the fitted prior for youngest group also had the largest mean, this did not vary significantly across groups. This provides some evidence that the youngest group may have been slower to learn the stimulus statistics.</p>
<p>As might be expected from the raw response variance shown in <xref ref-type="fig" rid="pcbi.1004865.g003">Fig 3</xref>, the fitted visual and auditory noise parameters also varied as a function of age (F<sub>4, 71</sub> = 6.8, <italic>p</italic>&lt;0.001; F<sub>4,71</sub> = 6.9, <italic>p</italic>&lt;0.0001, for <italic>σ</italic><sub><italic>V</italic></sub> and <italic>σ</italic><sub><italic>A</italic></sub>, respectively). Visual noise decreased monotonically with age, auditory noise decreased across each age group pair that shared a common stimulus ISI. Posthoc <italic>t</italic>-tests showed that, based on the fitted noise parameters, the youngest group was significantly more variable in both vision and audition than all other groups <italic>p</italic>&lt;0.01, after corrections for multiple comparisons). In the current paradigm ISI decreased with age (in order to broadly equate task difficulty). With a fixed ISI we would expect a larger increase in visual and auditory temporal acuity as a function of age.</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>A simple task was used to investigate the developmental trajectory of audio-visual integration. Importantly, we evaluated three different models of integration that together provide a good account of sensory integration behaviour at all stages of development. Key findings emerged:</p>
<list list-type="order">
<list-item><p>Observers of all ages combined visual and auditory information using appropriate sensory weights, as determined by the relative reliability of visual and auditory signals (see <xref ref-type="fig" rid="pcbi.1004865.g002">Fig 2a</xref>). This contrasts with previous findings in which younger observers relied entirely on one sensory estimate, and not always the most reliable one [<xref ref-type="bibr" rid="pcbi.1004865.ref010">10</xref>]. In that study, children viewed the front of the object, while touching the reverse side. It is has been suggested that the spatial offset and / or the fact that the active hand was obscured from view, prohibited cross-modal interactions in younger participants [<xref ref-type="bibr" rid="pcbi.1004865.ref022">22</xref>]; adults show reduced integration when sensory information is spatially offset [<xref ref-type="bibr" rid="pcbi.1004865.ref023">23</xref>]. Studies in which the sensory signals are aligned have found evidence for cross-modal interactions, i.e. switching behaviour, but not optimal integration, in younger children [<xref ref-type="bibr" rid="pcbi.1004865.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref022">22</xref>].</p></list-item>
<list-item><p>Integration was automatic, and younger children were far less able to ignore task-irrelevant sensory information than older observers. This is in broad agreement with recent work which suggests that 7–10 year olds are unable to ignore irrelevant visual stimuli when performing an auditory spatial discrimination task [<xref ref-type="bibr" rid="pcbi.1004865.ref024">24</xref>]. In the current study, the youngest observers fully combined auditory and visual information, such that for bi-modal stimuli they perceived the number of flashes and the number of beeps to be the same. Older observers’ behaviour was well modelled by a partial integration model in which the coupling of visual and auditory information was relatively weak.</p></list-item>
<list-item><p>Optimal integration, as indexed by increased reliability, emerged by 10 years—this is broadly in line with previous findings [<xref ref-type="bibr" rid="pcbi.1004865.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref012">12</xref>]. Before this age, model comparison suggests that observers do not integrate sensory information, but stochastically sample from each modality. Our finding suggests that optimal integration of auditory and visual signals develops at a similar to age to integration across and within other modalities. Why did a previous study fail to find optimal audio-visual integration of temporal signals [<xref ref-type="bibr" rid="pcbi.1004865.ref013">13</xref>]? The study used a temporal bi-section task in which observers estimated which of two empty intervals was longer. Subsequent work has shown that for this type of task, with empty intervals, observers integrate auditory and visual information to optimally estimate the time points at the ends of the interval, rather than integrating duration per se [<xref ref-type="bibr" rid="pcbi.1004865.ref004">4</xref>]. With filled intervals, it is likely that optimal integration of duration estimates would be found with children aged 10 or so, as it is in adults [<xref ref-type="bibr" rid="pcbi.1004865.ref004">4</xref>].</p></list-item>
</list>
<p>Sensory integration has the potential to provide benefits for virtually all of our everyday activities—precision is improved by combining redundant information sources either within or across modalities. An obvious question remains unanswered—why does this ability fail to appear until around 10 years? One proposed explanation is that the lack of integration is beneficial during early childhood, and facilitates recalibration [<xref ref-type="bibr" rid="pcbi.1004865.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref012">12</xref>]. During this period of growth and sensory development, constant sensory recalibration is required in order to maintain accurate (unbiased) perceptual estimates. Recalibration requires the estimation of inter-sensory conflict—if this were only possible in the absence of integration, i.e. by keeping sensory estimates separate, then the developing sensory system might forego integration in favour of recalibration. The importance of cross-sensory interaction for sensory calibration and development is supported by studies in populations with sensory impairments—congenital visual deficits appear to have a detrimental effect on the precision of haptic estimates and vice versa [<xref ref-type="bibr" rid="pcbi.1004865.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref026">26</xref>].</p>
<p>Studies with adult observers, however, suggest that integration and recalibration are not mutually exclusive. For example, when glasses distort the relationship between binocular disparity and depth, the perceptual system recalibrates accordingly, whilst continuing to integrate binocular disparity with other depth cues [<xref ref-type="bibr" rid="pcbi.1004865.ref027">27</xref>]. Moreover, the sensory system adapts relatively quickly (within hours) when sensory statistics change [<xref ref-type="bibr" rid="pcbi.1004865.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004865.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004865.ref030">30</xref>]. In fact, recalibration and integration both rely on establishing the correspondence between signals—identifying which signals are redundant and only integrating (or recalibrating) when they arise from the same source. It might be that younger children find this correspondence hard to learn [<xref ref-type="bibr" rid="pcbi.1004865.ref031">31</xref>]. In the current study, observers were told to ignore one modality—adults were able to do this to a large extent, whereas children were sub-optimal in the sense that cross-modal influences were larger, even though vision and audition were discrepant on the majority of trials. A previous study also found that the effect of auditory beeps on the reported number of flashes was larger in children than adults [<xref ref-type="bibr" rid="pcbi.1004865.ref032">32</xref>]. However, that study did not use a design that allowed optimal integration to be evaluated.</p>
<p>One recent study using a visual-proprioceptive reaching task did find some evidence of optimal integration, as evidenced by a reliability benefit, in children as young as 4–6 years [<xref ref-type="bibr" rid="pcbi.1004865.ref033">33</xref>]. However, this was only for the subset of observers who showed similar reliability for visual and proprioceptive estimates. Sub-optimal behaviour in other observers was attributed to inappropriate weighting. However, because the study did not include cue-conflict conditions, precise estimation of cue weightings was not possible. Our data suggest that, at least for the current task, the lack of integration shown by our observers was not due to a failure to weight the available signals appropriately.</p>
<p>In summary, the current work suggests that optimal integration does not emerge until around 10 years. Model comparison suggests that before that age, observers switch between the information provided by the two modalities, but do so in accordance with their relative reliabilities. This behaviour does result in responses centred on optimal values, but variance is larger than under optimal integration. In contrast with previous work, our younger observers did not rely on a single modality—in fact they were less able to ignore task-irrelevant information. Instead, they instead showed stronger, mandatory cross-sensory interactions than older observers.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec010">
<title>Stimuli</title>
<p>Visual stimuli were white discs subtending 2.2 degrees of visual angle (dva) at the viewing distance of 45 cm with a luminance of 196 cd/m<sup>2</sup>. These were presented briefly (1 flash = 16.7msec), centred at 5.7dva to the left or right (randomly across trials) of a central fixation cross on an otherwise black screen. Auditory stimuli were presented via small speakers placed either side of the screen. These consisted of short beeps: 440Hz tones in a Gaussian temporal envelope of σ = 21msec. To reduce the reliability of the auditory stimuli, these beeps were embedded in continuous white noise [<xref ref-type="bibr" rid="pcbi.1004865.ref004">4</xref>]. As in previous studies, sequences of flashes and beeps were temporally aligned [<xref ref-type="bibr" rid="pcbi.1004865.ref014">14</xref>], see <xref ref-type="fig" rid="pcbi.1004865.g009">Fig 9a</xref>.</p>
<fig id="pcbi.1004865.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004865.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Trial schematic.</title>
<p>(a) Instructions were shown at the start of each block of trials, and the voice of Stinker the dog gave the same instructions. A progress bar showed Stinker getting closer to his treats, as more trials were completed. (b) Either an ‘F’ or ‘B’ in the centre of the screen reminded the participant of the task. (c) After the letter was clicked, flashes, beeps or both were presented. The inset shows an example congruent trial (upper) and conflict trial (lower). (d) The participant was prompted to respond. An image of Stinker the dog appeared every few trials, with Stinker’s voice offering words of encouragement or comments, e.g. ‘You’re great’, or ‘I’m hungry’.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.g009" xlink:type="simple"/>
</fig>
<p>The spacing between events (the ISI) was varied as a function of age group, as determined by pilot work. This was done to roughly equate task difficulty across groups such that floor or ceiling effects were avoided: pilot work showed that a fixed ISI across groups resulted in floor effects for the youngest group (such that the number of perceived events did not systematically increase as a function of the true number of events) and / or ceiling effects in the adult group (no response errors). For children in school years 1–3 (infant school; age 4–7 years) beeps and / or flashes were spaced by an ISI of 200msec. For junior school children (school years 4–7; age 7–11 years) the ISI was 167msec and for adults it was 117msec.</p>
</sec>
<sec id="sec011">
<title>Procedure</title>
<p>All participants were given detailed instructions, and completed 8 practice trials in which they reported flashes (4 trials) or beeps (4 trials). When counting flashes, subjects were told to ignore any beeps and vice versa. To help with motivation and concentration, participants were told that they needed to help Stinker the dog count beeps or flashes in order to get his treats. At the start of each block of experimental trials, Stinker appeared on the screen and instructed the participant to ‘count the flashes’ or ‘count the beeps’.</p>
<p>Each trial began with an ‘F’ or a ‘B’ presented at the screen’s centre to remind participants of the current task. To ensure fixation, participants were required to use the mouse to click this letter. The letter then changed to a green fixation cross and the sequence of flashes and / or beeps was presented. Each sequence consisted of 0–3 beeps and 0–3 flashes, such that the trial could be uni-modal (only beeps or only flashes), bi-modal congruent (equal number– 1, 2, or 3 –of flashes and beeps) or bi-modal conflict (the number of beeps and flashes differed by 1 or 2). Uni-modal and bi-modal trials were randomly intermingled, but trials were blocked by focal modality (i.e. report flashes, or report beeps).</p>
<p>Participants gave their response on each trial by selecting the appropriate number on the keyboard (1–9); they were not told the maximum or minimum number of possible beeps or flashes. To keep the task duration within the concentration span of the child participants (approximately 20 minutes, based on pilot work), infant school children completed 140 trials (2 modalities: judging beeps or flashes) x 10 conditions (3 uni-modal, 7 bi-modal) x 7 repetitions. Junior school children completed 8 repetitions (160 trials) and adults completed 12 repetitions (240 trials).</p>
</sec>
<sec id="sec012">
<title>Participants</title>
<p>We report data from 76 observers (60 children, 16 adults). A further 5 children from the 4–6 age group were excluded who failed to complete the task and / or could not reliably count up to 3. To check for counting ability / task comprehension, we used leave-one-out cross validation to compare regression models for each observer’s data, to ensure that the reported number of events across uni-modal and bi-modal congruent trials increased significantly as a function of the true number of events.</p>
<p>Children were <italic>a priori</italic> divided into four age groups, by splitting the infant and junior school children at the midpoint of each age range, such that all children within a group were given the same stimulus set (i.e. the same ISI). The resultant 5 groups were (i) ‘4–6 Years’: Range: 4 years 9 months to 6 years 3 months, <italic>n</italic> = 9, 6 males (ii) ‘6–7 Years’: Range 6yrs 5m to 7yrs 8m, <italic>n</italic> = 11, 7 males (iii) ‘8–9 Years’: 7yrs 9m to 9 yrs 8m, <italic>n</italic> = 19, 9 males (iv) ‘10–11 Years’: 9yrs 11m to 11yrs 5m, <italic>n</italic> = 22, 8 males and (v) ‘Adults’: Range 18–41 years, <italic>n</italic> = 16, 8 males). The study was approved by the ethics committee at the University of Southampton and all participants gave informed consent. Parents / guardians gave consent on behalf of their children and children also provided consent on the day of the experiment.</p>
</sec>
</sec>
<sec id="sec013">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004865.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004865.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Description and evaluation of alternative models.</title>
<p>The supporting information provides a description and evaluation of alternative models of observers’ data. First, we apply the Causal Inference model [<xref ref-type="bibr" rid="pcbi.1004865.ref018">18</xref>], with three different decision rules [<xref ref-type="bibr" rid="pcbi.1004865.ref019">19</xref>]. Second, we test whether participants’ responses are more consistent with log-based coding of number. Finally, we show that more complex models, such as those in which the likelihoods can be biased, or noise changes as a function of the number of events, do not provide a significantly better fit to the data.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The author thanks Iona Kerrigan for conducting pilot work that informed the current study; Kristiana Newton for collecting data and Erich Graf for assisting in data collection and providing the voice for Stinker the dog. She also acknowledges the use of the IRIDIS High Performance Computing Facility, and associated support services at the University of Southampton, in the completion of this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004865.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>(<issue>6870</issue>):<fpage>429</fpage>–<lpage>33</lpage>. <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Current Biology</source>. <year>2004</year>;<volume>14</volume>(<issue>3</issue>):<fpage>257</fpage>–<lpage>62</lpage>. <object-id pub-id-type="pmid">14761661</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Helbig</surname> <given-names>HB</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>Optimal integration of shape information from vision and touch</article-title>. <source>Experimental Brain Research</source>. <year>2007</year>;<volume>179</volume>(<issue>4</issue>):<fpage>595</fpage>–<lpage>606</lpage>. <object-id pub-id-type="pmid">17225091</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hartcher-O'Brien</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Di Luca</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>The Duration of Uncertain Times: Audiovisual Information about Intervals Is Integrated in a Statistically Optimal Fashion</article-title>. <source>Plos One</source>. <year>2014</year>;<volume>9</volume>(<issue>3</issue>).</mixed-citation></ref>
<ref id="pcbi.1004865.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bresciani</surname> <given-names>J-P</given-names></name>, <name name-style="western"><surname>Dammeier</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>Vision and touch are automatically integrated for the perception of sequences of events</article-title>. <source>Journal of Vision</source>. <year>2006</year>;<volume>6</volume>(<issue>5</issue>):<fpage>554</fpage>–<lpage>64</lpage>. <object-id pub-id-type="pmid">16881788</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref006"><label>6</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>di Luca</surname> <given-names>M</given-names></name>. <chapter-title>Multisensory perception: from integration to remapping</chapter-title>. In: <name name-style="western"><surname>Trommershauser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>MS</surname> <given-names>L</given-names></name>, editors. <source>Sensory cue integration</source><year>2012</year>. p. <fpage>224</fpage>–<lpage>50</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004865.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bedford</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Braddick</surname> <given-names>O</given-names></name>. <article-title>Development of cue integration in human navigation</article-title>. <source>Current Biology</source>. <year>2008</year>;<volume>18</volume>(<issue>9</issue>):<fpage>689</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2008.04.021" xlink:type="simple">10.1016/j.cub.2008.04.021</ext-link></comment> <object-id pub-id-type="pmid">18450447</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>. <chapter-title>Computational models of sensorimotor integration</chapter-title>. In: <name name-style="western"><surname>Morasso</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>V</surname> <given-names>S</given-names></name>, editors. <source><italic>Self-organization</italic>, <italic>computational maps and motor control</italic></source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Elsevier Press</publisher-name>; <year>1997</year>. p. <fpage>117</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004865.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hillis</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Watt</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Slant from texture and disparity cues: Optimal cue combination</article-title>. <source>Journal of Vision</source>. <year>2004</year>;<volume>4</volume>(<issue>12</issue>):<fpage>967</fpage>–<lpage>92</lpage>. <object-id pub-id-type="pmid">15669906</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Del Viva</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sandini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>DC</given-names></name>. <article-title>Young children do not integrate visual and haptic form information</article-title>. <source>Current Biology</source>. <year>2008</year>;<volume>18</volume>(<issue>9</issue>):<fpage>694</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2008.04.036" xlink:type="simple">10.1016/j.cub.2008.04.036</ext-link></comment> <object-id pub-id-type="pmid">18450446</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petrini</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Remark</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>. <article-title>When vision is not an option: children's integration of auditory and haptic information is suboptimal</article-title>. <source>Developmental Science</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>):<fpage>376</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/desc.12127" xlink:type="simple">10.1111/desc.12127</ext-link></comment> PubMed WOS:000334693300005. <object-id pub-id-type="pmid">24612244</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bedford</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mareschal</surname> <given-names>D</given-names></name>. <article-title>Fusion of visual cues is not mandatory in children</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2010</year>;<volume>107</volume>(<issue>39</issue>):<fpage>17041</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1001699107" xlink:type="simple">10.1073/pnas.1001699107</ext-link></comment> <object-id pub-id-type="pmid">20837526</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sandini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>Development of visuo-auditory integration in space and time</article-title>. <source>Frontiers in integrative neuroscience</source>. <year>2012</year>;<volume>6</volume>:<fpage>77-</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnint.2012.00077" xlink:type="simple">10.3389/fnint.2012.00077</ext-link></comment> <object-id pub-id-type="pmid">23060759</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>. <article-title>Sound-induced flash illusion as an optimal percept</article-title>. <source>Neuroreport</source>. <year>2005</year>;<volume>16</volume>(<issue>17</issue>):<fpage>1923</fpage>–<lpage>7</lpage>. <object-id pub-id-type="pmid">16272880</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Morrone</surname> <given-names>MC</given-names></name>. <article-title>Auditory dominance over vision in the perception of interval duration</article-title>. <source>Experimental Brain Research</source>. <year>2009</year>;<volume>198</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-009-1933-z" xlink:type="simple">10.1007/s00221-009-1933-z</ext-link></comment> <object-id pub-id-type="pmid">19597804</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kamitani</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>. <article-title>Illusions—What you see is what you hear</article-title>. <source>Nature</source>. <year>2000</year>;<volume>408</volume>(<issue>6814</issue>):<fpage>788-</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004865.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>Learning to integrate arbitrary signals from vision and touch</article-title>. <source>Journal of Vision</source>. <year>2007</year>;<volume>7</volume>(<issue>5</issue>).</mixed-citation></ref>
<ref id="pcbi.1004865.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koerding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Causal Inference in Multisensory Perception</article-title>. <source>Plos One</source>. <year>2007</year>;<volume>2</volume>(<issue>9</issue>).</mixed-citation></ref>
<ref id="pcbi.1004865.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Probability Matching as a Computational Strategy Used in Perception</article-title>. <source>Plos Computational Biology</source>. <year>2010</year>;<volume>6</volume>(<issue>8</issue>).</mixed-citation></ref>
<ref id="pcbi.1004865.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adams</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Mamassian</surname> <given-names>P</given-names></name>. <article-title>Bayesian combination of ambiguous shape cues</article-title>. <source>Journal of Vision</source>. <year>2004</year>;<volume>4</volume>(<issue>10</issue>):<fpage>921</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">15595895</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sciutti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Saracco</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sandini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gori</surname> <given-names>M</given-names></name>. <article-title>Development of context dependency in human space perception</article-title>. <source>Experimental Brain Research</source>. <year>2014</year>;<volume>232</volume>(<issue>12</issue>):<fpage>3965</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-014-4021-y" xlink:type="simple">10.1007/s00221-014-4021-y</ext-link></comment> <object-id pub-id-type="pmid">25183158</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref022"><label>22</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Drewing</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jovanovic</surname> <given-names>B</given-names></name>. <chapter-title>Visuo-haptic Length Judgments in Children and Adults</chapter-title>. <source>Haptics: Generating and Perceiving Tangible Sensations, Pt Ii, Proceedings</source>. <year>2010</year>;<volume>6192</volume>:<fpage>438</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004865.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gepshtein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>The combination of vision and touch depends on spatial proximity</article-title>. <source>Journal of Vision</source>. <year>2005</year>;<volume>5</volume>(<issue>11</issue>):<fpage>1013</fpage>–<lpage>23</lpage>. <object-id pub-id-type="pmid">16441199</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petrini</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>. <article-title>Hearing Where the Eyes See: Children Use an Irrelevant Visual Cue When Localizing Sounds</article-title>. <source>Child Development</source>. <year>2015</year>;<volume>86</volume>(<issue>5</issue>):<fpage>1449</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/cdev.12397" xlink:type="simple">10.1111/cdev.12397</ext-link></comment> <object-id pub-id-type="pmid">26228618</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sandini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Martinoli</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>Poor Haptic Orientation Discrimination in Nonsighted Children May Reflect Disruption of Cross-Sensory Calibration</article-title>. <source>Current Biology</source>. <year>2010</year>;<volume>20</volume>(<issue>3</issue>):<fpage>223</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2009.11.069" xlink:type="simple">10.1016/j.cub.2009.11.069</ext-link></comment> <object-id pub-id-type="pmid">20116249</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tinelli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Sandini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cioni</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>Impaired visual size-discrimination in children with movement disorders</article-title>. <source>Neuropsychologia</source>. <year>2012</year>;<volume>50</volume>(<issue>8</issue>):<fpage>1838</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuropsychologia.2012.04.009" xlink:type="simple">10.1016/j.neuropsychologia.2012.04.009</ext-link></comment> <object-id pub-id-type="pmid">22569216</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adams</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>van Ee</surname> <given-names>R</given-names></name>. <article-title>Adaptation to three-dimensional distortions in human vision</article-title>. <source>Nature Neuroscience</source>. <year>2001</year>;<volume>4</volume>(<issue>11</issue>):<fpage>1063</fpage>–<lpage>4</lpage>. <object-id pub-id-type="pmid">11584290</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adams</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Graf</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>M</given-names></name>. <article-title>Experience can change the 'light-from-above' prior</article-title>. <source>Nature Neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>10</issue>):<fpage>1057</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">15361877</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerrigan</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>WJ</given-names></name>. <article-title>Learning different light prior distributions for different contexts</article-title>. <source>Cognition</source>. <year>2013</year>;<volume>127</volume>(<issue>1</issue>):<fpage>99</fpage>–<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2012.12.011" xlink:type="simple">10.1016/j.cognition.2012.12.011</ext-link></comment> <object-id pub-id-type="pmid">23376295</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>The statistical determinants of adaptation rate in human reaching</article-title>. <source>Journal of Vision</source>. <year>2008</year>;<fpage>8</fpage>(4).</mixed-citation></ref>
<ref id="pcbi.1004865.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>Multisensory integration: A late bloomer</article-title>. <source>Current Biology</source>. <year>2008</year>;<volume>18</volume>(<issue>12</issue>):<fpage>R519</fpage>–<lpage>R21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2008.05.002" xlink:type="simple">10.1016/j.cub.2008.05.002</ext-link></comment> <object-id pub-id-type="pmid">18579094</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Innes-Brown</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Barutchu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shivdasani</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Crewther</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Grayden</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Paolini</surname> <given-names>A</given-names></name>. <article-title>Susceptibility to the flash-beep illusion is increased in children compared to adults</article-title>. <source>Developmental Science</source>. <year>2011</year>;<volume>14</volume>(<issue>5</issue>):<fpage>1089</fpage>–<lpage>99</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-7687.2011.01059.x" xlink:type="simple">10.1111/j.1467-7687.2011.01059.x</ext-link></comment> <object-id pub-id-type="pmid">21884324</object-id></mixed-citation></ref>
<ref id="pcbi.1004865.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Begus</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mareschal</surname> <given-names>D</given-names></name>. <article-title>Multisensory Uncertainty Reduction for Hand Localization in Children and Adults</article-title>. <source>Journal of Experimental Psychology-Human Perception and Performance</source>. <year>2013</year>;<volume>39</volume>(<issue>3</issue>):<fpage>773</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0030719" xlink:type="simple">10.1037/a0030719</ext-link></comment> <object-id pub-id-type="pmid">23163790</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>