<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="other" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.3000125</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-18-00033</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Community Page</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data management</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Biological data management</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research facilities</subject><subj-group><subject>Information centers</subject><subj-group><subject>Archives</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research assessment</subject><subj-group><subject>Reproducibility</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Science policy</subject><subj-group><subject>Science and technology workforce</subject><subj-group><subject>Careers in research</subject><subj-group><subject>Scientists</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Scientists</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information technology</subject><subj-group><subject>Databases</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Industrial engineering</subject><subj-group><subject>Quality control</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Programming languages</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Developing a modern data workflow for regularly updated data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6969-1848</contrib-id>
<name name-style="western">
<surname>Yenni</surname>
<given-names>Glenda M.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5635-2502</contrib-id>
<name name-style="western">
<surname>Christensen</surname>
<given-names>Erica M.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bledsoe</surname>
<given-names>Ellen K.</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0072-029X</contrib-id>
<name name-style="western">
<surname>Supp</surname>
<given-names>Sarah R.</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0803-4734</contrib-id>
<name name-style="western">
<surname>Diaz</surname>
<given-names>Renata M.</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6728-7745</contrib-id>
<name name-style="western">
<surname>White</surname>
<given-names>Ethan P.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6026-8530</contrib-id>
<name name-style="western">
<surname>Ernest</surname>
<given-names>S. K. Morgan</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, Florida, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>School of Natural Resources and the Environment, University of Florida, Gainesville, Florida, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Data Analytics Program, Denison University, Granville, Ohio, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Informatics Institute, University of Florida, Gainesville, Florida, United States of America</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Biodiversity Institute, University of Florida, Gainesville, Florida, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">glenda@weecology.org</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>29</day>
<month>1</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>1</month>
<year>2019</year>
</pub-date>
<volume>17</volume>
<issue>1</issue>
<elocation-id>e3000125</elocation-id>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Yenni et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.3000125"/>
<abstract>
<p>Over the past decade, biology has undergone a data revolution in how researchers collect data and the amount of data being collected. An emerging challenge that has received limited attention in biology is managing, working with, and providing access to data under continual active collection. Regularly updated data present unique challenges in quality assurance and control, data publication, archiving, and reproducibility. We developed a workflow for a long-term ecological study that addresses many of the challenges associated with managing this type of data. We do this by leveraging existing tools to 1) perform quality assurance and control; 2) import, restructure, version, and archive data; 3) rapidly publish new data in ways that ensure appropriate credit to all contributors; and 4) automate most steps in the data pipeline to reduce the time and effort required by researchers. The workflow leverages tools from software development, including version control and continuous integration, to create a modern data management system that automates the pipeline.</p>
</abstract>
<abstract abstract-type="toc">
<p>This Community Page article describes a data management workflow that can be readily implemented by small research teams and which solves the core challenges of managing regularly updating data. It includes a template repository and tutorial to assist others in setting up their own regularly updating data management systems.</p>
</abstract>
<funding-group>
<funding-statement>This research was supported by the National Science Foundation through grant 1622425 to SKME and by the Gordon and Betty Moore Foundation’s Data-Driven Discovery Initiative through grant GBMF4563 to EPW. RMD was supported by a National Science Foundation Graduate Research Fellowship (Grant No. DGE-1315138 and DGE-1842473).</funding-statement>
</funding-group>
<counts>
<fig-count count="1"/>
<table-count count="0"/>
<page-count count="12"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-02-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/weecology/PortalData" xlink:type="simple">https://github.com/weecology/PortalData</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Biology has transitioned from a field in which data are collected in hand-written notes by lone scientists to a discipline that increasingly involves large amounts of data collected by collaborative teams. The impact of the increased volume of data being collected has been extensively discussed in biology [<xref ref-type="bibr" rid="pbio.3000125.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref002">2</xref>], but there has also been a revolution in the frequency with which data are collected [<xref ref-type="bibr" rid="pbio.3000125.ref003">3</xref>]. Instead of one-time data collection, biologists are increasingly collecting data that require databases to be regularly updated with new information. Long-term observational studies, experiments with repeated sampling, use of automatic sensors, and ongoing literature mining to build data compilations all produce continually updating data. These data are being used to address problems that require regularly updating data streams, including adaptive monitoring and management [<xref ref-type="bibr" rid="pbio.3000125.ref004">4</xref>], iterative near-term forecasting [<xref ref-type="bibr" rid="pbio.3000125.ref005">5</xref>], detecting and preventing ecological transitions [<xref ref-type="bibr" rid="pbio.3000125.ref006">6</xref>], and monitoring real-time cancer metabolism [<xref ref-type="bibr" rid="pbio.3000125.ref007">7</xref>]. Thus, whether studying changes in gene expression over time or long-term population dynamics, data that are being analyzed while they are still undergoing data collection are becoming a pervasive aspect of biology.</p>
<p>Data that are frequently updated present unique challenges for effective data management, reproducibility, and credit. Regularly updated data (<xref ref-type="boxed-text" rid="pbio.3000125.box001">Box 1</xref>) requires continual data entry, data integration, and error checking. This need for continually active data management places an extra burden on researchers and increases the potential for delays between when data are collected and when they are available to analyze. Since the data are continually changing, it is also essential to have methods for tracking, comparing, and archiving different versions of the data to support reproducibility [<xref ref-type="bibr" rid="pbio.3000125.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref008">8</xref>]. Finally, since new contributors often join ongoing projects, a method is needed that provides credit to new contributors while still allowing the impact of the project as a whole to be tracked. While strategies for managing large amounts of actively updated data exist in biology, they are typically limited to large, institutionalized data collection efforts with dedicated informatics groups. To reduce delays and burden on individual labs and small teams, researchers need accessible protocols that promote rapid, ongoing data entry, versioning, archiving, and documentation.</p>
<boxed-text id="pbio.3000125.box001" position="float">
<sec id="sec002">
<title>Box 1. Terminology</title>
<p>This regularly updated biological data differs from conventional “streaming data” in that it typically involves manually collected data, requires data entry, and is not truly continuous in nature. This type of data has been referred to by a variety of terms including “dynamic data” [<xref ref-type="bibr" rid="pbio.3000125.ref037">37</xref>], “evolving data” [<xref ref-type="bibr" rid="pbio.3000125.ref038">38</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref039">39</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref018">18</xref>], and “living data” [<xref ref-type="bibr" rid="pbio.3000125.ref040">40</xref>], but there is no general consensus on the appropriate terminology. To communicate more effectively across these different groups, we chose to simply describe the key aspect of this data that makes it a challenge to work with: the fact that this is data under continuing data collection that results in frequent updating of data files.</p>
</sec>
</boxed-text>
<p>As a small group of researchers managing an ongoing, long-term research project, we have grappled with the challenges of managing data that is regularly updated and making it publicly available. Our research involves automated and manual data collection efforts at daily to annual frequencies conducted over 40 years by a regularly changing group of personnel (<xref ref-type="boxed-text" rid="pbio.3000125.box002">Box 2</xref>; for details on our study and data collection see [<xref ref-type="bibr" rid="pbio.3000125.ref009">9</xref>]). Thus, our experience covers much of the range of challenges that biologists are struggling to manage when their data are continually being updated. We designed a modern workflow system to expedite the management of data streams, ranging from hourly data collected by automated weather stations to plant and animal data recorded on datasheets in the field. We have designed our process to mitigate the data management workload by automating much of the data management pipeline. We use a variety of tools that range from those commonly used in biology (e.g., Microsoft Excel and programming in R) to tools that have primarily been used in only the highly computational areas of biological research (e.g., version control and continuous integration; for more information see <xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref> and <xref ref-type="supplementary-material" rid="pbio.3000125.s002">S2 Box</xref>, respectively). We use these tools not only to help with the regular addition of new data but also to provide clear documentation when we find and fix existing errors in the database that evaded earlier quality assurance/quality control (QA/QC) procedures. Here, we describe our approach with the goal of allowing others to implement similar data management systems and to improve the data management of regularly updated data more broadly.</p>
<boxed-text id="pbio.3000125.box002" position="float">
<sec id="sec003">
<title>Box 2. The model system for this paper</title>
<p>Our data are generated by the Portal Project, a long-term study in ecology that is currently run by our research group [<xref ref-type="bibr" rid="pbio.3000125.ref009">9</xref>]. The project was established by Dr. James H. Brown in 1977 in the southwestern United States to study competition among rodents and ants and the impact of these species on desert plants [<xref ref-type="bibr" rid="pbio.3000125.ref041">41</xref>]. We collect data for several datasets that regularly update at different frequencies (hourly, monthly, biannually, and annually), and each data set presents its own challenges.</p>
<p><italic>Low-frequency, sample unit–level plant data</italic></p>
<p>We collect, on paper data sheets, information on the number of plant individuals per sampling quadrat but do not track particular individuals through time. These data are the least intensive to manage because data entry and quality control activities are more concentrated in time, and there are fewer potential issues for us to check.</p>
<p><italic>High-frequency, individual-level rodent data</italic></p>
<p>These data are time intensive to manage because they are recorded monthly on paper data sheets and require extra quality control efforts to maintain accurate individual-level data.</p>
<p><italic>Highest-frequency, automated weather data</italic></p>
<p>We also collect weather data, generated hourly, which we download weekly from an automated weather station at the field site. Because we do not transcribe these data, there are no human-introduced errors. We perform weekly quality control efforts for these data to detect any issues with the sensors, including checking for abnormal values and comparing output to regional stations to identify extreme deviations from regional conditions.</p>
<p>Given the variety of data that we collect, we require a generally flexible approach for managing the data coming from our study site. The diversity of regularly updating data that we manage makes it likely that our data workflow will address many of the data management situations that biologists collecting updating data regularly encounter.</p>
</sec>
</boxed-text>
</sec>
<sec id="sec004">
<title>Implementing a modern data workflow</title>
<p>Setting up an automated data management system for regularly updated data may initially seem beyond the skill set of most empirically focused lab groups. The approach we have designed and describe below does require some level of familiarity with computational tools such as a programming language (e.g., Python or R) and a version control system (e.g., git). However, data management and programming are increasingly becoming core skills in biology [<xref ref-type="bibr" rid="pbio.3000125.ref010">10</xref>], even for empirically focused lab groups, and training in the tools we used to build our data management system is available at many universities or through workshops at conferences. In designing and building the infrastructure for our study, our group consisted primarily of field ecologists who received their training in this manner and sought assistance from a computational ecologist for help with design and implementation of some of the more advanced aspects. We have aimed this paper and our associated tutorial at empirical groups with little background in the tools or approaches we implemented. Our goal is to provide an introduction to the concepts and tools, general information on how such a system can be constructed, and assistance—through a tutorial—for building data management systems to manage regularly updating data. <xref ref-type="boxed-text" rid="pbio.3000125.box003">Box 3</xref> contains a recipe for implementing our approach. Readers can also peruse our active data repository (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/weecology/PortalData" xlink:type="simple">www.github.com/weecology/PortalData</ext-link>) to see details of how we constructed our pipeline.</p>
<boxed-text id="pbio.3000125.box003" position="float">
<sec id="sec005">
<title>Box 3. Recipe for creating a regularly updating data pipeline</title>
<p>Visit <ext-link ext-link-type="uri" xlink:href="https://www.updatingdata.org/" xlink:type="simple">https://www.updatingdata.org/</ext-link> for click-through instructions on how to follow this recipe to build a regularly updating data repository.</p>
<list list-type="order">
<list-item><p>Clone the livedat repository</p></list-item>
<list-item><p>Configure the repository for your project</p></list-item>
<list-item><p>Connect to Zenodo</p></list-item>
<list-item><p>Connect to Travis</p></list-item>
<list-item><p>Give Travis access to update your data on GitHub</p></list-item>
<list-item><p>Add data</p></list-item>
<list-item><p>Add quality assurance/quality control (QA/QC) code</p></list-item>
<list-item><p>Update data</p></list-item>
</list>
</sec>
</boxed-text>
<sec id="sec006">
<title>QA in data entry</title>
<p>For data collected onto data sheets in the field or the lab, the initial processing requires human interaction to enter the data. Using tools and approaches that automatically check data for errors can make this process more efficient. Upon returning from the field, two different people manually enter the new data into Excel spreadsheets (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 1). We use the “data validation” feature in Excel to restrict possible entries as an initial method of quality control by restricting accepted species codes to those on a prespecified list and defining allowable ranges for numeric values. The two separately entered versions are compared to each other using an R script to find errors from data entry (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 2). The R script detects any discrepancies between the two versions and returns a list of row numbers in the spreadsheet where these discrepancies occur, which the researcher then uses to compare to the original data sheets and fix the errors.</p>
<fig id="pbio.3000125.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000125.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Data workflow for regularly updated data.</title>
<p>1. All field-collected data are double entered with automated checks to prevent invalid values from being entered. 2. The two versions of the double-entered data are compared using an R script, and mismatches are corrected. 3. A pull request is submitted to the data repository (i.e., GitHub), which triggers data checks run by the continuous integration system (i.e., Travis CI). 4. If the system detects any issues, the update is reviewed again, and corrections are made to the pull request, automatically triggering the data checks to run again. 5. Once the new data pass all automated checks, a data manager reviews the changes and merges the new data into the main data repository. 6. Addition of the new data triggers the continuous integration system to run additional scripts to get data from automated sensors (e.g., weather data) and to check for errors. 7. The system then runs scripts that automatically update the supporting tables (information not collected in the field that helps with data use) and updates the version number. 8. Once all tables have been automatically updated, the updates are automatically merged into the main repository. 9. The system automatically triggers a new release on GitHub. 10. The GitHub–Zenodo integration sends the new data release as a new version to Zenodo for archiving. CI, continuous integration.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000125.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Adding data to the central database</title>
<p>When data are regularly updated, often by multiple researchers, it is essential to have a central version of the database with all of the most current data. We store our data in comma separated values (csv) files in a system designed for managing and tracking changes to files called version control. Version control was originally designed for tracking changes to code but can track changes to any digital file, including data files [<xref ref-type="bibr" rid="pbio.3000125.ref011">11</xref>–<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>]. We use a specific version control system—git—and the associated GitHub website (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, “GitHub and Continuous Integration”) for managing version control (see <xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref> for more details). We store the master version of the data files online on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/weecology/PortalData" xlink:type="simple">https://github.com/weecology/PortalData</ext-link>). The data, along with the code for data management, are stored in the version control equivalent of a folder, called a repository (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, “Main Data Repository”). Through this online repository, everyone in the project has access to the most up-to-date or “master” version of both the data and the data management code. To add or change data in this central repository, we create a copy of the repository on a user’s local computer, which we then edit, save any changes along with a message describing the changes, and send a request through GitHub (called a “pull request”; <xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 3) to have these changes integrated into the central repository (<xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref>). While any user can suggest a change or addition, only select individuals have the authority to merge these changes into the central version (see <xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref>). This version control based process retains records of every change made to the data along with an explanation of that change [<xref ref-type="bibr" rid="pbio.3000125.ref011">11</xref>–<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>]. It also makes it possible to identify changes between different stages and to go back to any previous state of the data. As such, it protects data from accidental changes and makes it possible to track the provenance of the data.</p>
</sec>
<sec id="sec008">
<title>Automated data checks</title>
<p>Automating data checks (i.e., QA/QC) are essential for efficiently delivering regularly updated data of high quality. We automate a variety of aspects of our data management system, including the data checks, by using “continuous analysis” (sensu [<xref ref-type="bibr" rid="pbio.3000125.ref014">14</xref>]), an approach for automating computational analyses. Continuous analysis uses “continuous integration” tools from software engineering to automatically run a set of commands (in our case, this includes R code that is run to error check new data) when data or code is updated or at user-specified times ([<xref ref-type="bibr" rid="pbio.3000125.ref014">14</xref>]; see <xref ref-type="supplementary-material" rid="pbio.3000125.s002">S2 Box</xref>). Continuous integration systems (we use Travis CI; <ext-link ext-link-type="uri" xlink:href="https://travis-ci.com/" xlink:type="simple">https://travis-ci.com/</ext-link>) are designed to interact with version control systems, which makes it relatively easy to automate QA/QC checks of the data [<xref ref-type="bibr" rid="pbio.3000125.ref015">15</xref>]. When a “pull request” to add new data to the central database is submitted, it automatically triggers the continuous integration system to run a predetermined set of QA/QC checks. The QA/QC checks the validity and consistency of the new data (e.g., do data for all samples exist, are data values that should be similar through time self-consistent). This QA/QC system uses a software-testing approach called “unit testing” that is typically used to check that pieces of code work in the expected way [<xref ref-type="bibr" rid="pbio.3000125.ref016">16</xref>]. We use tests, written using the “testthat” package, to do our unit testing [<xref ref-type="bibr" rid="pbio.3000125.ref017">17</xref>]. Any identified issues with the data are automatically flagged in the pull request, indicating that they need to be fixed before the data are added to the main repository (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 4). The researcher then identifies the proper fix for the issue, fixes it in their local copy, and updates the pull request, which is then automatically retested to ensure that the data pass QA/QC (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 3).</p>
</sec>
<sec id="sec009">
<title>Human review and updating the central database</title>
<p>Human review of data updates is useful for identifying issues that are difficult to detect programmatically. Before field data are merged into the main repository, we require human review of the proposed changes by someone other than the researcher who initiated the pull request. This review is facilitated by the pull request functionality on GitHub, which shows the reviewer only the lines of data that have been changed [<xref ref-type="bibr" rid="pbio.3000125.ref011">11</xref>]. Once the changes have passed both the automated tests and human review, a data manager confirms the merge, and the changes are incorporated into the main version of the database (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 5). Records of all merged pull requests are retained in git and on GitHub, and it is possible to revert to previous states of the data at any time.</p>
</sec>
<sec id="sec010">
<title>Automatically integrating data from sensors</title>
<p>Many data collection efforts in biology involve some sort of automated data collection. We collect hourly weather data from an on-site weather station that transmits data over a cellular connection; we also download data from other weather stations in the region whose data are streamed online. While data collected by automated sensors do not require steps to correct human-entry errors, they still require QA/QC for sensor errors, and the raw data need to be processed into the most appropriate form for our database. To automate this process, we developed R scripts to download the data, transform them into the appropriate format, run QA/QC checks, and automatically update the weather table in the main repository (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, steps 6 and 8). The continuous integration system is scheduled to regularly download and add new weather data. Errors identified by the QA/QC checks will cause our continuous integration system to register an error, indicating that the data require human attention before being added to the main repository (similar to the QA/QC process described in <xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, steps 3 and 4). This process yields fully automated collection of weather data in near-real time.</p>
</sec>
<sec id="sec011">
<title>Automated updating of supporting tables</title>
<p>Once data from the field are merged into the main repository, there are often supporting data tables that need to be updated. Supporting tables contain information (e.g., about data collection events such as sampling intensity or timing) that cannot be efficiently stored in the main data file. Since this information can be derived from the entered data, we have automated the process of updating supporting tables in order to reduce the time and effort required to incorporate new sampling events into the database (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, steps 7 and 8). For each table that needs to be updated, we wrote a function to 1) confirm that the supporting table needs to be updated, 2) extract the relevant information from the new data in the main data table, 3) perform data checks, and 4) append the new information to the supporting table. The update process is triggered by the addition of new data into one of the main data tables, at which point the continuous integration service executes these functions (see <xref ref-type="supplementary-material" rid="pbio.3000125.s002">S2 Box</xref>). Automating curation of these supporting tables reduces the potential for data entry errors and allows researchers to allocate their time and effort to tasks that require intellectual input.</p>
</sec>
<sec id="sec012">
<title>Versioning</title>
<p>A common issue with data sets that are regularly updated is that the data available at one point in time are not the same as the data at some point in the future, which can cause difficulties for reproducing and comparing analyses [<xref ref-type="bibr" rid="pbio.3000125.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>]. Creating distinct versions of the database every time it changes and timestamping those versions allows analyses that can be run on a specific version of the data [<xref ref-type="bibr" rid="pbio.3000125.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>]. To address this issue, we automatically make a “release” every time new data are added to the database (using the GitHub application programming interface [API]; <xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 9). This allows specific versions of the data used for an analysis to be referenced directly, and the exact form of the data can be downloaded to allow fully reproducible analyses even as the data set is continually updated. Versions are named following the newly developed Frictionless Data data-versioning guidelines (<ext-link ext-link-type="uri" xlink:href="https://frictionlessdata.io/specs/patterns/" xlink:type="simple">https://frictionlessdata.io/specs/patterns/</ext-link>; see [<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>] for a similar approach).</p>
</sec>
<sec id="sec013">
<title>Archiving</title>
<p>Satisfying journal and funding agency data requirements increasingly requires depositing data in an archive that guarantees stable long-term availability of data under an open license. GitHub repositories can be deleted at any time and therefore cannot serve as an archive [<xref ref-type="bibr" rid="pbio.3000125.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref020">20</xref>]. Regularly updated data need an archive that supports easily automated archiving, data versioning, and DOIs for citation. In some fields, disciplinary repositories are the best choice for archiving some kinds of data, but often, these repositories do not support automatic updating. We archive our data with a Creative Commons 0 (CC0) license on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/" xlink:type="simple">https://zenodo.org/</ext-link>), a widely used general purpose repository, because it provides all of the necessary components. With its easy integration with GitHub, we can archive our data automatically with each update to the data (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>, step 10; [<xref ref-type="bibr" rid="pbio.3000125.ref013">13</xref>]). Zenodo’s data versioning provides DOIs that allow data users to cite both the exact version of the data used in their analyses (to allow for fully reproducible analyses) and the data set as a whole (to allow accurate tracking of the usage of the data set). To support the archiving of regularly updated data, data archives should support automatic updating (e.g., via an API) and data versioning.</p>
</sec>
<sec id="sec014">
<title>Citation and authorship</title>
<p>Regularly updating data also produces complexities for providing academic credit for collecting and sharing data, which is essential for a healthy culture supporting data collection and reuse [<xref ref-type="bibr" rid="pbio.3000125.ref021">21</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref022">22</xref>]. Data papers, which allow a data set to be treated like a publication for reporting and citation, are modeled on scientific papers and are effectively static. This limits their utility when data is being added repeatedly over time because there is no established way to update the data, metadata, or authorship. The ideal solution is a data paper that can be updated to include new authors, mention new techniques, and link directly to continually updating data in a data repository. This would allow the content and authorship to remain up-to-date and allow citations to acknowledge the use of the data set as a whole. We have addressed this problem by writing a data paper [<xref ref-type="bibr" rid="pbio.3000125.ref009">9</xref>] that resides on BioRxiv, a preprint server widely used in the biological sciences. The data paper can be updated with new versions as needed, providing the flexibility to add additional details, information on new data types, and new authors. BioRxiv supports versioning of preprints, which provides a record of changes to the data paper and to authorship. Citations to BioRxiv preprints are tracked by Google Scholar, providing academic credit that can be used to justify continued data collection to funders.</p>
</sec>
</sec>
<sec id="sec015" sec-type="conclusions">
<title>Discussion</title>
<p>Data management and sharing are receiving increasing attention in science, resulting in new requirements from journals and funding agencies. Discussions about modern data management focus primarily on two main challenges: making data used in scientific papers available in useful formats to increase transparency and reproducibility [<xref ref-type="bibr" rid="pbio.3000125.ref021">21</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref022">22</xref>] and the difficulties of working with exceptionally large data [<xref ref-type="bibr" rid="pbio.3000125.ref023">23</xref>]. An emerging data management challenge that has received significantly less attention in biology is managing, working with, and providing access to data that are undergoing continual active collection. These data present unique challenges in quality assurance and control, data publication, archiving, and reproducibility. The workflow we developed for our long-term study solves many of the challenges of managing this type of regularly updating data. We employ a combination of existing tools to reduce data errors, import and restructure data, archive and version the data, and automate most steps in the data pipeline to reduce the time and effort required by researchers. This workflow expands the idea of continuous analysis (sensu [<xref ref-type="bibr" rid="pbio.3000125.ref014">14</xref>]) to create a modern data management system that uses tools from software development to automate the data collection, processing, and publication pipeline.</p>
<p>We use our data management system to manage data collected both in the field by hand and automatically by machines, but our system is applicable to other types of data collection as well. For example, teams of scientists are increasingly interested in consolidating information scattered across publications and other sources into centralized databases, e.g., plant traits [<xref ref-type="bibr" rid="pbio.3000125.ref024">24</xref>,<xref ref-type="bibr" rid="pbio.3000125.ref025">25</xref>], tropical diseases [<xref ref-type="bibr" rid="pbio.3000125.ref026">26</xref>], biodiversity time series [<xref ref-type="bibr" rid="pbio.3000125.ref027">27</xref>], vertebrate endocrine levels [<xref ref-type="bibr" rid="pbio.3000125.ref028">28</xref>], and microRNA target interactions [<xref ref-type="bibr" rid="pbio.3000125.ref029">29</xref>]. Because new data are always being generated and published, literature compilations also have the potential for continual data expansion. Whether part of a large, international team such as the above efforts or single researchers interested in conducting meta-analyses, phylogenetic analyses, or compiling DNA reference libraries for barcodes, our approach is flexible enough to apply to most types of data collection activities for which data need to be ready for analysis before the endpoint is reached.</p>
<p>The main limitation on the infrastructure we have designed is that it cannot handle truly large data. Online services like GitHub and Travis CI typically limit the amount of storage and compute time that can be used by a single project. GitHub limits repository size to 1 GB and file size to 100 MB. As a result, remote sensing images, genomes, and other data types requiring large amounts of storage will not be suitable for the GitHub-centered approach outlined here. Travis CI limits the amount of time that code can run on its infrastructure for free to one hour. Most research data and data processing will fit comfortably within these limits (the largest file in the Portal database is currently &lt;20 MB, and it takes &lt;15 minutes for all data checking and processing code to run), so this type of system will work for the majority of research projects. However, in cases for which larger data files or longer run times are necessary, it is possible to adapt our general approach by using equivalent tools that can be run on local computing resources (e.g., GitLab for managing git repositories and Jenkins for continuous integration) and using tools that are designed for versioning large data (e.g., dat [<xref ref-type="bibr" rid="pbio.3000125.ref030">30</xref>] or git Large File Storage [<xref ref-type="bibr" rid="pbio.3000125.ref031">31</xref>]).</p>
<p>One advantage of our approach to the challenges of regularly updated data is that it can be accomplished by a small team composed of primarily empirical researchers. Our approach does not require dedicated information technology (IT) staff, but it does require some level of familiarity with tools that are not commonly used in biology. Many research groups will need computational training or assistance. The use of programming languages for data manipulation, whether in R, Python, or another language, is increasingly common, and many universities offer courses that teach the fundamentals of data science and data management (e.g., <ext-link ext-link-type="uri" xlink:href="http://www.datacarpentry.org/semester-biology/" xlink:type="simple">http://www.datacarpentry.org/semester-biology/</ext-link>). Training activities can also be found at many scientific society meetings and through workshops run by groups like the Carpentries, a nonprofit group focused on teaching data management and software skills—including git and GitHub—to scientists (<ext-link ext-link-type="uri" xlink:href="https://carpentries.org/" xlink:type="simple">https://carpentries.org/</ext-link>). A set of resources for learning the core skills and tools discussed in this paper is provided in <xref ref-type="supplementary-material" rid="pbio.3000125.s003">S3 Box</xref>. The tool that is most difficult to learn is continuous integration, both because it is a more advanced computational skill not covered in most biology training courses and because existing documentation is primarily aimed at people with high levels of technical training (e.g., software developers). To help researchers implement this aspect of the workflow, including the automated releasing and archiving of data, we have created a starter repository including reusable code (<ext-link ext-link-type="uri" xlink:href="http://github.com/weecology/livedat" xlink:type="simple">http://github.com/weecology/livedat</ext-link>). Our website (<ext-link ext-link-type="uri" xlink:href="https://www.updatingdata.org/" xlink:type="simple">https://www.updatingdata.org</ext-link>) provides a dynamic tutorial to help researchers set up continuous integration and automated archiving using Travis CI for their own GitHub repository. The value of the tools used here emphasizes the need for more computational training for scientists at all career stages, a widely recognized need in biology [<xref ref-type="bibr" rid="pbio.3000125.ref032">32</xref>–<xref ref-type="bibr" rid="pbio.3000125.ref034">34</xref>]. Given the importance of making continually collected data rapidly available for forecasting and other research, the field will continue to need to train, support, and retain scientists with advanced computational skills to assist with setting up and managing regularly updating data workflows.</p>
<p>The rise of technology to aid data collection in the sciences has fundamentally changed how we quantify and measure biological activities (e.g., [<xref ref-type="bibr" rid="pbio.3000125.ref035">35</xref>]) and facilitates our ability to find and compile information from the literature and across systems. While the resulting ability to generate data sets that are regularly updated with new information will help address complex issues facing our society (e.g. climate change, emerging diseases, cancer prevention and treatment), it also comes with unique challenges. We have described some of these challenges and our approach to solving them in the hope that it can serve as a catalyst for future development to make implementing data management protocols for this type of data more broadly accessible. All stages of the workflow for regularly updated data (<xref ref-type="fig" rid="pbio.3000125.g001">Fig 1</xref>) could be made easier to implement through improved tooling. A priority for investment in this area is simplifying the setup of continuous analysis systems for the data management–focused challenges of automated versioning and archiving. Additional training in automation and continuous analysis for biologists will also be important for helping the scientific community advance this new area of data management. There are also a number of important issues that, while not central to our project, need to be addressed to maximize the management of regularly updating data more generally. In particular, we see three areas to address: 1) data licensing issues for heterogeneous data sets (especially for data compilations [<xref ref-type="bibr" rid="pbio.3000125.ref036">36</xref>]); 2) properly crediting contributions to tool development (e.g., software, data management pipelines), especially for early career researchers; and 3) determining standards for authorship for large distributed collaborations. Continually updated data will become an increasingly more common data type in biology. This makes investment now in the tools, training, and culture of dealing with continually updated data critical for ensuring that scientists can maximize their use of this emerging data type to address pressing questions in biology. See <xref ref-type="boxed-text" rid="pbio.3000125.box004">Box 4</xref> for glossary of terms.</p>
<boxed-text id="pbio.3000125.box004" position="float">
<sec id="sec016">
<title>Box 4. Glossary</title>
<p>CI: Continuous integration (also see <xref ref-type="supplementary-material" rid="pbio.3000125.s002">S2 Box</xref>). The continuous application of quality control. A practice used in software engineering to continuously implement processes for automated testing and integration of new code into a project.</p>
<p>Git: (also see <xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref>) Git is an open source program for tracking changes in text files (version control) and is the core technology of which GitHub, the social and user interface, is built on top.</p>
<p>GitHub: (also see <xref ref-type="supplementary-material" rid="pbio.3000125.s001">S1 Box</xref>) A web-based hosting service for version control using git.</p>
<p>Github–Travis CI integration: Connects the Travis CI continuous integration service to build and test projects hosted at GitHub. Once set up, a GitHub project will automatically deploy CI and test pull requests through Travis CI.</p>
<p>Github–Zenodo integration: Connects a Github project to a Zenodo archive. Zenodo takes an archive of your GitHub repository each time you create a new release.</p>
<p>Pull request: A set of proposed changes to the files in a GitHub repository made by one collaborator, to be reviewed by other collaborators before being accepted or rejected.</p>
<p>QA/QC: Quality assurance/quality control. The process of ensuring the data in our repository meet a certain quality standard.</p>
<p>Repository: A location (folder) containing all of the files for a particular project. Files could include code, data files, or documentation. Each file’s revision history is also stored in the repository.</p>
<p>testthat: An R package that facilitates formal, automated testing.</p>
<p>Travis CI: (also see <xref ref-type="supplementary-material" rid="pbio.3000125.s002">S2 Box</xref>) A hosted continuous integration service that is used to test and build GitHub projects. Open source projects are tested at no charge.</p>
<p>Unit test: A software-testing approach that checks to make sure that pieces of code work in the expected way.</p>
<p>Version control: A system for managing changes made to a file or set of files over time that allows the user to a) see what changes were made when and b) revert back to a previous state if desired.</p>
<p>Zenodo: A general, open-access, research data repository.</p>
</sec>
</boxed-text>
</sec>
<sec id="sec017">
<title>Supporting information</title>
<supplementary-material id="pbio.3000125.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000125.s001" xlink:type="simple">
<label>S1 Box</label>
<caption>
<title>A brief introduction to version control using git and GitHub.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000125.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000125.s002" xlink:type="simple">
<label>S2 Box</label>
<caption>
<title>A brief introduction to continuous integration and an example setup using Travis.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000125.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000125.s003" xlink:type="simple">
<label>S3 Box</label>
<caption>
<title>Resources for learning data management tools.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank the scientific Twitter community and, in particular, Dr. Michael Kaspari for helping us figure out how to most clearly describe data that are being regularly updated. We’d also like to thank Dr. Doug Leavey, whose appreciation of our pipeline made us realize that others might be dealing with similar data management issues and would benefit from knowing about this approach.</p>
</ack>
<fn-group>
<fn fn-type="other" id="fn001">
<p><bold>Provenance:</bold> Not commissioned; externally peer reviewed</p>
</fn>
</fn-group>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>API</term>
<def><p>application programming interface</p></def>
</def-item>
<def-item><term>CC0</term>
<def><p>creative commons 0</p></def>
</def-item>
<def-item><term>CI</term>
<def><p>continuous integration</p></def>
</def-item>
<def-item><term>csv</term>
<def><p>comma separated values</p></def>
</def-item>
<def-item><term>IT</term>
<def><p>information technology</p></def>
</def-item>
<def-item><term>QA/QC</term>
<def><p>quality assurance/quality control</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.3000125.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hampton</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Strasser</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Tewksbury</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Gram</surname> <given-names>WK</given-names></name>, <name name-style="western"><surname>Budden</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Batcheller</surname> <given-names>AL</given-names></name>, <etal>et al</etal>. <article-title>Big data and the future of ecology</article-title>. <source>Frontiers in Ecology and the Environment</source>. <year>2013</year> <month>Apr</month> <day>1</day>;<volume>11</volume>(<issue>3</issue>):<fpage>156</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marx</surname> <given-names>V</given-names></name>. <article-title>Biology: The big challenges of big data [Internet]</article-title>. <source>Nature</source>. <year>2013</year> [cited 2018 Jun 11]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/498255a" xlink:type="simple">https://www.nature.com/articles/498255a</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raghupathi</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Raghupathi</surname> <given-names>V</given-names></name>. <article-title>Big data analytics in healthcare: promise and potential</article-title>. <source>Health Inf Sci Syst</source> [Internet]. <year>2014</year> <month>Feb</month> <day>7</day> [cited 2018 Oct 17];<volume>2</volume>. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341817/" xlink:type="simple">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341817/</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindenmayer</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Likens</surname> <given-names>GE</given-names></name>. <article-title>Adaptive monitoring: a new paradigm for long-term research and monitoring</article-title>. <source>Trends in Ecology &amp; Evolution</source>. <year>2009</year> <month>Sep</month> <day>1</day>;<volume>24</volume>(<issue>9</issue>):<fpage>482</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dietze</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Fox</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck-Johnson</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Betancourt</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Hooten</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Jarnevich</surname> <given-names>CS</given-names></name>, <etal>et al</etal>. <article-title>Iterative near-term ecological forecasting: Needs, opportunities, and challenges</article-title>. <source>PNAS</source>. <year>2018</year> <month>Jan</month> <day>30</day>;201710231.</mixed-citation></ref>
<ref id="pbio.3000125.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilkinson</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Cole</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Pace</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Batt</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Buelo</surname> <given-names>CD</given-names></name>, <etal>et al</etal>. <article-title>Early warning signals precede cyanobacterial blooms in multiple whole-lake experiments</article-title>. <source>Ecological Monographs</source>. <year>2018</year>;<volume>88</volume>(<issue>2</issue>):<fpage>188</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Misun</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Rothe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schmid</surname> <given-names>YRF</given-names></name>, <name name-style="western"><surname>Hierlemann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>O</given-names></name>. <article-title>Multi-analyte biosensor interface for real-time monitoring of 3D microtissue spheroids in hanging-drop networks</article-title>. <source>Microsystems &amp; Nanoengineering</source>. <year>2016</year> <month>Jun</month> <day>6</day>;<volume>2</volume>:<fpage>16022</fpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Errington</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Iorns</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gunn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Lomax</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nosek</surname> <given-names>BA</given-names></name>. <article-title>An open investigation of the reproducibility of cancer biology research</article-title>. <source>eLife</source> [Internet]. <fpage>3</fpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4270077/" xlink:type="simple">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4270077/</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernest</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Yenni</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Allington</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bledsoe</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Christensen</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Diaz</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>The Portal Project: a long-term study of a Chihuahuan desert ecosystem</article-title>. <source>bioRxiv</source>. <year>2018</year> <month>May</month> <day>28</day>;<fpage>332783</fpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hampton</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Wasser</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Schildhauer</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Supp</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Brun</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Skills and Knowledge for Data-Intensive Environmental Research</article-title>. <source>BioScience</source>. <year>2017</year> <month>Jun</month> <day>1</day>;<volume>67</volume>(<issue>6</issue>):<fpage>546</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biosci/bix025" xlink:type="simple">10.1093/biosci/bix025</ext-link></comment> <object-id pub-id-type="pmid">28584342</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ram</surname> <given-names>K</given-names></name>. <article-title>Git can facilitate greater reproducibility and increased transparency in science</article-title>. <source>Source Code for Biology and Medicine</source>. <year>2013</year> <month>Feb</month> <day>28</day>;<volume>8</volume>(<issue>1</issue>):<fpage>7</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1751-0473-8-7" xlink:type="simple">10.1186/1751-0473-8-7</ext-link></comment> <object-id pub-id-type="pmid">23448176</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pröll</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meixner</surname> <given-names>K</given-names></name>. <article-title>WGDC Pilot Git Reference</article-title> [Internet]. <source>RDA</source>. <year>2016</year> [cited 2018 Dec 14]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.rd-alliance.org/group/data-citation-wg/wiki/wgdc-pilot-git-reference" xlink:type="simple">https://www.rd-alliance.org/group/data-citation-wg/wiki/wgdc-pilot-git-reference</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Falster D, FitzJohn RG, Pennell MW, Cornwell WK. Versioned data: why it is needed and how it can be achieved (easily and cheaply) [Internet]. PeerJ Inc.; 2017 Nov [cited 2018 Dec 14]. Report No.: e3401v1. Available from: <ext-link ext-link-type="uri" xlink:href="https://peerj.com/preprints/3401" xlink:type="simple">https://peerj.com/preprints/3401</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beaulieu-Jones</surname> <given-names>BK</given-names></name>, <name name-style="western"><surname>Greene</surname> <given-names>CS</given-names></name>. <article-title>Reproducibility of computational workflows is automated using continuous analysis</article-title>. <source>Nature Biotechnology</source>. <year>2017</year> <month>Apr</month>;<volume>35</volume>(<issue>4</issue>):<fpage>342</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.3780" xlink:type="simple">10.1038/nbt.3780</ext-link></comment> <object-id pub-id-type="pmid">28288103</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fowler</surname> <given-names>D</given-names></name>. <article-title>Open Knowledge Labs</article-title> [Internet]. <source>Automated Data Validation with Data Packages</source>. <year>2016</year> [cited 2018 Oct 17]. Available from: <ext-link ext-link-type="uri" xlink:href="http://okfnlabs.org" xlink:type="simple">http://okfnlabs.org</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Aruliah</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>NPC</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Guy</surname> <given-names>RT</given-names></name>, <etal>et al</etal>. <article-title>Best Practices for Scientific Computing</article-title>. <source>PLoS Biol</source>. <year>2014</year> <month>Jan</month> <day>7</day>;<volume>12</volume>(<issue>1</issue>):<fpage>e1001745</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001745" xlink:type="simple">10.1371/journal.pbio.1001745</ext-link></comment> <object-id pub-id-type="pmid">24415924</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wickham</surname> <given-names>H</given-names></name>. <article-title>testthat: Get Started with Testing</article-title>. <source>The R Journal</source>. <year>2011</year>;<volume>3</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauber</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Asmi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Uytvanck</surname> <given-names>D van</given-names></name>, <name name-style="western"><surname>Proell</surname> <given-names>S</given-names></name>. <article-title>Data Citation of Evolving Data: Recommendations of the Working Group on Data Citation (WGDC)</article-title> [Internet]. <source>B2SHARE</source>. <year>2016</year> [cited 2018 Jul 23]. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.15497/RDA00016" xlink:type="simple">http://dx.doi.org/10.15497/RDA00016</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bergman</surname> <given-names>C</given-names></name>. <article-title>On the Preservation of Published Bioinformatics Code on Github</article-title> [Internet]. <source>An Assembly of Fragments</source>. <year>2012</year> [cited 2018 Jun 12]. Available from: <ext-link ext-link-type="uri" xlink:href="https://caseybergman.wordpress.com/2012/11/08/on-the-preservation-of-published-bioinformatics-code-on-github/" xlink:type="simple">https://caseybergman.wordpress.com/2012/11/08/on-the-preservation-of-published-bioinformatics-code-on-github/</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>White</surname> <given-names>EP</given-names></name>. <article-title>Some thoughts on best publishing practices for scientific software</article-title>. <source>Ideas in Ecology and Evolution</source>. <year>2015</year>;<volume>8</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reichman</surname> <given-names>OJ</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Schildhauer</surname> <given-names>MP</given-names></name>. <article-title>Challenges and Opportunities of Open Data in Ecology</article-title>. <source>Science</source>. <year>2011</year> <month>Feb</month> <day>11</day>;<volume>331</volume>(<issue>6018</issue>):<fpage>703</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1197962" xlink:type="simple">10.1126/science.1197962</ext-link></comment> <object-id pub-id-type="pmid">21311007</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Molloy</surname> <given-names>JC</given-names></name>. <article-title>The Open Knowledge Foundation: Open Data Means Better Science</article-title>. <source>PLoS Biol</source>. <year>2011</year> <month>Dec</month> <day>6</day>;<volume>9</volume>(<issue>12</issue>):<fpage>e1001195</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001195" xlink:type="simple">10.1371/journal.pbio.1001195</ext-link></comment> <object-id pub-id-type="pmid">22162946</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marx</surname> <given-names>V</given-names></name>. <article-title>Biology: The big challenges of big data</article-title> [Internet]. <source>Nature</source>. <year>2013</year> [cited 2018 Jun 11]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/498255a" xlink:type="simple">https://www.nature.com/articles/498255a</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kattge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Díaz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lavorel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Prentice</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Leadley</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bönisch</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>TRY–a global database of plant traits</article-title>. <source>Global Change Biology</source>. <year>2011</year> <month>Sep</month>;<volume>17</volume>(<issue>9</issue>):<fpage>2905</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Falster</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Duursma</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Ishihara</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Barneche</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>FitzJohn</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Vårhammar</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>BAAD: a Biomass And Allometry Database for woody plants</article-title>. <source>Ecology</source>. <year>2015</year>;<volume>96</volume>(<issue>5</issue>):<fpage>1445</fpage>–1445.</mixed-citation></ref>
<ref id="pbio.3000125.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hürlimann</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schur</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Boutsika</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Stensgaard</surname> <given-names>A-S</given-names></name>, <name name-style="western"><surname>Himpsl</surname> <given-names>ML de</given-names></name>,<name name-style="western"><surname>Ziegelbauer</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Toward an Open-Access Global Database for Mapping, Control, and Surveillance of Neglected Tropical Diseases</article-title>. <source>PLoS Negl Trop Dis</source>. <year>2011</year> <month>Dec</month> <day>13</day>;<volume>5</volume>(<issue>12</issue>):<fpage>e1404</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pntd.0001404" xlink:type="simple">10.1371/journal.pntd.0001404</ext-link></comment> <object-id pub-id-type="pmid">22180793</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dornelas</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Antão</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Moyes</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bates</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Magurran</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Adam</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>BioTIME: A database of biodiversity time series for the Anthropocene</article-title>. <source>Global Ecology and Biogeography</source>. <year>2018</year> <month>Jul</month> <day>1</day>;<volume>27</volume>(<issue>7</issue>):<fpage>760</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/geb.12729" xlink:type="simple">10.1111/geb.12729</ext-link></comment> <object-id pub-id-type="pmid">30147447</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vitousek</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Donald</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Fuxjager</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Goymann</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>HormoneBase, a population-level database of steroid hormone levels across vertebrates</article-title>. <source>Scientific Data</source>. <year>2018</year> <month>May</month> <day>22</day>;<volume>5</volume>:<fpage>180097</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2018.97" xlink:type="simple">10.1038/sdata.2018.97</ext-link></comment> <object-id pub-id-type="pmid">29786693</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname> <given-names>C-H</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>N-W</given-names></name>, <name name-style="western"><surname>Shrestha</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>S-D</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y-L</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>W-H</given-names></name>, <etal>et al</etal>. <article-title>miRTarBase 2016: updates to the experimentally validated miRNA-target interactions database</article-title>. <source>Nucleic Acids Res</source>. <year>2016</year> <month>Jan</month> <day>4</day>;<volume>44</volume>(<issue>D1</issue>):<fpage>D239</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/nar/gkv1258" xlink:type="simple">10.1093/nar/gkv1258</ext-link></comment> <object-id pub-id-type="pmid">26590260</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ogden</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McKelvey</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>MB</given-names></name>. <article-title>Dat—Distributed Dataset Synchronization And Versioning</article-title>. <source>Open Science Framework</source> [Internet]. <year>2017</year> <month>Jan</month> <day>31</day> [cited 2018 Jun 12]; Available from: <ext-link ext-link-type="uri" xlink:href="https://osf.io/nsv2c/" xlink:type="simple">https://osf.io/nsv2c/</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perez-Riverol</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gatto</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sachsenberg</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Uszkoreit</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Leprevost F da</surname> <given-names>V</given-names></name>, <etal>et al</etal>. <article-title>Ten Simple Rules for Taking Advantage of Git and GitHub</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year> <month>Jul</month> <day>14</day>;<volume>12</volume>(<issue>7</issue>):<fpage>e1004947</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004947" xlink:type="simple">10.1371/journal.pcbi.1004947</ext-link></comment> <object-id pub-id-type="pmid">27415786</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Teal</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Cranston</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Lapp</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ram</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Data Carpentry: Workshops to Increase Data Literacy for Researchers | International Journal of Digital Curation</article-title>. <source>International Journal of Digital Curation</source> [Internet]. <year>2015</year> [cited 2018 Oct 17];<volume>10</volume>(<issue>1</issue>). Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ijdc.net/article/view/10.1.135" xlink:type="simple">http://www.ijdc.net/article/view/10.1.135</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barone</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Micklos</surname> <given-names>D</given-names></name>. <article-title>Unmet needs for analyzing biological big data: A survey of 704 NSF principal investigators</article-title>. <source>PLoS Comput Biol</source>. <year>2017</year> <month>Oct</month> <day>19</day>;<volume>13</volume>(<issue>10</issue>):<fpage>e1005755</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005755" xlink:type="simple">10.1371/journal.pcbi.1005755</ext-link></comment> <object-id pub-id-type="pmid">29049281</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hampton</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Wasser</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Schildhauer</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Supp</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Brun</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Skills and Knowledge for Data-Intensive Environmental Research</article-title>. <source>BioScience</source>. <year>2017</year> <month>Jun</month> <day>1</day>;<volume>67</volume>(<issue>6</issue>):<fpage>546</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biosci/bix025" xlink:type="simple">10.1093/biosci/bix025</ext-link></comment> <object-id pub-id-type="pmid">28584342</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Estes</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Elsen</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Treuer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ahmed</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Caylor</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The spatial and temporal domains of modern ecology</article-title>. <source>Nature Ecology &amp; Evolution</source>. <year>2018</year> <month>May</month>;<volume>2</volume>(<issue>5</issue>):<fpage>819</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villata</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gandon</surname> <given-names>F</given-names></name>. <article-title>Licenses Compatibility and Composition in the Web of Data</article-title>. In <year>2012</year> [cited 2018 Oct 17]. Available from: <ext-link ext-link-type="uri" xlink:href="https://hal.inria.fr/hal-01171125/document" xlink:type="simple">https://hal.inria.fr/hal-01171125/document</ext-link></mixed-citation></ref>
<ref id="pbio.3000125.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>W</given-names></name>. <article-title>Data mining with big data</article-title>. <source>IEEE Transactions on Knowledge and Data Engineering</source>. <year>2014</year> <month>Jan</month>;<volume>26</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>107</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salzberg</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tsotras</surname> <given-names>VJ</given-names></name>. <article-title>Comparison of Access Methods for Time-evolving Data</article-title>. <source>ACM Comput Surv</source>. <year>1999</year> <month>Jun</month>;<volume>31</volume>(<issue>2</issue>):<fpage>158</fpage>–<lpage>221</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref039"><label>39</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ganti</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ramakrishnan</surname> <given-names>R</given-names></name>. <chapter-title>Mining and monitoring evolving data</chapter-title>. <source>In: Handbook of massive data sets</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2002</year>. p. <fpage>593</fpage>–<lpage>642</lpage>.</mixed-citation></ref>
<ref id="pbio.3000125.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>New</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Cripps</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Bonne Lee</surname> <given-names>B</given-names></name>. <article-title>Global maps of non-traumatic spinal cord injury epidemiology: towards a living data repository</article-title>. <source>Spinal Cord</source>. <year>2014</year> <month>Feb</month>;<volume>52</volume>(<issue>2</issue>):<fpage>97</fpage>–<lpage>109</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sc.2012.165" xlink:type="simple">10.1038/sc.2012.165</ext-link></comment> <object-id pub-id-type="pmid">23318556</object-id></mixed-citation></ref>
<ref id="pbio.3000125.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>JH</given-names></name>. <article-title>The Desert Granivory Experiments at Portal</article-title>. <source>In: Experimental ecology: Issues and perspectives</source> [Internet]. <year>1998</year>. p. <fpage>71</fpage>–<lpage>95</lpage>. (Experimental ecology: Issues and perspectives). Available from: ://PREV200000378306</mixed-citation></ref>
</ref-list>
</back>
</article>