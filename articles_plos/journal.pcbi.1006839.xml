<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01258</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006839</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Mathematical functions</subject><subj-group><subject>Exponential functions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject><subj-group><subject>Body limbs</subject><subj-group><subject>Arms</subject><subj-group><subject>Hands</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject><subj-group><subject>Body limbs</subject><subj-group><subject>Arms</subject><subj-group><subject>Hands</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The gradient of the reinforcement landscape influences sensorimotor learning</article-title>
<alt-title alt-title-type="running-head">Ascending the reinforcement gradient</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8642-6648</contrib-id>
<name name-style="western">
<surname>Cashaback</surname> <given-names>Joshua G. A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lao</surname> <given-names>Christopher K.</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Palidis</surname> <given-names>Dimitrios J.</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3113-0911</contrib-id>
<name name-style="western">
<surname>Coltman</surname> <given-names>Susan K.</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>McGregor</surname> <given-names>Heather R.</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1368-032X</contrib-id>
<name name-style="western">
<surname>Gribble</surname> <given-names>Paul L.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Human Performance Laboratory, University of Calgary, Calgary, Alberta, Canada</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Hotchkiss Brain Institute, University of Calgary, Calgary, Alberta, Canada</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Physiology and Pharmacology, Western University, London, Ontario, Canada</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Graduate Program in Neuroscience, Western University, London, Ontario, Canada</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Brain and Mind Institute, Western University, London, Ontario, Canada</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Department of Psychology, Western University, London, Ontario, Canada</addr-line>
</aff>
<aff id="aff007">
<label>7</label>
<addr-line>Haskins Laboratories, New Haven, Connecticut, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Haith</surname> <given-names>Adrian M</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Johns Hopkins University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">cashabackjga@gmail.com</email> (JGAC); <email xlink:type="simple">paul@gribblelab.org</email> (PLG)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>4</day>
<month>3</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e1006839</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>2</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Cashaback et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006839"/>
<abstract>
<p>Consideration of previous successes and failures is essential to mastering a motor skill. Much of what we know about how humans and animals learn from such reinforcement feedback comes from experiments that involve sampling from a small number of discrete actions. Yet, it is less understood how we learn through reinforcement feedback when sampling from a continuous set of possible actions. Navigating a continuous set of possible actions likely requires using gradient information to maximize success. Here we addressed how humans adapt the aim of their hand when experiencing reinforcement feedback that was associated with a continuous set of possible actions. Specifically, we manipulated the change in the probability of reward given a change in motor action—the reinforcement gradient—to study its influence on learning. We found that participants learned faster when exposed to a steep gradient compared to a shallow gradient. Further, when initially positioned between a steep and a shallow gradient that rose in opposite directions, participants were more likely to ascend the steep gradient. We introduce a model that captures our results and several features of motor learning. Taken together, our work suggests that the sensorimotor system relies on temporally recent and spatially local gradient information to drive learning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>In recent years it has been shown that reinforcement feedback may also subserve our ability to acquire new motor skills. Here we address how the reinforcement gradient influences motor learning. We found that a steeper gradient increased both the rate and likelihood of learning. Moreover, while many mainstream theories posit that we build a full representation of the reinforcement landscape, both our data and model suggest that the sensorimotor system relies primarily on temporally recent and spatially local gradient information to drive learning. Our work provides new insights into how we sample from a continuous action-reward landscape to maximize success.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002790</institution-id>
<institution>Canadian Network for Research and Innovation in Machining Technology, Natural Sciences and Engineering Research Council of Canada</institution>
</institution-wrap>
</funding-source>
<award-id>PJT-153447</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1368-032X</contrib-id>
<name name-style="western">
<surname>Gribble</surname> <given-names>Paul L.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000034</institution-id>
<institution>Institute of Neurosciences, Mental Health and Addiction</institution>
</institution-wrap>
</funding-source>
<award-id>RGPIN 238338</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1368-032X</contrib-id>
<name name-style="western">
<surname>Gribble</surname> <given-names>Paul L.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded through the Natural Sciences and Engineering Research Council of Canada (RGPIN 238338, <ext-link ext-link-type="uri" xlink:href="http://www.nserc-crsng.gc.ca" xlink:type="simple">http://www.nserc-crsng.gc.ca</ext-link>) awarded to PLG. This work was funded through the Canadian Institute of Health Research (PJT-153447, <ext-link ext-link-type="uri" xlink:href="http://www.cihr-irsc.gc.ca/e/193.html" xlink:type="simple">http://www.cihr-irsc.gc.ca/e/193.html</ext-link>) awarded to PLG. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="1"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-03-14</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Whether a previous action is successful or unsuccessful is an important contributor to sensorimotor learning. Indeed, binary reinforcement feedback (e.g., reward) is sufficient to cause adaptation of hand aim during a reaching task, independent from error feedback [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref007">7</xref>]. It has been proposed that updating aim of the hand based on reinforcement feedback is model-free and occurs by sampling a continuous set of possible motor actions until one or more actions are found that improve task success [<xref ref-type="bibr" rid="pcbi.1006839.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref009">9</xref>]. Sampling motor actions presumably allows the sensorimotor system to use information from the reinforcement landscape to drive adaptation.</p>
<p>Here we broadly define the reinforcement landscape as the mapping between all possible motor actions and the expected reward of those actions. In this context, the sensorimotor system can maximize expected reward by ascending the reinforcement landscape [<xref ref-type="bibr" rid="pcbi.1006839.ref010">10</xref>]. However, for a meaningful change in behaviour to occur there has to be an underlying process that either evaluates or accounts for whether one action is better than another. More specifically for learning to occur the sensorimotor system must account for the gradient of the reinforcement landscape, which defines the rate of change in the expected reward with respect to a change in motor action. Intuitively, the evaluation of different actions may be easier with a steeper gradient, as there would be a more salient change in the expected reward for a change in action.</p>
<p>The form of the reinforcement feedback influences the shape of the reinforcement landscape. Reinforcement feedback can be binary or graded, and can be provided deterministically [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref011">11</xref>] or probabilistically [<xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>]. Binary reinforcement feedback signifies only whether the action was successful or unsuccessful [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>]. Graded feedback varies the magnitude of positive feedback (reward) or negative feedback (punishment) as a function of motor action [<xref ref-type="bibr" rid="pcbi.1006839.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref012">12</xref>]. Thus, the reinforcement landscape gradient can be influenced by the magnitude and or the probability of feedback. Another consideration when using graded reinforcement feedback is that humans form a nonlinear relationship between different reward (or punishment) magnitudes and their perceived value [<xref ref-type="bibr" rid="pcbi.1006839.ref013">13</xref>]. This nonlinear relationship could potentially influence how the sensorimotor system evaluates perceived changes in expected reward.</p>
<p>Movement variability is also thought to influence the gradient of the reinforcement landscape by creating uncertainty between intended actions and actual actions. That is, the expected reward can change depending on whether it is a function of the intended action or the actual action [<xref ref-type="bibr" rid="pcbi.1006839.ref010">10</xref>]. Further, greater movement variability has been linked to faster learning in reinforcement-based tasks as it promotes exploration of the reinforcement landscape [<xref ref-type="bibr" rid="pcbi.1006839.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref015">15</xref>].</p>
<p>Here we designed two experiments to examine how humans adapt the aim of their hand when receiving binary reinforcement feedback. Specifically, we tested the hypothesis that the gradient of the reinforcement landscape influences sensorimotor adaptation. We manipulated the reinforcement landscape gradient by altering the expected reward (the probability of receiving reward) given the angular distance between the hand location and target. To maximize reward, participants had to update the aim of their unseen hand to a location that was not aligned with the visually displayed target. Importantly, we normalized the reinforcement landscapes to baseline movement variability on an individual basis. This normalization allowed us to assess the influence of the reinforcement landscape gradient on learning while accounting for individual differences in movement variability. We used binary reinforcement feedback to eliminate the potentially confounding nonlinear relationship between different magnitudes of reward and their perceived value.</p>
<p>We tested the prediction that a steep reinforcement landscape would lead to faster learning than a shallow landscape (<bold>Experiment 1</bold>). Building on these results, in <bold>Experiment 2</bold> we used a complex reinforcement landscape where each participant’s initial action was positioned in the ‘valley’ between two slopes that had different gradients (steep and shallow) and rose in the opposite direction (clockwise or counterclockwise). We predicted that participants would ascend the steeper portion of the complex reinforcement landscape. Finally, we introduce a model that relies on binary reinforcement feedback to update the aim of the hand during a reaching task.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Experimental design</title>
<p>In <bold>Experiments 1</bold> and <bold>2</bold>, 120 participants performed 450 forward reaching movements (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1A</xref>). For each trial they began at a starting position and attempted to pass their hand (unseen) through a virtually displayed target. We recorded reach angle, which was calculated relative to the line that intersected the visually displayed target and starting position, the moment their hand was 20 cm away from the starting position.</p>
<fig id="pcbi.1006839.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Apparatus and Experiment 1 design.</title>
<p><bold>A)</bold> Participants held the handle of a robotic arm. A semi-silvered mirror reflected images (target and home position) from an LCD screen onto a horizontal plane aligned with the shoulder, and occluded vision of the hand and arm. Participants made forward reaches from a home position, attempted to pass through a virtual target, and stopped once they passed over a horizontal line that disappeared when crossed. We informed participants that they would receive positive reinforcement for each target hit (target expanded, pleasant noise, and monetary reward). Unbeknownst to them, we manipulated <bold>B)</bold> the reinforcement landscape (steep or shallow), which dictated the probability of receiving reward (<italic>y</italic>-axis) as a function of their reach angle (<italic>x</italic>-axis). Reach angle was calculated relative to the line that intersected the target and home position, where the latter was the centre of rotation. To control for individual differences in movement variability, these reinforcement landscapes were scaled according to baseline reach behaviour. Accordingly, a z-score of 0.0 corresponds to their average baseline reach angle. A z-score of 1.0 corresponds to a reach angle that was 1.0 standard deviation, relative to baseline movement variability, away from the average baseline reach angle. We expected participants to adjust their aim such that they moved from their average baseline reach angle (0.0 z-score) to one that produced greater reward (z-score between 3-6). Critically, we predicted that participants experiencing the steep (blue) reinforcement landscape would learn to adjust their reach aim at faster rate than those experiencing the shallow (orange) reinforcement landscape (<bold>Experiment 1</bold>). <bold>C)</bold> Using Eqs <xref ref-type="disp-formula" rid="pcbi.1006839.e027">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006839.e032">9</xref>, we accounted for movement variability to calculate the probability of reward (<italic>y</italic>-axis) given intended reach aim (<italic>x</italic>-axis). The blue and orange vertical dashed lines correspond to the optimal intended reach aim (<inline-formula id="pcbi.1006839.e001"><alternatives><graphic id="pcbi.1006839.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1006839.e034">Eq 10</xref>) that maximized the probability of reward for the steep and shallow reinforcement landscapes, respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g001" xlink:type="simple"/>
</fig>
<p>Participants began by completing 50 baseline trials, where no feedback was received on whether reaches were successful or unsuccessful. During the next 350 experimental trials participants received binary reinforcement feedback according to their randomly assigned reinforcement landscape (see <bold>Experiment 1</bold> and <bold>Experiment 2</bold>). Like baseline, the final 50 washout trials were also performed without feedback.</p>
<p>We instructed participants to “hit the target”. We informed participants that no feedback would be received if they missed the target, and for each target hit 1) the target would expand, 2) they would hear a pleasant noise, and 3) they would receive monetary reward, such that they could earn up to $5.00 CAD.</p>
<p>To test the idea that the gradient of the reinforcement landscape influences sensorimotor learning, we manipulated the probability of receiving positive reinforcement feedback (i.e., reward) as a function of reach angle. In <bold>Experiment 1</bold> we tested the idea that the gradient of the reinforcement landscape would influence the rate of learning. In <bold>Experiment 2</bold> we tested the notion that the sensorimotor system would use gradient information from a complex reinforcement landscape to find the best of multiple solutions that improved performance.</p>
</sec>
<sec id="sec004">
<title>Experiment 1</title>
<p>We tested the idea that the gradient of the reinforcement landscape influences the rate of learning. We predicted that a steeper reinforcement landscape would lead to a faster learning rate.</p>
<p>Participants either experienced a steep reinforcement landscape (<italic>n</italic> = 40) or a shallow reinforcement landscape (<italic>n</italic> = 40). To control for direction, the probability of positive reinforcement (reward) rose either in the clockwise (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1B</xref>; <xref ref-type="disp-formula" rid="pcbi.1006839.e020">Eq 2</xref>) or counterclockwise direction (<xref ref-type="disp-formula" rid="pcbi.1006839.e023">Eq 3</xref>). We created these landscapes by manipulating the probability of reward as a function of reach angle. The width of each reinforcement landscape, that is the probability of reward given reach angle, was normalized to baseline movement variability on an individual basis. This normalization ensured that participants in an experimental group (steep or shallow) experienced the same gradient for a particular landscape, irrespective of movement variability. This also allowed us to calculate the change in reward probability for a change in intended aim (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1C</xref>, Eqs <xref ref-type="disp-formula" rid="pcbi.1006839.e027">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006839.e032">9</xref>) across participants, as well as the optimal intended reach aim (<inline-formula id="pcbi.1006839.e002"><alternatives><graphic id="pcbi.1006839.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) that maximized success (<xref ref-type="disp-formula" rid="pcbi.1006839.e034">Eq 10</xref>).</p>
<p>Reach angles were normalized by baseline movement variability on an individual basis and expressed as a z-score. Further, to allow for visual and statistical comparison irrespective of the direction that the reinforcement landscape rose (clockwise or counterclockwise), we multiplied the normalized reach angles by −1.0 for all participants that experienced a reinforcement landscape that increased in the counterclockwise direction [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref016">16</xref>].</p>
<p>Similar to others [<xref ref-type="bibr" rid="pcbi.1006839.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref018">18</xref>], we found two subpopulations of participants in <bold>Experiment 1</bold>: learner and non-learners. When examining the histogram of final reach position (average normalized reach angle of the last 100 experimental trials), we found a bimodal distribution (<xref ref-type="supplementary-material" rid="pcbi.1006839.s001">S1 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s005">S1 Fig</xref>). Based on this analysis, we found that a cutoff z-score of 1.0 did well to partition the bimodal distribution and separate the learners from the non-learners.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2A and 2B</xref> shows individual data from two participants. The participant experiencing a steep reinforcement landscape quickly changed their behaviour towards a reach angle that maximized reward (z-score between 3 and 6). The participant experiencing a shallow reinforcement landscape took comparatively longer to change their reaching behaviour. The difference in learning rates between these two participants is most evident during the first 50 experimental trials.</p>
<fig id="pcbi.1006839.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Behavioral data of Experiment 1.</title>
<p>Reach angle (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis): <bold>A)</bold> of a participant that experienced the steep reinforcement landscape, and <bold>B)</bold> of a participant that experienced the shallow reinforcement landscape, and <bold>C)</bold> averaged across participants that experienced either the steep (blue) or shallow (orange) reinforcement landscape, where shaded regions represent ± 1.0 SE. The grey vertical lines separate baseline trials (1-50), experimental trials (51-400) and washout trials (401-450). The dashed horizontal lines indicate the optimal intended reach aim (<inline-formula id="pcbi.1006839.e003"><alternatives><graphic id="pcbi.1006839.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) to maximize reward. In <bold>A)</bold> and <bold>B)</bold>, during the experimental trials, the blue and orange circles respectively indicate that the participant received reward on a given trial, while the black circles indicate no reward. In <bold>C)</bold>, the thick blue and orange curves represents the best-fit exponential functions to the reach angles of participants that experienced the steep or shallow reinforcement landscapes, respectively. The time constant (λ) of these exponential functions characterize the rate of learning and were found via a bootstrapping procedure. <bold>D)</bold> Posterior probability distributions of the exponential function time constants given the experimental reach angles (left <italic>y</italic>-axis) of participants that experienced the steep (λ<sub><italic>steep</italic></sub>, blue) or shallow (λ<sub><italic>shallow</italic></sub>, orange) reinforcement landscapes. The thick lines are the corresponding cumulative distributions (right <italic>y</italic>-axis). The inset represents the posterior probability distribution of the time constant differences between the shallow and steep participants (i.e., λ<sub><italic>shallow</italic></sub> − λ<sub><italic>steep</italic></sub>). As observed in <bold>C)</bold> and <bold>D)</bold>, we found that participants who experienced a steep reinforcement landscape had a significantly faster rate of learning (i.e., λ<sub><italic>steep</italic></sub> &lt; λ<sub><italic>shallow</italic></sub>) than those that experienced a shallow landscape (<italic>p</italic> = 0.012).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g002" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2C</xref> shows the average reach angle over trials for participants (learners) that experienced either a steep or shallow reinforcement landscape. To compare the rate of learning between these two groups of participants, we fit an exponential function (<xref ref-type="disp-formula" rid="pcbi.1006839.e026">Eq 6</xref>) over the experimental trials via bootstrapping (see <xref ref-type="sec" rid="sec011">Methods</xref> for details). We were interested in the time constant of the exponential function, λ, which defines the rate of learning. The exponential bootstrap fit analysis was performed separately first with the data from the learners alone, and then again with all participants (learners and non-learners together). As hypothesized, we found that the participants experiencing the steep landscape had faster learning (i.e., a lower exponential function time constant, λ) than those experiencing a shallow reinforcement landscape (<italic>p</italic> = 0.012 learners only, <italic>p</italic> = 0.021 for combined learners and non-learners, one-tailed). <xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2D</xref> shows the posterior probability distribution and cumulative distribution of the time constant λ given the reach angles of participants experiencing either a steep or shallow reinforcement landscape. The inset of <xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2D</xref> shows the posterior probability distribution of the time constant difference between the two experimental groups, from which we calculated the <italic>p</italic>-values reported directly above. The direction of the reinforcement landscape, clockwise or counterclockwise, did not influence the rate of learning (<italic>p</italic> = 0.540, two-tailed).</p>
<p>We also found that participants who experienced a steep landscape were more likely to be classified as learners than those who experienced a shallow reinforcement landscape (<italic>p</italic> = 0.036, two-tailed; <xref ref-type="table" rid="pcbi.1006839.t001">Table 1</xref>).</p>
<table-wrap id="pcbi.1006839.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.t001</object-id>
<label>Table 1</label>
<caption>
<title>Frequency of learners and non-learners.</title>
<p>Frequency of learners and non-learners partitioned based on whether the participants experienced a steep or shallow reinforcement landscape.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006839.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Group</th>
<th align="center">Learners</th>
<th align="center">Non-Learners</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Steep Reinforcement Landscape</td>
<td align="center">37</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">Shallow Reinforcement Landscape</td>
<td align="center">29</td>
<td align="center">11</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec005">
<title>Experiment 2</title>
<p>In this experiment we tested the notion that the sensorimotor system uses gradient information from a complex reinforcement landscape to find the solution that maximizes reward. The probability of reward was at a minimum for reaches toward mean baseline behaviour but increased at different gradients (steep or shallow) for reaches in either direction (clockwise or counterclockwise) away from the target. We predicted that a significantly greater number of participants would adapt their reach aim in the direction of the steeper gradient.</p>
<p>Two different reinforcement landscapes were used in this experiment: one landscape had a steep slope that rose in the clockwise direction and a shallow slope that rose in the counterclockwise direction (steep clockwise; <italic>n</italic> = 20; <xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3A</xref>; <xref ref-type="disp-formula" rid="pcbi.1006839.e024">Eq 4</xref>), and the other landscape had a steep slope that rose in the counterclockwise direction and a shallow slope that rose in the counterclockwise direction (steep counterclockwise; <italic>n</italic> = 20; <xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3C</xref>; <xref ref-type="disp-formula" rid="pcbi.1006839.e025">Eq 5</xref>). As in <bold>Experiment 1</bold>, for both reinforcement landscapes we calculated the probability of reward given intended aim (<xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3B and 3D</xref>; Eqs <xref ref-type="disp-formula" rid="pcbi.1006839.e027">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006839.e032">9</xref>), as well as the optimal intended reach aim (<inline-formula id="pcbi.1006839.e004"><alternatives><graphic id="pcbi.1006839.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) to maximize reward (<xref ref-type="disp-formula" rid="pcbi.1006839.e034">Eq 10</xref>).</p>
<fig id="pcbi.1006839.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Experimental 2 design.</title>
<p>Participants experienced a complex reinforcement landscape where they were initially positioned in the ‘valley’ between a steep and shallow slope. Participants experienced a reinforcement landscape where the steep slope rose in the <bold>A)</bold> clockwise direction (CW) or <bold>C)</bold> counterclockwise (CCW) direction. These reinforcement landscapes define the probability of reward (<italic>y</italic>-axis) as a function of reach angle (<italic>x</italic>-axis). Reach angle is normalized to baseline reach behaviour and is expressed as a z-score. <bold>B)</bold> and <bold>D)</bold> define the probability of reward (<italic>y</italic>-axis) given intended aim (<italic>x</italic>-axis) for the steep clockwise and steep counterclockwise reinforcement landscapes, respectively. In both these figures, <inline-formula id="pcbi.1006839.e005"><alternatives><graphic id="pcbi.1006839.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and the corresponding dashed vertical line correspond to the optimal intended reach aim that maximizes reward. We predicted that participants experiencing the steep clockwise reinforcement landscape to adjust their aim in the clockwise direction. Similarly, we expected that those experiencing the steep counterclockwise reinforcement landscape to adjust their aim in the counterclockwise direction.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g003" xlink:type="simple"/>
</fig>
<p>Here we were interested in the frequency of participants that changed their reach behaviour in the clockwise or counterclockwise direction, depending on whether they experienced the steep clockwise or steep counterclockwise reinforcement landscape. We used the average of the last 100 experimental trials to classify the direction of their final reach behaviour. Final reach direction was classified to be counterclockwise (z-score ≤ −1.0), center (−1.0 &lt; z-score &lt; +1.0) or clockwise (z-score ≥ +1.0). This classification was done separately for those experiencing a steep clockwise or steep counterclockwise reinforcement landscape.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006839.g004">Fig 4A and 4B</xref> show the average reach angle of steep learners, shallow learners and non-learners for participants experiencing the steep clockwise or steep counterclockwise reinforcement landscapes, respectively. The steep and shallower learners in <xref ref-type="fig" rid="pcbi.1006839.g004">Fig 4A</xref> respectively look qualitatively similar to the steep and shallow learners in <xref ref-type="fig" rid="pcbi.1006839.g004">Fig 4B</xref> when reflecting either of these figures about its <italic>x</italic>-axis. The behaviour of the non-learners was less consistent based on whether they experienced the clockwise or counterclockwise landscapes, but there was a limited frequency of non-learners (<italic>n</italic> = 2 and <italic>n</italic> = 3, respectively).</p>
<fig id="pcbi.1006839.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Behavioral data of Experiment 2.</title>
<p>Average reach angles (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis) of participants that experienced <bold>A)</bold> a steep clockwise or <bold>B)</bold> a steep counterclockwise reinforcement landscape. In both these subplots we partition the behaviour of participants that were classified as steep learners (solid lines), shallow learners (dashed lines), or non-learners (dotted lines). In <bold>A)</bold> positive reach angles correspond with the steep slope and the clockwise (CW) direction. In <bold>B)</bold> negative reach angles correspond with the steep slope and the counterclockwise (CCW) direction. The grey vertical lines separate baseline, experimental and washout trials. Shaded regions represent ± 1.0 SE. <bold>C)</bold> Frequency of participants whose final reach direction (average of the last 100 trials) was in the clockwise (z-score ≤ −1.0), central (−1.0 &lt; z-score &lt; +1.0), and (z-score ≥ +1.0) counterclockwise directions. As expected, we found significant differences in the frequency of final reach directions between participants that experienced the steep clockwise or steep counterclockwise reinforcement landscapes (<italic>p</italic> = 0.010). As predicted, we found the majority of participants used information from the complex reinforcement landscape to ascend up the steeper slope.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g004" xlink:type="simple"/>
</fig>
<p>As an additional classification, participants that had a final reach position corresponding to the direction of the steep slope, shallow slope or a central location, were deemed steep learners, shallow learners and non-learners, respectively. This was done separately for participants that experienced either the steep clockwise or steep counterclockwise reinforcement landscape.</p>
<p>For this experiment we predicted that participants would ascend the steeper gradient of their assigned reinforcement landscape. Specifically, we expected more participants who experienced the steep clockwise reinforcement landscape to have their final average reach angle to be classified as clockwise. Similarly, we expected participants who experienced the steep counterclockwise reinforcement landscape to have their final average reach angle to be classified as counterclockwise. Using z-score cutoffs of −1.0 and +1.0, we found that there were significant differences in the final average reach classification between participants who experienced a steep clockwise or steep counterclockwise reinforcement landscape (<italic>p</italic> = 0.010, two-tailed, <xref ref-type="fig" rid="pcbi.1006839.g004">Fig 4C</xref>). These results were robust to whether we used z-score cutoffs of ±0.5 (<italic>p</italic> = 0.016, two-tailed) or ±1.5 (<italic>p</italic> = 0.020, two-tailed) to classify final reach direction. Further, we found that the direction (clockwise or counterclockwise) did not influence behaviour in terms of whether a participant was classified as a steep learner, shallow learner or non-learner (<italic>p</italic> = 0.810, two-tailed). Thus, the direction of the reinforcement landscape had an effect on their final reach direction, but it did not impact the frequency of steep learners, shallow learners, and non-learners.</p>
</sec>
<sec id="sec006">
<title>Learning model and best-fit parameters</title>
<p>Here we introduce a learning model that predicts reach angle (<italic>θ</italic><sub><italic>n</italic></sub>) on a trial-by-trial basis (<xref ref-type="disp-formula" rid="pcbi.1006839.e006">Eq 1</xref>). This model takes the form
<disp-formula id="pcbi.1006839.e006"><alternatives><graphic id="pcbi.1006839.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> </mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>n</italic> and <italic>n</italic> + 1 represent the current and next trial, respectively.</p>
<p>The model considers whether the current reach angle was successful (<italic>r</italic> = 1) or unsuccessful (<italic>r</italic> = 0). The model explores small regions of the workspace in a natural way via movement variability. Here, the variance of movement variability on the current trial (<inline-formula id="pcbi.1006839.e007"><alternatives><graphic id="pcbi.1006839.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) is a function of motor (execution) variance (<inline-formula id="pcbi.1006839.e008"><alternatives><graphic id="pcbi.1006839.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) after a successful reach, and the addition of both motor variance and exploratory variance (<inline-formula id="pcbi.1006839.e009"><alternatives><graphic id="pcbi.1006839.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) after an unsuccessful reach [<xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>]. It was assumed that the variance of movement variability follows a Normal distribution <inline-formula id="pcbi.1006839.e010"><alternatives><graphic id="pcbi.1006839.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1006839.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref021">21</xref>], where <inline-formula id="pcbi.1006839.e011"><alternatives><graphic id="pcbi.1006839.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> represents the intended reach aim on the <italic>n</italic><sup><italic>th</italic></sup> trial.</p>
<p>Inspired by Haith and Krakauer (2014) [<xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>], the only action cached in memory is related to the location of the last successful reach. That is, an update in the intended reach aim (<inline-formula id="pcbi.1006839.e012"><alternatives><graphic id="pcbi.1006839.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) occurs only after a successful reach. Specifically, this update is some proportion (<italic>α</italic>) of the difference between the current intended aim (<inline-formula id="pcbi.1006839.e013"><alternatives><graphic id="pcbi.1006839.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) and the location of the last successful reach (<italic>θ</italic><sub><italic>n</italic></sub>). After an unsuccessful reach, the intended aim remains the same (i.e., <inline-formula id="pcbi.1006839.e014"><alternatives><graphic id="pcbi.1006839.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>n</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is still stored based on the last successful reach) but the subsequent movement has greater variance (<italic>σ</italic><sub><italic>m</italic></sub> + <italic>σ</italic><sub><italic>e</italic></sub>). This results in a similar formulation to the equation just recently published by Therrien and colleagues (2018) [<xref ref-type="bibr" rid="pcbi.1006839.ref023">23</xref>]. There are some slight differences between the present model and the Therrien et al. (2015, 2018) model in terms of how they update the intended aim following a successful reach [<xref ref-type="bibr" rid="pcbi.1006839.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref024">24</xref>] (<bold>see</bold> <xref ref-type="sec" rid="sec010">Discussion</xref>). Nevertheless, in the following we show the utility of this class of model in terms of replicating several features of sensorimotor adaptation. As previously suggested by van Beers (2009) [<xref ref-type="bibr" rid="pcbi.1006839.ref025">25</xref>] and Zhang et al. (2015) [<xref ref-type="bibr" rid="pcbi.1006839.ref026">26</xref>], our model assumes that the nervous system has some knowledge of movement variability when updating intended reach aim. This allows for an estimated difference between intended aim and actual reach angle, despite the participants have no vision of their hand during trials.</p>
<p>Our model has three free parameters: <italic>α</italic> = 0.40 (<italic>unitless</italic>), <italic>σ</italic><sub><italic>m</italic></sub> = 0.81 (z-score), and <italic>σ</italic><sub><italic>e</italic></sub> = 0.90 (z-score). The initial guesses of <italic>σ</italic><sub><italic>m</italic></sub> and <italic>σ</italic><sub><italic>e</italic></sub> for the fitting procedure were made with a trial-by-trial difference analysis (<xref ref-type="supplementary-material" rid="pcbi.1006839.s002">S2 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s006">S2 Fig</xref>) that we modified from Pekny et al. (2015). It is expected that <italic>σ</italic><sub><italic>m</italic></sub> is slightly lower than a z-score of 1, or baseline movement variability, since here we were interested in the movement variability on a single-trial and not the additive variance that results from repeatedly subtracting two successive trials (see <xref ref-type="supplementary-material" rid="pcbi.1006839.s002">S2 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s006">S2 Fig</xref> for further details). We found the best-fit parameters using a bootstrap optimization fitting procedure using only the data from <bold>Experiment 1</bold> (<xref ref-type="supplementary-material" rid="pcbi.1006839.s003">S3 Data</xref>).</p>
</sec>
<sec id="sec007">
<title>Simulating Experiment 1</title>
<p>With our learning model, we simulated 40 individuals experiencing the steep reinforcement landscape of <bold>Experiment 1</bold>, and then simulated another 40 individuals experiencing the shallow landscape.</p>
<p>We found that simulated individuals displayed similar trial-by-trial variance and rates of learning compared to the behavioural data (compare <xref ref-type="fig" rid="pcbi.1006839.g005">Fig 5A and 5B</xref> to <xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3A and 3B</xref>). We averaged across the 40 simulated individuals in each condition (steep or shallow reinforcement landscape). The model did well to capture between-subject variance. Similar to the behavioural data, we also found the emergence of exponential learning curves (<xref ref-type="fig" rid="pcbi.1006839.g005">Fig 5C</xref>).</p>
<fig id="pcbi.1006839.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Simulations of Experiment 1.</title>
<p>Reach angle (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis) when using our learning model to simulate an ‘individual’ experiencing <bold>A)</bold> the steep reinforcement landscape and <bold>B)</bold> the shallow reinforcement landscape. In <bold>A)</bold> and <bold>B)</bold>, during the experimental trials, the blue and orange circles respectively indicate that the model received reward on a given trial, while the black circles indicate no reward. At the individual level, the learning model does well to capture individual movement variability and a faster rate of learning when experiencing the steep reinforcement landscape (compare to <xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3A and 3B</xref>). For <bold>A)</bold>, <bold>B)</bold>, and <bold>C)</bold> the grey vertical lines separate baseline trials (1-50), experimental trials (51-400) and washout trials (401-450). The dashed horizontal lines indicate the optimal intended reach aim (<inline-formula id="pcbi.1006839.e015"><alternatives><graphic id="pcbi.1006839.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) to maximize reward. <bold>C)</bold> Average reach angle (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis) when using the learning model to simulate 40 ‘individuals’ for both the steep (blue) and shallow (orange) reinforcement landscape. Shaded regions represent ± 1.0 SE. The thick blue and orange curves represents the best-fit exponential functions to the average reach angles of simulated ‘individuals’ that experienced the steep or shallow reinforcement landscapes, respectively. For comparison, the inset displays the behavioural data of <bold>Experiment 1</bold> (also shown in <xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2C</xref>). At the group level, the learning model does well to capture between-subject variability, reproduces a faster rate of learning for the steep landscape, and gives rise to exponential learning curves.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g005" xlink:type="simple"/>
</fig>
<p>We then simulated 100, 000 individuals experiencing the steep landscape and 100, 000 individuals experiencing the shallow landscapes. Simulating a large number of individuals allowed us to numerically converge on the theoretical exponential learning curves produced by the model. We then averaged across simulated individuals in each group and fit an exponential function. The best-fit time constant, λ, of the exponential function for the steep and shallow reinforcement landscapes were 28.0 and 49.6, respectively. Both values fall within the 95<sup><italic>th</italic></sup> percentile confidence intervals of the corresponding behavioural data. (steep [10.7, 36.2], shallow [27.4, 102.1]; <xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2D</xref>).</p>
<p>In <xref ref-type="supplementary-material" rid="pcbi.1006839.s002">S2 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s006">S2 Fig</xref> we present a trial-by-trial analysis, as a function of reinforcement history, of both the model simulations and behavioural data. We show in <xref ref-type="supplementary-material" rid="pcbi.1006839.s004">S4 Data</xref> with model simulations that changing the initial reward probability of the shallow landscape has a marginal influence on learning rates.</p>
</sec>
<sec id="sec008">
<title>Simulating Experiment 2 with the best-fit parameters found in Experiment 1</title>
<p>Here we simulated <bold>Experiment 2</bold> using our learning model (<italic>n</italic> = 100, 000 simulated individuals) by using the best-fit parameters obtained from the behavioural data in <bold>Experiment 1</bold>. To compare the model to the behavioural results, we combined the data from all participants in <bold>Experiment 2</bold>. This was accomplished by multiplying the normalized reach angles by −1.0 for participants that experienced the steep counterclockwise reinforcement landscape.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6A</xref> shows a histogram of the final reach angle of both the behavioural data and model simulations. We then used the same final reach direction classification for the model simulations that we used for the behavioural data. Based on these classifications, we found that the model produced a similar frequency of steep learners, shallow learners and, to some extent, non-learners as the behavioural data (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6A and 6B</xref>). Further, we found that the model did well to explain reach angle over trials for these three different groups (<italic>R</italic><sup>2</sup> = 0.85; <xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6B</xref>).</p>
<fig id="pcbi.1006839.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Simulations of Experiment 2 (using the best-fit parameters found in Experiment 1).</title>
<p><bold>A)</bold>, <bold>B)</bold>, <bold>C)</bold>, <bold>D)</bold> We simulated 100,000 ‘individuals’ experiencing the steep clockwise reinforcement landscape with our learning model. <bold>A)</bold> Frequency (<italic>y</italic>-axis) of final reach direction (<italic>x</italic>-axis), the average of the last 100 reaches, of the behavioural data (bars) and model (dashed line). The inset shows the corresponding cumulative distribution. As shown, we found good agreement between the model and behaviour. <bold>B)</bold> Reach angle (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis) for the behaviour data and model outputs. Here we display the combined behavioural data of participants that experienced the steep clockwise and steep counterclockwise reinforcement landscape (see <xref ref-type="sec" rid="sec002">Results</xref> for details). We partitioned participants into steep learners (thin solid line), shallow learners (thin dashed line), and non-learners (thin dotted line) based on their final reach behaviour. Using the same classification criteria, the model also produced steep learners (thick solid line) and shallow learners (thick dashed line) at similar frequencies, and to some extent non-learner (thick dotted line). The model did well to capture the average reach angles of the steep learners (<italic>n</italic> = 26) and shallow learners (<italic>n</italic> = 9). It did not do well to capture the reach angles of non-learners, however there were only five participants in this group. Overall, the average reach angles of the model and behavioural data were highly correlated (<italic>R</italic><sup>2</sup> = 0.86). The grey vertical lines separate baseline, experimental and washout trials. Shaded regions represent ± 1.0 SE. <bold>C)</bold> and <bold>D)</bold> show the probability of becoming classified as a steep learner, shallow learner, or non-learner based based on whether the <italic>N</italic><sup><italic>th</italic></sup> successful reach was on the shallow slope or steep slope, respectively. Again, the participants (solid lines) and model (dashed lines) behaved similarly. These data highlight the importance of early exploration on whether an individual will maximize reward when experiencing a complex reinforcement landscape.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g006" xlink:type="simple"/>
</fig>
<p>We also performed an analysis to explore the influence of reinforcement feedback during the initial periods of experimental trials. To this end, we calculated how a participant’s <italic>N</italic><sup><italic>th</italic></sup> success predicted their final reach classification. This was done separately for successful reaches made on the shallow (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6C</xref>) and steep (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6D</xref>) slopes of the complex reinforcement landscape. We found that if a participant had their 1<sup><italic>st</italic></sup> success on the steep slope that they would likely be classified as a steep learner (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6D</xref>). Conversely, a 1<sup><italic>st</italic></sup> success on the shallow slope was not a good predictor of final reach classification (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6D</xref>). However, a participant was likely to be classified as a shallow learner if their 15<sup><italic>th</italic></sup> success and beyond was on the shallow slope. As shown, the model and data were highly correlated with each other (<italic>R</italic><sup>2</sup> = 0.933 and <italic>R</italic><sup>2</sup> = 0.995, respectively). This analysis shows that the participants and model simulations were both heavily influenced by early exploration and gradient information when they experienced a complex reinforcement landscape.</p>
</sec>
<sec id="sec009">
<title>Replicating previous work</title>
<p>Using the same set of best-fit parameters found from the data of <bold>Experiment 1</bold>, we replicated the results of Izawa and Shadmehr (2011) and our previous work [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>] (see <xref ref-type="fig" rid="pcbi.1006839.g007">Fig 7A and 7B</xref>, respectively). In the study by Izawa and Shadmehr (2011), participants were only provided binary feedback if they hit a target region that was gradually rotated from a visual displayed target. In our previous work [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>], cursor position was laterally shifted according to a skewed probability distribution and participants received binary feedback on whether the laterally shifted cursor hit the visually displayed target. In both these studies, participants had no vision of their hand or arm. We had our model experience the same reported conditions from both these studies. Our model did very well to capture average reach behaviour, between-subject variance, trial-by-trial movement variability as a function of reinforcement history (see [<xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>]; <xref ref-type="supplementary-material" rid="pcbi.1006839.s002">S2 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s006">S2 Fig</xref>), and suboptimality.</p>
<fig id="pcbi.1006839.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Replicating previous work using the best-fit parameters found in Experiment 1.</title>
<p><bold>A)</bold> We simulated the experiment of Izawa and Shadmehr (2011) using our learning model. Reach angle (<italic>y</italic>-axis) over trials (<italic>x</italic>-axis) as simulated by our model is shown in green (<italic>n</italic> = 18). The inset display the original behavioural data (black line) reported from Izawa and Shadmehr (2011; reprinted with permission). Our model captures both the linear change in reach angle and the between-subject variability. <bold>B)</bold> We then simulated a previous experiment of ours [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>]. Hand position (i.e., compensation, <italic>mm</italic>) over trials (<italic>x</italic>-axis) as simulated by our model is shown in green (<italic>n</italic> = 30). The inset shows the original behavioural data, where the dark red line represents the hand position of participants when they are receiving only binary reinforcement feedback to perform the task ([<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>]; reprinted with permission). <inline-formula id="pcbi.1006839.e017"><alternatives><graphic id="pcbi.1006839.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> represents the optimal location to aim the hand to maximize target hits (reward). Here, the model replicates the exponential learning curve, between-subject variability and suboptimal performance.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g007" xlink:type="simple"/>
</fig>
<p>Here, we define suboptimality as approaching but not quite reaching the optimal behaviour that maximizes reward (i.e., <inline-formula id="pcbi.1006839.e016"><alternatives><graphic id="pcbi.1006839.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> in <xref ref-type="fig" rid="pcbi.1006839.g007">Fig 7B</xref>). Suboptimality is often a feature of ‘greedy’ algorithms that place greater emphasis on locally optimal information rather than globally optimal information [<xref ref-type="bibr" rid="pcbi.1006839.ref027">27</xref>]. Our learning model would be considered a greedy algorithm since it samples from spatially local motor actions and updates its aim based on the last recent success. A greedy algorithm can lead to suboptimal performance in non-symmetrical landscapes (e.g., [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>], <xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1B and 1C</xref>) and complex landscapes with local maximums (e.g., <xref ref-type="fig" rid="pcbi.1006839.g002">Fig 2</xref>). Behaviourally, this was particularly evident in <bold>Experiment 2</bold> where a relatively high proportion of participants (22.5%) performed suboptimally by ascending the shallow slope and having a final reach direction aligned with a local maximum.</p>
<p>Further motivated by the model of Haith and Krakauer (2014) [<xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>], we ran simulations to examine how movement variability influences the rate of learning and whether our model could capture random-walk behaviour. There is some debate to whether movement variability is beneficial [<xref ref-type="bibr" rid="pcbi.1006839.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>] or detrimental [<xref ref-type="bibr" rid="pcbi.1006839.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref030">30</xref>] when learning from error feedback, which to some extent may be explained by the consistency (entropy) of the environment [<xref ref-type="bibr" rid="pcbi.1006839.ref031">31</xref>]. Recent work has suggested that movement variability is important when learning from reinforcement feedback and can influence the rate of learning [<xref ref-type="bibr" rid="pcbi.1006839.ref014">14</xref>]. Here we manipulated both motor (<italic>σ</italic><sub><italic>m</italic></sub>) and exploratory (<italic>σ</italic><sub><italic>e</italic></sub>) contributions to movement variability when simulating the experimental conditions of <bold>Experiment 1</bold>. We found that increasing the variance of movement variability, either <italic>σ</italic><sub><italic>m</italic></sub> or <italic>σ</italic><sub><italic>e</italic></sub>, led to increased rates of learning for both the steep (<xref ref-type="fig" rid="pcbi.1006839.g008">Fig 8A</xref>) and shallow (<xref ref-type="fig" rid="pcbi.1006839.g008">Fig 8B</xref>) reinforcement landscapes. However, it should be noted that with different amounts of movement variability there may exist a trade-off between the rate of learning and the probability of reward.</p>
<fig id="pcbi.1006839.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Predicting learning rates given motor and exploratory movement variability.</title>
<p>In <bold>A)</bold> and <bold>B)</bold> we predict the rate of learning (λ; <italic>y</italic>-axis) after varying motor (<italic>σ</italic><sub><italic>m</italic></sub>, <italic>deg</italic>; <italic>x</italic>-axis) and exploratory (<italic>σ</italic><sub><italic>e</italic></sub>, <italic>deg</italic>; different shaded lines) contributions to movement variability, when the our learning model experiences the steep (blue) and shallow (orange) landscapes, respectively (<italic>n</italic> = 10, 000 per data point). In both <bold>A)</bold> and <bold>B)</bold> we find that increasing either <italic>σ</italic><sub><italic>m</italic></sub> or <italic>σ</italic><sub><italic>e</italic></sub> leads to a faster rate of learning (i.e., lower magnitudes of λ).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g008" xlink:type="simple"/>
</fig>
<p>In previous literature, random-walk behaviour along task-irrelevant dimensions has been attributed solely to error-based learning [<xref ref-type="bibr" rid="pcbi.1006839.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref035">35</xref>]. In the study by van Beers and colleagues (2013), participants received error (visual) feedback when reaching to large targets (<xref ref-type="fig" rid="pcbi.1006839.g009">Fig 9D</xref>). They displayed random-walk behaviour (i.e., trial-by-trial correlations) along the task-irrelevant dimensions that had no bearing on task success. Here we tested whether reinforcement feedback can also lead to random-walk behaviour. To test this idea, we used our model to simulate the experiment van Beers et al. (2013). Critically however, we did not use error feedback as in the original study—instead we only provided binary reinforcement feedback to the model based on whether it had hit or missed the target. Interestingly, we found that random-walk behaviour along task-irrelevant dimensions also <italic>emerged</italic> from our model (<xref ref-type="fig" rid="pcbi.1006839.g009">Fig 9A, 9B and 9C</xref>). Thus, our simulations suggest that random-walk behaviour, at least in part, may be attributed to reinforcement-based processes.</p>
<fig id="pcbi.1006839.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006839.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Random-walk behavior along task-irrelevant dimensions.</title>
<p>In <bold>A), B), C)</bold>, we show that our learning model predicts that updating aim based on reinforcement feedback can lead to random-walk behaviour along task-irrelevant dimensions. <bold>A)</bold> Target with a long length (task-irrelevant dimension) and short width (task-relevant dimension). Endpoint position (green) as predicted by our model for each trial. <bold>B)</bold> Corresponding endpoints (<italic>y</italic>-axis) as partitioned into task-irrelevant (dark green) and task-relevant (light green) dimensions over trials (<italic>x</italic>-axis). Here <italic>AFC</italic>(1) represent the autocorrelation (lag 1 trial) of the task-irrelevant (dark green) and task-relevant (light green) endpoint components when simulating an individual. This autocorrelation provides insight if trials <italic>n</italic> and <italic>n</italic> + 1 are uncorrelated or correlated, where the latter is a feature of random-walk behaviour. <bold>C)</bold> The average AFC(1) in task-irrelevant (dark green) and task-relevant (light green) when simulating six subjects (± 1.0 SE bars). <bold>D)</bold> Original behavioural data from van Beers and colleagues (2013; reprinted with permission) showing task-irrelevant random-walk when participants received <italic>error feedback</italic>—not reinforcement feedback as simulated with our model. Although previously attributed to error-based processes, our model predicts that reinforcement-based processes may also give rise to random-walk behaviour.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.g009" xlink:type="simple"/>
</fig>
<p>Our model relies on updating intended reach aim by using only the recent success (temporally current information) based on sampling the reinforcement landscape via movement variability (spatially local information). Given the strong relationship between our model and the behavioural data throughout the simulations above, our results suggest that the sensorimotor system largely depends on temporally recent and spatially local information to update where to aim the hand during our reinforcement-based sensorimotor learning task.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>We found that manipulating the gradient of the reinforcement landscape influenced sensorimotor learning. First, we found that a steep reinforcement landscape led to faster learning. Second, participants were more likely to adjust their aim in the direction of the steepest portion of a complex reinforcement landscape. Our learning model that relies on reinforcement feedback to update aim of the hand was able to replicate the results in <bold>Experiment 1</bold> and predict the results found in <bold>Experiment 2</bold>. Taken together, our data and model suggest that the sensorimotor behaviour observed in our experiments does not necessitate a full representation of the entire reinforcement landscape (storing the expected reward for all possible actions). Rather, the majority of learning behaviour can be captured using temporally recent and spatially local information about actions and rewards.</p>
<p>Participants learned faster when they experienced a steep reinforcement landscape, compared to those experiencing a shallow landscape. To our knowledge this is the first work showing that the gradient of the reinforcement landscape influences the rate of learning. The present study may be distinguished from previous work showing that a graded reinforcement landscape can augment error-based learning [<xref ref-type="bibr" rid="pcbi.1006839.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref012">12</xref>]. Here we show that the gradient of a binary, positive reinforcement landscape influences learning in the absence of error feedback.</p>
<p>Using a visuomotor rotation task, Nikooyan and Ahmed (2014) used both graded reinforcement feedback and error feedback to study their effects on learning. Participants moved a cursor which was rotated from the unseen hand as it moved away from a start position towards a virtual target. Participants performed the task either with or without error (cursor) feedback. They experienced a graded reinforcement landscape, such that the magnitude of reward changed with the angular distance of the hand from the target, according to either a linear or cubic function. The maximum reward magnitude occurred when the rotated cursor hit the target. Relative to learning using only error feedback, linearly and cubically graded reinforcement landscapes combined with error feedback accelerated learning. They also found differences in the amount of adaptation between participants who experienced only graded reinforcement feedback (without any visual error feedback) based on either a linear or cubic reinforcement landscape. However, these differences reversed in direction during the course of the experiment and, in some instances, opposed theoretical predictions from a temporal-difference (TD) reinforcement algorithm [<xref ref-type="bibr" rid="pcbi.1006839.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref036">36</xref>]. These inconsistent findings may have been caused by not controlling for individual differences in movement variability [<xref ref-type="bibr" rid="pcbi.1006839.ref014">14</xref>] or the nonlinear relationship between different reward magnitudes and their perceived value [<xref ref-type="bibr" rid="pcbi.1006839.ref013">13</xref>].</p>
<p>In our experiments, we used binary feedback that always had the same magnitude of reward. This eliminated the nonlinear relationship between different reward magnitudes and their perceived value [<xref ref-type="bibr" rid="pcbi.1006839.ref013">13</xref>]. Further, we controlled for individual differences in movement variability, which can influence exploration and the rate of learning in reinforcement-based tasks [<xref ref-type="bibr" rid="pcbi.1006839.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref037">37</xref>]. Thus, our work is the first to our knowledge that has isolated how the gradient of the reinforcement landscape influences the rate of sensorimotor learning.</p>
<p>In our second experiment, each participant’s initial action was positioned in the ‘valley’ between two slopes that had different gradients (steep or shallow) and rose in opposite directions. As predicted, we found participants were more likely to ascend the steepest portion of a complex reinforcement landscape. While the majority of participants ascended the steep slope, several participants ascended the shallow slope. The probability of whether they would be classified as a steep learner or shallow learner seemed related to initial success on either the steep or shallow portion of the landscape. In particular, participants were very likely to be classified as a steep learner if their first successful reach was on the steep slope of the complex landscape.</p>
<p>Our learning model did well to capture trial-by-trial behaviour, between subject variability and exponential learning curves in <bold>Experiment 1</bold>. Using the same set of best-fit parameters found using <bold>Experiment 1</bold> data, we then simulated <bold>Experiment 2</bold>. The model produced similar distributions of steep-learners, shallow-learners and, to some extent, non-learners. The model was also able to capture several aspects of learning reported in previous work [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>].</p>
<p>As mentioned, the behavioural findings of <bold>Experiment 2</bold> were well predicted by our learning model. Critically, our model does not build up a full representation of the reinforcement landscape. Rather, it relies on using movement variability for spatially local exploration and temporally recent reinforcement feedback to update intended reach aim. Considering that the model does not build up a representation of the reinforcement landscape and that it was highly correlated with the behavioural results, suggests that whether participants ascended up the shallow portion or the steep portion of the complex reinforcement landscape was largely due to movement variability and the probability of reward. As an example, a participant’s initial reach angle had an equal probability of being aligned with either the steep or shallow slope due to movement variability. However, a participant’s initial reach was more likely to be rewarded on the steep slope because of its higher rate of reward. Moreover, the further a participant ascended either the steep or shallow slope it became increasingly unlikely that future successes would promote them from descending a slope. In particular, the steep slope had a stronger effect of promoting participants to ascend since its reward rate was double that of the shallow slope. This is evident in <xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6D</xref>, where both the participants and model simulations were very likely to be classified as a steep learner when they had their 1<sup><italic>st</italic></sup> success on the steep slope. Conversely, final reach classification for both the participants and model simulations only became reliable after approximately the 15<sup><italic>th</italic></sup> success on the shallow slope (<xref ref-type="fig" rid="pcbi.1006839.g006">Fig 6C</xref>). Thus, participants and the model were more likely to be initially rewarded on the steep slope and also more likely to ascend the steep slope. Taken together, our behavioural results and model simulations support the idea that the nervous system does not build up a representation of the reinforcement landscape. Rather, the nervous system seems to rely on spatially local movement variability for exploration and temporally recent reinforcement feedback to update hand aim. Importantly, our findings also suggest that early exploration is highly influential when attempting to avoid local maximums and discover a global maximum.</p>
<p>Several hallmarks of motor learning simply emerged from our phenomenological learning model. Specifically, we found that the model produces exponential learning curves, between-and within-subject movement variability, suboptimal performance, increased learning rates with greater movement variability, trial-by-trial variance given a successful or unsuccessful reach (<xref ref-type="supplementary-material" rid="pcbi.1006839.s002">S2 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s006">S2 Fig</xref>), reduced variability when hand aim approaches the optimal solution to maximize success, and random-walk behaviour in task-irrelevant dimensions. To our knowledge, random-walk behaviour has only been previously associated with error-based learning [<xref ref-type="bibr" rid="pcbi.1006839.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref035">35</xref>]. Future work should examine whether random-walk behaviour can be replicated with experiments involving only reinforcement feedback.</p>
<p>The model of Haith and Krakauer (2014) [<xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>] and the recently published model of Therrien and colleagues (2018) [<xref ref-type="bibr" rid="pcbi.1006839.ref023">23</xref>] would also be able to reproduce the rich set of behavioural phenomena mentioned in the above paragraph. These two models also rely on movement variability for exploration and caching a single aim direction that can be updated based on recent feedback. The Haith and Krakauer model stems from a Markov chain Monte Carlo (MCMC) algorithm and relies on sampling different motor actions. Actions are drawn from a probability distribution with a previously cached action acting as the distribution mean. If a recently experienced action is deemed less costly and or more rewarding than the previously cached action, this recent action becomes the newly cached action. Although this model was demonstrated with error-based tasks (i.e., visuomotor rotation and force-field adaptation), it could be extended to update hand aim using reinforcement feedback. As mentioned above, the work of Haith and Krakauer (2014) [<xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>] and Pekny et al. (2015) [<xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>] provided the motivation for our model. This resulted in a similar set of equations as recently proposed by Therrien and colleagues (2018) [<xref ref-type="bibr" rid="pcbi.1006839.ref023">23</xref>], albeit with some slight differences in terms of how the model updates hand aim. In their model, updating hand aim relies on the assumption that the sensorimotor system has perfect knowledge of additional exploratory movement variability following an unsuccessful reach and partial knowledge of the motor (execution) variability following a successful reach. Conversely, our model assumes that the same proportion of motor and exploratory movement variability are known by the sensorimotor system when updating hand aim. While some studies have explored the idea that the sensorimotor system has some awareness of movement variability [<xref ref-type="bibr" rid="pcbi.1006839.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref026">26</xref>], to our knowledge no study has explored what proportion of movement variability is known by the sensorimotor system following a successful or unsuccessful reach. Nevertheless, our present work highlights the utility of this class of models, which rely on movement variability for exploration and caching a single action, to predict sensorimotor adaptation.</p>
<p>Emergent behaviour and simplicity are perhaps the most attractive features of our learning model. The model uses movement variability to sample the reinforcement landscape locally, and temporally recent information to update where to aim the hand. These features distinguish our model from several mainstream reinforcement algorithms in the motor literature that rely on building a full representation of the reinforcement landscape [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref038">38</xref>]. The explicit goal of these algorithms is to maximize reward. For many of these reinforcement learning models, exploration and maximizing reward is accomplished by selecting actions using a soft-max function that considers the expected value of all possible actions. In general, such models rely on a large number free parameters and assumptions. Depending on the task and the discretization of considered actions and states, storing a representation of the reinforcement landscape in real-world situations could require vast amounts of memory and may be implausible. In comparison, our model (similarly, [<xref ref-type="bibr" rid="pcbi.1006839.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref023">23</xref>]) has a small number of free parameters, makes few assumptions, implicitly maximizes reward, and uses minimal memory.</p>
<p>Our learning model does well to capture several aspects of behaviour during learning. For the model to adapt however, there has to be a non-zero gradient within the range of naturally occurring movement variability. Thus, the model is limited to small areas of the workspace. It has been shown in previous studies that participants are unaware of a change in aim when operating over small areas of the workspace [<xref ref-type="bibr" rid="pcbi.1006839.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref039">39</xref>]. In our task, the average change in behaviour was ∼ 7.0 degrees, suggesting that the participants in our experiments were also likely unaware of the small shifts in reach angle [<xref ref-type="bibr" rid="pcbi.1006839.ref040">40</xref>]. Learning beyond these small areas of the workspace would likely also require active (cognitive) exploration strategies [<xref ref-type="bibr" rid="pcbi.1006839.ref041">41</xref>] and explicit awareness of the reinforcement landscape [<xref ref-type="bibr" rid="pcbi.1006839.ref017">17</xref>]. Nonetheless, our model did well to capture many features of sensorimotor adaptation over small areas of the workspace.</p>
<p>Behaviourally, we found that a steeper reinforcement landscape leads to faster learning. We also found that humans are more likely to ascend the steepest portion of a complex landscape. Our model was able to replicate our findings without the need to build up a representation of the reinforcement landscape. Further, several hallmarks of human learning simply emerged from this model. Taken together, our data and our model suggest that the sensorimotor system may not rely on building a representation of the reinforcement landscape. Rather, over small areas of the workspace, sensorimotor adaptation in reinforcement tasks may occur by using movement variability to locally explore the reinforcement landscape and recent successes to update where to aim the hand.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec012">
<title>Participants</title>
<p>80 individuals participated in <bold>Experiment 1</bold> (20.1 <italic>years</italic> ± 2.8 SD) and 40 individuals participated in <bold>Experiment 2</bold> (20.5 <italic>years</italic> ± 2.8 SD). Participants reported they were healthy, right-handed and provided informed consent to procedures approved by Western University’s Ethics Board.</p>
</sec>
<sec id="sec013">
<title>Apparatus</title>
<p>In both experiments, participants held the handle of a robotic arm (InMotion2, Interactive Motion Technologies, Cambridge, MA; <xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1A</xref>) and made right-handed reaching movements in a horizontal plane. An air-sled supported each participant’s right arm while providing minimal friction with the desk surface during the reaching movements. A semi-silvered mirror blocked vision of both the participant’s upper-limb and the robotic arm, and projected images from an LCD screen onto a horizontal plane passing through the participant’s shoulder. An algorithm controlled the robot’s torque motors and compensated for the dynamical properties of the robotic arm. The position of the robotic handle was recorded at 600 <italic>Hz</italic> and the data were stored for offline analysis.</p>
</sec>
<sec id="sec014">
<title>Protocol</title>
<sec id="sec015">
<title>Reaching task for Experiment 1 and 2</title>
<p>Participants were presented with virtual images of a start position (0.5 cm diameter, blue circle), a target (0.5 cm diameter, blue circle) located 20 cm forward of the start position, and a blue finish line located 2 cm beyond the target (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1A</xref>). For each trial, participants began from a start position, passed by or through the target, and then stopped their reach when the robot handle passed over the finish line that disappeared once crossed. After 1 sec, the finish line reappeared and the robotic arm returned the participant’s hand to the starting position.</p>
<p>Participants performed 450 reaching movements. We instructed them to “hit the target”. Participants received no feedback during baseline reaches (trials 1 − 50). During the experimental reaches (trials 51 − 400), they received binary reinforcement feedback that was dependent on their assigned reinforcement landscape. We told them that each time they hit the target: 1) it would expand (5<italic>x</italic>) in diameter, 2) they would hear a pleasant noise, and 3) that they could earn monetary reward. Participants were informed that they could earn up to 5.00 CAD based on their performance. We also told participants that if they missed the target, no feedback would be presented and the robot would return them to the start position after they passed the finish line. During washout (trials 401 − 450) participants received no feedback.</p>
</sec>
</sec>
<sec id="sec016">
<title>Reinforcement landscapes</title>
<p>During both experiments, participants were exposed to one of several different reinforcement landscapes. We manipulated the gradient of the reinforcement landscapes by controlling the probability of positive reinforcement (reward) as a function of reach angle. These landscapes were constructed such that participants had to learn to change their reach angle, relative to baseline performance, to maximize the probability of reward.</p>
<p>The width of the reinforcement landscape experienced by a participant was normalized to the variability of their baseline reach angles. Reach angle was measured at the position where the robot handle first became 20 cm away from the center of the starting position, and was calculated relative to the line that intersected the starting position and the displayed target. The last 25 baseline trials were used to calculate their average baseline reach angle and the standard deviation of their angular movement variability. All reach angles were converted into z-scores. Specifically, reach angles were expressed relative to the average baseline reach angle and then normalized by the participant’s average standard deviation recorded during baseline. Thus, a z-score of 0.0 corresponded with their average baseline reach angle. A z-score of 1.0 or −1.0 indicated that a reach angle was ± 1 SD away from their average baseline reach angle in the clockwise or counterclockwise direction, respectively.</p>
<p>Defining the reinforcement landscape in terms of a z-score served two purposes. First, we controlled for slight differences in individual aiming bias by positioning all participants on the same location of the reinforcement landscape during the start of the experimental trials. Second, we normalized the width of the reinforcement landscape for each participant based on baseline movement variability, allowing us to isolate how the reinforcement landscape gradient influenced learning.</p>
<sec id="sec017">
<title>Experiment 1</title>
<p>Here we tested the idea that the gradient (steep or shallow) of a reinforcement landscape influences the rate of learning. As a control, we also manipulated the direction that the reinforcement landscape increased along (clockwise or counterclockwise). Testing both directions assured us that changes in behaviour were not caused by systematic drift across trials. These manipulations resulted in four different reinforcement landscapes: a steep landscape increasing in the clockwise direction (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1B</xref>), a shallow landscape increasing in the clockwise direction (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1B</xref>), a steep landscape increasing in the counterclockwise direction, and a shallow landscape increasing in the counterclockwise direction. We predicted that participants would have faster learning in the steep condition relative to the shallow condition. Participants were pseudorandomly assigned to one of the four reinforcement landscapes (<italic>n</italic> = 20 participants per condition).</p>
<p>For the four reinforcement landscapes, average baseline behaviour (0.0 z-score) led to a 33.0% probability of receiving positive reinforcement. The probability of reward in the steep clockwise <inline-formula id="pcbi.1006839.e018"><alternatives><graphic id="pcbi.1006839.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>e</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and shallow clockwise <inline-formula id="pcbi.1006839.e019"><alternatives><graphic id="pcbi.1006839.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>h</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> reinforcement landscapes rose in the clockwise direction (<italic>CW</italic>) at a rate of 22.2% per z-score and 11.1% per z-score, respectively. These two reinforcement landscapes, which define the probability of success given reach angle [<italic>p</italic>(<italic>r</italic> = 1|<italic>θ</italic>)], can be summarized with
<disp-formula id="pcbi.1006839.e020"><alternatives><graphic id="pcbi.1006839.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>3</mml:mn> <mml:mrow><mml:mn>3</mml:mn> <mml:mo>·</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mfrac><mml:mn>3</mml:mn> <mml:mrow><mml:mn>3</mml:mn> <mml:mo>·</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>3</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
<italic>r</italic> = 1 denotes a successful reach. The maximal success rate, <italic>m</italic><sub><italic>i</italic></sub>, was between 3.0 to 6.0 z-score away from the average baseline reach angle in the clockwise direction. More specifically, <italic>m</italic><sub><italic>steep</italic></sub>(1.0) and <italic>m</italic><sub><italic>shallow</italic></sub>(2/3) define the maximal success rate of the steep and shallow landscapes, and are used to calculate both the landscape slopes and <italic>x</italic>-intercepts. Along the counterclockwise direction, the probability of success decreased linearly until 0.0%. Elsewhere, the probability of success was 0.0%. <italic>θ</italic> is expressed as a z-score.</p>
<p>The steep counterclockwise (<inline-formula id="pcbi.1006839.e021"><alternatives><graphic id="pcbi.1006839.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>e</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) and shallow counterclockwise (<inline-formula id="pcbi.1006839.e022"><alternatives><graphic id="pcbi.1006839.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>h</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) reinforcement landscapes are mirror images, reflected about the average baseline reach angle (0.0 z-score), of their clockwise counterparts. They are summarized as
<disp-formula id="pcbi.1006839.e023"><alternatives><graphic id="pcbi.1006839.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>3</mml:mn> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>6</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>3</mml:mn> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>−</mml:mo> <mml:mfrac><mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>3</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mfrac><mml:mn>3</mml:mn> <mml:mrow><mml:mn>3</mml:mn> <mml:mo>·</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>3</mml:mn> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mfrac><mml:mn>3</mml:mn> <mml:mrow><mml:mn>3</mml:mn> <mml:mo>·</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>3</mml:mn> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
</sec>
<sec id="sec018">
<title>Experiment 2</title>
<p>Here, we tested the idea that the sensorimotor system is able to use local gradient information to ascend the steepest slope of a complex reinforcement landscape. To investigate, participants were initially positioned between two slopes that rose at differing rates (steep and shallow) and opposite directions (<xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3A and 3C</xref>). We tested two landscapes: 1) a steep clockwise condition (<xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3A</xref>); where the steeper slope of the reinforcement landscape rose in the clockwise direction and the shallow slope rose in the counterclockwise direction and 2) a steep counterclockwise condition (<xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3C</xref>); where the steeper slope of the reinforcement landscape rose in the counterclockwise direction and the shallow slope rose in the clockwise direction. We predicted that a greater proportion of participants would ascend the steeper gradient, irrespective of direction (clockwise or counterclockwise). Similar to <bold>Experiment 1</bold>, we used two directions (steep clockwise or steep counterclockwise) to be assured that changes in behaviour were not due to systematic drift. Participants were pseudorandomly assigned to one of these two reinforcement landscapes (<italic>n</italic> = 20 participants per condition).</p>
<p>The steep clockwise condition (<italic>R</italic>(<italic>θ</italic>)<sup><italic>StCW</italic></sup>) can be summarized with
<disp-formula id="pcbi.1006839.e024"><alternatives><graphic id="pcbi.1006839.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>S</mml:mi> <mml:mi>t</mml:mi> <mml:mi>C</mml:mi> <mml:mi>W</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>S</mml:mi> <mml:mi>t</mml:mi> <mml:mi>C</mml:mi> <mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>6</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>3</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>3</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
The gradients of the steep and shallow slopes were identical to those described in <bold>Experiment 1</bold>. The maximal success rate (100.0%) in the clockwise direction occurred between 3.0 to 6.0 z-score, while the maximal success rate (66.7%) in the counterclockwise direction occurred between −3.0 to −6.0 z-score in the counterclockwise direction. Elsewhere, the probability of success was 0.0%.</p>
<p>The steep counterclockwise condition (<italic>R</italic>(<italic>θ</italic>)<sup><italic>StCCW</italic></sup>) was the mirror image of the steep clockwise condition, reflected about the average baseline reach angle (0.0 z-score). This is summarized by
<disp-formula id="pcbi.1006839.e025"><alternatives><graphic id="pcbi.1006839.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>S</mml:mi> <mml:mi>t</mml:mi> <mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:mi>W</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>S</mml:mi> <mml:mi>t</mml:mi> <mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>6</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mo>−</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mn>3</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>9</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>3</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>e</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>(</mml:mo> <mml:mn>5</mml:mn> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
</sec>
</sec>
<sec id="sec019">
<title>Data analysis</title>
<p>We performed data analysis using custom Python 2.7.11 scripts. For all participants in both Experiments, we recorded their endpoint reach angle for each of the 450 trials. Reach angles were normalized based on baseline reach behaviour, as described above, and expressed as a z-score.</p>
<sec id="sec020">
<title>Experiment 1</title>
<p>To perform comparisons across groups, we multiplied the normalized reach angles by −1.0 for all participants experiencing a reinforcement landscape that increased in the counterclockwise direction [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref016">16</xref>].</p>
<p>Here we were primarily interested in the rate of learning given the gradient (steep or shallow) of the assigned reinforcement landscape. The rate of learning is captured in the λ term of the following exponential function [<xref ref-type="bibr" rid="pcbi.1006839.ref042">42</xref>]:
<disp-formula id="pcbi.1006839.e026"><alternatives><graphic id="pcbi.1006839.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mi>i</mml:mi> <mml:mo>/</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>θ</italic><sub><italic>i</italic></sub> is the estimated reach angle (z-score) on the <italic>i</italic><sup><italic>th</italic></sup> experimental trial, <italic>e</italic> (2.71) is a constant, and <italic>a</italic> defines the asymptotic reach angle (z-score). We used least squares to fit this equation to the experimental trials (51 to 400) via bootstrapping. Specifically, we fit an exponential function for each bootstrap resample, allowing use to estimate the posterior distribution of each parameter given the data. The bootstrapping technique also allowed for statistical comparison to be made between the two groups. We expected participants experiencing a steep reinforcement landscape to learn faster (i.e., have a significantly lower λ) than those experiencing a shallow landscape.</p>
<p>When inspecting individual data, there seemed to be two distinct subpopulations of participants: learners and non-learners. For all participants in <bold>Experiment 1</bold>, we characterized their asymptotic reaching behaviour by calculating their average reach angle during the last 100 trials of the experimental trials. We found that a final reach angle of 1.0 z-score was an appropriate cutoff to separate these two subpopulations (<xref ref-type="supplementary-material" rid="pcbi.1006839.s001">S1 Data</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006839.s005">S1 Fig</xref>). We then summed the number of learners and non-learners based on whether they experienced a shallow or steep reinforcement landscape (<xref ref-type="table" rid="pcbi.1006839.t001">Table 1</xref>).</p>
</sec>
<sec id="sec021">
<title>Experiment 2</title>
<p>In this experiment, we were interested in the final reach direction after participants had been initially positioned between a shallow slope and steep slope acting in opposite directions. We averaged the last 100 experimental trials to calculate each participant’s asymptotic behaviour. We then classified each participant’s final asymptotic reach behaviour using the same cutoff used in <bold>Experiment 1</bold>. Specifically, final reach behaviour was classified to be counterclockwise (z-score ≤ −1.0), center (−1.0 &lt; z-score &lt; +1.0) or clockwise (z-score ≥ +1.0). Separately for those experiencing a steep clockwise or steep counterclockwise reinforcement landscape, we counted the number of participants whose asymptotic reach behaviour fell into these classifications.</p>
<p>We predicted that participants would ascend the steeper slope of the complex landscape. Consequently, we expected significant differences in the final average reach classification between participants that experienced a steep clockwise or steep counterclockwise reinforcement landscape. As a reminder, final reach position was calculated as the average of the last 100 experimental trials. For all participants in <bold>Experiment 2</bold>, those who had a final reach position corresponding to the direction of the steep slope, shallow slope or a central location were termed: steep learners, shallow learners and non-learners, respectively.</p>
<p>We also performed an analysis to explore the influence of reinforcement feedback during the initial periods of the experimental trials. To this end, we calculated how the location (steep or shallow slope) of their <italic>N</italic><sup><italic>th</italic></sup> success predicted the likelihood of their final reach classification. This analysis provides insight into the influence of both early exploration and gradient information on how a complex reinforcement landscape is experienced over the course of learning.</p>
</sec>
<sec id="sec022">
<title>Probability of reward given intended reach aim for Experiment 1 and 2</title>
<p>For all experimental conditions, we calculated the probability of reward given the intended reach aim (<xref ref-type="fig" rid="pcbi.1006839.g001">Fig 1C</xref>; <xref ref-type="fig" rid="pcbi.1006839.g003">Fig 3B and 3D</xref>). Critically, this analysis demonstrates that the experimentally imposed reinforcement landscapes still lead to different gradients (steep or shallow) when accounting for normalized movement variability.</p>
<p>The probability of reward, that is the expected utility (<italic>E</italic>[<italic>U</italic><sub>Θ</sub>(⋅)]) for the set of possible actions (Θ), is estimate by solving
<disp-formula id="pcbi.1006839.e027"><alternatives><graphic id="pcbi.1006839.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>Θ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>R</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>j</mml:mi></mml:msup> <mml:mi>d</mml:mi> <mml:mi>θ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where [<italic>R</italic>(<italic>θ</italic>)<sup><italic>j</italic></sup>] is the experimentally imposed reinforcement landscape and [<inline-formula id="pcbi.1006839.e028"><alternatives><graphic id="pcbi.1006839.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>] is the probability of some reach angle [<xref ref-type="bibr" rid="pcbi.1006839.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref044">44</xref>].</p>
<p>Reach angle (<italic>θ</italic>) was modelled with a Normal distribution [<xref ref-type="bibr" rid="pcbi.1006839.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref021">21</xref>],
<disp-formula id="pcbi.1006839.e029"><alternatives><graphic id="pcbi.1006839.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>σ</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>−</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>θ</italic><sup><italic>aim</italic></sup> represents an unbiased aim and <italic>σ</italic><sup>2</sup> is the overall reach angle variance. <italic>σ</italic><sup>2</sup> was estimated by considering both motor (execution) variance (<inline-formula id="pcbi.1006839.e030"><alternatives><graphic id="pcbi.1006839.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>m</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) and exploration variance (<inline-formula id="pcbi.1006839.e031"><alternatives><graphic id="pcbi.1006839.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>e</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) [<xref ref-type="bibr" rid="pcbi.1006839.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref025">25</xref>]. Pekny and colleagues (2015) proposed that the magnitude of exploration variability is inversely related to the probability of reward, the latter of which we manipulated as a function of reach angle [<italic>p</italic>(<italic>r</italic> = 1|<italic>θ</italic>)<sup><italic>j</italic></sup>] according to the assigned reinforcement landscape (<italic>j</italic>). Thus, by considering two potential sources of movement variability and the probability of reward, <italic>σ</italic><sup>2</sup> was approximated by the following equation:
<disp-formula id="pcbi.1006839.e032"><alternatives><graphic id="pcbi.1006839.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>j</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
Here, motor (execution) variance is constant from trial-to-trial. The influence of exploration variance scales inversely with the probability of receiving reward. The values of <italic>σ</italic><sub><italic>m</italic></sub>(0.81) and <italic>σ</italic><sub><italic>e</italic></sub>(0.9) were the best-fit parameters of our learning model (<xref ref-type="disp-formula" rid="pcbi.1006839.e006">Eq 1</xref>). Eqs <xref ref-type="disp-formula" rid="pcbi.1006839.e027">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006839.e032">9</xref> were solved numerically by convolving the reach angle probability distribution over each of the experimentally imposed reinforcement landscapes [<xref ref-type="bibr" rid="pcbi.1006839.ref044">44</xref>].</p>
<p>For each reinforcement landscape, the intended reach aim that maximizes the probability of reward (<inline-formula id="pcbi.1006839.e033"><alternatives><graphic id="pcbi.1006839.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) corresponds to the intended reach aim that maximizes the expected reward of <xref ref-type="disp-formula" rid="pcbi.1006839.e027">Eq 7</xref>. This is summarized by
<disp-formula id="pcbi.1006839.e034"><alternatives><graphic id="pcbi.1006839.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix">arg max</mml:mo> <mml:mrow><mml:msup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>∈</mml:mo> <mml:mo>Θ</mml:mo></mml:mrow></mml:munder> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mo>Θ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
In <bold>Experiment 1</bold>, the <inline-formula id="pcbi.1006839.e035"><alternatives><graphic id="pcbi.1006839.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for the steep and shallow clockwise reinforcement landscapes were 3.96 z-score and 3.8 z-score, respectively. In <bold>Experiment 2</bold>, the <inline-formula id="pcbi.1006839.e036"><alternatives><graphic id="pcbi.1006839.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006839.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for the steep clockwise and the steep counterclockwise reinforcement landscapes were 3.96 z-score and −3.96 z-score, respectively.</p>
</sec>
</sec>
<sec id="sec023">
<title>Statistical analysis</title>
<p>Tests between means were performed using bootstrapped hypothesis tests with 1, 000, 000 resamples (Python 2.7.11) [<xref ref-type="bibr" rid="pcbi.1006839.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006839.ref047">47</xref>]. Fisher’s exact test was used to test frequency tables (R 3.2.4). Coefficient of Determination (<italic>R</italic><sup>2</sup>) was used to compare model simulations to behavioural data (Python 2.7.11). One-sided tests were used for planned comparisons based on theory-driven predictions. For all other comparisons we used two-tailed tests. Multiple comparisons were corrected for Type-I error using the Holm-Bonferroni procedure [<xref ref-type="bibr" rid="pcbi.1006839.ref048">48</xref>]. Statistical tests were considered significant at <italic>p</italic> &lt; 0.05.</p>
</sec>
</sec>
<sec id="sec024">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006839.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s001" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Cutoff criterion used to separate learners and non-learners.</title>
<p>Cutoff criterion of final reach direction used to separate learners and non-learners in <bold>Experiment 1</bold>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006839.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s002" xlink:type="simple">
<label>S2 Data</label>
<caption>
<title>Trial-by-trial analyses.</title>
<p>Trial-by-trial analyses that examine: a) behavioural estimates of motor and exploratory contributions to movement variability, and b) movement variability as a function of reinforcement history.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006839.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s003" xlink:type="simple">
<label>S3 Data</label>
<caption>
<title>Best-fit parameters of the learning model.</title>
<p>Finds the best-fit parameters of the learning model using bootstrapping to minimize squared error.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006839.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s004" xlink:type="simple">
<label>S4 Data</label>
<caption>
<title>Influence of initial reward probability on learning rate.</title>
<p>Simulations to demonstrate that the initial reward probability of the shallow landscape has a marginal influence on learning rate.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006839.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s005" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Distribution of final reach direction.</title>
<p>The frequency (<italic>y</italic>-axis) of final reach position (<italic>x</italic>-axis) for the 80 participants collected in <bold>Experiment 1</bold>. We used a z-score cutoff of 1.0 (dashed, vertical black line) to separate the learners (z-score ≥ 1.0) from the non-learners (z-score &lt; 1.0).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006839.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006839.s006" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Movement variability as a function of reinforcement history.</title>
<p>Average standard deviation of changes in reach angle between trials <italic>n</italic> and <italic>n</italic> + 1 (<italic>y</italic>-axis) given reinforcement history (<italic>x</italic>-axis) for <bold>A)</bold> the behaviour data of participants in <bold>Experiment 1</bold>, and <bold>B)</bold> the learning model simulations. Base and Wash represent baseline (trials 25-50) and washout (trials 400-450), respectively. The best-fit parameters (<italic>x</italic>-axis) and their magnitudes (<italic>y</italic>-axis) of the variability state-space model developed by Pekny and Colleagues (2015) as applied to <bold>C)</bold> the behavioural data of participants in <bold>Experiment 1</bold> and <bold>D)</bold> the outputs of our learning model. For all subplots, the orange and blue colours represent participants (or model simulations) that experienced the shallow or steep reinforcement landscapes, respectively. Grey represents the average collapsed across all participants or simulations. Error bars are ±1.0 SE.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Dr. Michael Carter for constructive comments on this paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006839.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Izawa</surname> <given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Shadmehr</surname> <given-names>R.</given-names></name> (<year>2011</year>). <article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title>. <source>PLoS computational biology</source>, <volume>7</volume>(<issue>3</issue>), <fpage>e1002012</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002012" xlink:type="simple">10.1371/journal.pcbi.1002012</ext-link></comment> <object-id pub-id-type="pmid">21423711</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pekny</surname> <given-names>S. E.</given-names></name>, <name name-style="western"><surname>Izawa</surname> <given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Shadmehr</surname> <given-names>R.</given-names></name> (<year>2015</year>). <article-title>Reward-dependent modulation of movement variability</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>9</issue>), <fpage>4015</fpage>–<lpage>4024</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3244-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3244-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25740529</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shmuelof</surname> <given-names>L.</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>J. W.</given-names></name>, &amp; <name name-style="western"><surname>Mazzoni</surname> <given-names>P.</given-names></name> (<year>2012</year>). <article-title>How is a motor skill learned? Change and invariance at the levels of task success and trajectory control</article-title>. <source>Journal of neurophysiology</source>, <volume>108</volume>(<issue>2</issue>), <fpage>578</fpage>–<lpage>594</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00856.2011" xlink:type="simple">10.1152/jn.00856.2011</ext-link></comment> <object-id pub-id-type="pmid">22514286</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vaswani</surname> <given-names>P. A.</given-names></name>, <name name-style="western"><surname>Shmuelof</surname> <given-names>L.</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>A. M.</given-names></name>, <name name-style="western"><surname>Delnicki</surname> <given-names>R. J.</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>V. S.</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P.</given-names></name>, &amp; <name name-style="western"><surname>Krakauer</surname> <given-names>J. W.</given-names></name> (<year>2015</year>). <article-title>Persistent residual errors in motor adaptation tasks: reversion to baseline and exploratory escape</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>17</issue>), <fpage>6969</fpage>–<lpage>6977</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2656-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2656-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25926471</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cashaback</surname> <given-names>J. G.</given-names></name>, <name name-style="western"><surname>McGregor</surname> <given-names>H. R.</given-names></name>, <name name-style="western"><surname>Mohatarem</surname> <given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Gribble</surname> <given-names>P. L.</given-names></name> (<year>2017</year>). <article-title>Dissociating error-based and reinforcement-based loss functions during sensorimotor learning</article-title>. <source>PLoS computational biology</source>, <volume>13</volume>(<issue>7</issue>), <fpage>e1005623</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005623" xlink:type="simple">10.1371/journal.pcbi.1005623</ext-link></comment> <object-id pub-id-type="pmid">28753634</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van der Kooij</surname> <given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Smeets</surname> <given-names>J. B.</given-names></name> (<year>2018</year>). <article-title>Reward-based motor adaptation can generalize across actions</article-title>. <source>Journal of experimental psychology. Learning, memory, and cognition</source>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/xlm0000573" xlink:type="simple">10.1037/xlm0000573</ext-link></comment> <object-id pub-id-type="pmid">29698052</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Palidis D. J., Cashaback J., &amp; Gribble P. (2018). Neural Signatures of Reward and Sensory Prediction Error in Motor Learning. bioRxiv, 262576.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>V. S.</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>A.</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P.</given-names></name>, &amp; <name name-style="western"><surname>Krakauer</surname> <given-names>J. W.</given-names></name> (<year>2011</year>). <article-title>Rethinking motor learning and savings in adaptation paradigms: model-free memory for successful actions combines with internal models</article-title>. <source>Neuron</source>, <volume>70</volume>(<issue>4</issue>), <fpage>787</fpage>–<lpage>801</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.04.012" xlink:type="simple">10.1016/j.neuron.2011.04.012</ext-link></comment> <object-id pub-id-type="pmid">21609832</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref009">
<label>9</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haith</surname> <given-names>A.M.</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>J.W.</given-names></name> (<year>2013</year>). <chapter-title>Model-based and model-free mechanisms of human motor learning</chapter-title>. In <source>Progress in motor control</source> (pp. <fpage>1</fpage>–<lpage>21</lpage>). <publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dhawale</surname> <given-names>A. K.</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>M. A.</given-names></name>, &amp; <name name-style="western"><surname>Olveczky</surname> <given-names>B. P.</given-names></name> (<year>2017</year>). <article-title>The Role of Variability in Motor Learning</article-title>. <source>Annual Review of Neuroscience</source>, <volume>40</volume>, <fpage>479</fpage>–<lpage>498</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-072116-031548" xlink:type="simple">10.1146/annurev-neuro-072116-031548</ext-link></comment> <object-id pub-id-type="pmid">28489490</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nikooyan</surname> <given-names>A. A.</given-names></name>, &amp; <name name-style="western"><surname>Ahmed</surname> <given-names>A. A.</given-names></name> (<year>2014</year>). <article-title>Reward feedback accelerates motor learning</article-title>. <source>Journal of Neurophysiology</source>, <volume>113</volume>(<issue>2</issue>), <fpage>633</fpage>–<lpage>646</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00032.2014" xlink:type="simple">10.1152/jn.00032.2014</ext-link></comment> <object-id pub-id-type="pmid">25355957</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Galea</surname> <given-names>J. M.</given-names></name>, <name name-style="western"><surname>Mallia</surname> <given-names>E.</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Diedrichsen</surname> <given-names>J.</given-names></name> (<year>2015</year>). <article-title>The dissociable effects of punishment and reward on motor learning</article-title>. <source>Nature neuroscience</source>, <volume>18</volume>(<issue>4</issue>), <fpage>597</fpage>–<lpage>602</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3956" xlink:type="simple">10.1038/nn.3956</ext-link></comment> <object-id pub-id-type="pmid">25706473</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tversky</surname> <given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Kahneman</surname> <given-names>D.</given-names></name> (<year>1992</year>). <article-title>Advances in prospect theory: Cumulative representation of uncertainty</article-title>. <source>Journal of Risk and uncertainty</source>, <volume>5</volume>(<issue>4</issue>), <fpage>297</fpage>–<lpage>323</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00122574" xlink:type="simple">10.1007/BF00122574</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>H. G.</given-names></name>, <name name-style="western"><surname>Miyamoto</surname> <given-names>Y. R.</given-names></name>, <name name-style="western"><surname>Castro</surname> <given-names>L. N. G.</given-names></name>, <name name-style="western"><surname>Olveczky</surname> <given-names>B. P.</given-names></name>, &amp; <name name-style="western"><surname>Smith</surname> <given-names>M. A.</given-names></name> (<year>2014</year>). <article-title>Temporal structure of motor variability is dynamically regulated and predicts motor learning ability</article-title>. <source>Nature neuroscience</source>, <volume>17</volume>(<issue>2</issue>), <fpage>312</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3616" xlink:type="simple">10.1038/nn.3616</ext-link></comment> <object-id pub-id-type="pmid">24413700</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>He</surname> <given-names>K.</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>Y.</given-names></name>, <name name-style="western"><surname>Abdollahi</surname> <given-names>F.</given-names></name>, <name name-style="western"><surname>Bittmann</surname> <given-names>M. F.</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Wei</surname> <given-names>K.</given-names></name> (<year>2016</year>). <article-title>The statistical determinants of the speed of motor learning</article-title>. <source>PLoS computational biology</source>, <volume>12</volume>(<issue>9</issue>), <fpage>e1005023</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005023" xlink:type="simple">10.1371/journal.pcbi.1005023</ext-link></comment> <object-id pub-id-type="pmid">27606808</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Acerbi</surname> <given-names>L.</given-names></name>, <name name-style="western"><surname>Vijayakumar</surname> <given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Wolpert</surname> <given-names>D. M.</given-names></name> (<year>2014</year>). <article-title>On the origins of suboptimality in human probabilistic inference</article-title>. <source>PLoS computational biology</source>, <volume>10</volume>(<issue>6</issue>), <fpage>e1003661</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003661" xlink:type="simple">10.1371/journal.pcbi.1003661</ext-link></comment> <object-id pub-id-type="pmid">24945142</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Holland</surname> <given-names>P.</given-names></name>, <name name-style="western"><surname>Codol</surname> <given-names>O.</given-names></name>, &amp; <name name-style="western"><surname>Galea</surname> <given-names>J. M.</given-names></name> (<year>2018</year>). <article-title>Contribution of explicit processes to reinforcement-based motor learning</article-title>. <source>Journal of neurophysiology</source>, <volume>119</volume>(<issue>6</issue>), <fpage>2241</fpage>–<lpage>2255</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00901.2017" xlink:type="simple">10.1152/jn.00901.2017</ext-link></comment> <object-id pub-id-type="pmid">29537918</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Codol O., Holland P. J., &amp; Galea J. M. (2017). The relationship between reinforcement and explicit strategies during visuomotor adaptation. bioRxiv, 206284.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Trommershauser</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>L. T.</given-names></name>, &amp; <name name-style="western"><surname>Landy</surname> <given-names>M. S.</given-names></name> (<year>2003</year>). <article-title>Statistical decision theory and the selection of rapid, goal-directed movements</article-title>. <source>JOSA A</source>, <volume>20</volume>(<issue>7</issue>), <fpage>1419</fpage>–<lpage>1433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1364/JOSAA.20.001419" xlink:type="simple">10.1364/JOSAA.20.001419</ext-link></comment> <object-id pub-id-type="pmid">12868646</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Trommershauser</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Gepshtein</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>L. T.</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>M. S.</given-names></name>, &amp; <name name-style="western"><surname>Banks</surname> <given-names>M. S.</given-names></name> (<year>2005</year>). <article-title>Optimal compensation for changes in task-relevant movement variability</article-title>. <source>Journal of Neuroscience</source>, <volume>25</volume>(<issue>31</issue>), <fpage>7169</fpage>–<lpage>7178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1906-05.2005" xlink:type="simple">10.1523/JNEUROSCI.1906-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16079399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Landy</surname> <given-names>M. S.</given-names></name>, <name name-style="western"><surname>Trommershauser</surname> <given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Daw</surname> <given-names>N. D.</given-names></name> (<year>2012</year>). <article-title>Dynamic estimation of task-relevant variance in movement under risk</article-title>. <source>Journal of Neuroscience</source>, <volume>32</volume>(<issue>37</issue>), <fpage>12702</fpage>–<lpage>12711</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.6160-11.2012" xlink:type="simple">10.1523/JNEUROSCI.6160-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22972994</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref022">
<label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haith</surname> <given-names>A. M.</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>J. W.</given-names></name> (<year>2014</year>) <chapter-title>Motor learning by sequential sampling of actions</chapter-title>. In: <source>Translational and Computational Motor Control</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Society of Neurorehabilitation</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Therrien</surname> <given-names>A. S.</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>D. M.</given-names></name>, &amp; <name name-style="western"><surname>Bastian</surname> <given-names>A. J.</given-names></name> (<year>2018</year>). <article-title>Increasing motor noise impairs reinforcement learning in healthy individuals</article-title>. <source>eNeuro</source>, ENEURO-0050. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/ENEURO.0050-18.2018" xlink:type="simple">10.1523/ENEURO.0050-18.2018</ext-link></comment> <object-id pub-id-type="pmid">30105298</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Therrien</surname> <given-names>A. S.</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>D. M.</given-names></name>, &amp; <name name-style="western"><surname>Bastian</surname> <given-names>A. J.</given-names></name> (<year>2015</year>). <article-title>Effective reinforcement learning following cerebellar damage requires a balance between exploration and motor noise</article-title>. <source>Brain</source>, <volume>139</volume>(<issue>1</issue>), <fpage>101</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/awv329" xlink:type="simple">10.1093/brain/awv329</ext-link></comment> <object-id pub-id-type="pmid">26626368</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Beers</surname> <given-names>R. J.</given-names></name> (<year>2009</year>). <article-title>Motor learning is optimally tuned to the properties of motor noise</article-title>. <source>Neuron</source>, <volume>63</volume>(<issue>3</issue>), <fpage>406</fpage>–<lpage>417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.06.025" xlink:type="simple">10.1016/j.neuron.2009.06.025</ext-link></comment> <object-id pub-id-type="pmid">19679079</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>H.</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N. D.</given-names></name>, &amp; <name name-style="western"><surname>Maloney</surname> <given-names>L. T.</given-names></name> (<year>2015</year>). <article-title>Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions</article-title>. <source>Nature neuroscience</source>, <volume>18</volume>(<issue>8</issue>), <fpage>1152</fpage>–<lpage>1158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4055" xlink:type="simple">10.1038/nn.4055</ext-link></comment> <object-id pub-id-type="pmid">26120962</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaelbling</surname> <given-names>L. P.</given-names></name>, <name name-style="western"><surname>Littman</surname> <given-names>M. L.</given-names></name>, &amp; <name name-style="western"><surname>Moore</surname> <given-names>A. W.</given-names></name> (<year>1996</year>). <article-title>Reinforcement learning: A survey</article-title>. <source>Journal of artificial intelligence research</source>, <volume>4</volume>, <fpage>237</fpage>–<lpage>285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1613/jair.301" xlink:type="simple">10.1613/jair.301</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scheidt</surname> <given-names>R. A.</given-names></name>, <name name-style="western"><surname>Dingwell</surname> <given-names>J. B.</given-names></name>, &amp; <name name-style="western"><surname>Mussa-Ivaldi</surname> <given-names>F. A.</given-names></name> (<year>2001</year>). <article-title>Learning to move amid uncertainty</article-title>. <source>Journal of neurophysiology</source>, <volume>86</volume>(<issue>2</issue>), <fpage>971</fpage>–<lpage>985</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.2001.86.2.971" xlink:type="simple">10.1152/jn.2001.86.2.971</ext-link></comment> <object-id pub-id-type="pmid">11495965</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kording</surname> <given-names>K. P.</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>J. B.</given-names></name>, &amp; <name name-style="western"><surname>Shadmehr</surname> <given-names>R.</given-names></name> (<year>2007</year>). <article-title>The dynamics of memory as a consequence of optimal adaptation to a changing body</article-title>. <source>Nature neuroscience</source>, <volume>10</volume>(<issue>6</issue>), <fpage>779</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1901" xlink:type="simple">10.1038/nn1901</ext-link></comment> <object-id pub-id-type="pmid">17496891</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wei</surname> <given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Kording</surname> <given-names>K.</given-names></name> (<year>2010</year>). <article-title>Uncertainty of feedback and state estimation determines the speed of motor adaptation</article-title>. <source>Frontiers in computational neuroscience</source>, <volume>4</volume>, <fpage>11</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2010.00011" xlink:type="simple">10.3389/fncom.2010.00011</ext-link></comment> <object-id pub-id-type="pmid">20485466</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Castro</surname> <given-names>L. N. G.</given-names></name>, <name name-style="western"><surname>Hadjiosif</surname> <given-names>A. M.</given-names></name>, <name name-style="western"><surname>Hemphill</surname> <given-names>M. A.</given-names></name>, &amp; <name name-style="western"><surname>Smith</surname> <given-names>M. A.</given-names></name> (<year>2014</year>). <article-title>Environmental consistency determines the rate of motor adaptation</article-title>. <source>Current Biology</source>, <volume>24</volume>(<issue>10</issue>), <fpage>1050</fpage>–<lpage>1061</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.03.049" xlink:type="simple">10.1016/j.cub.2014.03.049</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dingwell</surname> <given-names>J. B.</given-names></name>, <name name-style="western"><surname>John</surname> <given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Cusumano</surname> <given-names>J. P.</given-names></name> (<year>2010</year>). <article-title>Do humans optimally exploit redundancy to control step variability in walking?</article-title>. <source>PLoS computational biology</source>, <volume>6</volume>(<issue>7</issue>), <fpage>e1000856</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000856" xlink:type="simple">10.1371/journal.pcbi.1000856</ext-link></comment> <object-id pub-id-type="pmid">20657664</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">van <name name-style="western"><surname>Beers</surname> <given-names>R. J.</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Smeets</surname> <given-names>J. B.</given-names></name> (<year>2013</year>). <article-title>Random walk of motor planning in task-irrelevant dimensions</article-title>. <source>Journal of neurophysiology</source>, <volume>109</volume>(<issue>4</issue>), <fpage>969</fpage>–<lpage>977</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00706.2012" xlink:type="simple">10.1152/jn.00706.2012</ext-link></comment> <object-id pub-id-type="pmid">23175799</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>John</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Dingwell</surname> <given-names>J. B.</given-names></name>, &amp; <name name-style="western"><surname>Cusumano</surname> <given-names>J. P.</given-names></name> (<year>2016</year>). <article-title>Error correction and the structure of inter-trial fluctuations in a redundant movement task</article-title>. <source>PLoS computational biology</source>, <volume>12</volume>(<issue>9</issue>), <fpage>e1005118</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005118" xlink:type="simple">10.1371/journal.pcbi.1005118</ext-link></comment> <object-id pub-id-type="pmid">27643895</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cardis</surname> <given-names>M.</given-names></name>, <name name-style="western"><surname>Casadio</surname> <given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Ranganathan</surname> <given-names>R.</given-names></name> (<year>2017</year>). <article-title>High variability impairs motor learning regardless of whether it affects task performance</article-title>. <source>Journal of neurophysiology</source>, <volume>119</volume>(<issue>1</issue>), <fpage>39</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00158.2017" xlink:type="simple">10.1152/jn.00158.2017</ext-link></comment> <object-id pub-id-type="pmid">28954891</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref036">
<label>36</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>R. S.</given-names></name>, &amp; <name name-style="western"><surname>Barto</surname> <given-names>A. G.</given-names></name> (<year>1998</year>). <source>Reinforcement learning: An introduction</source> (<volume>Vol. 1</volume>, <issue>No. 1</issue>). <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>X.</given-names></name>, <name name-style="western"><surname>Mohr</surname> <given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Galea</surname> <given-names>J. M.</given-names></name> (<year>2017</year>). <article-title>Predicting explorative motor learning using decision-making and motor noise</article-title>. <source>PLoS computational biology</source>, <volume>13</volume>(<issue>4</issue>), <fpage>e1005503</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005503" xlink:type="simple">10.1371/journal.pcbi.1005503</ext-link></comment> <object-id pub-id-type="pmid">28437451</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McDougle</surname> <given-names>S. D.</given-names></name>, <name name-style="western"><surname>Ivry</surname> <given-names>R. B.</given-names></name>, &amp; <name name-style="western"><surname>Taylor</surname> <given-names>J. A.</given-names></name> (<year>2016</year>). <article-title>Taking aim at the cognitive side of learning in sensorimotor adaptation tasks</article-title>. <source>Trends in cognitive sciences</source>, <volume>20</volume>(<issue>7</issue>), <fpage>535</fpage>–<lpage>544</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2016.05.002" xlink:type="simple">10.1016/j.tics.2016.05.002</ext-link></comment> <object-id pub-id-type="pmid">27261056</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kording</surname> <given-names>K. P.</given-names></name>, &amp; <name name-style="western"><surname>Wolpert</surname> <given-names>D. M.</given-names></name> (<year>2004</year>). <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source>, <volume>427</volume>(<issue>6971</issue>), <fpage>244</fpage>–<lpage>247</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02169" xlink:type="simple">10.1038/nature02169</ext-link></comment> <object-id pub-id-type="pmid">14724638</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taylor</surname> <given-names>J. A.</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>J. W.</given-names></name>, &amp; <name name-style="western"><surname>Ivry</surname> <given-names>R. B.</given-names></name> (<year>2014</year>). <article-title>Explicit and implicit contributions to learning in a sensorimotor adaptation task</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>8</issue>), <fpage>3023</fpage>–<lpage>3032</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3619-13.2014" xlink:type="simple">10.1523/JNEUROSCI.3619-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24553942</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cashaback</surname> <given-names>J. G.</given-names></name>, <name name-style="western"><surname>McGregor</surname> <given-names>H. R.</given-names></name>, &amp; <name name-style="western"><surname>Gribble</surname> <given-names>P. L.</given-names></name> (<year>2015</year>). <article-title>The human motor system alters its reaching movement plan for task-irrelevant, positional forces</article-title>. <source>Journal of neurophysiology</source>, <volume>113</volume>(<issue>7</issue>), <fpage>2137</fpage>–<lpage>2149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00901.2014" xlink:type="simple">10.1152/jn.00901.2014</ext-link></comment> <object-id pub-id-type="pmid">25589594</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howard</surname> <given-names>I. S.</given-names></name>, <name name-style="western"><surname>Ford</surname> <given-names>C.</given-names></name>, <name name-style="western"><surname>Cangelosi</surname> <given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Franklin</surname> <given-names>D. W.</given-names></name> (<year>2017</year>). <article-title>Active lead-in variability affects motor memory formation and slows motor learning</article-title>. <source>Scientific reports</source>, <volume>7</volume>(<issue>1</issue>), <fpage>7806</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-017-05697-z" xlink:type="simple">10.1038/s41598-017-05697-z</ext-link></comment> <object-id pub-id-type="pmid">28798355</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kording</surname> <given-names>K.</given-names></name> (<year>2007</year>). <article-title>Decision theory: what “should” the nervous system do?</article-title> <source>Science</source>, <volume>318</volume>(<issue>5850</issue>), <fpage>606</fpage>–<lpage>610</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1142998" xlink:type="simple">10.1126/science.1142998</ext-link></comment> <object-id pub-id-type="pmid">17962554</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tibshirani</surname> <given-names>R. J.</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Taylor</surname> <given-names>J.</given-names></name> (<year>2011</year>). <article-title>A statistician plays darts</article-title>. <source>Journal of the Royal Statistical Society: Series A (Statistics in Society)</source>, <volume>174</volume>(<issue>1</issue>), <fpage>213</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-985X.2010.00651.x" xlink:type="simple">10.1111/j.1467-985X.2010.00651.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Good</surname> <given-names>P. I.</given-names></name> (<year>2005</year>). <source>Permutation, parametric and bootstrap tests of hypotheses: a practical guide to resampling methods for testing hypotheses</source>. <volume>100</volume>(<issue>4</issue>), <fpage>1457</fpage>–<lpage>1458</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006839.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gribble</surname> <given-names>P. L.</given-names></name>, &amp; <name name-style="western"><surname>Scott</surname> <given-names>S. H.</given-names></name> (<year>2002</year>). <article-title>Overlap of internal models in motor cortex for mechanical loads during reaching</article-title>. <source>Nature</source>, <volume>417</volume>(<issue>6892</issue>), <fpage>938</fpage>–<lpage>941</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature00834" xlink:type="simple">10.1038/nature00834</ext-link></comment> <object-id pub-id-type="pmid">12087402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cashaback</surname> <given-names>J. G.</given-names></name>, <name name-style="western"><surname>McGregor</surname> <given-names>H. R.</given-names></name>, <name name-style="western"><surname>Pun</surname> <given-names>H. C.</given-names></name>, <name name-style="western"><surname>Buckingham</surname> <given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Gribble</surname> <given-names>P. L.</given-names></name> (<year>2017</year>). <article-title>Does the sensorimotor system minimize prediction error or select the most likely prediction during object lifting?</article-title>. <source>Journal of neurophysiology</source>, <volume>117</volume>(<issue>1</issue>), <fpage>260</fpage>–<lpage>274</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00609.2016" xlink:type="simple">10.1152/jn.00609.2016</ext-link></comment> <object-id pub-id-type="pmid">27760821</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006839.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Holm</surname> <given-names>S.</given-names></name> (<year>1979</year>). <article-title>A simple sequentially rejective multiple test procedure</article-title>. <source>Scandinavian journal of statistics</source>, <fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>