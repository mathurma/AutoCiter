<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00483</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006406</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Bats</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Echoes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Mechanical engineering</subject><subj-group><subject>Robotics</subject><subj-group><subject>Robots</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Remote sensing</subject><subj-group><subject>Sonar</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A fully autonomous terrestrial bat-like acoustic robot</article-title>
<alt-title alt-title-type="running-head">Bat-like robot</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1005-2424</contrib-id>
<name name-style="western">
<surname>Eliakim</surname> <given-names>Itamar</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cohen</surname> <given-names>Zahi</given-names></name>
<role content-type="http://credit.casrai.org/">Validation</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kosa</surname> <given-names>Gabor</given-names></name>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5429-9245</contrib-id>
<name name-style="western">
<surname>Yovel</surname> <given-names>Yossi</given-names></name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Mechanical Engineering Department, Tel Aviv University, Tel Aviv, Israel</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Intelligent Medical Micro/Nano Systems Group, University Hospital of Basel, Basel, Switzerland</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Sagol School of Neuroscience, Tel Aviv University, Tel Aviv, Israel</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>School of Zoology, Faculty of Life Sciences, Tel Aviv University, Tel Aviv, Israel</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Ayers</surname> <given-names>Joseph</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Northeastern University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">yossiyovel@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>6</day>
<month>9</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>9</issue>
<elocation-id>e1006406</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>3</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>7</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Eliakim et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006406"/>
<abstract>
<p>Echolocating bats rely on active sound emission (echolocation) for mapping novel environments and navigating through them. Many theoretical frameworks have been suggested to explain how they do so, but few attempts have been made to build an actual robot that mimics their abilities. Here, we present the ‘Robat’—a fully autonomous bat-like terrestrial robot that relies on echolocation to move through a novel environment while mapping it solely based on sound. Using the echoes reflected from the environment, the Robat delineates the borders of objects it encounters, and classifies them using an artificial neural-network, thus creating a rich map of its environment. Unlike most previous attempts to apply sonar in robotics, we focus on a biological bat-like approach, which relies on a single emitter and two ears, and we apply a biological plausible signal processing approach to extract information about objects’ position and identity.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Many animals are able of mapping a new environment even while moving through it for the first time. Bats can do this by emitting sound and extracting information from the echoes reflected from objects in their surroundings. In this study, we mimicked this ability by developing a robot that emits sound like a bat and analyzes the returning echoes to generate a map of space. Our Robat had an ultrasonic speaker mimicking the bat’s mouth and two ultrasonic microphones mimicking its ears. It moved autonomously through novel out-doors environments and mapped them using sound only. It was able to negotiate obstacles and move around them, to avoid dead-ends and even to recognize if the object in front of it is a plant or not. We show the great potential of using sound for future robotic applications.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Manna Food Security</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1005-2424</contrib-id>
<name name-style="western">
<surname>Eliakim</surname> <given-names>Itamar</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>ONRG grant</institution>
</funding-source>
<award-id>N62909-13-1-N066</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1005-2424</contrib-id>
<name name-style="western">
<surname>Eliakim</surname> <given-names>Itamar</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Israeli ministry of science technology and space</institution>
</funding-source>
<award-id>3-11874</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1005-2424</contrib-id>
<name name-style="western">
<surname>Eliakim</surname> <given-names>Itamar</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This project was funded by ONRG grant no. N62909-13-1-N066 and by the Israeli ministry of science technology and space, grant no. 3-11874. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="2"/>
<table-count count="2"/>
<page-count count="13"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The growing use of autonomous robots emphasizes the need for new sensory approaches to facilitate tasks such as obstacle avoidance, object recognition and path planning. One of the most challenging tasks, faced by many robots, is the problem of generating a map of an unknown environment, while simultaneously navigating through this environment for the first time [<xref ref-type="bibr" rid="pcbi.1006406.ref001">1</xref>]. This problem, is routinely solved by echolocating bats that perceive their surroundings acoustically (other animals also solve this task on a daily basis using a range of sensory modalities) [<xref ref-type="bibr" rid="pcbi.1006406.ref002">2</xref>]. By emitting sound signals and analyzing the returning echoes, bats can orient through a new environment and probably also map it [<xref ref-type="bibr" rid="pcbi.1006406.ref003">3</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref004">4</xref>]. Inspired by this ability, we present the ‘Robat’—a fully autonomous terrestrial robot that solely relies on bat-like SONAR to orient through a novel environment and map it. Using a biologically plausible system with two receivers (ears) and a single emitter(mouth) which produced frequency modulated (FM) chirps at a typical bat rate, the Robat managed to move through a large out-doors novel environment and map it in real-time.</p>
<p>There have been many attempts to use airborne sonar for mapping the environment and moving through it using non-biological approaches; for example by using an array of multiple narrow-band speakers [<xref ref-type="bibr" rid="pcbi.1006406.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006406.ref006">6</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref007">7</xref>] and or multiple microphones [<xref ref-type="bibr" rid="pcbi.1006406.ref008">8</xref>]. These studies proved, that by using multiple emitters, or by carefully scanning the environment with a sonar beam, as if it were a laser, one can map the environment acoustically, but these approaches are very far from the biological solution [<xref ref-type="bibr" rid="pcbi.1006406.ref009">9</xref>]. A bat emits relatively few sonar emissions towards an object, and it must rely on two receivers only (its ears) in order to extract spatial information from its very wide bio-sonar beam which can reach 60 degrees (6 dB double side drop in amplitude [<xref ref-type="bibr" rid="pcbi.1006406.ref010">10</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref011">11</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref012">12</xref>]). Unlike the narrow-band signals typically used in robotic applications, the bat’s wide-band signals provide ample spatial information allowing it to localize multiple reflectors within a single beam. This is the approach we aimed to test and mimic in this study.</p>
<p>Numerous studies have shown that echoes generated by emitting bat-like sonar signals contain spatial information that can be exploited for localization and identification of objects [<xref ref-type="bibr" rid="pcbi.1006406.ref013">13</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref014">14</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref015">15</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref016">16</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref017">17</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref018">18</xref>]. Several previous attempts have been made to model and mimic bats’ spatial abilities of localization and mapping [<xref ref-type="bibr" rid="pcbi.1006406.ref019">19</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref020">20</xref>]. One of the most comprehensive attempts to use a biological approach to map the environment was ‘BatSLAM’ [<xref ref-type="bibr" rid="pcbi.1006406.ref021">21</xref>], which relied on mammalian brain-like computation for simultaneous localization and mapping of a novel environment using biomimetic sonar. Using a biological representation of the data (the cochleogram) the BatSLAM algorithm generated topological maps in which the nodes represent unique places in the environment and the edges represent the robot’s displacements between them. The approach of recognizing a location based on its unique acoustic signature was further broadened by Vanderelst et al. [<xref ref-type="bibr" rid="pcbi.1006406.ref006">6</xref>] who classified a wide range of natural scenes based on their acoustic statistics, once again, without extraction of their spatial characteristics. Vanderelst et al. limited the information extracted from the echoes to the acoustic resolution available to a bat, and they were still successful in achieving useful scene recognition.</p>
<p>Our work differs from these former studies in two important respects: (1) Our Robat moved through the environment autonomously while the previous robots were driven by the user. (2) We mapped the 2D structure of the environment, while they mapped the position of the robot in the environment. Namely, in our approach the outline of the objects that were encountered by the Robat were delineated so that paths (free of obstacles) were revealed for future use. In these previous studies, objects in the environment were mapped as locations with a unique acoustic representation so that when encountered again, the agent could localize itself on the acoustic-map, but no spatial information about objects’ size or orientation was extracted. When moving autonomously, such information is essential for movement planning.</p>
<p>In addition to mapping, our Robat had to autonomously move through the environment while avoiding obstacles. Some previous attempts were made to model orientation and obstacle avoidance using a biological echolocation-based approach. For example, Vanderelst et al. [<xref ref-type="bibr" rid="pcbi.1006406.ref009">9</xref>], suggested a simple sensorimotor approach for obstacle avoidance based on turning away from the louder of the two echoes received by the ears. They showed that a simulated agent can move through a novel environment without any mapping of the positions or borders of the objects within it. This approach might be beneficial when an animal wants to move fast through the environment without an intention of returning to specific locations within it, but if the animal needs to find its way back to some point in this environment (e.g., to its roost), or to plan its movement to a specific location, some mapping must be performed. For example, the robust low-level sensorimotor heuristic presented in [<xref ref-type="bibr" rid="pcbi.1006406.ref009">9</xref>] could be combined with higher level mapping algorithms (e.g., [<xref ref-type="bibr" rid="pcbi.1006406.ref022">22</xref>]).</p>
<p>To our best knowledge, our Robat is the first fully autonomous bat-like biologically plausible robot that moves through a novel environment while mapping it solely based on echo information—delineating the borders of objects and the free paths between them and recognizing their type.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>The Robat’s goal was to move through an environment that it has never experienced before, finding its path between vegetation and other obstacles while mapping their locations, delineating their borders and identifying them (when possible) similar to a bat flying through a grove or a shrubbery which it encounters for the first time (<xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1A</xref>).</p>
<fig id="pcbi.1006406.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006406.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The system—The Robat, signal processing and mapping.</title>
<p>(A) Image of the Robat. Insert shows the Robat’s sensory unit including a speaker and two receivers. (B) An example of a single echo received by the Robat’s two ears. Top row shows right and left ear echo spectrograms where the emitted signal and 3 consecutive echoes can be observed. The first and loudest peak is the emitted signal. Bottom row shows the correlation signals with the peaks that were detected as returning from the same object numbered in each ear (1-3). (C) The Robat’s acoustic mapping. Objects recognized by the Robat are shown as turquoise points. Yellow shading shows the inflation of objects into a map deliniating the borders of the route. Note that turquoise points ahead of the robot have not yet been inflated, because inflation does not occur at each acquisition. Green dots show the locations of echo acquisition points (every 0.5m). The three directions of acquisition are depicted by the three-parts beam.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.g001" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Acquisition</title>
<p>The Robat moved through the environment emitting echolocation signals every 0.5m thus mimicking a bat flying at 5m/s while emitting a signal every 100ms which is within the range of flight-speeds and echolocation-rates used by many foraging bats [<xref ref-type="bibr" rid="pcbi.1006406.ref023">23</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006406.ref025">25</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref025">25</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref026">26</xref>]. Every 0.5m, the Robat emitted three bat-like wide-band frequency-modulated sound signals while pointing its sensors (emitter and receivers) in three different headings: -60, 0, 60 degrees relative to the direction of movement (<xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1A</xref>). This procedure aimed to overcome the narrow acoustic beam of the Robat and to better mimic a bat beam which is typically much wider than that of our speaker (see <xref ref-type="sec" rid="sec007">Methods</xref>) [<xref ref-type="bibr" rid="pcbi.1006406.ref027">27</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref028">28</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref010">10</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref029">29</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref011">11</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref012">12</xref>].</p>
</sec>
<sec id="sec004">
<title>Mapping</title>
<p>Following echo acquisition, acoustic peaks of interest (representing objects) were identified in the echoes (<xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1B</xref>). Equivalent peaks—i.e., peaks returning from the same object—received by the two ears were matched and the reflecting objects were localized. The time-delay between the emission and the arrival of the echoes was used to determine the distance of an object and the difference between the time of arrival of the echo to the two ears was used to determine its azimuth (i.e., Mapping was performed in 2D, <xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1C</xref>, Turquoise points depict objects’ location, see <xref ref-type="sec" rid="sec007">Methods</xref> for full details). Importantly, the Robat was able to localized multiple objects whose echoes were received within a single beam (<xref ref-type="supplementary-material" rid="pcbi.1006406.s001">S1 Fig</xref>). This ability has not been reported in previous studies and bats are likely able to do so. After every 5 steps (i.e., 2.5m) the Robat applied an inflation and interpolation algorithm that incorporated the newly mapped objects into the map that has been created so far (based on the previous echoes, <xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1C</xref>, yellow shaded area, see <xref ref-type="sec" rid="sec007">Methods</xref>). At each time step, following echo acquisition and object localization, the Robat planned its next movement according to the iterative map that has been created so far and according to the objects detected in the most recent acquisition. Movement planning was based on the bug algorithm [<xref ref-type="bibr" rid="pcbi.1006406.ref030">30</xref>] which can be simply described as turning 90 degrees to the right, whenever an obstacle is encountered ahead, and then turning left to maneuver around the obstacle.</p>
<p>The movement and mapping algorithms were tested in two outdoor environments: (1) The pteridophyte greenhouse (5m x 12m) and (2) The palm greenhouse (40m x 5m) both situated in the Tel Aviv University Botanical Garden.</p>
<p>The Robat successfully moved through both new environments without hitting objects and while mapping their locations and contour line (see Robat’s trajectory depicted in black in <xref ref-type="fig" rid="pcbi.1006406.g002">Fig 2A</xref>). When an obstacle was placed in the Robat’s way, it moved around it (<xref ref-type="fig" rid="pcbi.1006406.g002">Fig 2B</xref>). To quantify the mapping performance, we compared the contour of the objects as it was estimated by the Robat to the real contour (which we estimated from drone images in the Palm greenhouse and measured manually in the Pteridophyte greenhouse, see <xref ref-type="sec" rid="sec007">Methods</xref>). In the palm greenhouse, the mean distance between the two contours was 0.42 ± 0.74 (mean + STD) [m] meaning that along the 35m trail that the Robat passed and mapped in the Palm greenhouse, the estimated borders of the objects on both sides of the trail, were off by 42cm on average, relative to their real position. This might seem inaccurate when considering bats’ ability to estimate range with an accuracy of less than 1cm in a highly controlled experiment, [<xref ref-type="bibr" rid="pcbi.1006406.ref031">31</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref032">32</xref>] but it should be emphasized that the Robat only detected and localized parts of the objects while their borders were delineated based on our inflation an interpolation algorithm (<xref ref-type="sec" rid="sec007">Methods</xref>). Moreover, note that many of the objects in our environment were plants with multiple branches so that the exact borders of the objects were inherently difficult to define (even in the drone images). Similar performance (0.44 ± 0.25 (mean + STD) [m]) was observed in the second environment (the Pteridophyte greenhouse, <xref ref-type="supplementary-material" rid="pcbi.1006406.s002">S2 Fig</xref>).</p>
<fig id="pcbi.1006406.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006406.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Mapping and obstacle avoidance.</title>
<p>(a) Robat’s mapping of the greenhouse overlaid on drone images. Black line depicts the trajectory taken by the Robat. Yellow shaded areas show the objects mapped by the Robat and the blue line shows their borders. Turquoise points depict the center of the objects as they have been localized by the Robat. (b) Passing a cylinder obstacle (D = 0.8m) that has been placed in the Roba’ts way. Yellow arrows show the trajectory taken by the Robat to pass the obstacle (doing so fully autonomously). Turquoise points depict points on the obstacle as they have been localized by the Robat. (c) Examples of object classification. Two correct classifications and two wrong ones are presented. Note that the ‘non-plant’ classified as a plant includes a bamboo mesh. Such mesh objects create echoes that have plant-like acoustic characteristics.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Classification and decision making</title>
<p>When moving through the environment, a real bat can probably use echoes in order to classify objects into categories (e.g., rocks, trees, bushes) and even to identify specific objects (e.g., a specific beech tree in its favorite foraging site). Such recognition would greatly assist the bat to navigate, for example, by recognizing specific landmarks at important turning points along its flight route and it could also assist its foraging, for example, by recognizing specific vegetation that is rich in fruit or insects [<xref ref-type="bibr" rid="pcbi.1006406.ref033">33</xref>] [<xref ref-type="bibr" rid="pcbi.1006406.ref017">17</xref>]. So far we demonstrated that the Robat can translate a novel natural environment into a binary map of open spaces and obstacles. In order to improve the mapping, we added a classification step to the algorithm, which was performed using a neural-network that was trained to distinguish between two object categories—plants and non-plants. To this end, a set of acoustic features were extracted from the echoes and used as input for the network (<xref ref-type="sec" rid="sec007">Methods</xref>). The Robat was able to classify objects as plants or non plants significantly above chance level (<xref ref-type="fig" rid="pcbi.1006406.g002">Fig 2C</xref> and <xref ref-type="table" rid="pcbi.1006406.t001">Table 1</xref>) with a balanced accuracy of 68% (chance was 50%, P = 0.01, based on a permutation test with 100 permutations, the balanced accuracy is the number of correct classifications in each class, divided by the number of examples in each class, averaged over all classes. This measurement mitigates biases which could rise from unbalanced class sizes, see <xref ref-type="sec" rid="sec007">Methods</xref>). The classification performance is also shown in <xref ref-type="fig" rid="pcbi.1006406.g002">Fig 2a</xref> where colored points depict plants (green) and non-plants (gray).</p>
<table-wrap id="pcbi.1006406.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006406.t001</object-id>
<label>Table 1</label>
<caption>
<title>Classification performance on the test set, for the plant vs. non-plant task.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006406.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-right:thick;border-bottom:thick">True Label / Predicted Label</th>
<th align="left" style="border-bottom:thick">Plant</th>
<th align="left" style="border-bottom:thick">Non-Plant</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="border-right:thick">Plant</td>
<td align="left">77%</td>
<td align="left">23%</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Non-Plant</td>
<td align="left">42%</td>
<td align="left">58%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Finally, we tested the functionality of this classification ability by purposefully driving the Robat into a dead end where it faced obstacles in all directions ahead (i.e., right, left and straight ahead, <xref ref-type="supplementary-material" rid="pcbi.1006406.s007">S7 Fig</xref>). The Robat had to determine which of the three obstacles was a plant, through which it could drive, and it did so successfully at ~70% of the cases (in accordance with its ~70% accurate classification rate, see movie: <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=LzGGuzvYSH8-second" xlink:type="simple">https://www.youtube.com/watch?v=LzGGuzvYSH8-second</ext-link> 49 and onward).</p>
</sec>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we managed to build an autonomous robot that moves through a novel environment and maps it acoustically using bat-like Bio-sonar. We achieved high mapping accuracy, despite our simple approach, proving the great potential of using active wide-band sound emissions to map the environment. We created a (2D) topographic map which would allow us to plan future movements through the environment (and not a topological map). The statistical approach presented in [<xref ref-type="bibr" rid="pcbi.1006406.ref009">9</xref>] is therefore complementary to ours, allowing classifying specific locations based on their echoes. For example, when navigating back to a specific location using the map created by the Robat, their approach could be used to validate the arrival at the desired location and also to help adjust the map to improve its accuracy.</p>
<p>The Robat was much slower than a real bat, stopping for ca. 30 seconds every 0.5m to acquire echoes. This slowness was however, merely a result of the mechanical limitations of our system and mainly the gimbal that was slow. Using a speaker with a wider beam (that eliminates the need to turn at each location) would allow the Robat to acquire echoes on the move, while moving as fast as a bat. Importantly, despite our stopping for echo recording, the acoustic information we acquired did not differ from that received by a bat, except for the fact that a bat’s echoes would also be slightly Doppler-shifted (but this would probably not affect any of our results).</p>
<p>In some respects, our processing was not fully bat-like. We used a sampling rate of 250kHz, which is higher than the theoretical time precision of the auditory system [<xref ref-type="bibr" rid="pcbi.1006406.ref034">34</xref>]. Bats and other small mammals have been shown to estimate azimuth with an accuracy of &lt;10degrees (the exact accuracy depends on the azimuth, (e.g. [<xref ref-type="bibr" rid="pcbi.1006406.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006406.ref036">36</xref>])). This accuracy accounts for an inter-aural time difference of &lt;10<italic>μ</italic>s which is in accordance with our sampling rate (sampling at 250kHz is equivalent to an error of ~5<italic>μ</italic>s when estimating time differences between two ears). Therefore, even if our computation was different from that of a bat (which does not cross-correlate two highly sampled time signals) the overall accuracy allowed by our approach was not better than that of a bat. Moreover, due to the inflation and interpolation method that we used in order to delineate the borders of the objects, the effective accuracy of our mapping was much lower than that allowed by this high sampling rate, and probably much lower than that available to bats [<xref ref-type="bibr" rid="pcbi.1006406.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006406.ref032">32</xref>]. Therefore, we hypothesize that using an auditory preprocessing model like that used in Batslam for example [<xref ref-type="bibr" rid="pcbi.1006406.ref021">21</xref>] would probably not change our results dramatically. Another advantage that we had over real bats was the relatively large distance between the two ears which were spaced 7cm apart—ca. two times more than in a large bat. This probably allowed more accurate azimuth estimations, but once again, we hypothesize that because of the use of inflation, this did not improve our performance dramatically. Importantly, we managed to extract information about multiple objects within a single sonar beam. On average, in each echo that contained reflections (some echoes did not) we detected 4.1 objects positioned in a range of azimuths between -50—50 degrees. Another important difference between the Robat and an actual bat is the lack of an external ear in the Robat. The angle-dependent frequency response of the external ear allows bats (and other animals) to gain information about the location of a sound source in three dimensions. Because we relied on temporal information for object localization, we used a first approximation of an ear. Adding a structure mimicking the external ear could have further improved our localization performance and it would be essential in order to expand our mapping to 3D.</p>
<p>In order to better mimic the bat’s beam, we used three beams (directed 60 degrees apart), but this made our task easier than a bat’s because we could analyze the echoes returning from each direction separately. We therefore also tested an approach in which we sum the three echoes collected (with different headings) at each acquisition point, thus mimicking a wider beam. Even with this degraded data, we were able to map the environment with a decent accuracy of 1.14 ± 0.70 [m] (mean + STD, <xref ref-type="supplementary-material" rid="pcbi.1006406.s006">S6b Fig</xref>), an accuracy that would allow future planning of trajectories while avoiding obstacles on the way.</p>
<p>In some respects our approach was probably much more simplistic than a bat. For example, the obstacle avoidance algorithm was very simple and a better approach would probably use control-theory to steer the Robat around obstacles [<xref ref-type="bibr" rid="pcbi.1006406.ref037">37</xref>]. In terms of mission priority, we used serial processing where the Robat first processes new incoming sensory information; it then performs the urgent low-level task of obstacle avoidance and path planning, and only every several acquisitions, it performs the high-level process of map integration. There is much evidence that the mammalian brain also performs sensory tasks sequentially (e.g., [<xref ref-type="bibr" rid="pcbi.1006406.ref038">38</xref>]) but it would be interesting to test some procedures for parallel processing in the future.</p>
<p>In addition to mapping the positions of objects in the environment, a complete map should also include information about the objects such as their type or identity. To show that such information is available in the echoes, we developed a classifier that can categorize objects based on their echo. We hypothesize that the medium classification performance that we achieved (68%) was a result of our choice of categories. We trained the classifier to distinguish between plant and non-plant objects but these are not always two well distinct groups. For example, the echo of an artificial object such as a fence will have vegetation-like acoustic features and indeed most of the classifier’s mistakes were recognition of non-plants as plants. Bats might thus divide their world of objects differently, perhaps to diffusive vs. glint-reflecting objects.</p>
<p>Altogether, we show how a rather simple signal processing approach allows to autonomously move and map a new environment based on acoustic information. Our work thus proves the great potential of using acoustic echoes to map and navigate, a potential that is translated into action by echolocating bats on a daily basis.</p>
</sec>
<sec id="sec007" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec008">
<title>Acquisition</title>
<p>The Robat was based on the ‘Komodo’ robotic platform (Robotican, Israel). The Bio-sonar sensor was mounted on a DJI Ronin gimbal which allowed turning the sensing unit relatively to the base of the robot in a stable manner. The sensing unit included an ultrasonic speaker acting as the bat’s mouth (VIFA XT25SC90-04) and 2 ultrasonic microphones acting as the bat’s ears spaced 7cm apart (Avisoft-Bioacoustics CM16/CMPA40-5V Condenser). The speaker and the microphones were connected to A/D and D/A converters which were based on the USB-1608GX-2AO NI DAQ board, sampling at 250KS/s at each ear. The emitted signal was a 10ms FM chirp sweeping between 100-20kHz. It was amplified using a Sony Amplifier (XM-GS4). An uEye RGB camera, was used for image collection for validation purposes only. Three 2.4GHz/5.8GHz antennae were mounted at the rear of the Robat for wireless communication between the Robat and a stationary station. This allowed viewing the map created by the Robat in real time, but importantly, all calculations and decisions were performed on the Robat itself.</p>
</sec>
<sec id="sec009">
<title>Mapping</title>
<p>While moving, the Robat stopped every 0.5m (based on its odometry measurements) and the sonar system (emitter and receivers) was rotated to three different headings [0,60,-60 degrees] relative to the direction of movement, a sound signal (see above) was emitted, and echoes were recorded. Each recording was 0.035 sec long, equivalent to a range of ca. 6 meters (farther objects were thus ignored at each emission). The signal-to-echo delay time and the time of arrival differences of the echoes to the two ears (i.e., the Interaural Time Difference) were used together in order to map the environment. To this end, the received signals were cross-correlated with the theoretical emitted signal. The cross-correlated signal was normalized relative to the maximum value of the recording, and a peak detection function was used to find peaks of interest (python peakutil with a minimal peak distance of 0.002 sec, and a min amplitude of 0.3.).</p>
<p>To match peaks arriving at the right and left ears, for each peak detected in one ear, an equivalent peak was searched for in the other ear within a window of +/- 0.001 sec. If a peak was found, the Pearson correlation was used to determine if the two echoes were reflections of the same object. For this purpose, a segment of 0.01 seconds around each peak was cut and the correlation between the two time signals (one from each ear) was computed. Only correlations higher than 0.9 were accepted. This threshold was conservative thus potentially resulting in missing of objects, but it reduced the localization of artifact non-existent objects. Because the Robat emitted very 0.5 m—there was much overlap between echoes of consecutive emissions. We were therefore likely to detect an object several times, so a conservative approach was chosen. In addition to its position, each object on the map was defined by three parameters: “C |T |P”, where C is the Pearson correlation coefficient between the left and right ears for the specific point, T is the object’s type based on its acoustic classification—either artificial or a plant, and P is the classification probability (see more below about the classification process).</p>
<p>Results in the in-doors controlled environment showed that using two ears, the mean error in distance estimation was 1.3 ± 2.1 [cm] (mean + STD, <xref ref-type="supplementary-material" rid="pcbi.1006406.s003">S3 Fig</xref>) and the mean azimuth estimation error was 1.2 ± 0.7 [degrees] (mean+STD, <xref ref-type="supplementary-material" rid="pcbi.1006406.s003">S3 Fig</xref>). Importantly, these are the results for a single reflector, so accuracy in the real environment where many reflections are received at each point will be lower.</p>
<p>Every 5 Robat-steps, newly localized objects were integrated into the map that was created so far. This was done using an Iterative-Object-Inflation algorithm, which inflated points into squares and connected them. To this end, the entire area around the Robat was divided into a grid with 2000x2000 pixels (5x5cm<sup>2</sup> each). Each detected object was placed in the corresponding pixel on the map and was inflated to an area of 20x20 pixels around its center (i.e., 1x1 m<sup>2</sup>, <xref ref-type="supplementary-material" rid="pcbi.1006406.s004">S4 Fig</xref>). This procedure creates a binary map with 1’s depicting objects and 0’s depicting a free path. Pixels along the trajectory that the Robat previously moved through always received the value 0 depicting an open path (even if they were within the 20x20 window of a detected object).</p>
</sec>
<sec id="sec010">
<title>Movement and obstacle avoidance</title>
<p>We chose a very simple obstacle avoidance approach also known as the ‘bug algorithm’ [<xref ref-type="bibr" rid="pcbi.1006406.ref039">39</xref>]. During the exploration process, the Robat moved forward in steps of 0.5m between consecutive acquisition points. When detecting an obstacle less than 1.2m in front of it, the Robat turned 90 degrees towards the right, and performed a 1m step towards the right (after checking that there is no obstacle ahead). After performing a 1m step to the right, the Robat turned 90 degrees to the left and acquired an echo. If no obstacle was detected (meaning that the obstacle has been passed) the Robat continued straight (i.e., in its previous direction before turning right). If the way was still blocked (i.e., the obstacle was not passed), the Robat turned again to the right and kept moving towards the right (90 degrees relative to its original direction).</p>
</sec>
<sec id="sec011">
<title>Summing echoes from all three headings</title>
<p>In order to better mimic the bat, that has a beam much wider than Robat’s beam, we examine an approach of summing the echoes returning from the three different headings (mentioned above) into one superposition echo, and then running the same (detection, localization and mapping) algorithms as described above.</p>
</sec>
<sec id="sec012">
<title>Evaluation of the mapping accuracy</title>
<p>In order to examine the acoustic map generated by the Robat, inspired by [<xref ref-type="bibr" rid="pcbi.1006406.ref040">40</xref>], we collected aerial images using a drone (DJI Phantom 4, DJI), to construct a complete ground truth map of the area. This procedure was only performed for the large palm greenhouse (40x5 m<sup>2</sup>). The contour of the objects on both sides of the trail in the greenhouse was extracted and compared to the contour of the inflated map that was acoustically reconstructed by the Robat (both contours were marked manually). Each of the two contours was fit by a 55-coefficient order polynomial function which was then sampled at 500 points to get a high resolution description of the contour. The two contours (real and Robat-estimated) were compared by calculating the root-mean-square distance between them (the average over these 500 points, <xref ref-type="supplementary-material" rid="pcbi.1006406.s005">S5 Fig</xref>).</p>
</sec>
<sec id="sec013">
<title>Classification</title>
<p>Acoustic based object classification was performed using a neural-network that was trained on a binary task—classifying whether and object was a plant or not. Only objects that were located closer than 3[m] from the sensing unit were classified. 0.035 s long echoes were used from both the right and left ear. These recordings were passed through three band pass filters, without the transmitted echo, (20-40kHz, 40-60kHz and 60-100kHz). Each echo was represented by 6 signals—3 filters x two ears. Next, a set of 21 acoustic features (<xref ref-type="table" rid="pcbi.1006406.t002">Table 2</xref>) were extracted from each band-passed recording following T. Giannakopoulos [<xref ref-type="bibr" rid="pcbi.1006406.ref041">41</xref>]. Each echo was divided into seven windows equally spaced with an overlap of 40ms and the 21 features were extracted for each window generating a total of 147 dimensions per signal (21 features x 23 windows). The classifier was thus fed with 6 signals (483 dimensions each) and the decision of the majority of the six classifiers was used.</p>
<table-wrap id="pcbi.1006406.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006406.t002</object-id>
<label>Table 2</label>
<caption>
<title>Classification features.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006406.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-right:thick;border-bottom:thick">Feture Name</th>
<th align="left" style="border-bottom:thick">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="border-right:thick">ZCR</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Energy</td>
<td align="left">46</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Entropy</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Spectral Centroid</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Spectral Spread</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Spectral Flux</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Spectral Rolloff</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left" style="border-right:thick">Chrome Vector</td>
<td align="left">299</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The data was fed into a neural network with the following architecture:</p>
<list list-type="simple">
<list-item>
<p>Input layer—483 elements</p>
</list-item>
<list-item>
<p>First layer—105 elements with an RELU activation function</p>
</list-item>
<list-item>
<p>Dropout—0.5</p>
</list-item>
<list-item>
<p>Second Layer—50 elements with an RELU activation function</p>
</list-item>
<list-item>
<p>Third Layer—6 elements with an RELU activation function</p>
</list-item>
<list-item>
<p>Output Layer—1 element with a sigmoid activation function</p>
</list-item>
</list>
<p>We used Python’s TensorFlow to construct and train a three-layer neural-network (using the Keras directory).</p>
<p>The training sets included 788 plant examples and 628 non-plant examples collected on several sites on campus. We used the camera that was on the Robat to label the echoes.</p>
<p>Finally, to assess the statistical significance of our classification, we ran 100 permutations in which we assigned the training data randomly into the two classes (plants and non-plants), trained a classifier for each permutation and tested it on the same test-data.</p>
<p>We also tested several additional classification methods before choosing the neural-network. We tested a KNN (K nearest neighbors) classifier with five different distance measurements: Mahalanobis, Euclidean, Correlation, Minkowski and Canberra. We also tested two additional approaches for dimensionality reduction (before using the KNN) including PCA and LDA. In addition, we also tested a linear SVM classifier. For all classifiers, we used the same input features (see above).</p>
<p>The results were similar for most classifiers, but the neural network performed slightly better than the other (<xref ref-type="supplementary-material" rid="pcbi.1006406.s008">S8 Fig</xref>).</p>
</sec>
</sec>
<sec id="sec014">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006406.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>The algorithm recognized and localized multiple reflectors within a single beam.</title>
<p>Figure presents an echo train where three centers were localized. Spectrograms, correlation signals and the respective image (with centers) are depicted. Table shows the distances and azimuths of the three centers.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Acoustic mapping of the pteridophyte greenhouse.</title>
<p>Colors and symbols are the same as in <xref ref-type="fig" rid="pcbi.1006406.g001">Fig 1</xref>. Red lines depict the actual borders of the greenhouse while blue lines show the borders estimated by the Robat. The black line shows the Robat’s movement in the greenhouse, a total of ca. 20m. The mean mapping error was 0.44 ± 0.25 [m] (mean + STD).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Localization accuracy estimated in the lab.</title>
<p>Grey circles show real locations of a cylindrical object, while black crosses show estimated positions. Insert in bottom right shows an enlargement of the results when the object was at 0.8 m in front of the Robat.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>The iterative obstacle Inflation algorithm.</title>
<p>Each panel shows the result of another iteration of the algorithm,. The Robat used the map created after three iterations.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Mapping error estimation.</title>
<p>(a) Real and estimated binary maps of the upper side of the greenhouse, showing the contour of the objects in white. (b) The same as in ‘a’ but with the two overlaid on-top of each other. (c) The same as in ‘b’, but after the 500 point interpolations with a 55 degree polynomial fit. (d) The error—distance between the real and the mapped contours.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Map reconstruction based on summing three echoes.</title>
<p>The top three spectrograms represent three echoes recorded at the same location with different bearings. The bottom spectrogram represents the sum of the three echoes. The map on the bottom shows the result of using this mapping approach in the greenhouse (the same environment as in <xref ref-type="fig" rid="pcbi.1006406.g002">Fig 2a</xref>). Note that most of the errors are on one side, which was mostly composed of diffusive plant echoes.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Classification based decision making.</title>
<p>Schematic and real images of the decision making task in which we drove the Robat into a dead end and let it decide which of the three sides is a plant through which it could pass.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006406.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006406.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>The performance of different classification algorithms bars show correct classification of plants (blue) and non-plants (orange); BA—Balanced accuracy.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>Thanks to Manna Food Security fellowship supported I.Eliakim in this research, and for Kineret Manevich, Yuval Sapir from botanical garden, Tel Aviv University for helping with the outdoor experiments. Thanks to Aya Goldstein for aerial footage of the greenhouse.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006406.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leonard</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Durrant-Whyte</surname> <given-names>HF</given-names></name>. <article-title>Mobile robot localization by tracking geometric beacons</article-title>. <source>IEEE Transactions on Robotics and Automation</source>. <year>1991</year>;<volume>7</volume>(<issue>3</issue>):<fpage>376</fpage>–<lpage>382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/70.88147" xlink:type="simple">10.1109/70.88147</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tardos</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Neira</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Newman</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Leonard</surname> <given-names>JJ</given-names></name>. <article-title>Robust Mapping and Localization in Indoor Environments Using Sonar Data</article-title>. <source>The International Journal of Robotics Research</source>. <year>2002</year>;<volume>21</volume>(<issue>4</issue>):<fpage>311</fpage>–<lpage>330</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/027836402320556340" xlink:type="simple">10.1177/027836402320556340</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Voigt</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Frick</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Holderied</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Holland</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kerth</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mello</surname> <given-names>MAR</given-names></name>, <etal>et al</etal>. <article-title>Principles and Patterns of Bat Movements: From Aerodynamics to Ecology</article-title>. <source>The Quarterly Review of Biology</source>. <year>2017</year>;<volume>92</volume>(<issue>3</issue>):<fpage>267</fpage>–<lpage>287</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/693847" xlink:type="simple">10.1086/693847</ext-link></comment> <object-id pub-id-type="pmid">29861509</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jensen</surname> <given-names>ME</given-names></name>. <article-title>Echolocating bats can use acoustic landmarks for spatial orientation</article-title>. <source>Journal of Experimental Biology</source>. <year>2005</year>;<volume>208</volume>(<issue>23</issue>):<fpage>4399</fpage>–<lpage>4410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.01901" xlink:type="simple">10.1242/jeb.01901</ext-link></comment> <object-id pub-id-type="pmid">16339860</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lindsay</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Roman</surname> <given-names>K</given-names></name>. <article-title>Mobile Robot Sonar for Target Localization and Classification</article-title>. <source>The International Journal of Robotics Research</source>. <year>1995</year>;<volume>14</volume>(<issue>4</issue>):<fpage>295</fpage>–<lpage>318</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/027836499501400401" xlink:type="simple">10.1177/027836499501400401</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vanderelst</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Steckel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Boen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peremans</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Holderied</surname> <given-names>M</given-names></name>. <article-title>Place recognition using batlike bio-sonar</article-title>. <source>eLife</source>. <year>2016</year>;<volume>1</volume>:<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">L K, R K. An optimal sonar array for target localization and classification. In: Proceedings of the 1994 IEEE International Conference on Robotics and Automation; 1994. p. 3130–3135 vol.4.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref008">
<label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Evers C, Moore AH, Naylor PA. Acoustic simultaneous localization and mapping (A-SLAM) of a moving microphone array and its surrounding speakers. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing—Proceedings. 2016;2016-May:6Proceedings. 2016;2016-May:6–10.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vanderelst</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Holderied</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Peremans</surname> <given-names>H</given-names></name>. <article-title>Sensorimotor Model of Obstacle Avoidance in Echolocating Bats</article-title>. <source>PLoS Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004484" xlink:type="simple">10.1371/journal.pcbi.1004484</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ghose</surname> <given-names>K</given-names></name>. <article-title>Steering by Hearing: A Bat’s Acoustic Gaze Is Linked to Its Flight Motor Output by a Delayed, Adaptive Linear Law</article-title>. <source>Journal of Neuroscience</source>. <year>2006</year>;<volume>26</volume>(<issue>6</issue>):<fpage>1704</fpage>–<lpage>1710</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4315-05.2006" xlink:type="simple">10.1523/JNEUROSCI.4315-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16467518</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Falk</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Moss</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>. <article-title>Active control of acoustic field-of-view in a biosonar system</article-title>. <source>PLoS Biology</source>. <year>2011</year>;<volume>9</volume>(<issue>9</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001150" xlink:type="simple">10.1371/journal.pbio.1001150</ext-link></comment> <object-id pub-id-type="pmid">21931535</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jakobsen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ratcliffe</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Surlykke</surname> <given-names>A</given-names></name>. <article-title>Convergent acoustic field of view in echolocating bats</article-title>. <source>Nature</source>. <year>2013</year>;<volume>493</volume>(<issue>7430</issue>):<fpage>93</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11664" xlink:type="simple">10.1038/nature11664</ext-link></comment> <object-id pub-id-type="pmid">23172147</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reijniers</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vanderelst</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Peremans</surname> <given-names>H</given-names></name>. <article-title>Morphology-induced information transfer in bat sonar</article-title>. <source>Physical Review Letters</source>. <year>2010</year>;<volume>105</volume>(<issue>14</issue>):<fpage>1</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.105.148701" xlink:type="simple">10.1103/PhysRevLett.105.148701</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genzel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wiegrebe</surname> <given-names>L</given-names></name>. <article-title>Size does not matter: Size-invariant echo-acoustic object classification</article-title>. <source>Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology</source>. <year>2013</year>;<volume>199</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00359-012-0777-3" xlink:type="simple">10.1007/s00359-012-0777-3</ext-link></comment> <object-id pub-id-type="pmid">23180047</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simon</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Holderied</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>CU</given-names></name>, <name name-style="western"><surname>von Helversen</surname> <given-names>O</given-names></name>. <article-title>Floral Acoustics: Conspicuous Echoes of a Dish-Shaped Leaf Attract Bat Pollinators</article-title>. <source>Science</source>. <year>2011</year>;<volume>333</volume>(<issue>6042</issue>):<fpage>631</fpage>–<lpage>633</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1204210" xlink:type="simple">10.1126/science.1204210</ext-link></comment> <object-id pub-id-type="pmid">21798950</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wotton</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Simmons</surname> <given-names>JA</given-names></name>. <article-title>Spectral cues and perception of the vertical position of targets by the big brown bat, Eptesicus fuscus</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2000</year>;<volume>107</volume>(<issue>2</issue>):<fpage>1034</fpage>–<lpage>1041</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.428283" xlink:type="simple">10.1121/1.428283</ext-link></comment> <object-id pub-id-type="pmid">10687712</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Stilz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Franz</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Boonman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schnitzler</surname> <given-names>HU</given-names></name>. <article-title>What a plant sounds like: The statistics of vegetation echoes as received by echolocating bats</article-title>. <source>PLoS Computational Biology</source>. <year>2009</year>;<volume>5</volume>(<issue>7</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000429" xlink:type="simple">10.1371/journal.pcbi.1000429</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Franz</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Stilz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schnitzler</surname> <given-names>HU</given-names></name>. <article-title>Plant classification from bat-like echolocation signals</article-title>. <source>PLoS Computational Biology</source>. <year>2008</year>;<volume>4</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000032" xlink:type="simple">10.1371/journal.pcbi.1000032</ext-link></comment> <object-id pub-id-type="pmid">18369425</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref019">
<label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Yamada</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Oka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tateiwa</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ohta</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kobayashi</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <chapter-title>Obstacle-Avoidance Navigation by an Autonomous Vehicle Inspired by a Bat Biosonar Strategy</chapter-title>. In: <source>Biomimetic and Biohybrid Systems</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2015</year>. p. <fpage>135</fpage>–<lpage>144</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref020">
<label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Peremans</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Mey</surname> <given-names>FD</given-names></name>, <name name-style="western"><surname>Schillebeeckx</surname> <given-names>F</given-names></name>. <chapter-title>Man-made versus biological in-air sonar systems</chapter-title>. <source>Frontiers in Sensing: From Biology to Engineering</source>. <year>2012</year>;9783211997:<fpage>195</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-211-99749-9_13" xlink:type="simple">10.1007/978-3-211-99749-9_13</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steckel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Peremans</surname> <given-names>H</given-names></name>. <article-title>BatSLAM: Simultaneous localization and mapping using biomimetic sonar</article-title>. <source>PloS One</source>. <year>2013</year>;<volume>8</volume>(<issue>1</issue>):<fpage>e54076</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0054076" xlink:type="simple">10.1371/journal.pone.0054076</ext-link></comment> <object-id pub-id-type="pmid">23365647</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brooks</surname> <given-names>RA</given-names></name>. <article-title>A Robust Layered Control System For A Mobile Robot</article-title>. <source>IEEE Journal on Robotics and Automation</source>. <year>1986</year>;<volume>2</volume>(<issue>1</issue>):<fpage>14</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/JRA.1986.1087032" xlink:type="simple">10.1109/JRA.1986.1087032</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Surlykke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ghose</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Moss</surname> <given-names>CF</given-names></name>. <article-title>Acoustic scanning of natural scenes by echolocation in the big brown bat, Eptesicus fuscus</article-title>. <source>Journal of Experimental Biology</source>. <year>2009</year>;<volume>212</volume>(<issue>7</issue>):<fpage>1011</fpage>–<lpage>1020</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/jeb.024620" xlink:type="simple">10.1242/jeb.024620</ext-link></comment> <object-id pub-id-type="pmid">19282498</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Amichai</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Blumrosen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>. <article-title>Calling louder and longer: how bats use biosonar under severe acoustic interference from other bats</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>. <year>2015</year>;<volume>282</volume>(<issue>1821</issue>):<fpage>20152064</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2015.2064" xlink:type="simple">10.1098/rspb.2015.2064</ext-link></comment> <object-id pub-id-type="pmid">26702045</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>HU</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>CF</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>A</surname> <given-names>D</given-names></name>. <article-title>From spatial orientation to food acquisition in echolocating bats</article-title>. <source>Trends in Ecology and Evolution</source>. <year>2003</year>;<volume>18</volume>(<issue>8</issue>):<fpage>386</fpage>–<lpage>394</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0169-5347(03)00185-X" xlink:type="simple">10.1016/S0169-5347(03)00185-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nihoul</surname> <given-names>JCJ</given-names></name>. <article-title>Echolocation in Bats and Dolphins</article-title>. <source>Journal of Marine Systems</source>. <year>2004</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jmarsys.2004.01.009" xlink:type="simple">10.1016/j.jmarsys.2004.01.009</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daniell</surname> <given-names>H</given-names></name>. <article-title>Flying big brown bats emit a beam with two lobes in the vertical plane</article-title>. <source>J Acoust Soc Am 2007</source>. <year>2012</year>;<volume>76</volume>(<issue>October 2009</issue>):<fpage>211</fpage>–<lpage>220</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jakobsen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Brinkl⌀v</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Surlykke</surname> <given-names>A</given-names></name>. <article-title>Intensity and directionality of bat echolocation signals</article-title>. <source>Frontiers in Physiology</source>. <year>2013</year>;<volume>4</volume>:<fpage>89</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fphys.2013.00089" xlink:type="simple">10.3389/fphys.2013.00089</ext-link></comment> <object-id pub-id-type="pmid">23630501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kounitsky</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rydell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Amichai</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Boonman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Eitan</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Weiss</surname> <given-names>AJ</given-names></name>, <etal>et al</etal>. <article-title>Bats adjust their mouth gape to zoom their biosonar field of view</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>21</issue>):<fpage>6724</fpage>–<lpage>6729</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1422843112" xlink:type="simple">10.1073/pnas.1422843112</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Yufka A, Parlaktuna O. Performance Comparison of BUG Algorithms for Mobile Robots. Performance Comparison of BUG Algorithms for Mobile Robots. 2009;p. 61–65.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simmons</surname> <given-names>JA</given-names></name>. <source>Perception of echo phase information in bat sonar</source>; <year>1979</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simmons</surname> <given-names>JA</given-names></name>. <article-title>The resolution of target range by echolocating bats</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1973</year>;<volume>54</volume>(<issue>1</issue>):<fpage>157</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1913559" xlink:type="simple">10.1121/1.1913559</ext-link></comment> <object-id pub-id-type="pmid">4738624</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>M O Franz</surname> <given-names>PSYY</given-names></name>. <article-title>Complex echo classification by echo-locating bats: a review</article-title>. <source>Journal of Comparative Physiology</source>. <year>2011</year>;<volume>197</volume>:<fpage>475</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00359-010-0584-7" xlink:type="simple">10.1007/s00359-010-0584-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref034">
<label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Grinnell</surname> <given-names>A</given-names></name>. <source>Hearing in Bats: An Overview</source>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006406.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heffner</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Koay</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Heffner</surname> <given-names>HE</given-names></name>. <article-title>Sound localization in common vampire bats: Acuity and use of the binaural time cue by a small mammal</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2015</year>;<volume>137</volume>(<issue>1</issue>):<fpage>42</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.4904529" xlink:type="simple">10.1121/1.4904529</ext-link></comment> <object-id pub-id-type="pmid">25618037</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heffner</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Koay</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Heffner</surname> <given-names>HE</given-names></name>. <article-title>Sound localization in a new-world frugivorous bat, Artibeus jamaicensis: Acuity, use of binaural cues, and relationship to vision</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2001</year>;<volume>109</volume>:<fpage>412</fpage>–<lpage>421</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1329620" xlink:type="simple">10.1121/1.1329620</ext-link></comment> <object-id pub-id-type="pmid">11206172</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bar</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>Skogestad</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Marcal</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>Y</given-names></name>. <article-title>A Sensory-Motor Control Model of Animal Flight Explains Why Bats Fly Differently in Light Versus Dark</article-title>. <source>PLoS Biology</source>. <year>2015</year>;<volume>13</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002046" xlink:type="simple">10.1371/journal.pbio.1002046</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Brain Mechanisms of Serial and Parallel Processing during Dual-Task Performance</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>30</issue>):<fpage>7585</fpage>–<lpage>7598</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0948-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0948-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18650336</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lumelsky</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Skewis</surname> <given-names>T</given-names></name>. <article-title>Incorporating Range Sensing in the Robot Navigation Function</article-title>. <source>IEEE Transactions on Systems, Man and Cybernetics</source>. <year>1990</year>;<volume>20</volume>(<issue>5</issue>):<fpage>1058</fpage>–<lpage>1069</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/21.59969" xlink:type="simple">10.1109/21.59969</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kümmerle</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Steder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dornhege</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ruhnke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Grisetti</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Stachniss</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>On measuring the accuracy of SLAM algorithms</article-title>. <source>Autonomous Robots</source>. <year>2009</year>;<volume>27</volume>(<issue>4</issue>):<fpage>387</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10514-009-9155-6" xlink:type="simple">10.1007/s10514-009-9155-6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006406.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Giannakopoulos</surname> <given-names>T</given-names></name>. <article-title>pyAudioAnalysis: An Open-Source Python Library for Audio Signal Analysis</article-title>. <source>PLOS ONE</source>. <year>2015</year> <day>12</day>;<volume>10</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0144610" xlink:type="simple">10.1371/journal.pone.0144610</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>