<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01618</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007012</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Fluorescence imaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Condensed matter physics</subject><subj-group><subject>Anisotropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Materials science</subject><subj-group><subject>Material properties</subject><subj-group><subject>Anisotropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Microscopy</subject><subj-group><subject>Electron microscopy</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Equipment</subject><subj-group><subject>Optical equipment</subject><subj-group><subject>Prisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Microscopy</subject><subj-group><subject>Light microscopy</subject><subj-group><subject>Fluorescence microscopy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>DoGNet: A deep architecture for synapse detection in multiplexed fluorescence images</article-title>
<alt-title alt-title-type="running-head">DoGNet</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4264-8572</contrib-id>
<name name-style="western">
<surname>Kulikov</surname> <given-names>Victor</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Guo</surname> <given-names>Syuan-Ming</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Stone</surname> <given-names>Matthew</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Goodman</surname> <given-names>Allen</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Carpenter</surname> <given-names>Anne</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bathe</surname> <given-names>Mark</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lempitsky</surname> <given-names>Victor</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>CDISE, Skoltech, Moscow, Russian Federation</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Imaging Platform, Broad Institute of Harvard and MIT, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Vogelstein</surname> <given-names>Joshua</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Johns Hopkins University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">v.kulikov@skoltech.ru</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>5</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>13</day>
<month>5</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>5</issue>
<elocation-id>e1007012</elocation-id>
<history>
<date date-type="received">
<day>18</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>8</day>
<month>4</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Kulikov et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007012"/>
<abstract>
<p>Neuronal synapses transmit electrochemical signals between cells through the coordinated action of presynaptic vesicles, ion channels, scaffolding and adapter proteins, and membrane receptors. In situ structural characterization of numerous synaptic proteins simultaneously through multiplexed imaging facilitates a bottom-up approach to synapse classification and phenotypic description. Objective automation of efficient and reliable synapse detection within these datasets is essential for the high-throughput investigation of synaptic features. Convolutional neural networks can solve this generalized problem of synapse detection, however, these architectures require large numbers of training examples to optimize their thousands of parameters. We propose DoGNet, a neural network architecture that closes the gap between classical computer vision blob detectors, such as Difference of Gaussians (DoG) filters, and modern convolutional networks. DoGNet is optimized to analyze highly multiplexed microscopy data. Its small number of training parameters allows DoGNet to be trained with few examples, which facilitates its application to new datasets without overfitting. We evaluate the method on multiplexed fluorescence imaging data from both primary mouse neuronal cultures and mouse cortex tissue slices. We show that DoGNet outperforms convolutional networks with a low-to-moderate number of training examples, and DoGNet is efficiently transferred between datasets collected from separate research groups. DoGNet synapse localizations can then be used to guide the segmentation of individual synaptic protein locations and spatial extents, revealing their spatial organization and relative abundances within individual synapses. The source code is publicly available: <ext-link ext-link-type="uri" xlink:href="https://github.com/kulikovv/dognet" xlink:type="simple">https://github.com/kulikovv/dognet</ext-link>.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Multiplexed fluorescence imaging of synaptic proteins facilitates high throughput investigations in neuroscience and drug discovery. Currently, there are several approaches to synapse detection using computational image processing. Unsupervised techniques rely on the <italic>a priori</italic> knowledge of synapse properties, such as size, intensity, and co-localization of synapse markers in each channel. For each experimental replicate, these parameters are typically tuned manually in order to obtain appropriate results. In contrast, supervised methods like modern convolutional networks require massive amounts of manually labeled data, and are sensitive to signal/noise ratios. As an alternative, here we propose DoGNet, a neural architecture that closes the gap between classical computer vision blob detectors, such as Difference of Gaussians (DoG) filters, and modern convolutional networks. This approach leverages the strengths of each approach, including automatic tuning of detection parameters, prior knowledge of the synaptic signal shape, and requiring only several training examples. Overall, DoGNet is a new tool for blob detection from multiplexed fluorescence images consisting of several up to dozens of fluorescence channels that requires minimal supervision due to its few input parameters. It offers the ability to capture complex dependencies between synaptic signals in distinct imaging planes, acting as a trainable frequency filter.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100007455</institution-id>
<institution>Skolkovo Institute of Science and Technology</institution>
</institution-wrap>
</funding-source>
<award-id>NGP Program</award-id>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>R01 MH112694</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Guo</surname> <given-names>Syuan-Ming</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>R35 GM122547</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Carpenter</surname> <given-names>Anne</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the Skoltech NGP Program (MIT-Skoltech 1911/R). Funding from NSF PHY 1707999 to Matthew Stone and Mark Bathe, NIH U01-MH106011 and R01-MH112694 to Syuan-Ming Guo and Mark Bathe, and NIH R35 GM122547 to Allen Goodman and Anne Carpenter is gratefully acknowledged. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="4"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-23</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The source code is publicly available: <ext-link ext-link-type="uri" xlink:href="https://github.com/kulikovv/dognet" xlink:type="simple">https://github.com/kulikovv/dognet</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Neuronal synapses are the fundamental sites of electrochemical signal transmission within the brain that underlie learning and memory. The protein compositions within both presynaptic and postsynaptic synaptic densities crucially determine the stability and transmission sensitivity of individual synapses [<xref ref-type="bibr" rid="pcbi.1007012.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref002">2</xref>]. The analysis of synapse protein abundances, localizations, and morphologies offers better understanding of neuronal function, as well as ultimately psychiatric and neurological diseases [<xref ref-type="bibr" rid="pcbi.1007012.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref004">4</xref>]. However, the high spatial density and structural complexity of synapses both <italic>in vitro</italic> and <italic>in vivo</italic> requires new computational tools for the objective and efficient identification and structural profiling of diverse populations of synapses.</p>
<p>Fluorescence microscopy (FM) combines molecular discrimination with high-throughput, low-cost image acquisition of large fields of view of neuronal synapses within intact specimens using modern confocal imaging instruments. Immunostaining techniques [<xref ref-type="bibr" rid="pcbi.1007012.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>] can be used to identify synapses as puncta within fluorescence microscopy images to distinguish distinct types of synapses based on molecular composition. However, phenotypic classification of individual synapses in FM images is challenging because of the morphological complexities of variable structural features of synapses, including synaptic boutons, presynaptic vesicles, and synaptic clefts, which cannot be resolved using conventional light microscopy.</p>
<p>Manual synapse detection and classification quickly becomes intractable for even moderately sized datasets, thus necessitating automated processing. In recent years, deep convolutional neural networks (ConvNets) have become state-of-the-art tools for image classification [<xref ref-type="bibr" rid="pcbi.1007012.ref007">7</xref>] and segmentation [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>], and have been extended to electron microscopy images of neuronal synapses [<xref ref-type="bibr" rid="pcbi.1007012.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref010">10</xref>]. ConvNets, however, requires thousands of learnable parameters and therefore requires a large amount of training data to avoid overfitting. Furthermore, even when sufficient training data is available, ConvNets may fail to generalize to new experimental conditions that result in modified image properties. Both of these factors complicate the use of ConvNets for synapse detection in fluorescence microscopy images, often rendering traditional blob detection techniques such as [<xref ref-type="bibr" rid="pcbi.1007012.ref011">11</xref>] preferable.</p>
<p>In this work, we introduce a new neural network architecture for synapse detection in multiplexed immunofluorescence images. Compared with ConvNets, the new architecture achieves a considerable reduction in the number of learnable parameters by replacing the generic filters of ConvNets with Difference of Gaussians (DoG) filters [<xref ref-type="bibr" rid="pcbi.1007012.ref012">12</xref>].</p>
<p>This replacement is motivated by the fact that in FM images, typical mammalian synapses are close in size to the diffraction limit of light. Consequently, individual synapses are resolved as blobs due to the convolution of the microscope point spread function with the underlying fluorescence labels, and approximately Gaussian [<xref ref-type="bibr" rid="pcbi.1007012.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref014">14</xref>]. DoG filters are known to be good blob detectors and have few parameters. The DoGNet architecture uses multiple trainable DoG filters applied to multiple input channels and, potentially, in a hierarchical way (Deep DoGNets). The parameters of the DoG filters inside DoGNets are trained in an end-to-end fashion together with other network layers. We use linear weights layer to combine the response maps of different DoG filters together into a probabilistic map.</p>
<p>We post-process this probability map in order to estimate the centers of synapses and describe their properties. For each synapse, the output of our system gives the location and the shape of the punctum for each protein marker, with desired confidence level. The complete image processing pipeline is shown <xref ref-type="fig" rid="pcbi.1007012.g001">Fig 1</xref>.</p>
<fig id="pcbi.1007012.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Single layer DoGNet inference pipeline.</title>
<p>Synaptic protein channels from the PRISM [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>] dataset are used as input images. Each channel of the input images are convolved with a number of the Difference of Gaussian filters. This processing is performed using the sigmoid function convolved with (or multipled by) the per-pixel weighted sum of intermediate maps. The DoGNet is trained to predict the probability map for each pixel as belonging to a synapse. Synapses locations and parameters of their proteins (such as average intensities and shapes) are extracted by fitting Gaussians to the intensities of individual proteins in the vicinities of the local maxima of the resulting probability map. The scalebar on the large scale image equals 25 <italic>μ</italic>m (5 <italic>μ</italic>m in the cropped region).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g001" xlink:type="simple"/>
</fig>
<p>We have validated the performance of this new architecture by comparing several variations of DoGNets to popular types of ConvNet architectures including U-Nets [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>] and Fully Convolutional Networks [<xref ref-type="bibr" rid="pcbi.1007012.ref015">15</xref>] for the task of synapse detection. The comparison is performed on four different datasets including a synthetic dataset, an annotated real dataset from previous work [<xref ref-type="bibr" rid="pcbi.1007012.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref017">17</xref>], and another human annotated dataset acquired with PRISM multiplexed imaging [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>]. Apart from outperforming ConvNet architectures, the DoGNet approach achieves accuracy comparable to inter-human agreement on the dataset from [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>]. Finally, we have shown that a DoGNet trained on one correlated Array Tomography and Electron Microscopy dataset can be successfully applied to an Array Tomography (AT) dataset without associated Electron Microscopy images, which may facilitate accurate synapse detection in large datasets where correlated EM data are not available.</p>
<p>Overall, the system is based on the DoGNet detector and a post-processing pipeline that reveals synaptic structure consistent with known synaptic protein localization, and provides a wealth of data for further downstream phenotypic analysis, thereby achieving successful automation of synapse detection in neuronal FM images. Notably, the DoGNet architecture is not specific to such images, and can be applied to other microscopy modalities where objects of interest show a punctate spatial patterning, or where, more generally, a certain image analysis task may be performed via learnable blob detection such as single molecule segmentation in super-resolution microscopy and single particle tracking [<xref ref-type="bibr" rid="pcbi.1007012.ref018">18</xref>], detection of clusters or endosomes in immunofluorescence images [<xref ref-type="bibr" rid="pcbi.1007012.ref019">19</xref>], and detection of puncta in fluorescence in situ hybridization (FISH) datasets [<xref ref-type="bibr" rid="pcbi.1007012.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref021">21</xref>].</p>
<sec id="sec002">
<title>Related work</title>
<p>Automation of synapse detection and large-scale investigation of neuronal organization has seen considerable progress in recent years. Most work has been dedicated to the segmentation of electron microscopy datasets, with modern high-throughput pipelines for automated segmentation and morphological reconstruction of synapses [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1007012.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref023">23</xref>]. Much of this progress may be credited to deep convolutional networks. Segmentation accuracy of these approaches can be increased by making deeper networks [<xref ref-type="bibr" rid="pcbi.1007012.ref024">24</xref>], adding dilated/ a-trous convolution [<xref ref-type="bibr" rid="pcbi.1007012.ref025">25</xref>] or using hourglass architectures [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref026">26</xref>] that include downscaling/upscaling parts with so-called <italic>skip connections</italic>. ConvNets typically outperform random forest and other classical machine learning approaches that are dependent on hand-crafted features such as those proposed in [<xref ref-type="bibr" rid="pcbi.1007012.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref028">28</xref>]. At the same time, while it is possible to reduce the number of training examples needed by splitting the segmentation pipeline into several smaller pipelines [<xref ref-type="bibr" rid="pcbi.1007012.ref010">10</xref>], the challenge of reducnig the number of training parameters without sacrificing segmentation accuracy remains.</p>
<p>Within the context of neuronal immunofluorescence images, synapses are typically defined by the colocalization of pre- and postsynaptic proteins within puncta that have sizes on the order of the diffraction limit of 250 nm. One fully automated method using priors, which quantifies synaptic elements and complete synapses based on pre- and postsynaptic labeling plus a dendritic or cell surface marker, was previously proposed and applied successfully [<xref ref-type="bibr" rid="pcbi.1007012.ref029">29</xref>]. Alternatively, a machine learning approach to synapse detection was proposed in [<xref ref-type="bibr" rid="pcbi.1007012.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref031">31</xref>], where a support vector machine (SVM) was used to estimate the confidence of a pixel being a synapse, depending on a small number of neighboring pixels. Synapse positions were then computed from these confidence values by evaluating local confidence profiles and comparing them with a minimum confidence value. Finally, in [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>], a probabilistic approach to synapse detection on AT volumes was proposed. The principal idea of this approach was to estimate the probability of a pixel being a punctum within each tissue slice, and then calculating the joint distribution of presynapic and postsynapic proteins between neighbouring slices. Our work was mainly inspired by works [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>] and [<xref ref-type="bibr" rid="pcbi.1007012.ref011">11</xref>], that produced the state-of-the-art results in synapse detection on fluorescence images.</p>
<p>More conventional machine vision techniques have also been applied for synapse detection [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref012">12</xref>]. These methods aim at detecting regions that differ in brightness compared with neighboring regions. The most common approach for this task is convolution with a Laplacian filter [<xref ref-type="bibr" rid="pcbi.1007012.ref012">12</xref>]. The Laplacian filter can be computed as the limiting case of the difference between two Gaussian smoothed images. Since convolution with a Gaussian kernel is a linear operation, convolution with the difference of two Gaussian kernels can be used instead of seeking the difference between smooth images. The usage of Difference of Gaussians for synapse detection was proposed in [<xref ref-type="bibr" rid="pcbi.1007012.ref011">11</xref>] with manually defined filter parameters. Here, we introduce a new DoGNet architecture that integrates the use of simple DoG filters for blob detection with machine, deep learning, thereby combining the strengths of the preceding published approaches [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>]. Our approach offers the ability to capture complex dependencies between synaptic signals in distinct imaging planes, acting as a trainable frequency filter.</p>
</sec>
</sec>
<sec id="sec003" sec-type="materials|methods">
<title>Materials and methods</title>
<p>Our synapse puncta detection procedure consists of two steps: an application of the pre-trained DoGNet architecture to imaging planes of the source image and a post-processing of its output. In a nutshell, DoGNet is a standard convolutional neural network with convolution kernels reparametrized using the Difference-of-Gaussians (DoG) as shown in <xref ref-type="fig" rid="pcbi.1007012.g002">Fig 2</xref>. The DoGNet architecture applies a small number of DoG filters to each protein channel and then combines the outputs of the filtering operations. We train that network end-to-end using the backpropagation algorithm [<xref ref-type="bibr" rid="pcbi.1007012.ref033">33</xref>]. Accordingly, we describe the operation of our procedure by first discussing the properties of trainable DoG filters. We then discuss single layer and deep versions of the DoGNet architecture, and the training processes for both. Finally, we present in detail the post-processing procedure.</p>
<fig id="pcbi.1007012.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g002</object-id>
<label>Fig 2</label>
<caption>
<title/>
<p>(a) The architecture of shallow DoGNet. The input image channels (for example synapsin, vGlut, and PSD95) are each processed by five trainable DoG filters. The weighted sum (with trainable weights) combines the resulting 15 DoG layer output maps into a single map. The sigmoid function converts the latter map into a pixel probability map. (b,c,d) The variations of the Difference of Gaussians that we use in each DoG layer. (b) An isotropic Difference of Gaussians. (c) An anisotropic difference of Gaussians. Each Gaussian is described by a pair of variance values and a rotation angle. (d) A 3D Isotropic Difference of Gaussians. Surfaces show filter values along <italic>z</italic> slices.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g002" xlink:type="simple"/>
</fig>
<sec id="sec004">
<title>Difference-of-Gaussians filters</title>
<p>In classical computer vision, the DoG filter is perhaps the most popular operation for blob detection. As follows from its name, DoG filtering corresponds to applying two Gaussian filters to the same real-valued image and then subtracting the results. As the difference between two different low-pass filtered images, the DoG is actually a band-pass filter, which removes high frequency components representing noise as well as some low frequency components representing the background variation of the image. The frequency components in the preserved band are assumed to be associated with the edges and blobs that are of interest. DoG filters are often regarded as approximations to Laplacian-of-Gaussian filters that require more operations to compute.</p>
<p>Depending on the parameterization of the underlying Gaussian filters, DoG filters may vary in their complexity. For example, in the most common case, one considers the difference of two isotropic Gaussian probability distribution functions as the filter kernel:
<disp-formula id="pcbi.1007012.e001"><alternatives><graphic id="pcbi.1007012.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>DoG</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Isotropic</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
This version of the DoG filter depends on four parameters, namely the amplitude coefficients <italic>w</italic><sub>1</sub> and <italic>w</italic><sub>2</sub>, as well as the bandwidth parameters <italic>σ</italic><sub>1</sub> and <italic>σ</italic><sub>2</sub>. The shape of the resulting function is depicted in <xref ref-type="fig" rid="pcbi.1007012.g002">Fig 2(b)</xref>. The amplitudes <italic>w</italic><sub>1</sub> and <italic>w</italic><sub>2</sub> can be replaced by normalizing coefficients 1/2<italic>πσ</italic><sub>1</sub> and 1/2<italic>πσ</italic><sub>2</sub> respectively, reducing the number of trainable parameters to just two.</p>
<p>The four- and the two-parameter DoG filters described above are suitable for detecting isotropic blobs. For anisotropic blob detection, pairs of anisotropic Gaussians with zero means and shared orientations may be more suitable. In this case, we parameterize an anisotropic zero-mean Gaussian as:
<disp-formula id="pcbi.1007012.e002"><alternatives><graphic id="pcbi.1007012.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>w</mml:mi><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>a</mml:mi> <mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>b</mml:mi> <mml:mi>x</mml:mi> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where for an orientation angle <italic>α</italic> ∈ [0; <italic>π</italic>) the coefficients <italic>a</italic>, <italic>b</italic>, <italic>c</italic> are defined as:
<disp-formula id="pcbi.1007012.e003"><alternatives><graphic id="pcbi.1007012.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>α</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">sin</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>α</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula> <disp-formula id="pcbi.1007012.e004"><alternatives><graphic id="pcbi.1007012.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>b</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>α</mml:mi></mml:mrow></mml:mrow> <mml:mrow><mml:mn>4</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>α</mml:mi></mml:mrow></mml:mrow> <mml:mrow><mml:mn>4</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1007012.e005"><alternatives><graphic id="pcbi.1007012.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">sin</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>α</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>α</mml:mi></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
The anisotropic DoG filter is then defined as:
<disp-formula id="pcbi.1007012.e006"><alternatives><graphic id="pcbi.1007012.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>DoG</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Ansotropic</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>α</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
We refer to the DoG filter (<xref ref-type="disp-formula" rid="pcbi.1007012.e006">6</xref>) as the <italic>Anisotropic</italic> or <italic>seven-parameter DoG filter</italic> based on the number of associated parameters. The <italic>five-parameter DoG filter</italic> can be obtained by fixing the constants <italic>w</italic><sub>1</sub> and <italic>w</italic><sub>2</sub> to be normalizing, i.e. <inline-formula id="pcbi.1007012.e007"><alternatives><graphic id="pcbi.1007012.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:msqrt><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. An example of anisotropic Difference of Gaussians is depicted in <xref ref-type="fig" rid="pcbi.1007012.g002">Fig 2(c)</xref>. The usage of anisotropic difference of Gaussians allows detecting different kinds of elongated blobs with only three additional trainable parameters per filter (compared to the two- or four-parameter versions).</p>
<p>Overall, DoG filters provide a simple way to parameterize blob-detecting linear filters using a small number of parameters. They can also be extended to three-dimensional blob detection in a straightforward manner. Since in three dimensions generic linear filters come with an even larger number of parameters, the use of DoG parameterization is even better justified. Here, one natural choice would be to use differences of Gaussian filters that are isotropic within axial slices and use a different variance (bandwidth) along the axial dimensions:
<disp-formula id="pcbi.1007012.e008"><alternatives><graphic id="pcbi.1007012.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>w</mml:mi><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>z</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
<disp-formula id="pcbi.1007012.e009"><alternatives><graphic id="pcbi.1007012.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>DoG</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>3D</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
Generally, as axial resolution in 3D fluorescence microscopy is typically lower, <italic>σ</italic><sub><italic>i</italic>,<italic>z</italic></sub> is also taken to be larger than <italic>σ</italic><sub><italic>i</italic></sub>. The filter (<xref ref-type="disp-formula" rid="pcbi.1007012.e009">8</xref>) provides a six-parameter parameterization of a family of 3D blob detection filters (one of which is visualized in <xref ref-type="fig" rid="pcbi.1007012.g002">Fig 2(d)</xref>), whereas a generic 3D filter takes <italic>O</italic>(<italic>d</italic><sup>3</sup>) parameters, where <italic>d</italic> is the spatial window size.</p>
</sec>
<sec id="sec005">
<title>“Shallow” DoGNet</title>
<p>The shallow (single layer) Difference of Gaussians network (DoGNet) is a neural network built around DoG filters <xref ref-type="fig" rid="pcbi.1007012.g002">Fig 2(a)</xref>. It takes as an input a multiplexed fluorescence image, applies multiple DoG filters (<xref ref-type="disp-formula" rid="pcbi.1007012.e001">1</xref>),(<xref ref-type="disp-formula" rid="pcbi.1007012.e006">6</xref>) or (<xref ref-type="disp-formula" rid="pcbi.1007012.e009">8</xref>) to each of the input channels. Subsequently, DoGNet combines the obtained maps linearly (which in deep learning terminology corresponds to applying 1 × 1 convolution). The latter step obtains a single map of the same spatial resolution as the input image. Finally, a sigmoid non-linearity is applied to convert the applied maps into probability maps.</p>
<p>More formally, we define a single-layer DoGNet as
<disp-formula id="pcbi.1007012.e010"><alternatives><graphic id="pcbi.1007012.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>β</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>⊛</mml:mo> <mml:mi>D</mml:mi> <mml:mi>o</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mi>β</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊛</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>X</italic> denotes the input multiplexed image, ⊛ is the 2D convolution operation, and the vector <italic>β</italic> denotes the parameters of all DoG filters. Assuming that the input contains <italic>N</italic> channels, and each channel is filtered with <italic>M</italic> DoG filters, the application of all DoG results in <italic>M</italic> × <italic>N</italic> maps. Those maps are then combined into <italic>K</italic> maps using a pixel-wise linear operation (which can be treated as a convolution with 1 × 1 filters). The tensor corresponding to such linear combination and containing <italic>K</italic> × <italic>M</italic> × <italic>N</italic> values is denoted <italic>γ</italic>.</p>
<p>To each of the obtained <italic>K</italic> maps, the bias value <italic>ζ</italic><sub><italic>k</italic></sub> is added, and finally all obtained values are passed through the element-wise sigmoid non-linearity <italic>S</italic>(<italic>x</italic>) = 1/(1 + exp(−<italic>x</italic>)). Overall, <italic>θ</italic> in (<xref ref-type="disp-formula" rid="pcbi.1007012.e010">9</xref>) denotes all learnable parameters of the DoGNet.</p>
<p>In the case of the single-layer DoGNet, the output has a single map (i.e. <italic>K</italic> = 1). Except for the last sigmoid operation, the single-layer DoGNet contains only linear operations and can be regarded as a special parameterization of the linear filtering operator that maps the input <italic>M</italic> maps to several output maps, usually two maps.</p>
</sec>
<sec id="sec006">
<title>Deep DoGNet</title>
<p>The deep DoGNet architecture is obtained simply by stacking multiple DoGNet layers (<xref ref-type="disp-formula" rid="pcbi.1007012.e010">9</xref>):
<disp-formula id="pcbi.1007012.e011"><alternatives><graphic id="pcbi.1007012.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>…</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>T</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>…</mml:mo> <mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>…</mml:mo> <mml:mo>;</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>T</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>T</italic> is the number of stacked single layers DoGNets, and <italic>θ</italic><sub><italic>t</italic></sub> denotes the learnable parameters of the <italic>t</italic>-th layer. The final number of maps <italic>K</italic><sub><italic>T</italic></sub> is once again set to one, so that the whole network outputs a single probability map. However, the numbers of layers <italic>K</italic><sub><italic>t</italic></sub> that are output by the intermediate DoGNet layers would typically be greater than one. In our experiments the number of sequential layers <italic>T</italic> was set to three.</p>
</sec>
<sec id="sec007">
<title>Element-wise multiplication</title>
<p>Inspired by an idea from [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>], instead of producing a single probability map, our network delivers two independent maps and using the element-wise product of those maps we get the final map. We have implemented this approach as a separate layer and that does not require any trainable parameters. In the case of synapses, this step allows reducing the effect of displacement between pre- and postsynaptic punctae by learning probability maps independently for pre and postsynaptic signals. Given several probability maps (for pre- and postsynaptic punctae) the element-wise products will act as a logical operator “AND,” highlighting the intersection between those maps, where the synaptic cleft is located. In our research we use element-wise multiplication not only for DoGNets but for baselines as well, they all benefit from this layers.</p>
</sec>
<sec id="sec008">
<title>DoGNet initialization</title>
<p>We have found that appropriate parameter initialization is key to obtaining reproducible results with our approach. Popular neural networks have a redundant number of parameters and are initialized by sampling their values from a Gaussian distribution. This initialization is not suitable for DoGNets because of the relatively small number of parameters. Instead, we use a strategy from object detection frameworks [<xref ref-type="bibr" rid="pcbi.1007012.ref034">34</xref>]. This approach consists of initialization with a range of reasonable states (priors). An optimization procedure selects the best priors and tunes their parameters. In DoGNet we use Laplacian of Gaussians with different sizes that are sampled from a regular grid as priors. Specifically, we obtain the Gaussian variance (<italic>sigma</italic>) by splitting the line segment [0.5, 2] into equal parts. The number of parts depends on the number of DoGs reserved for each image plane (in our experiments that number was set to five). We set the difference-variance in the Laplacian of Gaussians to 0.01. For example, if we set the number of DoGs for a channel to 3, the sigmas will be 0.5, 1.25, and 2, respectively.</p>
</sec>
<sec id="sec009">
<title>Training DoGNets</title>
<p>We train the described architecture by minimizing the <italic>softdice</italic> loss (<xref ref-type="disp-formula" rid="pcbi.1007012.e012">11</xref>) proposed in [<xref ref-type="bibr" rid="pcbi.1007012.ref035">35</xref>] between the predicted probability map Ψ(<italic>X</italic>; <italic>θ</italic>) and a ground truth mask <italic>Y</italic><sub><italic>g</italic></sub>:
<disp-formula id="pcbi.1007012.e012"><alternatives><graphic id="pcbi.1007012.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007012.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mfrac><mml:mrow><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>Y</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>Ψ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow> <mml:mrow><mml:mo>∑</mml:mo> <mml:msup><mml:mrow><mml:mo>Ψ</mml:mo> <mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mo>∑</mml:mo> <mml:msup><mml:mrow><mml:msub><mml:mi>Y</mml:mi> <mml:mi>g</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Here, sums are taken over individual pixels, and in the ground-truth map <italic>Y</italic><sub><italic>g</italic></sub> all pixels belonging to synapses are marked with ones, while the background pixels are marked with zeros. In the experiments we found that on the imbalanced data typical for synapse detection problems, this loss performs better than standard binary cross entropy.</p>
<p>In order to optimize this loss function, partial derivatives with respect to DoGNet parameters <italic>dL</italic>/<italic>dθ</italic> must be obtained, which may be accomplished via backpropagation [<xref ref-type="bibr" rid="pcbi.1007012.ref033">33</xref>]. The backpropagation process computes the partial derivatives with respect to the filter parameters at each of the spatial positions within the spatial support of the filter (which we limit to 15 pixels). The partial derivatives with respect to the DoGNet parameters are then obtained by differentiating formulas (<xref ref-type="disp-formula" rid="pcbi.1007012.e001">1</xref>),(<xref ref-type="disp-formula" rid="pcbi.1007012.e006">6</xref>) or (<xref ref-type="disp-formula" rid="pcbi.1007012.e009">8</xref>) at each spatial location and multiplying by the respective derivatives.</p>
<p>The ground truth mask <italic>Y</italic><sub><italic>g</italic></sub> as well as the input images <italic>X</italic> for the training process are obtained using a combination of manual annotation and artificial augmentation. The synapse detection in FM images is a challenging and arguably ambiguous task even for human experts. Furthermore, even a small, 100 × 100 pixel region of an image might contain more than 80 synapses. In practice it is impossible to annotate the borders of each synapse accurately, therefore the experts were asked to mark the centroid of synapses only, corresponding to the synaptic cleft, after which all pixels within a radius of 0.8<italic>μm</italic> were assigned to the corresponding synapse. We trained DoGNets for 5000 epochs. Each epoch is a set of ten randomly cropped subsamples 64 × 64 from the annotated training dataset. Because DoGNets have few parameters, we found that the training processes converged rapidly typically requiring only several minutes on an NVidia Titan-X GPU for the datasets described below. Once trained, inference can be performed on a CPU as well as on a GPU using the implementations of Gaussian filtering that may be optimized for a particular computing architecture. Our implementation uses the PyTorch deep learning framework [<xref ref-type="bibr" rid="pcbi.1007012.ref036">36</xref>], which allows for concise code and benefits from automatic differentiation routines.</p>
</sec>
<sec id="sec010">
<title>Post-processing</title>
<p>Because both shallow and deep versions of DoGNet produce probability maps rather than lists of synapse locations and parameters, these probability maps need to be postprocessed in order to identify synapse locations and properties. Toward this end, first, we reject points with low confidence by truncating the probability maps using a threshold of <italic>τ</italic> of 0.5. In order to extract synapse locations from the probability map produced by the DoGNet, we need to find local maxima. In standard fashion, we greedily pick local maxima in the probability map, traversing them in the order of decreasing probability values while suppressing all maxima within a cut-off radius <italic>R</italic> = 1.6<italic>μm</italic> from previously identified maxima (so called <italic>non-maxima suppression</italic>) [<xref ref-type="bibr" rid="pcbi.1007012.ref037">37</xref>]. The output of this procedure is the <italic>x</italic> and <italic>y</italic> locations of synaptic puncta.</p>
<p>The next step is to describe each detected punctum with a vector containing the information about the detected synapse. To obtain a descriptor for a synapse, we select a small window of the same radius <italic>R</italic> = 1.6<italic>μm</italic> around its location, fit Gaussian distributions to each of the input channels, and for each protein marker we store the average intensity, the displacement of the Gaussian mean with respect to the window center, the Gaussian orientation, and its asymmetry. Evaluating the quality of such a descriptor is left for future work.</p>
</sec>
</sec>
<sec id="sec011" sec-type="results">
<title>Results</title>
<sec id="sec012">
<title>Datasets</title>
<p>The proposed method and a set of baselines were evaluated on four independent datasets for which synapses were annotated manually: <italic>[Collman15]</italic> dataset of conjugate array tomography (cAT) images [<xref ref-type="bibr" rid="pcbi.1007012.ref016">16</xref>], <italic>[Weiler14]</italic> dataset of array tomography (AT) images [<xref ref-type="bibr" rid="pcbi.1007012.ref017">17</xref>], <italic>[PRISM]</italic> dataset of multiplexed confocal microscopy images [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>], and a synthetic dataset that we generate here. Each published experimental dataset was obtained using fluorescence imaging based on commercially available antibodies, with synapsin, vGlut, and PSD-95 markers common to the datasets. At the end of section, we additionally perform comparisons using synthetic dataset with excitatory and inhibitory synapse sub-types.</p>
</sec>
<sec id="sec013">
<title>Compared methods</title>
<p>In each of our trials we compared several DoGNet configurations with several baseline methods including reduced version of the fully convolutional network (FCN) [<xref ref-type="bibr" rid="pcbi.1007012.ref015">15</xref>], and an encoder-decoder network with skip connections (U-net) [<xref ref-type="bibr" rid="pcbi.1007012.ref008">8</xref>]. An exhaustive comparison between different deep architectures is a nearly impossible task, mostly because of an infinite number of possible configurations. Nevertheless, we have done our best to tune the parameters of the baseline methods. The best-performing variants of the baseline architectures (FCN, Unet) were used in the experiments and are described in detail in the supplementary material. To make our evaluation more direct, we have designed the competitive networks to have the same receptive field (FOV) (arbitrarily chosen to 15 pixels). We have also evaluated two <italic>manually-tuned</italic> methods, namely the probabilistic synapse detection method [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>] and the image processing pipeline proposed in [<xref ref-type="bibr" rid="pcbi.1007012.ref038">38</xref>]. Detailed technical background on these architectures are described in supplementary materials.</p>
<p>The DoGNet architecture has two major options: Shallow and Deep, with the Shallow option corresponding to a single layer and the Deep option corresponding to number of sequential layers. The second word in our notation <italic>Isotropic</italic> or <italic>Anisotropic</italic> indicates the number of degrees of freedom in the DoG parameterization, e.g. Isotropic denotes four-degree DoG (<xref ref-type="disp-formula" rid="pcbi.1007012.e001">1</xref>). The number of DoG filters for each channel was arbitrary set to five. We also evaluated a simple ablation denoted as <italic>Direct</italic> that takes the Shallow Isotropic DoGNet architecture and replaces DoG-parameterized filters with 15 × 15 unconstrained filters (thus using Direct parameterization)(see Supplementary Information).</p>
</sec>
<sec id="sec014">
<title>Error metrics</title>
<p>The quality of synapse detection was estimated using the standard metrics: precision, recall, and F1-score, with the output of each method consisting of the set of points denoting synapse coordinates. True positives were estimated as the number of paired points between annotation and detection provided the distance between them was less than half of the mean synapse radius (<italic>ρ</italic> = 0.6<italic>μm</italic>). To avoid multiple detections of synapses (false positives), we require that each detected point can be matched at most once. Detections and annotations without pairs were considered to be false positives and false negatives, respectively. The <italic>precision</italic> measure was then computed as the ratio of true positives to all positives, and the <italic>recall</italic> measure as the ratio of true positives to all synapses contained in the annotation. The F1-score combines the precision and recall in one criterion by taking the double product of recall and precision divided over their sum. For evaluation purposes, we also added the AUC criterion corresponding to the area under the ROC curve obtained by varying the confidence threshold <italic>τ</italic>. This criterion is stable to the threshold choice and depends on the quality of the probability map produced by a method. For different thresholds, we estimated the conjunctions between probability map and ground truth binary segmentation pixel-wise.</p>
<p>For quantitative comparison, we have also used the absolute difference in counting (|<italic>DiC</italic>|). This metric merely computes the difference between the number of synapses detected using a method and the ground truth. This measure does not answer the question of how well a synapse was localized but still gives additional insight into quantitative results.</p>
<p>Since the training procedure is a probabilistic process depending on initialization and data sampling, we estimate each value as the mean of five independent runs.</p>
</sec>
<sec id="sec015">
<title>Results on PRISM dataset</title>
<p>To verify our method on PRISM data [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>], we performed manual dense annotation of several image regions of a dataset of FM images obtained using this technique. The manual annotation was performed by two experts using synapsin, vGlut, Bassoon and PSD-95 channels. Each expert annotated three regions. The total set was made of six regions and split into training, validation (392 synaptic locations) and testing subsets (173 synaptic locations). Each subset consisted of two regions annotated by different experts, with test regions overlapped in order to estimate inter-expert agreement. For synapse annotation, we developed a graphical user interface. This software allows selecting image channels and regions. As we solve the task of semantic segmentation during the training, we need a densely annotated image region. We mark each synapse with a point approximately at the synaptic cleft.</p>
<p>Evaluation against baselines is presented in <xref ref-type="table" rid="pcbi.1007012.t001">Table 1</xref>. Due to circular puncta shape and the relatively small displacement of markers, the optimal method was Shallow Anisotropic with only 107 trainable parameters. This configuration also performed considerably better than the Direct Ablation approach, highlighting the advantage of using DoG parameterization in place of direct parameterization of the filters.</p>
<table-wrap id="pcbi.1007012.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.t001</object-id>
<label>Table 1</label>
<caption>
<title>Comparison of several variations of DoGNets and several baselines on PRISM dataset.</title>
</caption>
<alternatives>
<graphic id="pcbi.1007012.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="center"># params</th>
<th align="center">F1 Score</th>
<th align="center">Precision</th>
<th align="center">Recall</th>
<th align="center">AUC</th>
<th align="center">|<italic>DiC</italic>|</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="7"><bold>ConvNets</bold></td>
</tr>
<tr>
<td align="left">Direct</td>
<td align="center">3392</td>
<td align="char" char=".">0.74</td>
<td align="char" char=".">0.66</td>
<td align="char" char=".">0.84</td>
<td align="char" char=".">0.85</td>
<td align="char" char=".">17.67</td>
</tr>
<tr>
<td align="left">FCN</td>
<td align="center">3002</td>
<td align="char" char=".">0.75</td>
<td align="char" char=".">0.73</td>
<td align="char" char=".">0.77</td>
<td align="char" char=".">0.84</td>
<td align="char" char=".">7.44</td>
</tr>
<tr>
<td align="left">Unet</td>
<td align="center">622</td>
<td align="char" char=".">0.80</td>
<td align="char" char=".">0.78</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">10.44</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>DoGNets</bold></td>
</tr>
<tr>
<td align="left">Shallow Isotropic</td>
<td align="center">62</td>
<td align="char" char=".">0.78</td>
<td align="char" char=".">0.72</td>
<td align="char" char=".">0.87</td>
<td align="char" char=".">0.91</td>
<td align="char" char=".">15.22</td>
</tr>
<tr>
<td align="left">Shallow Anisotropic</td>
<td align="center">107</td>
<td align="char" char="."><bold>0.83</bold></td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.86</td>
<td align="char" char=".">0.91</td>
<td align="char" char=".">4.89</td>
</tr>
<tr>
<td align="left">Deep Isotropic</td>
<td align="center">140</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.82</td>
<td align="char" char=".">0.89</td>
<td align="char" char=".">9.78</td>
</tr>
<tr>
<td align="left">Deep Anisotropic</td>
<td align="center">230</td>
<td align="char" char=".">0.80</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.80</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">7.89</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>Manually tuned methods</bold></td>
</tr>
<tr>
<td align="left">Nieland 2014 [<xref ref-type="bibr" rid="pcbi.1007012.ref038">38</xref>]</td>
<td align="center">-</td>
<td align="char" char=".">0.78</td>
<td align="char" char=".">0.72</td>
<td align="char" char=".">0.84</td>
<td align="char" char=".">0.82</td>
<td align="char" char=".">1.</td>
</tr>
<tr>
<td align="left">Simhal 2017 [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>]</td>
<td align="center">-</td>
<td align="char" char=".">0.50</td>
<td align="char" char=".">0.45</td>
<td align="char" char=".">0.58</td>
<td align="char" char=".">0.68</td>
<td align="char" char=".">21.</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We performed several analyses in order to evaluate agreement between three independent human experts as well as between the experts and our method (<xref ref-type="table" rid="pcbi.1007012.t002">Table 2</xref>). Importantly, the proposed network agreed with the Experts similarly to the agreement between the Experts themselves.</p>
<table-wrap id="pcbi.1007012.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.t002</object-id>
<label>Table 2</label>
<caption>
<title>Agreement between DoGNet and three independent human experts on the task of synapse detection on the PRISM dataset.</title>
</caption>
<alternatives>
<graphic id="pcbi.1007012.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Trial</th>
<th align="center">F1 Score</th>
<th align="center">Precision</th>
<th align="center">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Shallow Isotropic vs Expert 1</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">0.84</td>
</tr>
<tr>
<td align="left">Shallow Isotropic vs Expert 2</td>
<td align="char" char=".">0.87</td>
<td align="char" char=".">0.91</td>
<td align="char" char=".">0.83</td>
</tr>
<tr>
<td align="left">Shallow Isotropic vs Expert 3</td>
<td align="char" char=".">0.86</td>
<td align="char" char=".">0.90</td>
<td align="char" char=".">0.83</td>
</tr>
<tr>
<td align="left">Expert 1 vs Expert 2</td>
<td align="char" char=".">0.82</td>
<td align="char" char=".">0.86</td>
<td align="char" char=".">0.78</td>
</tr>
<tr>
<td align="left">Expert 3 vs Expert 2</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.8</td>
</tr>
<tr>
<td align="left">Expert 3 vs Expert 1</td>
<td align="char" char=".">0.77</td>
<td align="char" char=".">0.79</td>
<td align="char" char=".">0.8</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec016">
<title>Results on Collman15 dataset</title>
<p>In this dataset, the alignment of electron microscopy (EM) and array tomography (AT) images provides the ground truth for synapse detection using fluorescence markers. Using high resolution EM data synaptic clefts and pre- versus post- synaptic sites can be identified unambiguously, which was used as validation for the synapse detections from fluorescence data (<xref ref-type="fig" rid="pcbi.1007012.g003">Fig 3(a)</xref>). The dataset contains 27 slices of 6310 × 4520 pixels each, with a resolution of 2.23 × 2.23 × 70 <italic>nm</italic>, and contains annotation with pixel-level segmentation of synaptic clefts. In order to fit our training procedure, we have used only synaptic cleft centroid coordinates. The EM resolution is much greater, so AT data were interpolated to be aligned with EM data. Provided we utilize solely AT data, its original resolution of 0.1<italic>μm</italic> per pixel can be recovered without losing any information. The first five slices were used as the train dataset, whereas the remainder (slices 6-27) served as the test dataset.</p>
<fig id="pcbi.1007012.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Results of DoGNet synapse detection on distinct datasets.</title>
<p>Yellow arrows denote synapse orientation from presynaptic to postsynaptic sides. (a) The Collman15 dataset is a mixture of EM and FM images (EM is shown in grayscale, the red, green, and blue channels show the intensity of synapsin, vGlut, and PSD95 respectively). (b) The PRISM dataset. False color scheme has red channel corresponding to synapsin, blue to PSD95, and green to the cytoskeletal marker MAP2, which indicates how synapses are distributed along microtubules. (c) The Weiler14 dataset. The red, green, and blue channels show the intensity of synapsin, vGlut, and PSD95, respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g003" xlink:type="simple"/>
</fig>
<p>Results of our evaluation (<xref ref-type="table" rid="pcbi.1007012.t003">Table 3</xref>) show that shallow DoGNets exhibit highest performance in terms of the F1-measure. The receptive field 15 × 15 pixels followed by inter-channel element-wise multiplication allow capturing highly displaced markers puncta combinations. Displacements in marker punctae occur because synapses are 3D objects with random orientations. Therefore, the presynaptic and postsynaptic signals in the image plane produce displaced peaks up to a half of a micron. The closest-performing ConvNet architecture was U-net with 622 trainable parameters; increasing the number of its parameters led to overfitting and therefore lower performance on the test dataset examined here.</p>
<table-wrap id="pcbi.1007012.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.t003</object-id>
<label>Table 3</label>
<caption>
<title>Comparison of several variations of DoGNets and several baselines on the [Collman15] dataset.</title>
<p>The ‘Shallow3D’ network uses the 3D version of DoGNet, while other variants operate on 2D slices independently. Optimal performance was obtained using Shallow DoGNets.</p>
</caption>
<alternatives>
<graphic id="pcbi.1007012.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">params</th>
<th align="center">F1 Score</th>
<th align="center">Precision</th>
<th align="center">Recall</th>
<th align="center">AUC</th>
<th align="center">|<italic>DiC</italic>|</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="7"><bold>ConvNets</bold></td>
</tr>
<tr>
<td align="left">Direct</td>
<td align="center">3392</td>
<td align="char" char=".">0.69</td>
<td align="char" char=".">0.79</td>
<td align="char" char=".">0.62</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">11.19</td>
</tr>
<tr>
<td align="left">FCN</td>
<td align="center">3002</td>
<td align="char" char=".">0.71</td>
<td align="char" char=".">0.72</td>
<td align="char" char=".">0.70</td>
<td align="char" char=".">0.79</td>
<td align="char" char=".">4.12</td>
</tr>
<tr>
<td align="left">Unet</td>
<td align="center">622</td>
<td align="char" char=".">0.73</td>
<td align="char" char=".">0.73</td>
<td align="char" char=".">0.73</td>
<td align="char" char=".">0.91</td>
<td align="char" char=".">4.26</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>DoGNets</bold></td>
</tr>
<tr>
<td align="left">Shallow Isotropic</td>
<td align="center">62</td>
<td align="char" char="."><bold>0.75</bold></td>
<td align="char" char=".">0.74</td>
<td align="char" char=".">0.76</td>
<td align="char" char=".">0.90</td>
<td align="char" char=".">4.25</td>
</tr>
<tr>
<td align="left">Shallow Anisotropic</td>
<td align="center">107</td>
<td align="char" char="."><bold>0.75</bold></td>
<td align="char" char=".">0.75</td>
<td align="char" char=".">0.76</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">4.26</td>
</tr>
<tr>
<td align="left">Shallow3D</td>
<td align="center">61</td>
<td align="char" char=".">0.68</td>
<td align="char" char=".">0.62</td>
<td align="char" char=".">0.77</td>
<td align="char" char=".">0.65</td>
<td align="char" char=".">9.13</td>
</tr>
<tr>
<td align="left">Deep Isotropic</td>
<td align="center">140</td>
<td align="char" char=".">0.73</td>
<td align="char" char=".">0.77</td>
<td align="char" char=".">0.71</td>
<td align="char" char=".">0.97</td>
<td align="char" char=".">4.99</td>
</tr>
<tr>
<td align="left">Deep Anisotropic</td>
<td align="center">230</td>
<td align="char" char=".">0.71</td>
<td align="char" char=".">0.77</td>
<td align="char" char=".">0.33</td>
<td align="char" char=".">0.87</td>
<td align="char" char=".">7.72</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>Manually tuned methods</bold></td>
</tr>
<tr>
<td align="left">Nieland 2014 [<xref ref-type="bibr" rid="pcbi.1007012.ref038">38</xref>]</td>
<td align="center">-</td>
<td align="char" char=".">0.37</td>
<td align="char" char=".">0.49</td>
<td align="char" char=".">0.32</td>
<td align="char" char=".">0.63</td>
<td align="char" char=".">16.5</td>
</tr>
<tr>
<td align="left">Simhal 2017 [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>]</td>
<td align="center">-</td>
<td align="char" char=".">0.65</td>
<td align="char" char=".">0.52</td>
<td align="char" char="."><bold>0.89</bold></td>
<td align="char" char=".">0.74</td>
<td align="center">-</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The AT stains include markers specific for excitatory (vGlut, PSD95) and inhibitory (GABAergic, gephyrin) synapses. In our experiments, the use of inhibitory markers did not improve the detection scores. Moreover, the precision of all trainable methods was considerably lower using only inhibitory markers (synapsin, GABA, gephyrin).</p>
</sec>
<sec id="sec017">
<title>Results on Weiler dataset</title>
<p>The Weiler dataset [<xref ref-type="bibr" rid="pcbi.1007012.ref017">17</xref>] consists of 12 different neural tissue samples. Each sample was stained with a number of distinct antibodies including synapsin vGlut, and PSD-95. For each stain, 70 aligned slices were acquired using array tomography (AT). Each slice was a 3164 × 1971 pixel image with spatial resolution of 0.2<italic>μm</italic> per-pixel. This dataset does not have any published annotation.</p>
<p>We investigated the ability of DoGNets to generalize across distinct datasets by applying networks trained on the well-annotated [Collman15] dataset, which was annotated using serial electron microscopy data, to the previously unlabeled AT dataset [Weiler14] [<xref ref-type="bibr" rid="pcbi.1007012.ref017">17</xref>]. Generally, the staining of [<xref ref-type="bibr" rid="pcbi.1007012.ref017">17</xref>] is similar to the Collman15 dataset [<xref ref-type="bibr" rid="pcbi.1007012.ref016">16</xref>]. Thus, we first performed a coarse alignment by resizing [Collman15] images and applying linear transforms to the intensities of each channel so that the magnification factors, means, and standard deviations of the intensity distributions were matched. The architectures trained on [Collman15] were then evaluated on [Weiler14].</p>
<p>Qualitative examples of this cross-dataset transfer are shown in <xref ref-type="fig" rid="pcbi.1007012.g003">Fig 3</xref>. For quantitative validation we generated manual annotations of two randomly selected regions of the [Weiler14] dataset using the same software that we have used for [PRISM] annotation. We observed that the levels of agreement between the results of the DoGNet Shallow Anisotropic trained on [Collman15] dataset and each of the experts were similar to the level of inter-expert agreement (in terms of the F1 score).</p>
<p>The results of this cross-dataset validation are shown in <xref ref-type="table" rid="pcbi.1007012.t004">Table 4</xref>. Importantly, while the performance of compared methods, did not diminish dramatically. In fact, the DoGNets actually improved in their performance, which we attribute to the fact that in the Weiler dataset all expert annotations were based on FM images, rendering the analysis more straightforward in comparison with the [Collman15] synapses that are visible in EM data but not in the FM data that were not included.</p>
<table-wrap id="pcbi.1007012.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.t004</object-id>
<label>Table 4</label>
<caption>
<title>The quantitative validation of DoGNet trained on [Collman15] cAT dataset and applied to [Weiler14] dataset.</title>
<p>Differences with F1 scores on [Collman15] cAT dataset are shown in parentheses.</p>
</caption>
<alternatives>
<graphic id="pcbi.1007012.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="center"># params</th>
<th align="center">F1 Score</th>
<th align="center">Prec.</th>
<th align="center">Recall</th>
<th align="center">AUC</th>
<th align="center">|<italic>DiC</italic>|</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="7"><bold>ConvNets</bold></td>
</tr>
<tr>
<td align="left">Direct</td>
<td align="center">3392</td>
<td align="center">0.72 (0.03)↑</td>
<td align="char" char=".">0.79</td>
<td align="char" char=".">0.66</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">5.33</td>
</tr>
<tr>
<td align="left">FCN</td>
<td align="center">3002</td>
<td align="center">0.64 (-0.07)↓</td>
<td align="char" char=".">0.85</td>
<td align="char" char=".">0.51</td>
<td align="char" char=".">0.84</td>
<td align="char" char=".">19.</td>
</tr>
<tr>
<td align="left">Unet</td>
<td align="center">622</td>
<td align="center">0.79 (0.06)↑</td>
<td align="char" char=".">0.85</td>
<td align="char" char=".">0.74</td>
<td align="char" char=".">0.97</td>
<td align="char" char=".">4.33</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>DoGNets</bold></td>
</tr>
<tr>
<td align="left">Shallow Isotropic</td>
<td align="center">62</td>
<td align="center">0.85 (0.1)↑</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">0.96</td>
<td align="char" char=".">3.33</td>
</tr>
<tr>
<td align="left">Shallow Anisotropic</td>
<td align="center">107</td>
<td align="center">0.83 (0.08)↑</td>
<td align="char" char=".">0.88</td>
<td align="char" char=".">0.78</td>
<td align="char" char=".">0.94</td>
<td align="char" char=".">3.33</td>
</tr>
<tr>
<td align="left">Deep Isotropic</td>
<td align="center">140</td>
<td align="center"><bold>0.88</bold> (0.15)↑</td>
<td align="char" char=".">0.83</td>
<td align="char" char=".">0.95</td>
<td align="char" char=".">0.93</td>
<td align="char" char=".">3.33</td>
</tr>
<tr>
<td align="left">Deep Anisotropic</td>
<td align="center">230</td>
<td align="center">0.71 (0.0)</td>
<td align="char" char=".">0.80</td>
<td align="char" char=".">0.63</td>
<td align="char" char=".">0.90</td>
<td align="char" char=".">7.33</td>
</tr>
<tr>
<td align="left" colspan="7"><bold>Manually tuned methods</bold></td>
</tr>
<tr>
<td align="left">Nieland 2014 [<xref ref-type="bibr" rid="pcbi.1007012.ref038">38</xref>]</td>
<td align="center">-</td>
<td align="center">0.64 (0.27)↑</td>
<td align="char" char=".">0.66</td>
<td align="char" char=".">0.62</td>
<td align="char" char=".">0.44</td>
<td align="char" char=".">2.</td>
</tr>
<tr>
<td align="left">Simhal 2017 [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>]</td>
<td align="center">-</td>
<td align="center">0.65 (0.0)</td>
<td align="char" char=".">0.81</td>
<td align="char" char=".">0.55</td>
<td align="char" char=".">0.55</td>
<td align="char" char=".">13.</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec018">
<title>Synthetic dataset</title>
<p>In order to further evaluate our approach rigorously in a fully controlled setting, we also applied it to a synthetic dataset. The goal of the evaluation of DoGNet using synthetic data was to estimate the quality of synapse detection compared with baseline procedures for distinct levels of signal-to-noise ratio; including the presence of spurious synapses; and for different presynaptic-to-postsynaptic markers displacements on image planes to emulate the 3D structure of synapse. Further, this systematic evaluation using synthetic data addresses questions regarding meta-parameter choice, methodological limitations, and the justification of neural network usage for synapse detection tasks. Because the number of training samples was unlimited, deep networks with a large number of parameters were unlikely to overfit the data.</p>
<p>Our dataset models three entities: true synapses, spurious synapses that emulates false bindings, and random noise. We emulated true synapses and spurious synapses using Gaussian probability density functions placed in different image planes with additive white noise, where each image plane refers to a specific protein marker such as synapsin, vGlut, PSD-95, vGat or gephyrin. To assess the generalized performance of different architectures, in our synthetic experiments we simulated both excitatory and inhibitory synapses.</p>
<p>Spurious synapses are made to emulate false bindings in combination with random noise in order to act as a distraction for the classifier to evaluate its robustness. An actual synapse has intensity peaks at least in one presynaptic and in one postsynaptic image plane, while spurious synapses have peaks only in presynaptic or postsynaptic channels, but never in both. An example of a true excitatory synapse might be a signal that has a punctum in synapsin, vGlut and PSD-95 markers separated by a distance less than a half of a micron. An inhibitory synapse would have punctae in synapsin, vGat and gephyrin. The displacement in markers punctae, caused by the 3D structure of synapses, makes the process of differentiation between actual and spurious synapses considerably more challenging, thereby rendering the simulation more realistic. The intensity of the synaptic signal were emulated using Log-Normal distribution with zero mean and <italic>σ</italic> = 0.1.</p>
<p>Modeling synapses using isotropic Gaussians in our synthetic dataset enables the initial evaluation of purely isotropic DoGNets. First the sensitivity of the approach to signal-to-noise ratio was evaluated (<xref ref-type="fig" rid="pcbi.1007012.g004">Fig 4</xref>). Results indicate that small convolutional neural networks are sensitive to initialization and may become trapped in local minima, whereas DoGNet performance was more robust, although DoGNets initialized randomly rather than using our initialization scheme also suffered from local minima. Importantly, deeper architectures were capable of handling larger displacements between punctae (<xref ref-type="fig" rid="pcbi.1007012.g005">Fig 5</xref>). This result is anticipated because multi-layer architectures have larger receptive fields and capture more non-linearities, allowing the capture of more complex relations in the data. For example, in the presence of substantial displacements, at least one additional convolution layer followed by an element-wise multiplication was needed to perform a logical AND operation between pre and post synaptic channels after blob detection [<xref ref-type="bibr" rid="pcbi.1007012.ref032">32</xref>].</p>
<fig id="pcbi.1007012.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Method sensitivity to signal-to-noise ratio.</title>
<p>A comparison of manually-tuned methods, deep architecture baselines, and DoGNets. The bar chart shows differences in methods in area-under-curve (AUC) measure for different signal-to-noise ratios. DoGNets are more robust to noise than manually tuned methods, with low variation in AUC between DoGNet runs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1007012.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Methods sensitivity to punctae displacement.</title>
<p>With increasing displacement, it is more difficult to discriminate between true synapses and spurious synapses. The quality of the segmentation map produced by DoGNets decreases more slowly than that of other methods.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g005" xlink:type="simple"/>
</fig>
<p>We also present a study of training with limited examples. We have evaluated trainable methods (Direct, FCN, U-Net, Shallow Isotropic, Deep Isotropic) on fixed size crop without any augmentation in search of minimal size of image region when each method starts work suitable the signal-to-noise ration was sent to approx 4.5 and the maximal displacement to two pixels. We present the results of this study in (<xref ref-type="fig" rid="pcbi.1007012.g006">Fig 6</xref>). We show that Shallow and Deep DoGNets are able to learn a simple signal like a multiplexed blob form only few samples.</p>
<fig id="pcbi.1007012.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007012.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Methods sensitivity to small numbers of training examples.</title>
<p>Comparison of different trainable architecture baselines with DoGNets for various amount of training data. In this experiment, the training sets corresponded to patches of different sizes, ranging from 45 × 45 pixels (with approximately 12 synapses) to 128 × 128 (with approximately 96 synapses). The maximal displacement was set to two pixels, the signal-to-noise ratio was fixed to 3.0, and no augmentation such as random cropping was applied. Shallow DoGNets need only few examples to reach acceptable performance. With a sufficient number of examples the baseline architecture can perform as well as or better than DoGNets.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.g006" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>We introduce an efficient architecture (DoGNet) for the automatic detection of neuronal synapses in both cultured primary neurons and brain tissue slices from multiplexed fluorescence images. Under some conditions, the accuracy of DoGNet accuracy approaches the level of agreement between human annotations. DoGNet also outperforms ConvNets when the number of training examples is limited. Importantly, the DoGNet approach is capable of efficiently integrating a number of different input images from multiplexed microscopy data with a larger number of channels, which can be prohibitively difficult for human experts to accomplish efficiently. This allows for the detection of synapses in large datasets and facilitates downstream quantitative analysis of synaptic features including brightness or intensity, size, and asymmetry.</p>
<p>The robust automated detection of synapses is important for downstream synapse classification, particularly as multiplexed imaging modalities such as PRISM are applied to larger-scale genetic and compound screens, which rely on phenotypic classification of synapses to understand the molecular basis of neurological diseases. By integrating features of synapses detected using machine learning techniques, the proposed method can be used to classify synapses to study their identities and spatial distributions. In conjunction with dendrite and axon tracking [<xref ref-type="bibr" rid="pcbi.1007012.ref039">39</xref>], this approach may be used to build connectivity maps, tracing synaptic connections for each individual neuron.</p>
<p>DoGNet is computationally efficient during both training and inference. Training the simplest model Simple Isotropic required only 7.37 seconds on an NVidia TitanX GPU and 37.84 seconds on Intel i7 CPU for 2000 epochs, which is several times faster than training U-Net and FCN ConvNets. Each epoch is an array of ten patches 64 × 64 pixels randomly cropped from the training set. The inference process for a 1000 × 1000 image requires only 0.001 second on a Titan-X GPU and only 0.1 second on Intel i7 CPU. Most of this time is consumed by post-processing, making it suitable for both high-throughput studies and small-scale experiments without GPU acceleration. The proposed architecture is not specific to synaptic images, and can be applied to other cellular or tissue features where objects of interest show punctate spatial patterning, such as single molecule annotation in super-resolution imaging and single-particle tracking, detection of exocytic vesicles, and detection of puncta in mRNA FISH and in situ sequencing datasets [<xref ref-type="bibr" rid="pcbi.1007012.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref021">21</xref>]. In cases where high precision estimates of puncta features, such as their spatial extent and centroid positions exists, it may be beneficial to follow DoGNet segmentation with dedicated point spread function (PSF) fitting methods such as Maximum Likelihood Estimation or Least Squares fitting. In this case, DoGNet could be used to improve and streamline initial segmentation tasks that generally occur prior to more robust PSF fitting methods in analysis pipelines [<xref ref-type="bibr" rid="pcbi.1007012.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1007012.ref041">41</xref>].</p>
<p>Despite the preceding strengths, the proposed method also has several limitations, most of which are common to supervised methods. First, DoGNet is useful for synapses because synapse sizes are on the order of the resolution of the light microscope, and thus present as diffraction limited spots. However, this approach would be unsuitable to more complex, larger objects such as nuclei, bacterial cells, or possibly large organelles. In summary, DoGNets are limited to the class of 2D signals with a convex shape and limited radius (blobs). A second limitation is the dependency on the proper parameter initialization scheme. For DoGNets, which have fewer parameters, improper initialization of a single parameter, for example setting <italic>σ</italic> close to zero, can cause the entire network to diverge. In contrast, ConvNets with a larger number of parameters can more easily recover from improper initialization. Notwithstanding, we have found that our initialization scheme for DoGNets works reliably across multiple runs and distinct datasets. For practical use, shallow DoGNet seems to be more reliable than deep DoGNets. We note that shallow DoGNet can still become a part of more complex networks.</p>
<p>We have also shown the ability of DoGNets to transfer across datasets by training them on one AT dataset [<italic>Collman15</italic>] and applying them to another, distinct dataset [<italic>Weiler14</italic>]. This type of transfer may prove useful in the detection of synapses with high confidence by training DoGNet on either cAT data sets such as [<italic>Collman15</italic>] [<xref ref-type="bibr" rid="pcbi.1007012.ref016">16</xref>] or highly multiplexed datasets such as [<italic>PRISM</italic>] [<xref ref-type="bibr" rid="pcbi.1007012.ref006">6</xref>], which are more difficult to acquire experimentally but facilitate synapse annotation with higher certainty. Specifically, electron microscopy allows for highly robust synaptic annotation through conserved features of the synaptic cleft and the post-synaptic density, whereas multiplexed fluorescence data allow for accurate annotation of synapses through the colocalization of multiple synaptic markers.</p>
<sec id="sec020">
<title>Conclusion</title>
<p>We present DoGNet—a new architecture for blob detection. While DoGNets are applied here to synapse detection in multiplexed fluorescence and electron microscopy datasets, they are more broadly applicable to other blob detection tasks in biomedical image analysis.</p>
<p>Due to their low number of parameters, DoGNets can be trained in a matter of minutes, and are suitable for non-GPU architectures because the application of a pretrained DoGNet amounts to a sequence of Gaussian filtering and elementwise operations. In our experiments, DoGNets were able to robustly detect millions of synapses within several minutes in a fully automated manner, with accuracy comparable to human annotations. This computational efficiency and robustness may prove essential for the application of multiplexed imaging to high-throughput experimentation including genetic and drug screens of neuronal and other cellular systems.</p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007012.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Baseline network architectures.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007012.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Results of Shallow Isotropic DoGNet on PRISM dataset.</title>
<p>The top image is the original one, the middle is the probability map produced by DoGNet and on the bottom is the overlay of detected synapses on the original image. Detected synapses are denoted with a red arrow, indicating their orientation concerning pre- and postsynaptic sides. The ground truth synapses locations are depicted using white crosses. Yellow bounding box highlights the densely annotated region.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007012.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Results of Deep Anisotropic DoGNet on the Weiler14 dataset.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007012.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007012.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Results of Deep Isotropic DoGNet on the Collman15 dataset.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007012.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yagi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Takeichi</surname> <given-names>M</given-names></name>. <article-title>Cadherin superfamily genes: functions, genomic organization, and neurologic diversity</article-title>. <source>Genes &amp; development</source>. <year>2000</year>;<volume>14</volume>(<issue>10</issue>):<fpage>1169</fpage>–<lpage>1180</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Sheng</surname> <given-names>M</given-names></name>. <article-title>Some assembly required: the development of neuronal synapses</article-title>. <source>Nature reviews Molecular cell biology</source>. <year>2003</year>;<volume>4</volume>(<issue>11</issue>):<fpage>833</fpage>–<lpage>841</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrm1242" xlink:type="simple">10.1038/nrm1242</ext-link></comment> <object-id pub-id-type="pmid">14625534</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zikopoulos</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Barbas</surname> <given-names>H</given-names></name>. <article-title>Changes in prefrontal axons may disrupt the network in autism</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>44</issue>):<fpage>14595</fpage>–<lpage>14609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2257-10.2010" xlink:type="simple">10.1523/JNEUROSCI.2257-10.2010</ext-link></comment> <object-id pub-id-type="pmid">21048117</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peça</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Feliciano</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ting</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wells</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Venkatraman</surname> <given-names>TN</given-names></name>, <etal>et al</etal>. <article-title>Shank3 mutant mice display autistic-like behaviours and striatal dysfunction</article-title>. <source>Nature</source>. <year>2011</year>;<volume>472</volume>(<issue>7344</issue>):<fpage>437</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09965" xlink:type="simple">10.1038/nature09965</ext-link></comment> <object-id pub-id-type="pmid">21423165</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Micheva</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SJ</given-names></name>. <article-title>Array tomography: a new tool for imaging the molecular architecture and ultrastructure of neural circuits</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>55</volume>(<issue>1</issue>):<fpage>25</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.06.014" xlink:type="simple">10.1016/j.neuron.2007.06.014</ext-link></comment> <object-id pub-id-type="pmid">17610815</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref006">
<label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Guo SM, Veneziano R, Gordonov S, Li L, Park D, Kulesa AB, et al. Multiplexed confocal and super-resolution fluorescence imaging of cytoskeletal and neuronal synapse proteins. bioRxiv. 2017; p. 111625.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems; 2012. p. 1097–1105.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref008">
<label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer; 2015. p. 234–241.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref009">
<label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee K, Zlateski A, Ashwin V, Seung HS. Recursive training of 2D-3D convolutional networks for neuronal boundary prediction. In: Advances in Neural Information Processing Systems; 2015. p. 3573–3581.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref010">
<label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Santurkar S, Budden D, Matveev A, Berlin H, Saribekyan H, Meirovitch Y, et al. Toward streaming synapse detection with compositional convnets. arXiv preprint arXiv:170207386. 2017.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Iwabuchi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kakazu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Koh</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Harata</surname> <given-names>NC</given-names></name>. <article-title>Evaluation of the effectiveness of Gaussian filtering in distinguishing punctate synaptic signals from background noise during image analysis</article-title>. <source>Journal of neuroscience methods</source>. <year>2014</year>;<volume>223</volume>:<fpage>92</fpage>–<lpage>113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2013.12.003" xlink:type="simple">10.1016/j.jneumeth.2013.12.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lindeberg</surname> <given-names>T</given-names></name>. <article-title>Feature detection with automatic scale selection</article-title>. <source>International journal of computer vision</source>. <year>1998</year>;<volume>30</volume>(<issue>2</issue>):<fpage>79</fpage>–<lpage>116</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1008097225773" xlink:type="simple">10.1023/A:1008097225773</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Zerubia</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Olivo-Marin</surname> <given-names>JC</given-names></name>. <article-title>Gaussian approximations of fluorescence microscope point-spread function models</article-title>. <source>Applied optics</source>. <year>2007</year>;<volume>46</volume>(<issue>10</issue>):<fpage>1819</fpage>–<lpage>1829</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1364/AO.46.001819" xlink:type="simple">10.1364/AO.46.001819</ext-link></comment> <object-id pub-id-type="pmid">17356626</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Airy</surname> <given-names>GB</given-names></name>. <article-title>On the diffraction of an object-glass with circular aperture</article-title>. <source>Transactions of the Cambridge Philosophical Society</source>. <year>1835</year>;<volume>5</volume>:<fpage>283</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2015. p. 3431–3440.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Buchanan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Phend</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Micheva</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Weinberg</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SJ</given-names></name>. <article-title>Mapping synapses by conjugate light-electron array tomography</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>14</issue>):<fpage>5792</fpage>–<lpage>5807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4274-14.2015" xlink:type="simple">10.1523/JNEUROSCI.4274-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25855189</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Weiler</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Vogelstein</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SJ</given-names></name>. <article-title>Synaptic molecular imaging in spared and deprived columns of mouse barrel cortex with array tomography</article-title>. <source>Scientific data</source>. <year>2014</year>;<volume>1</volume>:<fpage>140046</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2014.46" xlink:type="simple">10.1038/sdata.2014.46</ext-link></comment> <object-id pub-id-type="pmid">25977797</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Stallinga</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lidke</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Rieger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Grunwald</surname> <given-names>D</given-names></name>. <article-title>Probability-based particle detection that enables threshold-free and robust in vivo single-molecule tracking</article-title>. <source>Molecular biology of the cell</source>. <year>2015</year>;<volume>26</volume>(<issue>22</issue>):<fpage>4057</fpage>–<lpage>4062</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1091/mbc.E15-06-0448" xlink:type="simple">10.1091/mbc.E15-06-0448</ext-link></comment> <object-id pub-id-type="pmid">26424801</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Morphew</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yeh</surname> <given-names>FL</given-names></name>, <name name-style="western"><surname>Dong</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>All three components of the neuronal SNARE complex contribute to secretory vesicle docking</article-title>. <source>J Cell Biol</source>. <year>2012</year>;<volume>198</volume>(<issue>3</issue>):<fpage>323</fpage>–<lpage>330</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1083/jcb.201106158" xlink:type="simple">10.1083/jcb.201106158</ext-link></comment> <object-id pub-id-type="pmid">22869597</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Gudla PR, Nakayama K, Pegoraro G, Misteli T. SpotLearn: Convolutional Neural Network for Detection of Fluorescence In Situ Hybridization (FISH) Signals in High-Throughput Imaging Approaches. In: Cold Spring Harbor symposia on quantitative biology. Cold Spring Harbor Laboratory Press; 2017. p. 033761.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tan</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Chia</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>GHC</given-names></name>, <name name-style="western"><surname>Choo</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Tai</surname> <given-names>DWM</given-names></name>, <name name-style="western"><surname>Chua</surname> <given-names>CWL</given-names></name>, <etal>et al</etal>. <article-title>Gastric peritoneal carcinomatosis-a retrospective review</article-title>. <source>World journal of gastrointestinal oncology</source>. <year>2017</year>;<volume>9</volume>(<issue>3</issue>):<fpage>121</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4251/wjgo.v9.i3.121" xlink:type="simple">10.4251/wjgo.v9.i3.121</ext-link></comment> <object-id pub-id-type="pmid">28344747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Ciresan D, Giusti A, Gambardella LM, Schmidhuber J. Deep neural networks segment neuronal membranes in electron microscopy images. In: Advances in neural information processing systems; 2012. p. 2843–2851.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Shavit N. A Multicore Path to Connectomics-on-Demand. In: Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures. ACM; 2016. p. 211–211.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–778.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:151107122. 2015.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Paszke A, Chaurasia A, Kim S, Culurciello E. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:160602147. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kreshuk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Straehle</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Koethe</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Cantoni</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Knott</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Automated detection and segmentation of synaptic contacts in nearly isotropic serial electron microscopy images</article-title>. <source>PloS one</source>. <year>2011</year>;<volume>6</volume>(<issue>10</issue>):<fpage>e24899</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0024899" xlink:type="simple">10.1371/journal.pone.0024899</ext-link></comment> <object-id pub-id-type="pmid">22031814</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Becker</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ali</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Knott</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Fua</surname> <given-names>P</given-names></name>. <article-title>Learning context cues for synapse segmentation</article-title>. <source>IEEE transactions on medical imaging</source>. <year>2013</year>;<volume>32</volume>(<issue>10</issue>):<fpage>1864</fpage>–<lpage>1877</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TMI.2013.2267747" xlink:type="simple">10.1109/TMI.2013.2267747</ext-link></comment> <object-id pub-id-type="pmid">23771317</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schätzle</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wuttke</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ziegler</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Sonderegger</surname> <given-names>P</given-names></name>. <article-title>Automated quantification of synapses by fluorescence microscopy</article-title>. <source>Journal of neuroscience methods</source>. <year>2012</year>;<volume>204</volume>(<issue>1</issue>):<fpage>144</fpage>–<lpage>149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2011.11.010" xlink:type="simple">10.1016/j.jneumeth.2011.11.010</ext-link></comment> <object-id pub-id-type="pmid">22108140</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Herold J, Friedenberger M, Bode M, Rajpoot N, Schubert W, Nattkemper TW. Flexible synapse detection in fluorescence micrographs by modeling human expert grading. In: Biomedical Imaging: From Nano to Macro, 2008. ISBI 2008. 5th IEEE International Symposium on. IEEE; 2008. p. 1347–1350.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Herold</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schubert</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Nattkemper</surname> <given-names>TW</given-names></name>. <article-title>Automated detection and quantification of fluorescently labeled synapses in murine brain tissue sections for high throughput applications</article-title>. <source>Journal of biotechnology</source>. <year>2010</year>;<volume>149</volume>(<issue>4</issue>):<fpage>299</fpage>–<lpage>309</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jbiotec.2010.03.004" xlink:type="simple">10.1016/j.jbiotec.2010.03.004</ext-link></comment> <object-id pub-id-type="pmid">20230863</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simhal</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Aguerrebere</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Vogelstein</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Micheva</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Weinberg</surname> <given-names>RJ</given-names></name>, <etal>et al</etal>. <article-title>Probabilistic fluorescence-based synapse detection</article-title>. <source>PLoS computational biology</source>. <year>2017</year>;<volume>13</volume>(<issue>4</issue>):<fpage>e1005493</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005493" xlink:type="simple">10.1371/journal.pcbi.1005493</ext-link></comment> <object-id pub-id-type="pmid">28414801</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Rumelhart DE, Hinton GE, Williams RJ. Learning internal representations by error propagation. California Univ San Diego La Jolla Inst for Cognitive Science; 1985.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Gkioxari G, Dollar P, Girshick R. Mask R-CNN. In: The IEEE International Conference on Computer Vision (ICCV); 2017.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Milletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 3D Vision (3DV), 2016 Fourth International Conference on. IEEE; 2016. p. 565–571.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">PyTorch Tensors and Dynamic neural networks in Python with strong GPU acceleration; 2017. <ext-link ext-link-type="uri" xlink:href="http://pytorch.org" xlink:type="simple">http://pytorch.org</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Brown M, Szeliski R, Winder S. Multi-image matching using multi-scale oriented patches. In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. vol. 1. IEEE; 2005. p. 510–517.</mixed-citation>
</ref>
<ref id="pcbi.1007012.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nieland</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Logan</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Saulnier</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lam</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Root</surname> <given-names>DE</given-names></name>, <etal>et al</etal>. <article-title>High content image analysis identifies novel regulators of synaptogenesis in a high-throughput RNAi screen of primary neurons</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>3</issue>):<fpage>e91744</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0091744" xlink:type="simple">10.1371/journal.pone.0091744</ext-link></comment> <object-id pub-id-type="pmid">24633176</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bosch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Martínez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Masachs</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Teixeira</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Fernaud</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ulloa</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>FIB/SEM technology and high-throughput 3D reconstruction of dendritic spines and synapses in GFP-labeled adult-generated neurons</article-title>. <source>Frontiers in neuroanatomy</source>. <year>2015</year>;<volume>9</volume>:<fpage>60</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnana.2015.00060" xlink:type="simple">10.3389/fnana.2015.00060</ext-link></comment> <object-id pub-id-type="pmid">26052271</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Deschout</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zanacchi</surname> <given-names>FC</given-names></name>, <name name-style="western"><surname>Mlodzianoski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Diaspro</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bewersdorf</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hess</surname> <given-names>ST</given-names></name>, <etal>et al</etal>. <article-title>Precisely and accurately localizing single emitters in fluorescence microscopy</article-title>. <source>Nature methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>253</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.2843" xlink:type="simple">10.1038/nmeth.2843</ext-link></comment> <object-id pub-id-type="pmid">24577276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007012.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kirshner</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Aguet</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Sage</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Unser</surname> <given-names>M</given-names></name>. <article-title>3-D PSF fitting for fluorescence microscopy: implementation and localization application</article-title>. <source>Journal of microscopy</source>. <year>2013</year>;<volume>249</volume>(<issue>1</issue>):<fpage>13</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1365-2818.2012.03675.x" xlink:type="simple">10.1111/j.1365-2818.2012.03675.x</ext-link></comment> <object-id pub-id-type="pmid">23126323</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>