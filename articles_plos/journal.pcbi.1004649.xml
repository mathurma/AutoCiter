<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004649</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00572</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Biases in Visual, Auditory, and Audiovisual Perception of Space</article-title>
<alt-title alt-title-type="running-head">Biases in Visual, Auditory, and Audiovisual Perception of Space</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Odegaard</surname>
<given-names>Brian</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wozny</surname>
<given-names>David R.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Shams</surname>
<given-names>Ladan</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, University of California, Los Angeles, Los Angeles, California, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of BioEngineering, University of California, Los Angeles, Los Angeles, California, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Neuroscience Interdepartmental Program, University of California, Los Angeles, Los Angeles, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Faisal</surname>
<given-names>Aldo A</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Imperial College London, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: DRW LS. Performed the experiments: BO DRW. Analyzed the data: BO LS DRW. Contributed reagents/materials/analysis tools: BO DRW. Wrote the paper: BO LS DRW.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">odegaard.brian@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>8</day>
<month>12</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>12</issue>
<elocation-id>e1004649</elocation-id>
<history>
<date date-type="received">
<day>7</day>
<month>4</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>11</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Odegaard et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004649" xlink:type="simple"/>
<abstract>
<p>Localization of objects and events in the environment is critical for survival, as many perceptual and motor tasks rely on estimation of spatial location. Therefore, it seems reasonable to assume that spatial localizations should generally be accurate. Curiously, some previous studies have reported biases in visual and auditory localizations, but these studies have used small sample sizes and the results have been mixed. Therefore, it is not clear (1) if the reported biases in localization responses are real (or due to outliers, sampling bias, or other factors), and (2) whether these putative biases reflect a bias in sensory representations of space or <italic>a priori</italic> expectations (which may be due to the experimental setup, instructions, or distribution of stimuli). Here, to address these questions, a dataset of unprecedented size (obtained from 384 observers) was analyzed to examine presence, direction, and magnitude of sensory biases, and quantitative computational modeling was used to probe the underlying mechanism(s) driving these effects. Data revealed that, on average, observers were biased towards the center when localizing visual stimuli, and biased towards the periphery when localizing auditory stimuli. Moreover, quantitative analysis using a Bayesian Causal Inference framework suggests that while pre-existing spatial biases for central locations exert some influence, biases in the sensory representations of both visual and auditory space are necessary to fully explain the behavioral data. How are these opposing visual and auditory biases reconciled in conditions in which both auditory and visual stimuli are produced by a single event? Potentially, the bias in one modality could dominate, or the biases could interact/cancel out. The data revealed that when integration occurred in these conditions, the visual bias dominated, but the <italic>magnitude</italic> of this bias was <italic>reduced</italic> compared to unisensory conditions. Therefore, multisensory integration not only improves the <italic>precision</italic> of perceptual estimates, but also the <italic>accuracy</italic>.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Almost all daily tasks performed by humans require localizing objects and events. Since spatial localization is critical for survival, it is expected that the brain performs this task accurately. We tested the accuracy of localizing simple sounds and sights in hundreds of human observers. We found that, on average, localizations of sounds and sights were <italic>not</italic> accurate, and observers made systematic errors: flashes of light were perceived to be <italic>nearer</italic> the center of the visual field than they actually were, and bursts of noise were perceived farther <italic>away</italic> from center. Surprisingly, computational analyses revealed that these biases in perceived location of sounds and sights are at least partly due to a bias in how sights and sounds are encoded by the sensory systems; the visual representation of the world is compressed towards the center and the auditory representation is expanded away from the center. When flashes and noise bursts were presented at the same location simultaneously and the observers perceived a common source for both stimuli, the bias in the perceived location was smaller, showing that synthesizing information across modalities helps increase accuracy in the perceived location of objects and events, compared to when only one sensation is available.</p>
</abstract>
<funding-group>
<funding-statement>LS was supported by a grant from National Science Foundation 1057969. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="2"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Data from our 384 subjects is available at the following URL: <ext-link ext-link-type="uri" xlink:href="http://shamslab.psych.ucla.edu/people/odegaard/PLOSCompBioData.zip" xlink:type="simple">http://shamslab.psych.ucla.edu/people/odegaard/PLOSCompBioData.zip</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Nearly every function critical for human survival depends directly on accurate localization of objects and events in the environment. For example, our capacity to locate food, find potential mates, avoid predators, navigate new terrain, avoid obstacles, act upon objects, and orient towards sudden sounds requires a skilled aptitude to detect stimuli in the surrounding world. Spatial localization is thus a fundamental task that the brain has to solve at any given moment, and evolution has had millions of years to refine this function. Therefore, one would expect that the representation of space in all sensory modalities is generally accurate. However, the existing literature on human spatial localization contains many findings that are at odds with this assumption: representations of space in different sensory modalities appear to be biased, and no consensus yet exists regarding the direction and magnitude of the bias in a given modality.</p>
<p>For instance, while a majority of studies investigating localizations of simple visual stimuli that differ in azimuth have shown that visual localizations often show a bias towards the center of visual space [<xref rid="pcbi.1004649.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004649.ref008" ref-type="bibr">8</xref>], a few studies have reported peripheral biases or veridical perception [<xref rid="pcbi.1004649.ref009" ref-type="bibr">9</xref>–<xref rid="pcbi.1004649.ref012" ref-type="bibr">12</xref>]. In the auditory domain, findings of various studies have been mixed as well: some indicate that, on average, auditory localizations are often biased increasingly farther away from center as eccentricity increases [<xref rid="pcbi.1004649.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1004649.ref013" ref-type="bibr">13</xref>–<xref rid="pcbi.1004649.ref015" ref-type="bibr">15</xref>], while others have reported central biases for average auditory localizations [<xref rid="pcbi.1004649.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004649.ref017" ref-type="bibr">17</xref>]. Thus, previous research has provided little consensus regarding the bias present in each sensory modality.</p>
<p>The previous investigations on spatial localization have used relatively small sample sizes, and consequently, variability in the reported findings could be due to sampling bias and/or the undue influence of outliers. Similar issues also apply to the previous reports of <italic>precision</italic> of spatial localization, particularly in the auditory domain. One study reported that the variability in auditory localizations remains relatively consistent across differences in azimuth [<xref rid="pcbi.1004649.ref018" ref-type="bibr">18</xref>], but two recent investigations found that auditory precision declines for peripheral targets [<xref rid="pcbi.1004649.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004649.ref019" ref-type="bibr">19</xref>].</p>
<p>Here, we present findings from an extraordinarily large dataset (384 subjects) to obtain more definitive answers regarding not only the direction and magnitude of biases in both the visual and auditory modalities, but also the precision in spatial localizations over a range of spatial positions. Previous speculations about the source of the variability in findings regarding the presence and direction of bias have focused on the specifics of the experimental paradigms, including the mechanism used to report location [<xref rid="pcbi.1004649.ref011" ref-type="bibr">11</xref>], the role of visibility of response location [<xref rid="pcbi.1004649.ref020" ref-type="bibr">20</xref>], the visibility of external boundaries [<xref rid="pcbi.1004649.ref021" ref-type="bibr">21</xref>], and the memory demands of the task as related to the time before response [<xref rid="pcbi.1004649.ref008" ref-type="bibr">8</xref>]. However, the variability in the results of previous studies could be a result of any of these factors or small sample sizes. In this study, we implement a simple yet rigorous paradigm to assess bias in which subjects’ localized briefly presented flashes of light and bursts of sounds; the response cursor was visible at all times to eliminate the influence of motor error, the locations of stimuli were unknown to subjects, the display was boundary-free, the response delay was minimal, and hundreds of subjects participated, all in an attempt to capture the true perceptual biases present in each modality. By analyzing responses from our immense dataset, we aim to clarify discrepant results in the literature regarding the accuracy and precision of both visual and auditory localizations.</p>
<p>Additionally, while previous studies on unisensory biases and variances have enhanced an overall understanding of how the senses process information from the surrounding world, in real-world settings, objects usually produce signals that simultaneously stimulate multiple sensory modalities. In conditions where auditory and visual stimuli co-occur in close spatial proximity, it has been well-documented that auditory localizations are often pulled in the direction of a visual stimulus [<xref rid="pcbi.1004649.ref018" ref-type="bibr">18</xref>,<xref rid="pcbi.1004649.ref022" ref-type="bibr">22</xref>–<xref rid="pcbi.1004649.ref026" ref-type="bibr">26</xref>]. Known as the “Ventriloquist illusion” [<xref rid="pcbi.1004649.ref027" ref-type="bibr">27</xref>], this effect has been demonstrated in many experiments, lending support to the idea that the most reliable sense in a given dimension exerts a strong influence over other noisier senses as estimates about the world are produced [<xref rid="pcbi.1004649.ref028" ref-type="bibr">28</xref>].</p>
<p>While many studies have focused on how far this “spatial window” of audiovisual integration extends [<xref rid="pcbi.1004649.ref029" ref-type="bibr">29</xref>], one important question remains: when visual and auditory stimuli <italic>co-occur</italic> at the <italic>same</italic> spatial location (which is generally the case when an event produces both visual and auditory stimulation), which modality’s bias emerges during localization? Is it the visual bias, the auditory bias, or something else? If, as many studies indicate, visual localization is biased towards the center, and auditory localization is biased towards the periphery, three possibilities exist: vision dominates on most trials, and a central bias emerges; the auditory bias dominates, and a peripheral bias emerges; the biases interact, and a hybrid bias emerges in the bimodal localizations. It has been shown previously that even when the stimuli are congruent in both space and time, they may be perceived to have a common cause or independent causes. Therefore, it is also important to explore whether the nature and presence of bias in multisensory conditions is influenced by the inference of common cause.</p>
<p>Finally, another important question that has not been previously explored is the following: if indeed the visual and/or auditory localization of stimuli in space is biased and inaccurate, what underlies this bias? Is this bias in localization a result of bias in the sensory representation of the space, a result of general a priori expectation of location of events in the world, or the result of a combination of these two possibilities? In the former case, the sensory encoding of the stimuli is compressed towards the center of the visual space, and this early sensory bias leads to a mislocalization of stimuli towards the center without any effect of prior expectations for the location of the stimuli. In the latter case, the sensory representation of the visual space may be perfectly accurate and unbiased, but due to the task demands or expectations about the location of the stimuli (in this task or in general), the localization may be biased towards the center. The third possibility is that the localization bias stems from a combination of both of these mechanisms. Teasing apart these three different scenarios requires quantitative computational modeling of the data, as explained below.</p>
<p>The behavioral findings in this study showed that both visual and auditory localizations were indeed biased, with visual localizations on average biased towards the center and auditory localizations on average biased away from the center. To address whether these biases in spatial perception stem from a bias in sensory representations or from a prior expectation of location, we employed the causal inference model of multisensory perception [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>–<xref rid="pcbi.1004649.ref032" ref-type="bibr">32</xref>] to quantitatively characterize both sensory representations and prior expectations of each individual observer. This model has been very successful in accounting for human observers’ data in a variety of multisensory tasks [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>–<xref rid="pcbi.1004649.ref035" ref-type="bibr">35</xref>] and a recent brain imaging study has provided further support for the brain utilizing this computation in the spatial localization task that is used in this study [<xref rid="pcbi.1004649.ref036" ref-type="bibr">36</xref>,<xref rid="pcbi.1004649.ref037" ref-type="bibr">37</xref>]. Importantly, this model can reliably provide a quantitative estimate of several components of perceptual processing for each individual observer.</p>
<p>In a Bayesian framework, the final perceptual estimate is based upon a combination of the sensory representation (i.e. likelihood distribution) and pre-existing expectation (i.e. prior). <xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1</xref> shows different kinds of underlying mechanisms that could produce biases similar to those shown by most subjects in our localization task. In the visual domain, either (1) an <italic>a priori</italic> bias for center in expectation of location of visual stimuli (<xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1A</xref>), or (2) a bias in the sensory representation of visual stimuli towards the center (the likelihood means shifted towards center with the amount of shift being proportional to the degree of stimulus eccentricity) could account for biased perception (<xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1B</xref>). In the auditory domain, either (1) an a priori bias towards periphery (<xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1C</xref>), or (2) a bias away from the center in sensory representations (<xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1D</xref>; shifts of likelihood means away from the center with the degree of shift proportional to stimulus eccentricity) could account for participants’ behavior. The exhibited behavioral biases may also be due to a combination of biased likelihood and biased priors. To investigate which of these options is indeed the mechanism underlying the observed biases, we implemented all three types of mechanisms (biased likelihoods, bias priors, a combination of the two) into a Bayesian Causal Inference model and performed quantitative model comparisons to determine which computational mechanism best accounts for the behavioral data.</p>
<fig id="pcbi.1004649.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Possible underlying mechanisms for the biases observed in subjects’ localization responses.</title>
<p>Zero on the x-axis represents the center. In panels (B) and (D), the likelihood and posterior functions are overlapping and are shown in blue. In the visual domain, (A) a prior distribution located at the center of visual space could draw localization estimates towards the center, or (B) the likelihood distributions themselves could be biased, drawing sensory estimates towards a central location. In the auditory domain, (C) a prior bias for periphery could push perception of peripheral target estimates further away from the center of space, or (D) the likelihood distributions themselves maybe biased away from the center. Various combinations of these computational mechanisms were tested in the different proposed models (see <xref rid="sec012" ref-type="sec">Model Comparisons</xref> section).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g001" position="float" xlink:type="simple"/>
</fig>
<p>All previous models of multisensory spatial localization assume that sensory representations are unbiased, and that the main benefit in integrating stimuli is improvement in the overall <italic>precision</italic> of the combined estimate. For example, maximum likelihood estimation models have proposed that auditory and visual signals can be represented by distributions centered at the true location of each stimulus, with the combined audiovisual estimate exhibiting a smaller variance than the unisensory estimates [<xref rid="pcbi.1004649.ref038" ref-type="bibr">38</xref>–<xref rid="pcbi.1004649.ref040" ref-type="bibr">40</xref>]. The reduced variance of the combined estimate has been considered to be the main reason why it may be advantageous for the brain to integrate redundant sensory information in a complex world [<xref rid="pcbi.1004649.ref041" ref-type="bibr">41</xref>,<xref rid="pcbi.1004649.ref042" ref-type="bibr">42</xref>]. Here, we explore the effect of integration on the <italic>bias</italic> of the estimates and examine whether localization estimates become less biased, more biased or remain the same as a result of integration as compared to unisensory conditions.</p>
<p>In order to address all of these questions, we analyzed psychophysical data and quantitatively characterized the perceptual components involved in the spatial perceptual process using computational modeling. Specifically, the study had five aims: (1) to quantify unisensory spatial biases and variances in the visual and auditory modalities using a large dataset, (2) to investigate how biases are reconciled when subjects localize spatially congruent bisensory stimuli, (3) to determine whether the biases that emerge in spatially congruent bisensory trials depend on an observer’s inference about a common cause, (4) to determine whether the biases in spatial localization are due to a bias in the sensory representations, prior expectations, or both, and (5) to examine how the biases in bisensory conditions compare with biases in unisensory conditions.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec003">
<title>Ethics Statement</title>
<p>This study was conducted according to the principles expressed in the Declaration of Helsinki. Each subject also signed a consent form approved by the UCLA IRB.</p>
</sec>
<sec id="sec004">
<title>Participants</title>
<p>A total of 412 subjects (ages 18–55) participated in our experiment; since our measurement of localization biases are means that can be influenced by extreme outliers, 28 subjects were excluded because their responses for each of the five unisensory conditions in either the visual or auditory modalities were more than three times the inter-quartile standard deviation for the tested location. This exclusion criterion ensured that we would avoid analyzing data that may have been due to sloppiness or negligence with the response device. All participants verbally reported that they did not have a history of any neurological conditions (seizures, epilepsy, stroke), had normal or corrected-to-normal vision and hearing, and had not experienced head trauma.</p>
</sec>
<sec id="sec005">
<title>Apparatus and Stimuli</title>
<p>Eligible participants sat at a desk in a dimly lit room with their chins positioned on a chinrest 52cm from a projection screen. The screen was a black, acoustically transparent cloth subtending much of the visual field (134° width ° x 60° height). Behind the screen were 5 free-field speakers (5 x 8 cm, extended range paper cone), that differed in azimuth by 6.5° and were placed 7° below fixation. The middle speaker was positioned directly below the fixation point, and two speakers were positioned to the right and two to the left of fixation. The visual stimuli were presented overhead from a ceiling mounted projector set to a resolution of 1280 x 1024 pixels with a refresh rate of 75 Hz.</p>
<p>Prior to the presentation of any stimuli in the experiment, participants were required to have their gaze centered on the central fixation point. To ensure that participants’ gaze for each trial was starting from the same location, gaze position and fixation time were recorded at 60Hz with a ViewPoint eye tracker (Arrington Research, Scottsdale, AZ) fixed to the chinrest and PC-60 software (version 2.8.5,000). Stimuli were not displayed until the recorded gaze angle was within 3.0° of the fixation point and the fixation time was greater than 250 ms. Viewing of the stimuli was binocular, although only movements of the right eye were tracked. The eye tracker was adjusted for each participant before the test session to ensure that the entire eye was being monitored, and a calibration task was performed before trials for the experiment began. A separate computer controlled presentation of stimuli and recorded behavioral responses using MATLAB (version 7.6.0, R2008a). A wireless mouse was used to record the behavioral responses.</p>
<p>The visual stimuli used in the experiments were white disks (.41 cd/m2) with a Gaussian envelope of 1.5° FWHM, presented 7° below the fixation point on a black background (.07cd/m2), for 35 ms. The center of visual stimuli overlapped the center of one of the five speakers behind the screen positioned at -13°, -6.5°, 0°, 6.5°, and 13°. Auditory stimuli were ramped white noise bursts of 35 ms measuring 59 dB(A) sound pressure level at a distance of 52 cm. The speaker locations were unknown to all of the participants in the experiment.</p>
</sec>
<sec id="sec006">
<title>Procedure</title>
<p>Participants began each session with 10 practice trials requiring localization of unisensory auditory stimuli. This practice session ensured that participants were using the mouse properly, understood the instructions, and were fulfilling the fixation requirements for each trial. Each trial started with the fixation cross, followed after 750 ms (if the subject was fixating properly) by the presentation of stimuli. 450 ms after the stimuli, fixation was removed and a cursor appeared on the screen vertically just above the horizontal line where the stimuli were presented at a random horizontal location in order to minimize response bias. Following the removal of fixation, the scene was entirely dark except for the cursor. The cursor was controlled by the trackball mouse placed in front of the subject, and could only be moved in the horizontal direction. Participants were instructed to “move the cursor as quickly and accurately as possible to the exact location of the stimulus and click the mouse.” This enabled the capture of continuous responses with a resolution of 0.1 degree/pixel. No feedback about the correctness of responses was given. Participants were allowed to move their eyes as they made their response, so the fixation requirement was dropped following the presentation of the stimuli.</p>
<p>Following the brief practice session, participants began the localization session, which consisted of 525 trials of interleaved auditory, visual, and audiovisual stimuli presented in pseudorandom order. The stimulus conditions included 5 unisensory auditory locations, 5 unisensory visual locations, and 25 combinations of auditory and visual locations (bisensory conditions), for a total of 35 stimulus conditions, shown in <xref rid="pcbi.1004649.g002" ref-type="fig">Fig 2</xref> below. Fifteen trials of each of the 35 conditions were presented in pseudorandom order, lasting about 45 minutes, including breaks.</p>
<fig id="pcbi.1004649.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The spatial localization paradigm.</title>
<p>Stimuli could be presented from one of five locations, ranging from -13 to +13 degrees.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g002" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Bayesian Causal Inference Model</title>
<p>The Bayesian causal inference model has been shown in multiple studies to account for the human multisensory spatial perception very well [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>–<xref rid="pcbi.1004649.ref033" ref-type="bibr">33</xref>] and it has been shown to outperform other proposed models of multisensory spatial perception [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>]. Moreover, a human neuroimaging study recently provided further support for the Bayesian Causal Inference being carried out in human brain in the spatial localization task, using the same task used in current study [<xref rid="pcbi.1004649.ref037" ref-type="bibr">37</xref>]. Therefore, we used this model to investigate the characteristics and perceptual components of human spatial processing in our localization task. In previous formulations of this model it had been assumed that (1) the likelihood distributions of sensory stimuli are unbiased (centered at the true locations of stimuli in the world), (2) visual and auditory localizations are both impacted by a meta-modal central prior over space, and (3) the variance of sensory representations (likelihood functions) is uniform across all eccentricities. Considering our behavioral results, which reveal non-zero biases of differing direction and magnitude in the visual and auditory modalities, it appears that these simplifying assumptions are not quite warranted, and the model would need to be enhanced to allow flexibility in the representation of likelihoods and/or priors.</p>
<p>The nervous system does not have access to the true events <italic>s</italic> in the world. Instead, it only has access to the noisy sensory representations, <italic>x</italic><sub><italic>v</italic></sub> <italic>and x</italic><sub><italic>a</italic></sub> (denoting visual and auditory sensations, respectively). Taking into account both the noisy sensory representations (likelihoods) and prior knowledge of the world (priors), the brain makes an inference about whether the sensory signals come from the same source (<italic>C</italic> = 1) and should be integrated or the signals come from different sources (<italic>C</italic> = 2) and should be segregated (see [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>] for an image of the graphical model).</p>
<p>Thus, the posterior probability of an event <italic>s</italic> is conditioned on the causal structure of the stimuli (C = 1 or C = 2):
<disp-formula id="pcbi.1004649.e001">
<alternatives>
<graphic id="pcbi.1004649.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mspace width="0.1em"/><mml:mo>;</mml:mo><mml:mspace width="0.05em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1004649.e002">
<alternatives>
<graphic id="pcbi.1004649.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mspace width="0.1em"/><mml:mo>;</mml:mo><mml:mspace width="0.05em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
<disp-formula id="pcbi.1004649.e003">
<alternatives>
<graphic id="pcbi.1004649.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mspace width="0.1em"/><mml:mo>;</mml:mo><mml:mspace width="0.05em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>The estimate of the location of the stimuli will be based on these posterior probabilities. However, since the causal scenario is not known by the nervous system, this also must be inferred based on the available sensory evidence and prior information. This can also be computed using Bayes’ Rule:
<disp-formula id="pcbi.1004649.e004">
<alternatives>
<graphic id="pcbi.1004649.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e004" xlink:type="simple"/>
<mml:math display="block" id="M4" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>Therefore,
<disp-formula id="pcbi.1004649.e005">
<alternatives>
<graphic id="pcbi.1004649.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e005" xlink:type="simple"/>
<mml:math display="block" id="M5" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where P<sub>c</sub> is the prior probability of a common cause, and the likelihood terms can be computed by integrating over the latent variable <italic>s</italic>:
<disp-formula id="pcbi.1004649.e006">
<alternatives>
<graphic id="pcbi.1004649.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e006" xlink:type="simple"/>
<mml:math display="block" id="M6" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi>∫</mml:mi><mml:mspace width="0.15em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
<disp-formula id="pcbi.1004649.e007">
<alternatives>
<graphic id="pcbi.1004649.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e007" xlink:type="simple"/>
<mml:math display="block" id="M7" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi>∫</mml:mi><mml:mspace width="0.15em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>∫</mml:mi><mml:mspace width="0.15em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>The posterior probability of independent causes is computed as follows:
<disp-formula id="pcbi.1004649.e008">
<alternatives>
<graphic id="pcbi.1004649.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e008" xlink:type="simple"/>
<mml:math display="block" id="M8" overflow="scroll">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>Having calculated the probabilities of each causal structure, and having calculated the optimal estimates for spatial localization under each causal structure, we now need to obtain estimates given <italic>uncertainty</italic> about the causal structure. Previously it has been shown [<xref rid="pcbi.1004649.ref024" ref-type="bibr">24</xref>] that the vast majority of observers use a probability matching strategy as follows:
<disp-formula id="pcbi.1004649.e009">
<alternatives>
<graphic id="pcbi.1004649.e009g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.e009" xlink:type="simple"/>
<mml:math display="block" id="M9" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.15em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.05em"/><mml:mo>&gt;</mml:mo><mml:mi>ξ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>ξ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.1em"/><mml:mo>:</mml:mo><mml:mspace width="0.1em"/><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mspace width="0.25em"/><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.25em"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>ξ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.25em"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>ξ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>ξ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.1em"/><mml:mo>:</mml:mo><mml:mspace width="0.1em"/><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mspace width="0.25em"/><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.25em"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>ξ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.25em"/><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Therefore, here we used this strategy to model the observers’ data. We used a generative model to simulate 10,000 <italic>x</italic><sub><italic>A</italic></sub>’s and <italic>x</italic><sub><italic>V</italic></sub>’s for each experiment condition (35 total) using the preceding equations. Trial-to-trial variability is introduced by sampling the likelihoods from Eqs <xref rid="pcbi.1004649.e001" ref-type="disp-formula">1</xref>–<xref rid="pcbi.1004649.e003" ref-type="disp-formula">3</xref> from a normal distribution centered at the true sensory location, plus a bias term that scales linearly with eccentricity of the stimulus. Thus, if subjects’ sensory representations are not centered at the true locations, this bias term could potentially reflect this systematic shift in the likelihoods. The variability in the normal distribution from which the likelihoods are sampled scales with eccentricity of the stimulus. The prior described in the above equations changes depending on the model version, and can either be the biased central prior, the biased peripheral prior, or both, (see <xref rid="pcbi.1004649.g001" ref-type="fig">Fig 1</xref>).</p>
<p>We quantitatively compared six models to determine which model could best account for participants’ data. Different mechanisms were incorporated in the different models, resulting in different combinations of free parameters. All of these parameters are shown in <xref rid="pcbi.1004649.t001" ref-type="table">Table 1</xref>, with a brief description of their corresponding mechanism.</p>
<table-wrap id="pcbi.1004649.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.t001</object-id>
<label>Table 1</label> <caption><title>Glossary table of all potential parameters.</title></caption>
<alternatives>
<graphic id="pcbi.1004649.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="1" colspan="1">Parameter</th>
<th align="center" rowspan="1" colspan="1">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="1" colspan="1">P<sub>c</sub></td>
<td align="left" rowspan="1" colspan="1">The prior probability of integrating visual and auditory signals; the tendency to bind</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">σ<sub>V</sub></td>
<td align="left" rowspan="1" colspan="1">Standard deviation of the visual likelihood; the noise in the visual sensory representation</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">σ<sub>A</sub></td>
<td align="left" rowspan="1" colspan="1">Standard deviation of the auditory likelihood; noise in the auditory sensory representation</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Δx<sub>V</sub></td>
<td align="left" rowspan="1" colspan="1">The shift in the center of the visual likelihood; the bias in the visual representation</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Δx<sub>A</sub></td>
<td align="left" rowspan="1" colspan="1">The shift in the center of the auditory likelihood; the bias in the auditory representation</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Δσ<sub>V</sub></td>
<td align="left" rowspan="1" colspan="1">The increase in visual likelihood std dev with each increase in eccentric position</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">σ<sub>P</sub></td>
<td align="left" rowspan="1" colspan="1">Standard deviation of the metamodal spatial prior</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">x<sub>P</sub></td>
<td align="left" rowspan="1" colspan="1">The mean of the metamodal spatial prior</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">σ<sub>VP</sub></td>
<td align="left" rowspan="1" colspan="1">Standard deviation of the visual spatial prior</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">x<sub>VP</sub></td>
<td align="left" rowspan="1" colspan="1">The mean of the visual spatial prior</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">σ<sub>AP</sub></td>
<td align="left" rowspan="1" colspan="1">Standard deviation of the auditory spatial prior</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">x<sub>AP</sub></td>
<td align="left" rowspan="1" colspan="1">The mean(s) of the auditory spatial prior</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Three parameters were included in every model: the prior probability of a common cause (P<sub>c</sub>), the standard deviation of the visual likelihood (σ<sub>V</sub>), and the standard deviation of the auditory likelihood (σ<sub>A</sub>). Two models incorporated the biased likelihood mechanisms, including a symmetric shift in the visual likelihood mean that increases with eccentricity (Δx<sub>V</sub>), a symmetric shift in the auditory likelihood mean that increases with eccentricity (Δx<sub>A</sub>); one of these models included a change in the visual likelihood variance which increases with eccentricity (Δσ<sub>V</sub>), while the other did not. Two models incorporated the biased prior mechanisms, including a visual central prior to capture a central bias (σ<sub>VP,</sub> x<sub>VP</sub>) and a peripheral spatial prior to capture a peripheral auditory bias (with the mean and variance, σ<sub>AP,</sub> x<sub>AP,</sub> reflecting the symmetric aspects of each part of the bimodal distribution); one of these included an additional metamodal spatial prior, while the other did not. We also included the original model [<xref rid="pcbi.1004649.ref031" ref-type="bibr">31</xref>], which does not include either the biased likelihood nor the biased prior mechanisms, as the baseline for comparison. Finally, we included a hybrid model that incorporated all biased likelihood mechanisms (Δx<sub>V</sub>, Δx<sub>A</sub>, Δσ<sub>V</sub>) as well as a metamodal spatial prior. These models are summarized in the results section. Since the different models have different numbers of free parameters, in addition to reporting their model fits (in log likelihood) we also report the BIC value of each model, which is a measure of goodness of fit that penalizes models with larger number of free parameters. The six models described above were applied to the data from each of the 384 observers, and log likelihood and BIC values were computed for each model’s fit to each observer.</p>
</sec>
</sec>
<sec id="sec008" sec-type="results">
<title>Results</title>
<p>The trials in which only a visual stimulus or only an auditory stimulus was presented were analyzed for characterizing localization in “unisensory” conditions. The trials in which both visual and auditory stimuli were presented and occurred in the same location (congruent audiovisual trials) were used to characterize the localization behavior in the “bisensory” condition.</p>
<sec id="sec009">
<title>Accuracy of Localization in Unisensory Conditions</title>
<p>For each observer, the average error in localization (across 15 trials) in the unisensory trials was calculated for each spatial position and for each modality. The mean and distribution of these biases across observers are shown in <xref rid="pcbi.1004649.g003" ref-type="fig">Fig 3A, 3C and 3B</xref>, respectively. Participants’ average localizations for unisensory visual stimuli exhibited a bias for localizing peripheral stimuli closer to the center of visual space. Analysis of unisensory-visual trials showed that 63% of subjects (242) exhibited central biases for all four peripheral locations of visual stimuli, and 84% of subjects (319) exhibited central biases for three out of the four peripheral locations. This trend for a central bias tended to increase as the eccentricity of the visual stimulus increased. In the auditory domain, subjects’ average localizations for each location revealed the opposite trend: average localizations for eccentric stimuli exhibited a peripheral bias for localizing the stimuli (<xref rid="pcbi.1004649.g003" ref-type="fig">Fig 3A and 3C</xref>). However, further analysis revealed heterogeneity among subjects for this trend: 25% of the subjects exhibited consistent peripheral biases for all four eccentric locations, but 15% of participants exhibited consistent central biases. Overall, though, 45% of the participants exhibited a peripheral bias for three out of the four locations, indicating that a substantial proportion of subjects localized eccentric auditory stimuli further away from the true locations of the sounds.</p>
<fig id="pcbi.1004649.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Biases present in localizations on unisensory trials.</title>
<p>A) Average biases across subjects. B) The distribution of subjects’ individual biases (based on each subjects’ mean across 15 trials) for each of the five locations, from -13 degrees in the leftmost column, to +13 degrees in the rightmost column. Visual biases are shown in blue, and auditory biases are shown in red. C) Positive numbers indicate rightward biases, and negative numbers indicate leftward biases. SEM bars (computed over all subjects) are shown around the mean bias in each modality for each location.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g003" position="float" xlink:type="simple"/>
</fig>
<p>Bonferonni-corrected one-sample t-tests for all peripheral localizations in both modalities were all highly significant (p &lt; .0001), indicating the responses in both the visual and auditory modalities for peripheral locations were biased away from the true location of the stimulus. For the central (0°) localizations, there was no bias in the visual modality (t(383) = 1.5, p &gt; .05), but a small bias to the left for the auditory modality was observed (t(383) = -2.9, p = .0042). Previous research has indicated that participants often display an auditory localization bias contralateral to the preferred hand [<xref rid="pcbi.1004649.ref043" ref-type="bibr">43</xref>], so this slight bias could be explained by the near certain-assumption that the majority of our participants were right-handed.</p>
<p>Finally, as can be seen in <xref rid="pcbi.1004649.g003" ref-type="fig">Fig 3B</xref>, the distributions of biases in both the auditory and visual modalities are unimodal, indicating that despite some variability in biases across subjects, there do not appear to be distinct subgroups (i.e. “central-bias” subjects and “peripheral-bias” subjects) that are easily classified. Importantly, the distributions in biases shown in this figure reveal the value of acquiring large sample sizes when investigating spatial perception: reporting averages from small samples could not only reflect skewed trends based on the influence of outliers, but also obscure important variability across subjects in terms of the extant biases. Especially in the auditory domain, studies with smaller sample sizes have not previously revealed this amount of heterogeneity in biases in auditory space [<xref rid="pcbi.1004649.ref014" ref-type="bibr">14</xref>,<xref rid="pcbi.1004649.ref016" ref-type="bibr">16</xref>].</p>
</sec>
<sec id="sec010">
<title>Accuracy of Localization in Bisensory Conditions</title>
<p>Given that the average biases in the visual and auditory modalities are in opposite directions, an intriguing question arises: in which direction is this conflict resolved when auditory and visual stimuli co-occur at the same location? Analysis of congruent bisensory trials (i.e., trials on which the visual and auditory stimulus co-occurred at the same spatial location) revealed that on average, both the visual and auditory modality exhibited <italic>central</italic> biases (<xref rid="pcbi.1004649.g004" ref-type="fig">Fig 4</xref>). However, as shown below (see <xref rid="pcbi.1004649.g005" ref-type="fig">Fig 5</xref>), further analysis of these trials revealed that the bias that emerged in the auditory modality was dependent on whether the observer inferred a common cause or distinct causes for the stimuli.</p>
<fig id="pcbi.1004649.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Visual and auditory biases on spatially congruent bisensory trials.</title>
<p>First, each subject’s average bias (over 15 trials) was computed for each of the five locations. Then, the mean across 384 subjects was calculated. Error bars represent standard error of the mean across subjects’ averages.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g004" position="float" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004649.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Bisensory trials, classified by inferring one common cause, or two independent causes.</title>
<p>Panel A shows average biases and panel B displays the distribution of subjects’ individual biases for each location. In Panel C, positive numbers indicate rightward biases, and negative numbers indicate leftward biases. SEM bars are shown around the mean bias in each modality for each location.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g005" position="float" xlink:type="simple"/>
</fig>
<p>Previous research investigating auditory localizations has shown that the bias that emerges in the auditory domain is contingent upon whether observers perceive a simultaneous visual stimulus as unified with or independent from the auditory stimulus (see <xref rid="pcbi.1004649.g005" ref-type="fig">Fig 5</xref> in [<xref rid="pcbi.1004649.ref026" ref-type="bibr">26</xref>]). Observers’ explicit reports indicate that even with spatially and temporally congruent flashes and sound bursts, sometimes a common cause is perceived, but at other times discrepant causes are perceived, and this inference can affect the amount of bias that emerges during the localization task. This phenomenon has been explained quantitatively by a Bayesian causal inference model [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>]. Furthermore, recalibration of auditory space by vision has been shown to strongly depend on whether or not a common cause is perceived for the two stimuli [<xref rid="pcbi.1004649.ref044" ref-type="bibr">44</xref>].</p>
<p>Therefore, we investigated whether the inference of a common cause influences the bias in bisensory conditions. As in [<xref rid="pcbi.1004649.ref044" ref-type="bibr">44</xref>], we classified the congruent bisensory trials into “common-cause” and “independent-cause” trials based on the observer’s responses. If the visual and auditory localizations were within two degrees of each other, we considered the trial as a common-cause trial (as the stimuli are perceived to have the same location, thus reflecting the inference of a common cause). If the visual and auditory localizations were more than five degrees apart, we considered the trial to be generated by independent causes (as this degree of discrepancy between the two percepts is inconsistent with the inference of a common cause, and is unlikely to be due to motor error). Trials in which the discrepancy between visual and auditory responses were in between 2 and 5 degrees were excluded from analysis (due to uncertainty about the inference of causal structure). The results are shown in <xref rid="pcbi.1004649.g005" ref-type="fig">Fig 5</xref>.</p>
<p>As can be seen in <xref rid="pcbi.1004649.g005" ref-type="fig">Fig 5C</xref>, on “common cause” trials both modalities show a considerable central bias, which increases as eccentricity of the target increases. Interestingly, comparing these common cause trials with the corresponding unisensory trials in each modality reveals an important effect: when trials are integrated, the amount of bias <italic>decreases</italic> in each modality, and the localizations become closer to the veridical location of the stimulus (p &lt; .005 for t-tests for all eccentric locations). For example, in the visual modality, unisensory localizations at the -13 degree location exhibited a bias towards the center of 1.53 degrees, but when visual localizations are integrated with an auditory stimulus at this location, the bias decreases to 1.24 degrees. This reveals an important new finding: multisensory integration not only increases precision in the final spatial estimates of stimuli (which was previously well-established), but also increases the <italic>accuracy</italic> of the estimates by reducing biases and bringing the final estimates closer to the true stimulus location.</p>
<p>Localizations on trials where independent causes were inferred are much more variable: while the visual modality shows a central bias that appears to increase as eccentricity increases, the bias that emerges in the auditory modality is irregular, and the trend less clear. This variability is apparent in <xref rid="pcbi.1004649.g005" ref-type="fig">Fig 5C</xref>, where the irregularity in auditory localizations on trials where independent causes were inferred is evident.</p>
<p>Thus, to summarize: in spatially congruent audiovisual trials in which a common cause is perceived, the visual bias dominates, however, the degree of bias is smaller relative to unisensory trials. In contrast, on segregated trials, while the visual modality still exhibits a central bias, auditory biases are extremely variable.</p>
</sec>
<sec id="sec011">
<title>Precision of Localization</title>
<p>Analysis of the variability in unisensory trials (<xref rid="pcbi.1004649.g006" ref-type="fig">Fig 6</xref>) showed the following: the standard deviation of visual localizations was not equivalent across conditions (F(4,1532) = 39.467, p &lt; .001), and increased as the eccentricity of the stimulus increased (p &lt; .001 for all paired-samples t-tests between adjacent conditions). The standard deviation of auditory localizations also revealed differences contingent on stimulus eccentricity (F(4, 1532) = 16.32, p &lt; .001), but in a different way. That is to say, variability in the two auditory standard deviations surrounding the zero-degree location (+6.5 and -6.5) degrees) were not significantly different from the variability at the central location (p &gt; .05 for both paired-samples t-tests), but variability in the peripheral locations (-13 and +13 degrees) were significantly different from the variability in localizations at the spatially adjacent neighbors (p &lt; .001 for both paired-samples t-tests).</p>
<fig id="pcbi.1004649.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Standard deviations for all five stimulus positions.</title>
<p>(A) Unisensory visual (blue) and unisensory auditory (red) localizations, and (B) bisensory visual (blue) and bisensory auditory (red) localizations are shown for each modality.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g006" position="float" xlink:type="simple"/>
</fig>
<p>We also evaluated the precision in visual and auditory localizations on trials where the visual and auditory stimuli co-occurred at the same locations in space. In the visual modality, similar to the unisensory trials, the standard deviations for visual localizations increased as eccentricity increased (F(4,1532) = 29.89, p &lt; .001)). In the auditory modality, although a significant effect emerged (F(4,1532) = 22.73, p &lt; .001), none of the peripheral locations were significantly different from one another, and the only significant difference was that variance in peripheral locations was larger than that of the center (p &lt; .001 for all paired-samples t-tests).</p>
</sec>
<sec id="sec012">
<title>Model Comparisons</title>
<p><xref rid="pcbi.1004649.t002" ref-type="table">Table 2</xref> shows the results of model fits for each of the 6 models. The first two columns describe each model, columns 3 and 4 show the log likelihood and BIC values averaged across all observers, column 5 shows the number of subjects best fit by each model, and column 6 shows the average R<sup>2</sup> values [<xref rid="pcbi.1004649.ref045" ref-type="bibr">45</xref>] across all observers. As can be seen, the 8-parameter model accounts for the observers’ data the best, as it has the best-fitting average log likelihood value, the lowest average BIC value (i.e. the best account of the data even after penalizing it for having larger number of free parameters than some of the other models), the largest R<sup>2</sup> value, and the highest number of subjects that are best fit by its iteration. In fact, the number of participants whose data is best accounted for by the 8-parameter model is one order of magnitude larger than the runner up model, providing compelling evidence for the superiority of this model over the other models in explaining the data.</p>
<table-wrap id="pcbi.1004649.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.t002</object-id>
<label>Table 2</label> <caption><title>Summary of modeling results.</title></caption>
<alternatives>
<graphic id="pcbi.1004649.t002g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.t002" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="1" colspan="1">Model Description</th>
<th align="center" rowspan="1" colspan="1">Model Parameters</th>
<th align="center" rowspan="1" colspan="1">Log Likelihood</th>
<th align="center" rowspan="1" colspan="1">BIC</th>
<th align="center" rowspan="1" colspan="1"># of Subjects Best Fit</th>
<th align="center" rowspan="1" colspan="1">R<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><underline>4-Param Original Model</underline>: Meta-modal Central Bias prior</td>
<td align="left" rowspan="1" colspan="1">Pc, σ<sub>V</sub>, σ<sub>A,</sub> σ<sub>P</sub></td>
<td align="center" rowspan="1" colspan="1">-2416.0</td>
<td align="center" rowspan="1" colspan="1">4857.1</td>
<td align="center" rowspan="1" colspan="1">2</td>
<td align="center" rowspan="1" colspan="1">.807</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><underline>5-Param Likelihood Model</underline>: Shift in Likelihood Means</td>
<td align="left" rowspan="1" colspan="1">Pc, σ<sub>V</sub>, σ<sub>A,</sub> Δx<sub>V,</sub> Δx<sub>A</sub></td>
<td align="center" rowspan="1" colspan="1">-2357.9</td>
<td align="center" rowspan="1" colspan="1">4747.2</td>
<td align="center" rowspan="1" colspan="1">0</td>
<td align="center" rowspan="1" colspan="1">.827</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><underline>6-Param Likelihood Model</underline>: Shift in Likelihood Means, Variable Visual Likelihood Variance</td>
<td align="left" rowspan="1" colspan="1">Pc, σ<sub>V</sub>, σ<sub>A,</sub> Δx<sub>V,</sub> Δx<sub>A,</sub> Δσ<sub>V</sub></td>
<td align="center" rowspan="1" colspan="1">-2333.9</td>
<td align="center" rowspan="1" colspan="1">4705.3</td>
<td align="center" rowspan="1" colspan="1">33</td>
<td align="center" rowspan="1" colspan="1">.831</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><underline><bold>8-Param Hybrid Model</bold></underline>: <bold>Shift in Likelihood Means, Variable Visual Likelihood Variance, Meta-Modal Central Bias Prior</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>Pc, σ</bold><sub><bold>V</bold></sub><bold>, σ</bold><sub><bold>A,</bold></sub> <bold>Δx</bold><sub><bold>V,</bold></sub> <bold>Δx</bold><sub><bold>A,</bold></sub> <bold>Δσ</bold><sub><bold>V,</bold></sub> <bold>σ</bold><sub><bold>P,</bold></sub> <bold>x</bold><sub><bold>P</bold></sub></td>
<td align="center" rowspan="1" colspan="1"><bold>-2311.8</bold></td>
<td align="center" rowspan="1" colspan="1"><bold>4673.7</bold></td>
<td align="center" rowspan="1" colspan="1"><bold>290</bold></td>
<td align="center" rowspan="1" colspan="1"><bold>.842</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><underline>7-Param Prior Model</underline>: Visual Central Prior &amp; Auditory Peripheral Prior</td>
<td align="left" rowspan="1" colspan="1">Pc, σ<sub>V</sub>, σ<sub>A,</sub> σ<sub>VP,</sub> x<sub>VP,</sub> σ<sub>AP,</sub> x<sub>AP</sub></td>
<td align="center" rowspan="1" colspan="1">-2352.3</td>
<td align="center" rowspan="1" colspan="1">4748.4</td>
<td align="center" rowspan="1" colspan="1">21</td>
<td align="center" rowspan="1" colspan="1">.827</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><underline>9-Param Prior Model</underline>: Visual Central Prior, Auditory Peripheral Prior, &amp; General Spatial Prior</td>
<td align="left" rowspan="1" colspan="1">Pc, σ<sub>V</sub>, σ<sub>A,</sub> σ<sub>VP,</sub> x<sub>VP,</sub> σ<sub>AP,</sub> x<sub>AP,</sub> σ<sub>P,</sub> x<sub>P</sub></td>
<td align="center" rowspan="1" colspan="1">-2351.7</td>
<td align="center" rowspan="1" colspan="1">4765.7</td>
<td align="center" rowspan="1" colspan="1">38</td>
<td align="center" rowspan="1" colspan="1">.828</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>A one-way repeated-measures ANOVA applied to the BIC values [<xref rid="pcbi.1004649.ref046" ref-type="bibr">46</xref>] showed that the model fits were significantly different from one another (F(5, 1915) = 248.1, p &lt; .001). Furthermore, a post-hoc t-test between the two best models (the 6-parameter and 8-parameter model) showed that the 8-parameter model was significantly superior (t(383) = 10.2, p = .001). Thus, a model that incorporates biases in the sensory representations and a general (metamodal) prior bias for center best captures observers’ performance in our multisensory localization task.</p>
<p>Previous versions of the model have fit data quite well (as also evidenced in the high R<sup>2</sup> values for all model fits) [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>,<xref rid="pcbi.1004649.ref031" ref-type="bibr">31</xref>]. It is therefore important to note that simpler versions of the model in this study did not do an inadequate job of fitting the overall data; instead, they were limited primarily in their abilities to account for the unisensory data, which revealed sensory biases in eccentric positions. Shown in <xref rid="pcbi.1004649.g007" ref-type="fig">Fig 7</xref> are model fits for the auditory-only conditions from one randomly selected subject. When plotted, it becomes clear that the simple 4-parameter model fails to account for peripheral auditory biases, and thus more complex models are needed to fully account for the phenomenon of sensory biases.</p>
<fig id="pcbi.1004649.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004649.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Auditory-alone condition model fits for one randomly selected subject.</title>
<p>Plotted across the columns are model fits for each of the five auditory locations, from -13 to +13. The dotted light blue line shows the true stimulus location, the subject’s data is plotted in the shaded light blue regions, and the solid dark blue line shows the model’s fit to the data. As shown by the red arrows, simple models that assume unbiased sensory representations make considerable errors in estimating subjects’ response, as simulated response distributions are centered near the true stimulus locations. By allowing the sensory representations to vary, biases can be more fully accounted for, as is shown in the fits for the 8-parameter model in the second row.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004649.g007" position="float" xlink:type="simple"/>
</fig>
<p>In the winning hybrid model (8 parameters), the optimized value of the mean for the general meta-modal prior was almost zero, reaffirming the central bias that had been assumed in the original 4-parameter model [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>,<xref rid="pcbi.1004649.ref031" ref-type="bibr">31</xref>]. Therefore, the only difference between the winning model and the original 4-parameter model is in the bias in visual and auditory sensory representations, and the increase in visual likelihood variance as a function of eccentricity.</p>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec014">
<title>Auditory and Visual Perception of Space Is Indeed Biased</title>
<p>Many previous studies have suggested that humans’ visual and auditory localizations in surrounding space are biased and inaccurate [<xref rid="pcbi.1004649.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1004649.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004649.ref013" ref-type="bibr">13</xref>,<xref rid="pcbi.1004649.ref015" ref-type="bibr">15</xref>]. If such biases are truly present in human perception of space, they would pose a riddle and warrant further investigation into their characteristics and function. However, previous studies have been based on relatively small sample sizes that render the results prone to outliers and other statistical irregularities. To examine whether these biases are real (or reflect noise or other phenomena), we used a very large sample size (384 subjects) and an experimental design minimizing the potential for induction of any perceptual or response biases. The experiment consisted of a large variety of unisensory and bisensory stimulus conditions presented in pseudo-random order with a uniform distribution across space, and a fairly large number of trials per condition. Furthermore, the positions of the auditory and visual sources were unknown to the observers, no feedback was provided, and the initial position of the response cursor was randomized. Thus, the experimental design could not induce any perceptual or response bias for any given position in space. By using a rigorous experimental design and acquiring a large dataset, we aimed to obtain a more definitive answer to the question of the existence and nature of any biases in the visual and auditory perception of space.</p>
<p>The results demonstrated that when observers were asked to localize briefly presented visual and auditory targets that differ in azimuth, on average, localizations of unisensory visual targets displayed a central bias that increased with eccentricity, and localizations of unisensory auditory targets displayed a peripheral bias that increased with eccentricity. Our finding of a central bias in visual localization is consistent with most studies in this area, especially those in which the cursor was visible during response, which thus minimizes motor error [<xref rid="pcbi.1004649.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1004649.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1004649.ref013" ref-type="bibr">13</xref>,<xref rid="pcbi.1004649.ref015" ref-type="bibr">15</xref>]. While the vast majority of observers in our sample showed a clear and consistent bias towards center in vision, the biases exhibited by observers in the auditory modality were more variable and less consistent, but still on average displayed a peripheral bias. This finding is also consistent with those of several previous studies of auditory spatial localizations [<xref rid="pcbi.1004649.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1004649.ref013" ref-type="bibr">13</xref>,<xref rid="pcbi.1004649.ref014" ref-type="bibr">14</xref>].</p>
<p>Localizations for spatially congruent bisensory stimuli exhibited biases that were contingent upon the subject’s inference of the causal structure of stimuli: when a common cause was inferred, the visual bias dominated. When independent causes were inferred, vision continued to exhibit a bias towards the center of space, but the auditory modality on average showed a peripheral bias (as in the unisensory auditory condition).</p>
<p>These findings are quite consistent with research requiring explicit judgments of unity or discrepancy [<xref rid="pcbi.1004649.ref026" ref-type="bibr">26</xref>], indicating that sensory systems implicitly perform inference processes, and that the biases that emerge are contingent upon whether or not sensory signals were integrated or segregated. Interestingly, on trials for which a common cause was perceived, the opposing biases were reconciled in the following manner: the visual bias dominated, although its magnitude was reduced. This finding strongly suggests that the observed biases in the unisensory conditions stem from a bias in sensory representations. Because visual representations are significantly more precise (and reliable) than the auditory representations, in bisensory conditions the visual signals dominate, and therefore, the visual bias dominates the auditory bias. However, in principle, any bias in perception could originate from either a bias in sensory representations (modeled by likelihood functions) or a bias in the expectations or model of the world (prior).</p>
</sec>
<sec id="sec015">
<title>The Biases in Localization Largely Reflect Biases in Sensory Representations</title>
<p>While the findings discussed above suggest that biases in bisensory conditions are more consistent with a bias in the sensory representations, to address this question rigorously, one needs to quantitatively model these scenarios and perform model comparisons to determine which mechanism(s) account for the behavioral data the best.</p>
<p>To do this, we used the Bayesian Causal Inference model that has been shown to account for human observers’ multisensory perception quite effectively [<xref rid="pcbi.1004649.ref030" ref-type="bibr">30</xref>–<xref rid="pcbi.1004649.ref035" ref-type="bibr">35</xref>,<xref rid="pcbi.1004649.ref037" ref-type="bibr">37</xref>]. We compared models incorporating a bias in likelihoods to those incorporating a bias in prior expectations, and also tested one model that incorporated both mechanisms. We fitted the model parameters to each observer’s data, and compared the models based on data from 384 observers. Consistent with the qualitative observation discussed above, the quantitative model comparison results revealed that indeed a bias in auditory and visual spatial sensory representations is necessary to account for the participants’ data. But over and above these sensory biases, a general (metamodal) prior bias for center appears to also exist in perception of space.</p>
</sec>
<sec id="sec016">
<title>Why Do Biases Exist?</title>
<p>Why should the visual system be biased in perceiving locations towards the center? Upon first glance, this would appear to be a tremendously suboptimal strategy, but insights from phenomena in other perceptual domains may provide some possible explanations. For instance, saccadic undershooting for eye movements to peripheral stimuli has been demonstrated conclusively and would seem to be a suboptimal strategy [<xref rid="pcbi.1004649.ref047" ref-type="bibr">47</xref>,<xref rid="pcbi.1004649.ref048" ref-type="bibr">48</xref>], but one potential explanation that has been posited is that it represents an advantageous trade-off between accuracy and the movement time needed to reasonably localize the stimulus [<xref rid="pcbi.1004649.ref049" ref-type="bibr">49</xref>]. Here, we find spatial localization biases in sensory representations. If visual sensory representations were built <italic>only</italic> for this task, then biases could be suboptimal. But sensory representations are calibrated to be optimal for acting in a complex and changing world in a variety of tasks and conditions, and thus, biases in the visual and auditory systems could be optimal if they are due to an advantageous trade-off; for example, between foveal acuity and peripheral accuracy. Due to the much larger number of neurons representing the fovea compared to peripheral regions [<xref rid="pcbi.1004649.ref050" ref-type="bibr">50</xref>], the representation of space may be skewed in that direction if spatial representation involves a population code [<xref rid="pcbi.1004649.ref051" ref-type="bibr">51</xref>]. It seems possible that this cortical magnification effect may skew visual localizations towards the center.</p>
<p>One recent study investigating the perception of visual orientation and spatial frequency posits another possible account: for a Bayesian observer, perceptual priors match stimulus distributions in the world, and sensory likelihoods (i.e. sensory representations) become distorted through a process of “efficient encoding” [<xref rid="pcbi.1004649.ref052" ref-type="bibr">52</xref>]. In this framework, likelihood functions are constrained by the Fisher information in the encoding process, and the Fisher information matches the stimulus distribution in the world. Thus, rather than positing that biased perception is driven entirely by priors (as many previous models have proposed), this study shows that <italic>sensory representations</italic> can themselves be biased, and this approach can account for biases in perceptual behavior that previously appeared to be “anti-Bayesian.” Here, in our study, we similarly find perceptual biases best characterized by biases in sensory representations. Future work should investigate whether behaviors such as orienting to sources of sensory stimulation (objects and events) may produce a disproportional amount of visual stimuli being located in the center and in turn result in the representation of visual space to become biased accordingly.</p>
<p>Additionally, based on everyday subjective experience, visual biases would seem to be quite counterintuitive, as the surrounding spatial world does not appear to be compressed. However, as noted by Rahnev et al. [<xref rid="pcbi.1004649.ref053" ref-type="bibr">53</xref>], while subjective perception of the periphery <italic>appears</italic> to be accurate and colorful, studies have demonstrated that the visual periphery has proportionally less processing resolution, likely impacting capacities for form and motion perception [<xref rid="pcbi.1004649.ref054" ref-type="bibr">54</xref>], and color sensitivity [<xref rid="pcbi.1004649.ref055" ref-type="bibr">55</xref>]. Therefore, subjective assessments of perceptual capacities in the periphery are inaccurate, and comprehensive investigations are needed to quantify the relative impairments in perception outside the fovea. Similar to lack of subjective awareness of color deficiencies in the periphery, here, in the spatial domain, the observers seem to be unaware of their bias in spatial localization.</p>
<p>It should also be noted that this study is not the first to document the counterintuitive phenomenon of distortions in perception of surrounding visual space. For instance, inaccuracies in visual estimates of both egocentric distance estimates and objects’ shapes have been well documented [<xref rid="pcbi.1004649.ref056" ref-type="bibr">56</xref>]. Subsequently, Bingham et al. have raised the question of how accurate movements are possible despite these visual distortions [<xref rid="pcbi.1004649.ref057" ref-type="bibr">57</xref>], and indeed, many studies report that perceptual responses and visually-guided reaching actions are dissociated or even uncorrelated [<xref rid="pcbi.1004649.ref058" ref-type="bibr">58</xref>–<xref rid="pcbi.1004649.ref061" ref-type="bibr">61</xref>]. As these authors note, it appears likely that feedback plays an important role in minimizing the impact these distortions can have on our movement accuracy, and it also seems that (as we report here between the auditory and visual modalities) multisensory integration can assist in reducing biases present in a given pair of sensory modalities [<xref rid="pcbi.1004649.ref057" ref-type="bibr">57</xref>].</p>
<p>The findings regarding peripheral auditory biases also raise the question of why these biases should exist in the auditory domain. As noted by [<xref rid="pcbi.1004649.ref015" ref-type="bibr">15</xref>], prolonged fixation in an eccentric direction shifts the entire map of auditory space in the direction of the fixation point, even for auditory stimuli that are at the same location as fixation. While auditory localizations still exhibit a slight peripheral bias when central fixation is maintained, it appears that when the eyes are allowed to move, auditory space is shifted in the direction of the eye movement. Further support for this idea comes from a study which included one condition where the eyes were allowed to move when localizing the target, and another condition where central fixation was enforced [<xref rid="pcbi.1004649.ref016" ref-type="bibr">16</xref>]. Results indicated that enforcing fixation greatly <italic>reduced</italic> the magnitude of auditory biases, suggesting that auditory biases may be driven in part by eye movements. Thus, in our task, as eye position was not constrained, the auditory bias may be at least partially due to this mechanism.</p>
<p>Moreover, much is still unknown about how auditory spatial perception is encoded in the human brain. One current idea is that rather than having explicit auditory spatial <italic>maps</italic>, some form of a distributed population code is built by neurons that may have broad tuning curves [<xref rid="pcbi.1004649.ref062" ref-type="bibr">62</xref>]. As noted in a recent review [<xref rid="pcbi.1004649.ref063" ref-type="bibr">63</xref>], this population rate code may be able to be characterized as a “hemifield code,” where the firing rates of the relevant auditory neuron population are highest for stimuli in the periphery (-90 and +90 degrees) that maximize the ITD and ILD information available. This idea has substantial support from experimental findings indicating that sound source locations could be encoded by the relative firing rates of the right-tuned or left-tuned auditory cortical neurons [<xref rid="pcbi.1004649.ref064" ref-type="bibr">64</xref>,<xref rid="pcbi.1004649.ref065" ref-type="bibr">65</xref>] and has been further supported by recent neuroimaging work [<xref rid="pcbi.1004649.ref066" ref-type="bibr">66</xref>,<xref rid="pcbi.1004649.ref067" ref-type="bibr">67</xref>]. Thus, it seems possible that when peripheral stimuli are presented, enhanced activity in the auditory population code could skew perception of auditory space towards eccentric locations. Future research will need to determine exactly how activity in both cortical and subcortical regions contributes to humans’ perception of auditory space.</p>
</sec>
<sec id="sec017">
<title>Multisensory Integration Reduces the Bias in Spatial Perception</title>
<p>Multisensory integration may be one of the strategies the nervous system attempts to reduce the problem of unisensory biases. Our behavioral data showed that biases in spatial perception were much smaller in bisensory congruent trials (in which both sound and light were presented at the same location), relative to either the unisensory visual or unisensory auditory trials. Therefore, it appears that multisensory integration not only benefits perception by improving precision (reducing variability), but also by reducing bias; a win-win approach.</p>
</sec>
<sec id="sec018">
<title>Open Questions</title>
<p>While the present work provides valuable insight into the computational mechanism underlying spatial biases in the visual and auditory systems, future research must address several pertinent questions. For example, future studies will need to address the extent of these biases in more complex environments, including more naturalistic settings. For instance, it has been reported that the localization of objects is often biased towards other elements in the visual field [<xref rid="pcbi.1004649.ref007" ref-type="bibr">7</xref>]. Known as the “landmark attraction effect” [<xref rid="pcbi.1004649.ref068" ref-type="bibr">68</xref>,<xref rid="pcbi.1004649.ref069" ref-type="bibr">69</xref>], this effect is particularly relevant for the current paradigm; while this investigation illuminates the biases present in each modality in a sparse visual scene (i.e. with only a fixation cross present at the start of each trial), the question remains as to how biases change as objects that are potential sources of the relevant auditory and visual signals are introduced to the environment.</p>
<p>Finally, we also acknowledge that factors such as stimulus duration, response mechanism, and eye position could potentially influence the magnitude of the biases that emerge in a given modality. For example, one previous study presenting auditory stimuli for a much longer duration than our study (10s) reported greater peripheral biases for eccentric auditory stimuli than what we found in this investigation [<xref rid="pcbi.1004649.ref014" ref-type="bibr">14</xref>]. Thus, future experiments should seek to systematically manipulate each of these factors to reveal the impact of each variable.</p>
</sec>
<sec id="sec019">
<title>Conclusion</title>
<p>Nearly every function critical for human survival depends directly on accurate localization of objects and events in the environment. While numerous studies have investigated spatial perception in humans, little consensus exists on whether representation of space in the human brain is accurate or biased, and if the latter is true, how so and why.</p>
<p>The findings of this large-scale study revealed that on average, observers’ localizations are <italic>inaccurate</italic>, as visual localizations show a bias towards the center of space, while auditory localizations show a bias towards the periphery. Even more surprisingly, the observed biases in localization appear to be at least partly due to biases in sensory representations of space as opposed to <italic>a priori</italic> expectations of the spatial layout of objects in the environment (which may be due to a number of non-sensory factors such as the experimental setup, instructions, learning the distribution of stimuli during experiment, etc.).</p>
<p>In real-world settings, objects frequently produce signals that simultaneously stimulate multiple sensory modalities. Considering the opposing biases in the visual and auditory modalities, when visual and auditory stimuli <italic>co-occur</italic> at the <italic>same</italic> spatial location, which bias emerges during localization? The results show that visual bias dominates, however, the magnitude of the central bias in the visual modality is <italic>reduced</italic> when the visual stimulus is integrated with a co-occurring auditory stimulus, thus revealing an additional advantage of multisensory integration: <italic>the reduction of perceptual biases</italic>.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004649.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adam</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Davelaar</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>van der Gouw</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Willems</surname> <given-names>P</given-names></name>. <article-title>Evidence for attentional processing in spatial localization</article-title>. <source>Psychological Research</source>. <year>2008</year> <month>Jul</month> <day>1</day>;<volume>72</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">17899176</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fortenbaugh</surname> <given-names>FC</given-names></name>, <name name-style="western"><surname>Robertson</surname> <given-names>LC</given-names></name>. <article-title>When here becomes there: attentional distribution modulates foveal bias in peripheral localization</article-title>. <source>Atten Percept Psychophys</source>. <year>2011</year> <month>Apr</month> <day>1</day>;<volume>73</volume>(<issue>3</issue>):<fpage>809</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13414-010-0075-5" xlink:type="simple">10.3758/s13414-010-0075-5</ext-link></comment> <object-id pub-id-type="pmid">21264747</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mateeff</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gourevich</surname> <given-names>A</given-names></name>. <article-title>Peripheral vision and perceived visual direction</article-title>. <source>Biol Cybernetics</source>. <year>1983</year> <month>Dec</month> <day>1</day>;<volume>49</volume>(<issue>2</issue>):<fpage>111</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Müsseler</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Heijden</surname> <given-names>AHCVD</given-names></name>, <name name-style="western"><surname>Mahmud</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Deubel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ertsey</surname> <given-names>S</given-names></name>. <article-title>Relative mislocalization of briefly presented stimuli in the retinal periphery</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1999</year> <month>Dec</month> <day>1</day>;<volume>61</volume>(<issue>8</issue>):<fpage>1646</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Osaka</surname> <given-names>N</given-names></name>. <article-title>Effect of Refraction on Perceived Locus of a Target in the Peripheral Visual Field</article-title>. <source>The Journal of Psychology</source>. <year>1977</year>;<volume>95</volume>(<issue>1</issue>):<fpage>59</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parise</surname> <given-names>CV</given-names></name>, <name name-style="western"><surname>Spence</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>When Correlation Implies Causation in Multisensory Integration</article-title>. <source>Current Biology</source>. <year>2012</year> <month>Jan</month> <day>10</day>;<volume>22</volume>(<issue>1</issue>):<fpage>46</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2011.11.039" xlink:type="simple">10.1016/j.cub.2011.11.039</ext-link></comment> <object-id pub-id-type="pmid">22177899</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerzel</surname> <given-names>D</given-names></name>. <article-title>Memory for the position of stationary objects: disentangling foveal bias and memory averaging</article-title>. <source>Vision Research</source>. <year>2002</year> <month>Jan</month>;<volume>42</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>67</lpage>. <object-id pub-id-type="pmid">11809470</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sheth</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>. <article-title>Compression of space in visual memory</article-title>. <source>Vision Research</source>. <year>2001</year> <month>Feb</month>;<volume>41</volume>(<issue>3</issue>):<fpage>329</fpage>–<lpage>41</lpage>. <object-id pub-id-type="pmid">11164448</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bock</surname> <given-names>O</given-names></name>. <article-title>Contribution of retinal versus extraretinal signals towards visual localization in goal-directed movements</article-title>. <source>Exp Brain Res</source>. <year>1986</year> <month>Nov</month> <day>1</day>;<volume>64</volume>(<issue>3</issue>):<fpage>476</fpage>–<lpage>82</lpage>. <object-id pub-id-type="pmid">3803485</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bock</surname> <given-names>O</given-names></name>. <article-title>Localization of objects in the peripheral visual field</article-title>. <source>Behavioural Brain Research</source>. <year>1993</year> <month>Jul</month> <day>30</day>;<volume>56</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>84</lpage>. <object-id pub-id-type="pmid">8397856</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruno</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Morrone</surname> <given-names>MC</given-names></name>. <article-title>Influence of saccadic adaptation on spatial localization: Comparison of verbal and pointing reports</article-title>. <source>J Vis</source>. <year>2007</year> <month>Dec</month> <day>28</day>;<volume>7</volume>(<issue>5</issue>):<fpage>16</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/7.5.16" xlink:type="simple">10.1167/7.5.16</ext-link></comment> <object-id pub-id-type="pmid">18217856</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henriques</surname> <given-names>DYP</given-names></name>, <name name-style="western"><surname>Klier</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Lowy</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Crawford</surname> <given-names>JD</given-names></name>. <article-title>Gaze-Centered Remapping of Remembered Visual Space in an Open-Loop Pointing Task</article-title>. <source>J Neurosci</source>. <year>1998</year> <month>Feb</month> <day>15</day>;<volume>18</volume>(<issue>4</issue>):<fpage>1583</fpage>–<lpage>94</lpage>. <object-id pub-id-type="pmid">9454863</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cui</surname> <given-names>QN</given-names></name>, <name name-style="western"><surname>O’Neill</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Paige</surname> <given-names>GD</given-names></name>. <article-title>Advancing age alters the influence of eye position on sound localization</article-title>. <source>Exp Brain Res</source>. <year>2010</year> <month>Oct</month> <day>1</day>;<volume>206</volume>(<issue>4</issue>):<fpage>371</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-010-2413-1" xlink:type="simple">10.1007/s00221-010-2413-1</ext-link></comment> <object-id pub-id-type="pmid">20857091</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewald</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ehrenstein</surname> <given-names>WH</given-names></name>. <article-title>Auditory-visual spatial integration: A new psychophysical approach using laser pointing to acoustic targets</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1998</year> <month>Sep</month> <day>1</day>;<volume>104</volume>(<issue>3</issue>):<fpage>1586</fpage>–<lpage>97</lpage>. <object-id pub-id-type="pmid">9745742</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Razavi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>O’Neill</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Paige</surname> <given-names>GD</given-names></name>. <article-title>Auditory Spatial Perception Dynamically Realigns with Changing Eye Position</article-title>. <source>J Neurosci</source>. <year>2007</year> <month>Sep</month> <day>19</day>;<volume>27</volume>(<issue>38</issue>):<fpage>10249</fpage>–<lpage>58</lpage>. <object-id pub-id-type="pmid">17881531</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dobreva</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>O’Neill</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Paige</surname> <given-names>GD</given-names></name>. <article-title>Influence of age, spatial memory, and ocular fixation on localization of auditory, visual, and bimodal targets by human subjects</article-title>. <source>Exp Brain Res</source>. <year>2012</year> <month>Dec</month> <day>1</day>;<volume>223</volume>(<issue>4</issue>):<fpage>441</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-012-3270-x" xlink:type="simple">10.1007/s00221-012-3270-x</ext-link></comment> <object-id pub-id-type="pmid">23076429</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zahn</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Abel</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Dell’Osso</surname> <given-names>LF</given-names></name>. <article-title>Audio-ocular response characteristics</article-title>. <source>Sens Processes</source>. <year>1978</year> <month>Mar</month>;<volume>2</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>7</lpage>. <object-id pub-id-type="pmid">705354</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hairston</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Wallace</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Vaughan</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Norris</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Schirillo</surname> <given-names>JA</given-names></name>. <article-title>Visual Localization Ability Influences Cross-Modal Bias</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2003</year> <month>Jan</month> <day>1</day>;<volume>15</volume>(<issue>1</issue>):<fpage>20</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">12590840</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Charbonneau</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Véronneau</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boudrias-Fournier</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lepore</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Collignon</surname> <given-names>O</given-names></name>. <article-title>The ventriloquist in periphery: Impact of eccentricity-related reliability on audio-visual localization</article-title>. <source>J Vis</source>. <year>2013</year> <month>Oct</month> <day>28</day>;<volume>13</volume>(<issue>12</issue>):<fpage>20</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/13.12.20" xlink:type="simple">10.1167/13.12.20</ext-link></comment> <object-id pub-id-type="pmid">24167163</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uddin</surname> <given-names>MK</given-names></name>. <article-title>Visual spatial localization and the two-process model</article-title>. <source>Psychological research of Kyushu University</source>. <year>2006</year> <month>Mar</month>;<volume>7</volume>:<fpage>65</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fortenbaugh</surname> <given-names>FC</given-names></name>, <name name-style="western"><surname>Sanghvi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Robertson</surname> <given-names>LC</given-names></name>. <article-title>Exploring the edges of visual space: the influence of visual boundaries on peripheral localization</article-title>. <source>J Vis</source>. <year>2012</year>;<volume>12</volume>(<issue>2</issue>).</mixed-citation></ref>
<ref id="pcbi.1004649.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Aschersleben</surname> <given-names>G</given-names></name>. <article-title>Automatic visual bias of perceived auditory location</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>1998</year> <month>Sep</month> <day>1</day>;<volume>5</volume>(<issue>3</issue>):<fpage>482</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Radeau</surname> <given-names>M</given-names></name>. <article-title>Cross-modal bias and perceptual fusion with auditory-visual spatial discordance</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1981</year> <month>Nov</month> <day>1</day>;<volume>29</volume>(<issue>6</issue>):<fpage>578</fpage>–<lpage>84</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewald</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Guski</surname> <given-names>R</given-names></name>. <article-title>Cross-modal perceptual integration of spatially and temporally disparate auditory and visual stimuli</article-title>. <source>Cognitive Brain Research</source>. <year>2003</year> <month>May</month>;<volume>16</volume>(<issue>3</issue>):<fpage>468</fpage>–<lpage>78</lpage>. <object-id pub-id-type="pmid">12706226</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Slutsky</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>. <article-title>Temporal and spatial dependency of the ventriloquism effect</article-title>. <source>Neuroreport</source>. <year>2001</year> <month>Jan</month> <day>22</day>;<volume>12</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>10</lpage>. <object-id pub-id-type="pmid">11201094</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallace</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Roberson</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Hairston</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Vaughan</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Schirillo</surname> <given-names>JA</given-names></name>. <article-title>Unifying multisensory signals across time and space</article-title>. <source>Exp Brain Res</source>. <year>2004</year> <month>Sep</month> <day>1</day>;<volume>158</volume>(<issue>2</issue>):<fpage>252</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">15112119</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pick</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Hay</surname> <given-names>JC</given-names></name>. <article-title>Sensory conflict in judgments of spatial direction</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1969</year> <month>Jul</month> <day>1</day>;<volume>6</volume>(<issue>4</issue>):<fpage>203</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welch</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>DH</given-names></name>. <article-title>Immediate perceptual response to intersensory discrepancy</article-title>. <source>Psychological Bulletin</source>. <year>1980</year>;<volume>88</volume>(<issue>3</issue>):<fpage>638</fpage>–<lpage>67</lpage>. <object-id pub-id-type="pmid">7003641</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vroomen</surname> <given-names>J</given-names></name>. <article-title>Intersensory binding across space and time: A tutorial review</article-title>. <source>Atten Percept Psychophys</source>. <year>2013</year> <month>Jul</month> <day>1</day>;<volume>75</volume>(<issue>5</issue>):<fpage>790</fpage>–<lpage>811</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13414-013-0475-4" xlink:type="simple">10.3758/s13414-013-0475-4</ext-link></comment> <object-id pub-id-type="pmid">23709064</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Causal Inference in Multisensory Perception</article-title>. <source>PLoS ONE</source>. <year>2007</year> <month>Sep</month> <day>26</day>;<volume>2</volume>(<issue>9</issue>):<fpage>e943</fpage>. <object-id pub-id-type="pmid">17895984</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Probability Matching as a Computational Strategy Used in Perception</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year> <month>Aug</month> <day>5</day>;<volume>6</volume>(<issue>8</issue>):<fpage>e1000871</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000871" xlink:type="simple">10.1371/journal.pcbi.1000871</ext-link></comment> <object-id pub-id-type="pmid">20700493</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Computational Characterization of Visually Induced Auditory Spatial Adaptation</article-title>. <source>Front Integr Neurosci [Internet]</source>. <year>2011</year> <month>Nov</month> <day>4</day> [cited 2013 Feb 14];5. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3208186/" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3208186/</ext-link></mixed-citation></ref>
<ref id="pcbi.1004649.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Bayesian priors are encoded independently from likelihoods in human multisensory perception</article-title>. <source>J Vis</source>. <year>2009</year> <month>May</month> <day>21</day>;<volume>9</volume>(<issue>5</issue>):<fpage>23</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/9.5.23" xlink:type="simple">10.1167/9.5.23</ext-link></comment> <object-id pub-id-type="pmid">19757901</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samad</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chung</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Perception of Body Ownership Is Driven by Bayesian Sensory Inference</article-title>. <source>PLoS ONE</source>. <year>2015</year> <month>Feb</month> <day>6</day>;<volume>10</volume>(<issue>2</issue>):<fpage>e0117178</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0117178" xlink:type="simple">10.1371/journal.pone.0117178</ext-link></comment> <object-id pub-id-type="pmid">25658822</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Human trimodal perception follows optimal statistical inference</article-title>. <source>J Vis [Internet]</source>. <year>2008</year> <month>Mar</month> <day>27</day> [cited 2013 Mar 3];8(3). Available from: <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/8/3/24" xlink:type="simple">http://www.journalofvision.org/content/8/3/24</ext-link></mixed-citation></ref>
<ref id="pcbi.1004649.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Multisensory Causal Inference in the Brain</article-title>. <source>PLoS Biol</source>. <year>2015</year> <month>Feb</month> <day>24</day>;<volume>13</volume>(<issue>2</issue>):<fpage>e1002075</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1002075" xlink:type="simple">10.1371/journal.pbio.1002075</ext-link></comment> <object-id pub-id-type="pmid">25710476</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Cortical Hierarchies Perform Bayesian Causal Inference in Multisensory Perception</article-title>. <source>PLoS Biol</source>. <year>2015</year> <month>Feb</month> <day>24</day>;<volume>13</volume>(<issue>2</issue>):<fpage>e1002073</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1002073" xlink:type="simple">10.1371/journal.pbio.1002073</ext-link></comment> <object-id pub-id-type="pmid">25710328</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>The Ventriloquist Effect Results from Near-Optimal Bimodal Integration</article-title>. <source>Current Biology</source>. <year>2004</year> <month>Feb</month> <day>3</day>;<volume>14</volume>(<issue>3</issue>):<fpage>257</fpage>–<lpage>62</lpage>. <object-id pub-id-type="pmid">14761661</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Battaglia</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Aslin</surname> <given-names>RN</given-names></name>. <article-title>Bayesian integration of visual and auditory signals for spatial localization</article-title>. <source>J Opt Soc Am A</source>. <year>2003</year> <month>Jul</month> <day>1</day>;<volume>20</volume>(<issue>7</issue>):<fpage>1391</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year> <month>Jan</month> <day>24</day>;<volume>415</volume>(<issue>6870</issue>):<fpage>429</fpage>–<lpage>33</lpage>. <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>. <article-title>Merging the senses into a robust percept</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2004</year> <month>Apr</month>;<volume>8</volume>(<issue>4</issue>):<fpage>162</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">15050512</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref042"><label>42</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>. <chapter-title>Computation and psychophysics of sensorimotor integration [Internet]</chapter-title>. <publisher-name>Massachusetts Institute of Technology</publisher-name>; <year>1995</year> [cited 2015 Feb 18]. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.learning.eng.cam.ac.uk/zoubin/papers/thesis.pdf" xlink:type="simple">http://www.learning.eng.cam.ac.uk/zoubin/papers/thesis.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1004649.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ocklenburg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hirnstein</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hausmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lewald</surname> <given-names>J</given-names></name>. <article-title>Auditory space perception in left- and right-handers</article-title>. <source>Brain and Cognition</source>. <year>2010</year> <month>Mar</month>;<volume>72</volume>(<issue>2</issue>):<fpage>210</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bandc.2009.08.013" xlink:type="simple">10.1016/j.bandc.2009.08.013</ext-link></comment> <object-id pub-id-type="pmid">19786316</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Recalibration of Auditory Space following Milliseconds of Cross-Modal Discrepancy</article-title>. <source>J Neurosci</source>. <year>2011</year> <month>Mar</month> <day>23</day>;<volume>31</volume>(<issue>12</issue>):<fpage>4607</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6079-10.2011" xlink:type="simple">10.1523/JNEUROSCI.6079-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21430160</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagelkerke</surname> <given-names>NJD</given-names></name>. <article-title>A note on a general definition of the coefficient of determination</article-title>. <source>Biometrika</source>. <year>1991</year> <month>Sep</month> <day>1</day>;<volume>78</volume>(<issue>3</issue>):<fpage>691</fpage>–<lpage>2</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>. <article-title>How much to trust the senses: Likelihood learning</article-title>. <source>J Vis</source>. <year>2014</year> <month>Nov</month> <day>14</day>;<volume>14</volume>(<issue>13</issue>):<fpage>13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/14.13.13" xlink:type="simple">10.1167/14.13.13</ext-link></comment> <object-id pub-id-type="pmid">25398975</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kapoula</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>DA</given-names></name>. <article-title>Saccadic undershoot is not inevitable: Saccades can be accurate</article-title>. <source>Vision Research</source>. <year>1986</year>;<volume>26</volume>(<issue>5</issue>):<fpage>735</fpage>–<lpage>43</lpage>. <object-id pub-id-type="pmid">3750853</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname> <given-names>CM</given-names></name>. <article-title>Does saccadic undershoot minimize saccadic flight-time?</article-title> <source>A Monte-Carlo study. Vision Research</source>. <year>1995</year> <month>Mar</month>;<volume>35</volume>(<issue>5</issue>):<fpage>691</fpage>–<lpage>701</lpage>. <object-id pub-id-type="pmid">7900307</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>The Main Sequence of Saccades Optimizes Speed-accuracy Trade-off</article-title>. <source>Biol Cybern</source>. <year>2006</year> <month>Mar</month> <day>23</day>;<volume>95</volume>(<issue>1</issue>):<fpage>21</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">16555070</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Connolly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>D</given-names></name>. <article-title>The representation of the visual field in parvicellular and magnocellular layers of the lateral geniculate nucleus in the macaque monkey</article-title>. <source>J Comp Neurol</source>. <year>1984</year> <month>Jul</month> <day>10</day>;<volume>226</volume>(<issue>4</issue>):<fpage>544</fpage>–<lpage>64</lpage>. <object-id pub-id-type="pmid">6747034</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fischer</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Peña</surname> <given-names>JL</given-names></name>. <article-title>Owl’s behavior and neural representation predicted by Bayesian inference</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Aug</month>;<volume>14</volume>(<issue>8</issue>):<fpage>1061</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2872" xlink:type="simple">10.1038/nn.2872</ext-link></comment> <object-id pub-id-type="pmid">21725311</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wei</surname> <given-names>X-X</given-names></name>, <name name-style="western"><surname>Stocker</surname> <given-names>AA</given-names></name>. <article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title>. <source>Nat Neurosci [Internet]</source>. <year>2015</year> <month>Sep</month> <day>7</day> [cited 2015 Sep 18];advance online publication. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4105.html" xlink:type="simple">http://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4105.html</ext-link></mixed-citation></ref>
<ref id="pcbi.1004649.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rahnev</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Maniscalco</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Graves</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>de Lange</surname> <given-names>FP</given-names></name>, <name name-style="western"><surname>Lau</surname> <given-names>H</given-names></name>. <article-title>Attention induces conservative subjective biases in visual perception</article-title>. <source>Nat Neurosci</source>. <year>2011</year> <month>Dec</month>;<volume>14</volume>(<issue>12</issue>):<fpage>1513</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2948" xlink:type="simple">10.1038/nn.2948</ext-link></comment> <object-id pub-id-type="pmid">22019729</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Azzopardi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cowey</surname> <given-names>A</given-names></name>. <article-title>Preferential representation of the fovea in the primary visual cortex</article-title>. <source>Nature</source>. <year>1993</year> <month>Feb</month> <day>25</day>;<volume>361</volume>(<issue>6414</issue>):<fpage>719</fpage>–<lpage>21</lpage>. <object-id pub-id-type="pmid">7680108</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gordon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Abramov</surname> <given-names>I</given-names></name>. <article-title>Color vision in the peripheral retina. II. Hue and saturation</article-title>. <source>J Opt Soc Am, JOSA</source>. <year>1977</year> <month>Feb</month> <day>1</day>;<volume>67</volume>(<issue>2</issue>):<fpage>202</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Tittle</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>JF</given-names></name>. <article-title>Distortions of three-dimensional space in the perceptual analysis of motion and stereo</article-title>. <source>Perception</source>. <year>1995</year>;<volume>24</volume>(<issue>1</issue>):<fpage>75</fpage>–<lpage>86</lpage>. <object-id pub-id-type="pmid">7617420</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bingham</surname> <given-names>GP</given-names></name>, <name name-style="western"><surname>Zaal</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Robin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>J</given-names></name>. <article-title>Distortions in definite distance and shape perception as measured by reaching without and with haptic feedback</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2000</year>;<volume>26</volume>(<issue>4</issue>):<fpage>1436</fpage>–<lpage>60</lpage>. <object-id pub-id-type="pmid">10946724</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pagano</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Bingham</surname> <given-names>GP</given-names></name>. <article-title>Comparing measures of monocular distance perception: Verbal and reaching errors are not correlated</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1998</year>;<volume>24</volume>(<issue>4</issue>):<fpage>1037</fpage>–<lpage>51</lpage>. <object-id pub-id-type="pmid">9706709</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref059"><label>59</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Milner</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Goodale</surname> <given-names>MA</given-names></name>. <chapter-title>The visual brain in action</chapter-title>. <publisher-loc>New York, NY, US</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1995</year>. 248 p.</mixed-citation></ref>
<ref id="pcbi.1004649.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodale</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Milner</surname> <given-names>AD</given-names></name>. <article-title>Separate visual pathways for perception and action</article-title>. <source>Trends in Neurosciences</source>. <year>1992</year> <month>Jan</month>;<volume>15</volume>(<issue>1</issue>):<fpage>20</fpage>–<lpage>5</lpage>. <object-id pub-id-type="pmid">1374953</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref061"><label>61</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Goodale</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Jakobson</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Servos</surname> <given-names>P</given-names></name>. <chapter-title>The visual pathways mediating perception and prehension</chapter-title>. In: <name name-style="western"><surname>Wing</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Haggard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Flanagan</surname> <given-names>JR</given-names></name>, editors. <source>Hand and brain: The neurophysiology and psychology of hand movements</source>. <publisher-loc>San Diego, CA, US</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>1996</year>. p. <fpage>15</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>Different Stimuli, Different Spatial Codes: A Visual Map and an Auditory Rate Code for Oculomotor Space in the Primate Superior Colliculus</article-title>. <source>PLoS ONE</source>. <year>2014</year> <month>Jan</month> <day>15</day>;<volume>9</volume>(<issue>1</issue>):<fpage>e85017</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0085017" xlink:type="simple">10.1371/journal.pone.0085017</ext-link></comment> <object-id pub-id-type="pmid">24454779</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salminen</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>Tiitinen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>PJC</given-names></name>. <article-title>Auditory Spatial Processing in the Human Cortex</article-title>. <source>Neuroscientist</source>. <year>2012</year> <month>Dec</month> <day>1</day>;<volume>18</volume>(<issue>6</issue>):<fpage>602</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1073858411434209" xlink:type="simple">10.1177/1073858411434209</ext-link></comment> <object-id pub-id-type="pmid">22492193</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Harrington</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Location Coding by Opponent Neural Populations in the Auditory Cortex</article-title>. <source>PLoS Biol</source>. <year>2005</year> <month>Feb</month> <day>22</day>;<volume>3</volume>(<issue>3</issue>):<fpage>e78</fpage>. <object-id pub-id-type="pmid">15736980</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werner-Reiss</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>A Rate Code for Sound Azimuth in Monkey Auditory Cortex: Implications for Human Neuroimaging Studies</article-title>. <source>J Neurosci</source>. <year>2008</year> <month>Apr</month> <day>2</day>;<volume>28</volume>(<issue>14</issue>):<fpage>3747</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5044-07.2008" xlink:type="simple">10.1523/JNEUROSCI.5044-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18385333</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salminen</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>PJC</given-names></name>, <name name-style="western"><surname>Alku</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tiitinen</surname> <given-names>H</given-names></name>. <article-title>A Population Rate Code of Auditory Space in the Human Cortex</article-title>. <source>PLoS ONE</source>. <year>2009</year> <month>Oct</month> <day>26</day>;<volume>4</volume>(<issue>10</issue>):<fpage>e7600</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0007600" xlink:type="simple">10.1371/journal.pone.0007600</ext-link></comment> <object-id pub-id-type="pmid">19855836</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salminen</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>Tiitinen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yrttiaho</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>PJC</given-names></name>. <article-title>The neural code for interaural time difference in human auditory cortex</article-title>. <source>J Acoust Soc Am</source>. <year>2010</year> <month>Feb</month>;<volume>127</volume>(<issue>2</issue>):<fpage>EL60</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3290744" xlink:type="simple">10.1121/1.3290744</ext-link></comment> <object-id pub-id-type="pmid">20136180</object-id></mixed-citation></ref>
<ref id="pcbi.1004649.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubbard</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Ruppel</surname> <given-names>SE</given-names></name>. <article-title>Representational momentum and the landmark attraction effect</article-title>. <source>Canadian Journal of Experimental Psychology/Revue canadienne de psychologie expérimentale</source>. <year>1999</year>;<volume>53</volume>(<issue>3</issue>):<fpage>242</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004649.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubbard</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Ruppel</surname> <given-names>SE</given-names></name>. <article-title>Spatial memory averaging, the landmark attraction effect, and representational gravity</article-title>. <source>Psychological Research Psychologische Forschung</source>. <year>2000</year> <month>Oct</month> <day>1</day>;<volume>64</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>55</lpage>. <object-id pub-id-type="pmid">11109866</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>