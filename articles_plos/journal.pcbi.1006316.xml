<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006316</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01787</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Hippocampus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Hippocampus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Behavioral conditioning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Rodents</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Agent-based modeling</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Agent-based modeling</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Agent-based modeling</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Neostriatum</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Neostriatum</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Model-based spatial navigation in the hippocampus-ventral striatum circuit: A computational analysis</article-title>
<alt-title alt-title-type="running-head">Model-based spatial navigation in the hippocampus-ventral striatum circuit</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Stoianov</surname>
<given-names>Ivilin Peev</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8328-1175</contrib-id>
<name name-style="western">
<surname>Pennartz</surname>
<given-names>Cyriel M. A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lansink</surname>
<given-names>Carien S.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6813-8282</contrib-id>
<name name-style="western">
<surname>Pezzulo</surname>
<given-names>Giovani</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>University of Amsterdam, Swammerdam Institute for Life Sciences–Center for Neuroscience Amsterdam, The Netherlands</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Daunizeau</surname>
<given-names>Jean</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Brain and Spine Institute (ICM), FRANCE</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">giovanni.pezzulo@istc.cnr.it</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>17</day>
<month>9</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>9</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>9</issue>
<elocation-id>e1006316</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>6</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Stoianov et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006316"/>
<abstract>
<p>While the neurobiology of simple and habitual choices is relatively well known, our current understanding of goal-directed choices and planning in the brain is still limited. Theoretical work suggests that goal-directed computations can be productively associated to model-based (reinforcement learning) computations, yet a detailed mapping between computational processes and neuronal circuits remains to be fully established. Here we report a computational analysis that aligns Bayesian nonparametrics and model-based reinforcement learning (MB-RL) to the functioning of the hippocampus (HC) and the ventral striatum (vStr)–a neuronal circuit that increasingly recognized to be an appropriate model system to understand goal-directed (spatial) decisions and planning mechanisms in the brain. We test the MB-RL agent in a contextual conditioning task that depends on intact hippocampus and ventral striatal (shell) function and show that it solves the task while showing key behavioral and neuronal signatures of the HC—vStr circuit. Our simulations also explore the benefits of biological forms of look-ahead prediction (forward sweeps) during both learning and control. This article thus contributes to fill the gap between our current understanding of computational algorithms and biological realizations of (model-based) reinforcement learning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computational reinforcement learning theories have contributed to advance our understanding of how the brain implements decisions—and especially simple and habitual choices. However, our current understanding of the neural and computational principles of complex and flexible (goal-directed) choices is comparatively less advanced. Here we design and test a novel (model-based) reinforcement learning model, and align its learning and control mechanisms to the functioning of the neural circuit formed by the hippocampus and the ventral striatum in rodents—which is key to goal-directed spatial cognition. In a series of simulations, we show that our model-based reinforcement learning agent replicates multi-level constraints (behavioral, neural, systems) emerged from rodent cue- and context- conditioning studies, thus contributing to establish a map between computational and neuronal mechanisms of goal-directed spatial cognition.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010661</institution-id>
<institution>Horizon 2020 Framework Programme</institution>
</institution-wrap>
</funding-source>
<award-id>PIEF-GA- 2013-622882</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Stoianov</surname>
<given-names>Ivilin</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000854</institution-id>
<institution>Human Frontier Science Program</institution>
</institution-wrap>
</funding-source>
<award-id>RGY0088/2014</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6813-8282</contrib-id>
<name name-style="western">
<surname>Pezzulo</surname>
<given-names>Giovani</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research has received funding from the European Union's Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 785907 (Human Brain Project SGA2 to GP and CP), the European Commission's Seventh Framework Programme (PIEF-GA- 2013-622882 to IS), and the Human Frontier Science Program (grant no. RGY0088/2014 to GP). The GEFORCE Titan GPU card used for this research was donated by the NVIDIA Corp. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="11"/>
<table-count count="0"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-09-27</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Custom software implementing the computational model described in the study is fully available (in a self-maintained repository) at <ext-link ext-link-type="uri" xlink:href="https://github.com/stoianov/MBRL" xlink:type="simple">https://github.com/stoianov/MBRL</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The neurobiology of goal-directed decisions and planning in the brain is still incompletely known. From a theoretical perspective, goal-directed systems have been often associated model-based reinforcement learning (MB-RL) computations [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref002">2</xref>]; yet, a detailed mapping between specific components (or computations) of MB-RL controllers and their brain equivalents remains to be established. Much work has focused on brain implementations of single aspects of MB-RL controllers, such as action-outcome predictions or model-based prediction errors [<xref ref-type="bibr" rid="pcbi.1006316.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref004">4</xref>]. A more challenging task consists in mapping MB-RL computations to a systems-level neuronal circuit that provides a complete solution to decision and control problems in dynamic environments–or in other words, identifying the biological implementation of a complete MB-RL agent rather than only one or more components.</p>
<p>The neuronal circuit formed by the (rodent) hippocampus and ventral striatum circuit is particularly appealing, and can be productively taken as a “model system” to understand biological implementations of model-based computations during spatial navigation (see <xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1A</xref>). The hippocampus (HC) has long been implied in place-based and goal-directed navigation [<xref ref-type="bibr" rid="pcbi.1006316.ref005">5</xref>]. Recent findings suggest that the role of hippocampus in goal-directed navigation may be mediated by the strong projections from the hippocampal CA1 and subicular areas to the ventral striatum (vStr) [<xref ref-type="bibr" rid="pcbi.1006316.ref006">6</xref>], which might convey spatial-contextual information and permit forming place-reward associations [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref010">10</xref>]. From a computational perspective, the hippocampus and ventral striatum may jointly implement a <italic>model-based controller</italic> for goal-directed choice [<xref ref-type="bibr" rid="pcbi.1006316.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref017">17</xref>]. In this scheme, HC and vStr might be mapped to the two essential components of a model-based reinforcement learning (MB-RL) controller [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref018">18</xref>]: the <italic>state-transition model</italic>, which is essentially a model of the task that permits to predict the next location given the current state (say, a given place) and chosen action, and the <italic>state-value model</italic>, which encodes the (expected) reward associated to each state, respectively.</p>
<fig id="pcbi.1006316.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Spatial navigation in rodents: functional organization, scenario, and overall architecture of the model.</title>
<p>(A) Structures in the rodent brain that are involved in goal-directed navigation. HC-VS constitute the essential structures of the putative model-based control system, which supports goal-directed behavior. AT and MEC provide input to the model-based control system. Output of the HC-VS circuitry may reach the cortex/mPFC via VP and MD. (B) The Y-maze used here and in [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. Each room contains 3 goal locations, which can be cued with a light with different probability in three different phases (separate panels) according to the color legend. See the main text for more explanation. (C) Sample grid cells providing systematic (spatial) information about the environment. (D) Architecture of our biologically inspired model-based reinforcement-learning model for spatial navigation that combines non-parametric clustering of the input signal P<sub>c</sub>(s|x), a state-transition model P<sub>M</sub>(s′|s,a), and a state-value model P<sub>V</sub>(r|g,s) with lookahead prediction mechanism for learning and control. See the <xref ref-type="sec" rid="sec009">Methods</xref> for details. Abbreviations: AT, anterior thalamus; HC, hippocampus; MD, mediodorsal thalamus; MEC, medial entorhinal cortex; mPFC, medial prefrontal cortex, VP, ventral pallidum, VS, ventral striatum.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g001" xlink:type="simple"/>
</fig>
<p>Different from a model-free controller, a model-based controller can use an explicit form of look-ahead prediction (or internal simulation) that permits to imagine future trajectories and covertly evaluate them [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>]. Similar look-ahead predictions have been reported in the HC: at difficult decision points, such as when they are at the junction of a T-maze, rodents sometimes stop and produce <italic>internally generated sequences</italic> of neuronal activity in the HC that resemble the sequential neuronal activity observed in the same area when they navigate through the left or right branches of the T-maze [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]. These internally generated sequences may serve to serially simulate future spatial trajectories (e.g., a trajectory to the left and successively a trajectory to the right). In turn, these look-ahead predictions might elicit covert reward expectations in the vStr [<xref ref-type="bibr" rid="pcbi.1006316.ref009">9</xref>]. By linking spatial locations with reward information, the HC-vStr might thus jointly implement a model-based mechanism that allows an animal to covertly simulate and evaluate spatial trajectories [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref021">21</xref>], using a serial scheme that has some analogies with machine learning algorithms (e.g., forward simulations in Bayes nets [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>] or Monte Carlo rollouts in decision trees [<xref ref-type="bibr" rid="pcbi.1006316.ref024">24</xref>]). Internally generated sequences were also reported during sleep or rest periods in the hippocampus and associated structures such as the ventral striatum [<xref ref-type="bibr" rid="pcbi.1006316.ref025">25</xref>]; in this case, they have been associated with multiple functions such as planning [<xref ref-type="bibr" rid="pcbi.1006316.ref026">26</xref>] and the off-line “replay” of past trajectories for memory consolidation [<xref ref-type="bibr" rid="pcbi.1006316.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref028">28</xref>]–a mechanism that has inspired early (DYNA [<xref ref-type="bibr" rid="pcbi.1006316.ref029">29</xref>]) as well as more recent (“experience replay” [<xref ref-type="bibr" rid="pcbi.1006316.ref030">30</xref>]) machine learning schemes.</p>
<p>Here we present a biologically-grounded computational analysis of model-based RL, by comparing side-by-side the behavior and neuronal activity of living organisms (rodents) and of a probabilistic MB-RL agent, in <italic>cue</italic>- and <italic>context- conditioning</italic> tasks that depend on intact hippocampal and ventral striatal (shell) function [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref032">32</xref>]. The results we report indicate that the MB-RL agent replicates the multi-level constraints (behavioral, neural, systems) emerged from rodent cue- and context- conditioning studies. First, we show that the MB-RL agent correctly learns to obtain reward in multiple goal locations and develops contextual preferences that are analogous to those of rodents. Second, we show that latent states emerging in the <italic>state-transition</italic> and <italic>state-value</italic> models of the MB-RL agent show key coding properties of HC and vStr neurons, respectively. Third, by comparing multiple variants of the MB-RL algorithm, we show that forward (look-ahead) predictions—of the kind that can be associated to hippocampal forward sweeps [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]—improve both action selection and learning. This latter result is important as the computational efficacy of model-based schemes based on forward sweeps has been put into question [<xref ref-type="bibr" rid="pcbi.1006316.ref033">33</xref>].</p>
<p>Taken together, the results of this study speak to the possibility of aligning the MB-RL scheme to the HC—vStr circuit to reproduce both behavioral and neuronal data. Establishing this kind of mappings is particularly important to foster cross-fertilizations between reinforcement learning and neurophysiology; for example, to identify specific algorithmic solutions that the brain uses to face problems that are still challenging in machine learning, or conversely, to advance novel computationally-guided theories of neural processing [<xref ref-type="bibr" rid="pcbi.1006316.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref034">34</xref>].</p>
<p>Finally, our MB-RL scheme has two main features that go beyond the state of the art from a computational perspective, and which are necessary for an adaptive agent to deal with open-ended situations. First, the MB-RL agent successfully faces the challenge to learn simultaneously three components: (the latent categories that form) the state space, the state-transition and the state-value models. Our novel scheme that combines non-parametric and reinforcement learning ensures that only the latent categories that afford accurate state-transition and state-value functions are retained, thus linking inextricably the three learning processes. Second, the MB-RL agent can deal with multiple goals in a native way, without the necessity of re-learning.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We report the results of a series of simulations, which aim to assess whether the novel MB-RL agent introduced here replicates the multi-level constraints (behavioral, neural, systems) that emerge from a context conditioning task, in which neural ensembles in rat hippocampus and ventral striatum were simultaneously recorded [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>].</p>
<sec id="sec003">
<title>Experimental set-up: Y-maze</title>
<p>The maze used in the animal study and in our simulations is a y-shaped symmetric arena consisting of 3 identical square chambers rotated 120 degrees from each other and connected through a central triangular passage, see <xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1B</xref>. Each chamber contained three goal locations located along the chamber walls, where reward was (probabilistically) delivered. Each reward location had a cue light above it.</p>
<p>Our simulation follows the protocol used in the animal study [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>] and consists of three phases. In the first phase (<italic>Cue</italic>: <italic>cue conditioning</italic>), the correct goal location was cued with a light; reward was delivered if the animal reached the goal location and made a nose-poke (but note that we do not simulate the nose poke here). In the second phase (<italic>Context</italic>: <italic>contextual conditioning</italic>), the correct goal location was cued with a light, too; but the probability of reward (following a nose poke) depended on the chamber’s spatial position: south room goals brought reward in 75% of the trials, whereas the other goals brought 25% reward. Finally, a <italic>context-conditioning test</italic> (<italic>CX test</italic>) was performed, with no cues or rewards (i.e., "free run"), to probe the animals’ acquired contextual place preferences.</p>
<p>The key behavioral result of the animal study, which we aim to reproduce in the MB-RL agent model, is that the contextual preference for the south room goals is preserved in the last (CX test) phase. We also aim to test whether the internal representations acquired by the two key components (<italic>state-transition</italic> and <italic>state-value</italic> model) of the MB-RL agent during learning, have coding properties that resemble HC and vStr activations, respectively, in the animal study.</p>
</sec>
<sec id="sec004">
<title>Brief introduction to the computational model</title>
<p>The computational model (MB-RL) is fully explained in the Methods section; however, we shortly summarize it here for the sake of the reader. The model essentially includes three interconnected components: a latent state categorization mechanism, which permits to learn the state representations that are useful to solve a task (e.g., place cells); a state-transition model, which learns the contingencies between actions and future states (e.g., what location do I reach by going left?) and a state-value model, which learns the utility of the states (e.g., is reaching location x good or bad?).</p>
<p>There are three important aspects of the model to note (see the <xref ref-type="sec" rid="sec009">Methods</xref> section for further details). First, we assume that state-transition and state-value models correspond to HC and vStr, respectively, and they jointly permit to steer look-ahead predictions (analogous to hippocampal forward sweeps [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]). As we will discuss below, our simulations will compare different versions of the same MB-RL agent, which use, or not use, look-ahead predictions for learning, control (action selection) or both.</p>
<p>Second, the MB-RL agent includes a mechanism that adaptively selects whether or not to do a sweep, and the depth of the sweep, depending on the agent's uncertainty and confidence about the choice. Essentially, the depth of sweeps decreases when the agent becomes sufficiently confident about its choice. However, a change in reward contingencies (e.g., the location of reward) would produce a reversal of this effect—and the generation of longer sweeps. This is because after failing to reach a reward at the expected location, the agent would become again uncertain about its choice, hence triggering sweeps to minimize it before a choice [<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>]. Here we are interested in validating the computational efficiency of this mechanism, which may potentially explain why VTE behavior increases rapidly when animals make errors following a switch in reward contingency [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>].</p>
<p>Third, most model-based planning algorithms start from a state space that is predefined by the programmers. However, the brain has to simultaneously learn internal models to generate predictions, and the states to be used by the model. In keeping, in the MB-RL agent, the learning processes required to acquire latent states and the (state transition and state value) models that use the latent states are interdependent (see <xref ref-type="disp-formula" rid="pcbi.1006316.e026">Eq 1</xref> below). Here we are interested in validating this learning scheme, which permits learning latent states that not only entail high perceptual accuracy but also afford good prediction and control when used by the state-transition and state-value models.</p>
</sec>
<sec id="sec005">
<title>Behavioral analysis: Accuracy</title>
<p>Successful context conditioning was evaluated in [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>] with a <italic>CX test</italic>: a “free-run” target preference behavioral test in which the rats freely explored the maze for several minutes without any cue or reward. During this period, to probe the animals' learned preferences, statistics were collected about their visitations of (and “nose pokes” in) rooms that in the previous <italic>Contextual Conditioning</italic> phase were associated with high- and low-reward probabilities. Analogously, to test the MB-RL agent’s learned preferences, we ran a free-run simulated test for 2,000 time steps, in which the agent started from a small central area (3x3 units) and with random orientation. In this simulation, action selection maximized the likelihood for (expected) reward pooled from all the 9 goal locations; and the agent did not learn. To better clarify the behavioral effects of conditioning in the MB-RL agent, we performed the same free-run test (and without learning) also at the end of cue conditioning. Finally, and importantly, we envisaged to compare different versions of the same MB-RL agent that uses, or not uses, look-ahead predictive mechanisms (analogous to hippocampal forward sweeps [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]) for both control / action selection and (reward) learning—in order to assess the computational importance of these mechanisms. Specifically, we compared four different versions of the MB-RL agent using a 2x2 design, which considers 2 controllers (i.e., using or not using forward sweeps) and 2 learning procedures (using or not using forward sweeps); see Section 4 for details.</p>
<p>We expected that look-ahead prediction (or forward sweeps) to provide the agent significant advantages for both control / action selection and (reward) learning; this result would be particularly intriguing for action selection, given that the baseline controller uses the full factorized distribution of reward and is thus computationally demanding. The results shown in <xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2</xref> confirm our hypotheses, by showing an additive advantage of forward sweeps in both action selection and learning. As shown in <xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2A</xref>, the MB-RL agent that uses forward sweeps in both action selection and learning (swControl+Reward), with average success of μ = 69% (σ = 2) during the entire <italic>Cue Conditioning</italic> phase outperforms in amount of collected reward the agent that uses forward sweeps only for control (swControl, μ = 62% (σ = 3), t(1,9) = 6.9, n = 10, p&lt;0.001), the agent that uses forward sweeps only for learning (swReward, μ = 39% (σ = 2), t(1,9) = 33.4, n = 10, p&lt;0.001), and the agent that lacks them during both control and learning (Baseline, μ = 36% (σ = 1), t(1,9) = 39.6, n = 10, p&lt;0.001) and this advantage is evident from the very beginning of learning. For all the four agents, performance decreases after the vertical dotted bar, in correspondence of the <italic>Contextual Conditioning</italic> phase—but this is expected, given that less reward is available in this phase.</p>
<fig id="pcbi.1006316.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Benefits of forward sweeps for action selection and control.</title>
<p>(A,B) Learning performance (accuracy and length of the agent path to the goal). (C) Length of sweeps used for control / action selection. (D) Decision (un)certainty during control / action selection.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g002" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2B</xref> illustrates the same advantage in overall path length needed to reach the reward site of the four agents. Along with the increased performance, the length of sweeps used for control by the swControl+Reward and swControl agents decreases with time (<xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2C</xref>). This is because the agent’s decision certainty progressively increases (<xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2D</xref>) as their state-value model becomes more effective; in turn, the mechanism for sweep length control explained in the Methods Section decreases length (we obtained equivalent accuracy and path length results using a controller having a constant length of 9 and thus requiring more computational results; results not shown). This result aligns well with evidence that hippocampal forward sweeps at decision points progressively diminish during learning [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]—until they disappear when the animal develops a habit (which we do not model here, but see [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>]).</p>
</sec>
<sec id="sec006">
<title>Behavioral analysis: Preferences after conditioning</title>
<p>As the above analysis confirmed the advantages of the MB-RL agent that uses forward sweeps in both action selection and learning (swControl+Reward), we used this MB-RL agent for the rest of our behavioral and neural analyses.</p>
<p>Our target rodent study reported that during "free run" in the <italic>CX test</italic> phase, animals showed an increased preference for the room that yielded more rewards in the previous (<italic>Context</italic>) phase [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. The MB-RL agent correctly reproduces this pattern of behavioral results. As shown on <xref ref-type="fig" rid="pcbi.1006316.g003">Fig 3A</xref>, in the <italic>CX test</italic> phase (in the absence of reward), the agent shows a significant behavioral preference (number of reward-site visits) for the room that was previously most rewarded (room1 vs. room3, t(1,9) = 4.3, n = 10, p = 0.002; room2 vs. room3, t(1,9) = 7.6, n = 10, p&lt;0.001), which matches the animal's learned preference (and differed from an analogue test executed right after cue conditioning: room1 vs. room3, t(1,9) = 1.5, n = 10, p = 0.16; room2 vs.–room3, t(1,9) = 0.1, n = 10, p = 0.92). <xref ref-type="fig" rid="pcbi.1006316.g003">Fig 3B</xref> provides three representative trajectories before (blue trajectories) and after (red trajectories) context conditioning–the latter exemplifying the increased preference for the room that included highly rewarded locations in the Context phase (black dots).</p>
<fig id="pcbi.1006316.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Behavioral results of the simulations.</title>
<p>(A) Preferences of the MB-RL agent after <italic>Cue Conditioning</italic> and after Contextual conditioning (i.e., <italic>CX test</italic>). In the CX test, both agent and animals show a preference to visit targets in the room that was previously most rewarded. (B) Sample trajectories of the agents during these tests (after <italic>Cue Conditioning</italic>, blue; after Context Conditioning, or <italic>CX test</italic>, red).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Neural-level analysis of conditioning</title>
<p>To further establish a parallel between the MB-RL agent and the HC—vStr circuit, we analyzed the <italic>content</italic> of the latent states that emerged in the agent’s <italic>transition model P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) and <italic>value function P</italic><sub><italic>V</italic></sub>(<italic>r</italic>|<italic>g</italic>,<italic>s</italic>) at the end of the <italic>Cue</italic> and <italic>Context Conditioning</italic> phases. We decoded the spatial location corresponding to each latent state and created: (a) value maps for each target showing for each spatial location the greatest reward across all head-directions and (b) transition maps showing how position could change after applying an(y) action at any orientation. We also analyzed the effect of conditioning at the neural level: how the learned internal models change and how this change affects behavior.</p>
<p>The neuronal analysis of HC and vStr cells after the cue conditioning task reported different coding characteristics in the two areas, with the former showing high spatial selectivity and the latter showing selectivity for reward- and task-related information, and place/reward combinations [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. The two components of the MB-RL agent acquire analogous coding preferences after conditioning. <xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4</xref> shows the transition probabilities learned by the state-state (transition) model after <italic>Cue Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4A</xref>) and how they changed after <italic>Context Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4B</xref>). In <xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4</xref>, each green square corresponds to a place in the maze (i.e., the central square corresponds to the central place of the maze) and the colors of the smaller squares code the greatest probability of transitioning from the location of that state, regardless the direction, to the location of any other state of the maze (shown are only the possible successors, i.e., nearby locations), by executing any of the 3 actions available to the agent (see <xref ref-type="sec" rid="sec009">Methods</xref> section). The more red the color of the cell, the higher the probability. In other words, this map shows the spatial projection of the learned "successor states" (small squares) of each latent state (green squares) implicitly coded in the agent's probabilistic transition model—analogous to "successor representations" [<xref ref-type="bibr" rid="pcbi.1006316.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref036">36</xref>]. For example, the green squares near the center include transitions to all directions while the green squares on the borders only include transitions towards the center. In other words, the learned transition probabilities encode the actual transitions available in the environment, reflecting the idea of HC encoding a navigational "cognitive map" [<xref ref-type="bibr" rid="pcbi.1006316.ref005">5</xref>]. Furthermore, importantly for our analysis, these codes only change (remap) to a minor extent between the <italic>Cue</italic> and <italic>Context Conditioning</italic> phases (<xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4B</xref>)—in keeping to what was reported empirically [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. Essentially, the changes reflect a consolidation of the same transition model, not a remapping, due to additional training during the <italic>Context conditioning</italic> phase.</p>
<fig id="pcbi.1006316.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Neural representation of state transitions in the state-state model.</title>
<p>Latent states developed by the state-transition models averaged across all 10 learners after the <italic>Cue Conditioning</italic> phase (A); and the changes due to <italic>Contextual Conditioning</italic>, i.e., the differences between probabilities before and after <italic>Contextual Conditioning</italic> (B). Each image from the transition model <italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) encodes the greatest likelihoods <italic>P</italic><sub><italic>M</italic></sub>(<italic>xy</italic>(<italic>s</italic>′)|<italic>xy</italic>(<italic>s</italic>), <italic>a</italic>) across all head-directions and actions to step from the location <italic>xy</italic>(<italic>s</italic>) of a given latent state (place) <italic>s</italic> to nearby places <italic>xy</italic>(<italic>s</italic>′) located within the range of an(y) action from the location of the current place, following any of the available actions, i.e., the (probabilistic) location of the successors of every state. The locations <italic>xy</italic>(<italic>s</italic>) and <italic>xy</italic>(<italic>s′</italic>) of <italic>s</italic> and <italic>s</italic>′ are decoded using an inverse of the function providing input to the Dirichlet model. Note that, as expected, the decoding procedure is not perfect—hence the gaps in the maps.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g004" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5</xref> shows the value of each place (i.e., the learned probability of obtaining reward in the place) when a goal location is selected or cued, as learned by the state-value function component of MB-RL agent after <italic>Cue Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5A</xref>) and how these values changed after <italic>Context Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5B</xref>). The nine outer big Y-shapes represent the possible goal sites; inside each of them, the colors code the probabilities of obtaining reward while starting from any specific location in the maze—the darker red the color, the higher the probability. The central Y-shape represents the combined value function across all goal sites, which is effective when the goal is not cued, as in the behavioral CX test. These probabilities relate well with key coding properties of vStr neurons in [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. If one assumes that the current goal is unknown, as in the CX-test, one can see the same rotational invariance in the model (<xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5A</xref>, the central Y-shape) as in sample vStr neurons (<xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5C</xref>; for more samples, see [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]). This aligns well with the idea that vStr neurons might encode place-reward associations and these in turn can be conditioned on a specific goal, possibly provided by prefrontal areas [<xref ref-type="bibr" rid="pcbi.1006316.ref037">37</xref>] or vicariously from hippocampal forward sweeps [<xref ref-type="bibr" rid="pcbi.1006316.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref038">38</xref>].</p>
<fig id="pcbi.1006316.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Neural representation of value in the state-value model.</title>
<p>Latent states developed by the state-value model averaged across all 10 learners, after the <italic>Cue Conditioning</italic> phase (A); and how they change after <italic>Contextual Conditioning</italic>, i.e., the differences between probabilities before and after <italic>Contextual Conditioning</italic> (B); and the activity of three sample vStr neurons drawn from the experiment in [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>] (C). The images in (A, B) represent the greatest value <italic>P</italic><sub><italic>V</italic></sub>(<italic>r</italic> = 1|<italic>g</italic>,<italic>xy</italic>(<italic>s</italic>)) across all head-directions attributed to a given spatial position <italic>xy</italic>(<italic>s</italic>) for a given target <italic>g</italic>. Each image represents one target and its location in the plot corresponds to its location in the Y-maze. The central image represents the combined value function across all targets. It is rotationally symmetric after the rotationally symmetric reward delivered during Cue Conditioning (as some of the vStr rodent-neurons; see insets C) and becomes asymmetric during the context conditioning phase. The spatial positions <italic>xy</italic>(<italic>s</italic>) are decoded from the latent states <italic>s</italic> using an inverse of the function providing input from the grid cells.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g005" xlink:type="simple"/>
</fig>
<p>Note that the probabilities emerging in the state-value model are closely related to reward functions of RL, which can be used to learn a policy from the current state [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>] and, in a model-based scheme, to retrieve covert expectations of reward using mental simulation or forward sweeps [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>]. As usual in RL, the value of reward places is temporally discounted such that places farther from goal sites have lower values. This creates a sort of gradient or "tessellation" of the task, which might reflect not only proximal distance but also other information such as phases or subtasks that are necessary to secure a reward [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>].</p>
<p>Importantly, the coding of value in the state-value model changes drastically after <italic>Contextual Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5B</xref>). While after Cue Conditioning one can observe full rotational symmetry, after the Context Conditioning a marked preference is evident for the goal locations in the most rewarded (lower; south) room—which also explains the behavioral results of <xref ref-type="fig" rid="pcbi.1006316.g002">Fig 2</xref>. This change of preference can be better appreciated if one considers the distribution of the change of values associated with latent states in the state-value model after <italic>Contextual Conditioning</italic> (<xref ref-type="fig" rid="pcbi.1006316.g006">Fig 6</xref>). Specifically, there is a decrease of all the state values (given that reward is less frequently available), but the value of states corresponding to the less rewarded rooms (blue, mean change of -0.29 across all learners) decreases significantly more than those of the more rewarded rooms (red, mean change of -0.17 across all learners; t(1,9) = 25.0, p&lt;0.001). This result is coherent with a body of literature implying vStr in the coding of reward expectancies [<xref ref-type="bibr" rid="pcbi.1006316.ref021">21</xref>] and would imply a sensitivity for changing reward contingencies (see <xref ref-type="sec" rid="sec008">Discussion</xref>). This is in sharp contrast with the states in the state-state transition model (black), which essentially does not change (mean change of 0.01 across all learners).</p>
<fig id="pcbi.1006316.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Changes in the state-state and state-value models after contextual conditioning.</title>
<p>This figure shows what changes the Contextual Conditioning procedure produces in the probability values of states in the state-state or transition model (black), and in the state-value model, where states correspond to the most rewarded (red) or less rewarded (blue) rooms. For clarity, we only show the changes of states having a probability that is greater than 0.05 (for the state-state model) and 0.5 (for the state-value function). This choice of thresholds in motivated by the fact that in the state-value function we are interested in verifying changes in the states carrying significant value information (e.g., those regarding the goal states or their neighbor’s), not in the many states that have a low probability value in all situations (see <xref ref-type="fig" rid="pcbi.1006316.g005">Fig 5</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g006" xlink:type="simple"/>
</fig>
<p>Finally, <xref ref-type="fig" rid="pcbi.1006316.g007">Fig 7</xref> provides analysis of the sweeps for action selection generated during the two conditioning phases. During <italic>Cue Conditioning</italic>, sweep length increases at decision points, e.g., the center of the maze and nearby densely located targets (<xref ref-type="fig" rid="pcbi.1006316.g007">Fig 7A</xref>). This simulates the rodent data that consistently show longer internally generated sequences at branch points [<xref ref-type="bibr" rid="pcbi.1006316.ref015">15</xref>], [<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>]. In turn, <xref ref-type="fig" rid="pcbi.1006316.g007">Fig 7B</xref> shows how sweep length changes after <italic>Context Conditioning</italic>. Notably, when the agent is in a location that is far away from targets with low reward probability, it needs longer sweeps to accumulate sufficient evidence about the most valuable action. Instead, sweep length remains the same or decreases for highly rewarded targets.</p>
<fig id="pcbi.1006316.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Analysis of sweeps.</title>
<p>Length of sweeps during control / action selection, for each target (separate Y-maze images) and each spatial location (dots in the Y-mazes). Sweep length is color-coded. (A) Sweep length after <italic>Cue Conditioning</italic>. (B). Change of sweep length after <italic>Context Conditioning</italic>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>Computational methods of reinforcement learning have been widely used to analyze neuronal data [<xref ref-type="bibr" rid="pcbi.1006316.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref039">39</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref041">41</xref>]. However, this has been done far more systematically for model-free RL–mostly associated to habitual behavior—than for model-based RL methods [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref042">42</xref>]–more associated to goal-directed behavior and planning. A challenge consists in mapping the components of a MB-RL agent to a neural circuit that offers a complete, systems-level solution to goal-directed choice, rather than a single aspect of it (e.g., the dynamics of dopaminergic neurons and their relations to reward prediction error).</p>
<p>Rodent research suggests that the neuronal circuit formed by the hippocampus (HC) and ventral striatum (vStr) might implement model-based computations in spatial navigation. In this scheme, the HC learns a state-transition model (permitting to predict future states) and the vStr learns a state-value model (encoding goal-dependent state values). The HC might then predict future trajectories (e.g., using forward sweeps and ripple-associated replay) and vStr might covertly evaluate them, permitting to select the best (that is, reward-maximizing) course of action [<xref ref-type="bibr" rid="pcbi.1006316.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref021">21</xref>]. To provide a computationally-guided analysis of this idea, here we designed a Bayesian model-based reinforcement learning (MB-RL) agent and tested it in a challenging contextual conditioning experiment that is dependent on hippocampal and ventral striatal function [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>].</p>
<p>The results of our simulation show that the MB-RL agent procedure can reproduce animal data at multiple—behavioral, neural, systems—levels. At the behavioral level, the agent shows place preferences that are analogous to rodents [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. Key to this result is the fact that the agent's value function is conditioned on goal information. Here, in other words, goal information acts as a “context” and permits the agent to learn multiple goals, as required by the experimental set-up [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. This possibility is precluded to other RL controllers that are only optimized for a single value function (or goal) and thus lack context sensitivity [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>] but is important to scale up RL methods to real-world situations that involve multiple potential goals. At the neural and systems levels, the agent develops internal codes that reproduce key signature of hippocampal and ventral striatal neurons [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. This is important to establish deep relations between formal methods and neurophysiology that potentially span multiple levels of analysis—from computational to algorithmic and implementational [<xref ref-type="bibr" rid="pcbi.1006316.ref043">43</xref>]. Finally, in our proposed model, hippocampal sweeps can contribute to both action selection and learning. Our comparison of multiple MB-RL schemes shows that mechanisms of look-ahead prediction that resemble hippocampal forward sweeps improve both control and learning. There has been some recent skepticism around the idea of using look-ahead prediction for control and learning, based on the fact that prediction errors tend to accumulate over time [<xref ref-type="bibr" rid="pcbi.1006316.ref033">33</xref>], which would cast into doubt the idea that hippocampal sequences may serve predictive or planning roles. Furthermore, alternative proposals have cast model-based control (and hippocampal function) in terms of backward [<xref ref-type="bibr" rid="pcbi.1006316.ref042">42</xref>] or a combination of forward and backward inference [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>]. Our results suggest that the limited form of look-ahead prediction that we adopt is computationally advantageous, thus lending computational-level support for theories that assign forward sweeps a predictive role [<xref ref-type="bibr" rid="pcbi.1006316.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref045">45</xref>].</p>
<p>The MB-RL agent is related to other computational models of model-based spatial decisions [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref052">52</xref>] that use various forms of forward planning. Different from these architectures, the MB-RL agent includes a nonparametric module that learns a “state space” in an unsupervised manner while the agent learns the task, and which works synergistically with the other components of the architecture. Indeed, the state space representations emerging from the nonparametric learning procedure ensure that only states that afford good prediction and control are retained. Using end-to-end learning (i.e., from perception to action) helps keeping the various learning procedures (state space, action-state and state-value learning) coordinated and is more effective and biologically realistic than using a staged approach—still popular in RL—in which unsupervised state space learning is seen as a generic preprocessing phase. From a computational perspective, this approach can be considered to be a novel, model-based extension of a family of Bayesian methods that have been successfully applied to decision-making problems [<xref ref-type="bibr" rid="pcbi.1006316.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref057">57</xref>]. It is also important to note that our approach does not require learning a separate state space (or sensory mapping) for each goal; rather, multiple spatial goals share the same state space—which implies that our MB-RL agent can deal natively with multiple goals. This is different from most current deep RL approaches, which require learning a new state space for each problem or task (but see [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>]). In our set-up, sharing the state space is effective because all goals correspond to spatial locations—or in other words, they all lie in the same low-dimensional state space that supports spatial navigation. This reliance on a relatively low-dimensional state space makes spatial navigation quite special and tractable, both in machine learning and the brain; it is thus possible that brain areas that support goal-directed processing outside navigation domains rely on more complex computational solutions that remain to be fully established [<xref ref-type="bibr" rid="pcbi.1006316.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref058">58</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref059">59</xref>].</p>
<p>At the neurophysiological level, the latent states that emerged in the agent's state-state and state-value models during conditioning share important similarities with HC and vStr coding, respectively—and in particular the selectivity for space vs. reward-predictive information and place/reward combinations, respectively. Our computational model thus provides a detailed (yet to some extent abstract) mapping between specific components of model-based controllers and neuronal circuits. Establishing this sort of mappings is important if one aims to the cross-fertilization of current research in reinforcement learning / artificial intelligence and neurophysiology; for example, to probe in detail neuronal solutions to challenging machine learning problems. Indeed, one should expect that the neuronal networks in the HC and vStr (as well as other brain structures) have been optimized over time to solve the challenging problems that animals must face every day, and cracking this neural code might be helpful to design artificial agents with similar abilities. In this respect, a limitation of the model proposed here concerns the simplifying assumptions on HC and vStr coding and their respective roles in model-based control. For simplicity, we mapped one-to-one HC to state-transition and vStr to state-value models. However, this sharp division is likely to be simplistic and both neural coding and computational roles of the HC-vStr circuit are certainly more complex. HC cells show some sensitivity to goal and reward information [<xref ref-type="bibr" rid="pcbi.1006316.ref060">60</xref>] and remap from cue-on to cue-off and spontaneous goal site approach [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. Furthermore, vStr can code information that goes beyond scalar reward expectations; it can code for example item- or object-specific information, such as the expected reward associated to a specific food or other characteristics of a task, including (for example) intermediate steps to goal locations [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. This suggests a more complex architecture in which HC and vStr do not map one-to-one to components of a model-based controller but may realize model-based computations in a more distributed manner; and in which they form a latent task model that encodes task variables that are more abstract than spatial codes, permitting “tessellating” tasks into behaviorally meaningful events (e.g., task phases that lend to reward delivery) and forming hierarchies. Understanding how the brain tessellates and organizes the state space hierarchically may be particularly important for the success of future model-based reinforcement learning algorithms. Current model-based RL methods suffer from the problem that, during long look-ahead searches, errors sum up; for this, in practice, most practical applications use model-free solutions. However, in principle, endowing model-based methods with the ability to exploit relevant task structure (e.g., milestones or subgoals) may mitigate this problem, by making forward search more abstract and hierarchical (or “saltatory”) [<xref ref-type="bibr" rid="pcbi.1006316.ref061">61</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref063">63</xref>] or by avoiding chaining too many consecutive predictions [<xref ref-type="bibr" rid="pcbi.1006316.ref033">33</xref>]. To this aim, unsupervised state representation methods (including deep learning methods) that are able to identify latent task structure may be productively incorporated into model-based control systems—yet it remains to be studied in future research how to handle or bound (hierarchical) state representations that may solve realistic problems [<xref ref-type="bibr" rid="pcbi.1006316.ref036">36</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref064">64</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref068">68</xref>]. An alternative possibility consists in learning a more complex state space (successor representations) that afford implicit prediction [<xref ref-type="bibr" rid="pcbi.1006316.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>]. Finally, it is important to remark that overall brain architecture for model-based spatial navigation plausibly includes other areas in addition to HC and vStr. For example, in rodents prelimbic cortex may be particularly important to decide whether or not to engage the model-based system (or to initiate a deliberative event [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>]) and to code for spatial goals [<xref ref-type="bibr" rid="pcbi.1006316.ref037">37</xref>]. More broadly, the hippocampus cross-talks extensively with cortical areas, especially during sleep—and this cross talk can be essential to train not only the cortex as usually assumed [<xref ref-type="bibr" rid="pcbi.1006316.ref070">70</xref>] but also the hippocampus [<xref ref-type="bibr" rid="pcbi.1006316.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref071">71</xref>]. Realizing a complete, systems-level architecture for goal-directed spatial cognition remains an open challenge for future research.</p>
<p>In sum, our results highlight the viability of model-based methods to study goal-directed spatial navigation, in the same way model-free methods have been widely adopted to study habitual action control [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref042">42</xref>]. Our comparison of alternative MB-RL schemes shows that forward sweeps are useful for both learning and control / action selection. Indeed, our results show that the MB-RL agent that includes both kinds of forward sweeps is the most accurate and the one with lowest uncertainty. This result, again, connects well to neurophysiological evidence that implicates (prospective) internally generated hippocampal sequences to both learning / memory function [<xref ref-type="bibr" rid="pcbi.1006316.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref072">72</xref>] and decision [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref026">26</xref>]. Our approach has a number of implications from both computational and neurophysiological perspectives, which we address next.</p>
<p>From a computational perspective, it is worth noting that the results that we present here may be obtained by a model-free RL system, augmented in various ways (e.g., by using various goals as part of the state classification system). Two points are however important, the former related to coding parsimony, and the latter related to biological plausibility. First, separating the transition function from the value function (which is typical of model-based systems) makes the realization of multiple goals more “economic” from a coding perspective. In the classical RL approach, a Q-value function encodes the expected cumulative future reward for each state and action, but for one single goal [<xref ref-type="bibr" rid="pcbi.1006316.ref073">73</xref>]. The addition of multiple goals in the Q-value function would increase learning time dramatically. In contrast, adding novel goals to our model-based RL approach requires just learning the new value associated to each state. Second, and importantly, a pure model-free RL approach would fail in navigation tasks that are typically associated with the hippocampus, such as detour tasks [<xref ref-type="bibr" rid="pcbi.1006316.ref074">74</xref>]. This is because model-free RL methods lack the required flexibility to rapidly adapt to novel contingencies (e.g., a change of the reward or goal location, or of the state transitions, as in the case of detours).</p>
<p>Another interesting approach to understand predictive aspects of hippocampal coding is in terms of successor representations (SR) [<xref ref-type="bibr" rid="pcbi.1006316.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref075">75</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>]—or prospective codes at the single cell level. Our approach is distinct from the successor representation (SR) approach [<xref ref-type="bibr" rid="pcbi.1006316.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref075">75</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>], for four main reasons. Firstly, the two approaches target two distinct (possibly complementary) forms of predictive coding in the hippocampus. Our approach addresses predictive coding at the level of sequential activation of multiple cells, e.g., theta sequences and forward sweeps at decision points [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>]. Rather, the SR addresses predictive coding at the level of single cells; and in particular the backward expansion of place fields (in CA1), which can be considered a form of predictive representation of future states or locations [<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>]. Secondly, the two approaches use two distinct computational schemes (model-based versus model-free or hybrid) to engage in predictive processing. In our model-based approach, predictions about future locations (and their value) are generated by engaging the state-transition and state-value models on-line, to perform forward sequential planning and policy selection. This requires learning a one-step probability transition model <italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) and a probabilistic cumulative value function <italic>P</italic><sub><italic>V</italic></sub>(<italic>r</italic>|<italic>g</italic>,<italic>s</italic>). Rather, the SR approach does not require engaging an internal model for forward prediction during planning (although some variants of SR learn a model and engage it off-line, to "train" SR [<xref ref-type="bibr" rid="pcbi.1006316.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>]). This is because a SR essentially caches a series of predictions (e.g., of future occupancies) into a single state representation. More formally, the SR approach learns the future occupancy <italic>M</italic><sub><italic>s</italic></sub>(<italic>s</italic>′) function, or the probability to occupy any state <italic>s</italic>′ following a specific policy (standard, on-policy method) or any policy (extended, off-policy method [<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>]) starting from state <italic>s</italic>. Using this function, the SR approach can be sensitive to future events without engaging a model on-line. Thirdly, our approach and the SR have distinct trade-offs. Our model-based approach has the highest flexibility (because all knowledge embedded in the model is used on line) but also the highest computational cost. The usefulness of SR rests on the possibility to decompose the cumulative value function into a product of SR and local reward. The SR approach permits using predictive representations in a model-free manner, thus skipping (costly) model-based computations during planning and choice. Extensions of the SR approach permit to have roughly the same flexibility as model-based approaches, in challenging situations like detour and revaluation tasks [<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>]. At the same time, a prediction generated using a SR is usually less specific than a model-based prediction, as the SR marginalizes over all possible sequences of actions. Finally, and most importantly here, the two approaches would assign different roles to internally generated hippocampal sequences. Our model-based approach uses internally generated hippocampal sequences for both learning and on-line decision-making and planning. Rather, in the SR approach internally generated hippocampal sequences are not required for on-line decision-making or learning (although some variants of SR use sequences for off-line training, i.e., experience replay [<xref ref-type="bibr" rid="pcbi.1006316.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>]). In most navigation scenarios, decisions can be done using a single SR (comparable to a single place cell or a small population of place cells [<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>]), hence this approach would not explain per se why the hippocampus should encode theta sequences, after sufficient learning. Of course, the two forms of prediction entailed by our model-based and the SR approach are not mutually exclusive, but can be productively combined; for example, by using sequences of SR (rather than "standard" place cells) within a model-based scheme. The efficiency and biological plausibility of such combined scheme remains to be tested.</p>
<p>At the neurophysiological level, the proposal advanced here is that the hippocampus encodes a model or cognitive map of spatial transitions and uses this model for state estimation (self localization) and forward inference (prediction / planning). In our scheme, these computations are intimately related. The same model can support different functions (e.g., self localization, forward or even retrospective inference), depending on the specific "message passing": state estimation may rely on the cross-talk between input structures of the hippocampus that encode the observations or inputs of the model (putatively, LEC and MEC) and the hippocampus proper (putatively, dentate gyrus and CA3-CA1) [<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>]; forward prediction is more dependent on the hippocampus proper (recurrent dynamics in CA3, and CA3- CA1 connections) [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref022">22</xref>]; whereas a complete goal-directed decision requires the interplay of the hippocampus with other brain areas that may putatively encode goal states (mPFC [<xref ref-type="bibr" rid="pcbi.1006316.ref077">77</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref078">78</xref>]) and/or state values (vStr [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref025">25</xref>]). The complete systems-level circuit supporting goal-directed decisions has not been systematically mapped. However, various studies point to the importance of dorsolateral and ventromedial prefrontal cortex, and orbitofrontal cortex in supporting model-based computations [<xref ref-type="bibr" rid="pcbi.1006316.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref079">79</xref>] (but see [<xref ref-type="bibr" rid="pcbi.1006316.ref080">80</xref>] for evidence that orbitofrontal cortex may participate in post-decision processes rather than model-based decision).</p>
<p>Our computational scheme is quintessentially model-based; however, it permits to dynamically modulate the degree of model-basedness and its temporal horizon. In this scheme, there is no fundamental difference between theta sequences observed during "standard" navigation, and forward sweeps that occur at choice points, and which stretch much farther in space [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>]. These are part and parcel of the same model-based (theta-paced) inferential mechanism that runs continuously during navigation (or outside navigation, during sequential processing [<xref ref-type="bibr" rid="pcbi.1006316.ref081">81</xref>]). Whether or not a more intensive, far-reaching sweep occurs depends on a trade-off between its costs (e.g., costs of computation and the time consumed by it) and benefits (e.g., whether or not engaging in far-looking prediction is useful for action selection). From a computational perspective, one can characterize the benefits of a far-reaching sweep by considering the (epistemic) value of information that it may make accessible [<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref052">52</xref>]. This would explain why sweeps predominantly occur at difficult choice points [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>], and why repeated exposure to the same contingencies lowers choice uncertainty, thus rendering deep search unnecessary. In keeping, our information-theoretic approach automatically determines the utility of performing a forward sweep and sets its depth, by considering the initial uncertainty and the value of the to-be-acquired information [<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref082">82</xref>]. Neurophysiologically, the decision of whether or not to engage in deep search has been hypothesized to involve the prelimbic area (in rodents) [<xref ref-type="bibr" rid="pcbi.1006316.ref020">20</xref>].</p>
<p>The current MB-RL implementation has a number of limitations, which we briefly summarize here. First, the neurophysiological implementation of internally generated neuronal sequences is more sophisticated than our simplified implementation; for example, there are at least two classes of internally generated sequential activity patterns, termed theta sequences (and sometimes forward sweeps) [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref038">38</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref083">83</xref>] and sharp wave ripple sequences (also called replay) [<xref ref-type="bibr" rid="pcbi.1006316.ref084">84</xref>], which have different neurophysiological signatures and possibly different (albeit coordinated) roles; see [<xref ref-type="bibr" rid="pcbi.1006316.ref013">13</xref>] for a review. Replay activity in the hippocampus has already inspired various methods for improving learning (e.g., experience replay [<xref ref-type="bibr" rid="pcbi.1006316.ref030">30</xref>]) or hybridizing model-free and model-based computations (e.g., DYNA [<xref ref-type="bibr" rid="pcbi.1006316.ref029">29</xref>]); understanding neuronal sequential activity in more detail might permit going beyond these metaphorical mappings and potentially design improved algorithms. A second, related limitation of the current model is that only focuses on forward replays and does not take into account backward replay, which may have a separate computational role. Both forward and backward replays are observed at rest periods (e.g., during sleep), suggesting that they are useful for consolidating the internal model [<xref ref-type="bibr" rid="pcbi.1006316.ref014">14</xref>]. However, recent evidence suggests that reverse replays are more prominent during the awake state, after (novel) reward observations [<xref ref-type="bibr" rid="pcbi.1006316.ref085">85</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref086">86</xref>]. This makes sense if one considers that backward replays may help learning from recent rewards, possibly using some sort of "eligibility trace" [<xref ref-type="bibr" rid="pcbi.1006316.ref087">87</xref>] to update the current model or the current policy. More broadly, one may conceptualize replays in terms of <italic>epistemic actions</italic> that aim at gathering (the best) evidence to improve the internal model [<xref ref-type="bibr" rid="pcbi.1006316.ref052">52</xref>]. In this perspective, it becomes plausible that different kinds of memory contents need to be accessed during sleep, before a choice and after obtaining a reward; see [<xref ref-type="bibr" rid="pcbi.1006316.ref088">88</xref>] for a recent computational characterization of hippocampal replay in terms of prioritized memory access. Endowing the current model with the ability to "direct" (forward or backward) replays to informative memory content—possibly using a mechanism that marks memories or places with saliency information [<xref ref-type="bibr" rid="pcbi.1006316.ref089">89</xref>]—is an open research objective. Third, since head direction input is used to classify latent states, the resulting place fields are directional. While place cells often have some directionality in their fields, our model does not account for non-directional or omnidirectional aspects of the fields. This also implies that in order to select an action, the MB-RL agent needs to know its orientation (using e.g., head direction cells). This design choice was made for the sake of simplicity and has no major implications for the phenomena we were interested in; however, extending the model to obtaining omnidirectional place fields (e.g., as shown in <xref ref-type="fig" rid="pcbi.1006316.g004">Fig 4</xref>, in which place fields are averaged across all directions) would help reproducing more accurately neurophysiological data. Furthermore, our model treats the (hippocampal) state space as flat. An interesting alternative possibility that deserves future investigation is that the hippocampal code is hierarchically organized along a septo-temporal axis, with more temporal cells that encode information that are broader in space (e.g., place and grid cells having larger firing fields [<xref ref-type="bibr" rid="pcbi.1006316.ref090">90</xref>–<xref ref-type="bibr" rid="pcbi.1006316.ref092">92</xref>]). Hierarchical organization of the state space can be productively investigated in our framework by rendering the Bayesian nonparametric approach hierarchical, or manipulating its concentration (α) parameter. A final limitation of the current model is that it uses simplified grid cells. Several modeling approaches have been proposed that explain grid cells in terms of (for example) attractors [<xref ref-type="bibr" rid="pcbi.1006316.ref093">93</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref094">94</xref>], oscillators [<xref ref-type="bibr" rid="pcbi.1006316.ref095">95</xref>] or an eigendecomposition of state space [<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>], which may be exploited to develop more realistic grid cells within our proposed model.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec010">
<title>MB-RL agent: Sensors and action primitives</title>
<p>The MB-RL agent is characterized by position and orientation information, provided as real values, relative to a coordinate system with a unit step size. The y-maze is centered on the origin of the agent’s coordinate system and the overall diameter of the maze is about 16 arbitrary units. The agent could move with constant speed using three action primitives: (i) step 1.5 units forward, (ii) turn 90 degrees to the left and make a 1.5-units step, and (iii) turn 90 degrees to the right and make a 1.5-units step. The effective turn and step size were noisy values (Gaussian noise, σ = 0.1). At each discrete time moment, the agent selects one of the three actions and moves; after each action, it obtains sensory information and (sometimes) reward.</p>
<p>The MB-RL agent receives sensory information from two sensors: a head-direction (i.e., orientation) sensor <italic>h</italic> and (a simplified model of) grid cells <italic>G</italic>; the agent has no further position or proximity sensors. In the rat, head-direction cells located in the postsubiculum part of the HC and in the anterior thalamic nuclei discharge as a function of the horizontal orientation of the head, also in darkness, within a preferred sector extending about 90 degrees that persists even for weeks [<xref ref-type="bibr" rid="pcbi.1006316.ref096">96</xref>] and are hypothesized to drive place fields [<xref ref-type="bibr" rid="pcbi.1006316.ref097">97</xref>]. In keeping, our orientation sensory unit <italic>h</italic> provides a discrete, 4-level signal that reduces the true orientation to four non-overlapping 90-degree sectors.</p>
<p>In turn, information about the spatial regularities derives by the so-called grid cells located in the medial entorhinal cortex (MEC) as reviewed in [<xref ref-type="bibr" rid="pcbi.1006316.ref098">98</xref>]. The discharge pattern of each grid cell resembles a hexagonal grid characterized by spatial frequency (spatial range from, e.g., 0.3 to 3 m), phase, and orientation (see <xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1C</xref>). Several models propose detailed explanations of how this pattern could be produced, e.g., oscillatory interference [<xref ref-type="bibr" rid="pcbi.1006316.ref095">95</xref>] and attractor networks [<xref ref-type="bibr" rid="pcbi.1006316.ref093">93</xref>]. Here we used a simplified mathematical model that constructs the typical hexagonal pattern of the grid cells by summing three 2D sinusoidal gratings oriented <italic>π</italic>/3 apart [<xref ref-type="bibr" rid="pcbi.1006316.ref099">99</xref>]: <inline-formula id="pcbi.1006316.e001"><alternatives><graphic id="pcbi.1006316.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi mathvariant="normal">G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where k<sub>i = 1…3</sub> stand for 2D oriented ramps with spatial phase <italic>r</italic><sub>0</sub> = [<italic>x</italic><sub>0</sub>,<italic>y</italic><sub>0</sub>]. We equipped our model with 11 grid cells with spatial frequencies ranging from 2 to 7 cycles per maze (step 0.5) and randomly drawn grid orientation and phase (samples in <xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1C</xref>). Each grid cell provides a binary signal sampled on a unit-step grid. The binary signal from all grid cells was combined multiplicatively to provide a signal <italic>G</italic> with about 500 different levels.</p>
<p>Finally, in order to simplify the recognition of target goal sites, we endowed the agent with an implicit target-recognition mechanism, which provides target identity (but not position, which has to be inferred on the basis of the above sensory information). Upon approaching the target within a unit distance, the MB-RL agent receives reward with a given probability, which depends on the experimental phase (see below).</p>
</sec>
<sec id="sec011">
<title>MB-RL agent: Architecture</title>
<p>The synthetic agent was implemented as a (Bayesian) model-based reinforcement learning (MB-RL) controller, having three key components (<xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1D</xref>). This and the following figures illustrate the model using the formalism of Bayesian networks, which show the model variables (circles) and the conditional distributions (rectangles) linking them.</p>
<sec id="sec012">
<title>Latent state categorization mechanism</title>
<p>The latent state categorization mechanism learns to categorize the combined head-direction and grid-cell input signal <italic>x</italic> = <italic>G</italic>×<italic>h</italic> into latent states <italic>s</italic> containing implicit spatial information. This component abstracts the acquisition of place cells [<xref ref-type="bibr" rid="pcbi.1006316.ref005">5</xref>], which form the model's emergent (latent) state space on top of which state-transition and state-value models are learned. The non-parametric categorization of the internal input signal is implemented using a growing probability distribution <italic>P</italic><sub><italic>c</italic></sub>(<italic>s</italic>|<italic>x</italic>) whereby each novel input signal extends the distribution with a new conditional entry. The new entry is initialized according to a Dirichlet process mixture, also known as the Chinese restaurant process (CRP) [<xref ref-type="bibr" rid="pcbi.1006316.ref100">100</xref>], with a prior that accounts for the popularity of the categories developed thus far. Critically, this categorization is further updated to account for behaviorally relevant transition contingencies (see later).</p>
</sec>
<sec id="sec013">
<title>State-transition model</title>
<p>The state-transition model learns the effects of executing a given action primitive <italic>a</italic> in the emergent latent state-space domain: s × a → s′ where s′ is the expected latent state after the transition. This model is intended to abstract the functions of the hippocampus. It is implemented as a conditional multinomial probability distribution P<sub>M</sub>(s′|s,a)~Cat(θ) parametrized by a Dirichlet conjugate prior θ<sub>s,a</sub>(s′)~Dir(α) that keeps a count of the number of time each state <italic>s</italic>′ is reached when the agent started from state <italic>s</italic> and selected action <italic>a</italic>.</p>
</sec>
<sec id="sec014">
<title>State-value model</title>
<p>The state-value model learns the value <italic>r</italic> of each latent state <italic>s</italic> given a goal or target <italic>g</italic>. This model is intended to abstract the functions of the ventral striatum. The <italic>state-value model</italic> defining the reinforcement contingencies is implemented as a conditional binomial probability distribution of reinforcement P<sub>v</sub>(r|g,s)~B(φ) (rewarded: <italic>r = 1</italic>, not rewarded: <italic>r = 0</italic>) parameterized by a Beta-conjugate prior φ<sub>g,s</sub>(r)~Beta(β). For each combination of <italic>g</italic> and <italic>s</italic>, the model (through its conjugate prior) accounts for the number of successes (reward) and failures (no reward) during the entire learning experience (see later for details). Note that at difference with most RL models, the value of states depends on the current goal <italic>g</italic>. This is in keeping with evidence that vStr encodes item-specific values, not just scalar values [<xref ref-type="bibr" rid="pcbi.1006316.ref008">8</xref>].</p>
</sec>
</sec>
<sec id="sec015">
<title>MB-RL agent: Control mechanism</title>
<p>We tested the behavior of the simulated agent in conditions that mimic the experimental manipulations of the animal study [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>]. During the <italic>cue conditioning</italic> (<italic>Cue</italic>) and <italic>contextual conditioning</italic> (<italic>Context</italic>) phases, action selection (i.e., the Bayesian inference of the next action) was conditioned on a specific cued target <italic>g</italic>; this corresponds to the fact that, in the animal study, the correct goal location was cued with a light. Rather, during <italic>the context conditioning test</italic> (<italic>CX test</italic>) phase, action selection operated in a “free-running mode” in which the target was not externally cued and the animal was able to navigate and search for a reward freely.</p>
<p>To select an action, the control mechanism first reads the combined input signal <italic>x</italic><sub><italic>t</italic></sub> from the head-direction sensor and the grid cells and uses the categorization distribution to select the corresponding most likely latent category, or state <italic>s</italic><sub><italic>t</italic></sub> = <italic>argmax</italic><sub><italic>s</italic></sub> <italic>P</italic><sub><italic>c</italic></sub>(<italic>s</italic>|<italic>x</italic><sub><italic>t</italic></sub>). Then, assuming a Markov Decision Process, inferential action selection maximizes the likelihood to obtain reward for a given target or goal: <italic>a</italic><sub><italic>t</italic></sub> = <italic>argmax</italic><sub><italic>a</italic></sub><italic>P</italic>(<italic>r</italic> = 1|<italic>g</italic>,<italic>s</italic><sub><italic>t</italic></sub>,<italic>a</italic>). Assuming further that the value depends only on the landing state following the selected action, we can factorize the value distribution: P(r|g,s,a) = ∑<sub>s′</sub> P<sub>v</sub>(r|g,s′) P<sub>M</sub>(s′|s,a). Thus, the control mechanism could use the state-transition model <italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) to simulate action execution, and the state-value model <italic>P</italic><sub><italic>V</italic></sub>(<italic>r</italic> = 1|<italic>g</italic>,<italic>s</italic>′) to evaluate the (immediate) expected outcomes under the various possible actions. Note that in practice, evaluating all the latent states would require high computational costs, especially for a large latent state-space–and such exhaustive evaluation would consider subsequent states that are highly unlikely, e.g., two states that lie in different corners of a maze. A more effective approach would consist in using an approximation that only evaluates the state <inline-formula id="pcbi.1006316.e002"><alternatives><graphic id="pcbi.1006316.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> that most likely would be visited upon executing a given action <inline-formula id="pcbi.1006316.e003"><alternatives><graphic id="pcbi.1006316.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi>a</mml:mi><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>′</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>; and this approach could generalize to simulated further transitions for deeper value prediction. In the simulations below, we compare the behavior of two control mechanisms: a controller that uses a form of lookahead prediction (forward sweeps) and one that dispenses from using it (baseline).</p>
</sec>
<sec id="sec016">
<title>Baseline (Shallow) controller</title>
<p>The baseline controller shown in <xref ref-type="fig" rid="pcbi.1006316.g008">Fig 8</xref> selects the action <italic>a</italic><sup><italic>i</italic></sup> that maximizes the immediately obtainable reward, i.e., <inline-formula id="pcbi.1006316.e004"><alternatives><graphic id="pcbi.1006316.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> locally predicted with the help of the state-transition and state-value models: <inline-formula id="pcbi.1006316.e005"><alternatives><graphic id="pcbi.1006316.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>′</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006316.e006"><alternatives><graphic id="pcbi.1006316.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, which we can combine in a single expression: <italic>a</italic><sub><italic>t</italic></sub> = <italic>argmax</italic><sub><italic>a</italic></sub> <italic>P</italic><sub><italic>V</italic></sub>(<italic>r</italic> = 1|<italic>g</italic>,<italic>argmax</italic><sub><italic>s</italic>′</sub><italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic><sub><italic>t</italic></sub>,<italic>a</italic>)). However, this one-step prediction method is quite myopic as it does not consider future events.</p>
<fig id="pcbi.1006316.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Shallow control mechanism.</title>
<p>It exploits the state-transition and state-value models and local maximization to predict the expected value <inline-formula id="pcbi.1006316.e007"><alternatives><graphic id="pcbi.1006316.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> of each action primitive <italic>a</italic><sup><italic>i</italic></sup> and select the most valuable one. For each action primitive, the mechanism first finds the latent state that most likely would be achieved by applying that action and then finds among those states the predictably most valuable one. The action bringing to that state is the selected one: <italic>a</italic><sub><italic>t</italic></sub> = <italic>argmax</italic><sub><italic>a</italic></sub> <italic>P</italic><sub><italic>v</italic></sub>(<italic>r</italic> = 1|<italic>g</italic>,<italic>argmax</italic><sub><italic>s</italic>′</sub><italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic><sub><italic>t</italic></sub>,<italic>a</italic>)).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec017">
<title>Controller using look-ahead prediction or forward sweeps</title>
<p>We designed an action selection mechanism based on forward sweeps (<xref ref-type="fig" rid="pcbi.1006316.g009">Fig 9</xref>) that generalizes the idea of local value maximization by using a limited form of forward search: it performs one forward sweep for each available action primitive <italic>a</italic><sub><italic>i</italic></sub> in order to estimate the future value obtainable as a result of applying this action. For each action <italic>a</italic><sup><italic>i</italic></sup>, the policy first simulates one transition that starts from the current state and applies that action: <inline-formula id="pcbi.1006316.e008"><alternatives><graphic id="pcbi.1006316.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> where <inline-formula id="pcbi.1006316.e009"><alternatives><graphic id="pcbi.1006316.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Then it iteratively performs a series of simulated transitions, each one locally maximizing the expected reward without restriction on the action: <inline-formula id="pcbi.1006316.e010"><alternatives><graphic id="pcbi.1006316.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Reward evidence <inline-formula id="pcbi.1006316.e011"><alternatives><graphic id="pcbi.1006316.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow/><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for each action <italic>a</italic><sup><italic>i</italic></sup> gradually accumulates along each step of each sweep: <inline-formula id="pcbi.1006316.e012"><alternatives><graphic id="pcbi.1006316.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The accumulated reward evidence drives stochastic action selection using the <italic>softmax</italic> function (here, with exponent coefficient β = 80).</p>
<fig id="pcbi.1006316.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Controller using look-ahead prediction (or forward sweeps).</title>
<p>The mechanism includes <italic>k</italic> sweeps, one for each available action primitive, each consisting of <italic>l</italic> steps. At each step <italic>j</italic>, the mechanism first iteratively predicts the next latent state of each sweep <inline-formula id="pcbi.1006316.e013"><alternatives><graphic id="pcbi.1006316.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and then accumulates the predicted value for that state: <inline-formula id="pcbi.1006316.e014"><alternatives><graphic id="pcbi.1006316.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. The first transition of the i-th sweep departs from the current latent state <italic>s</italic><sub><italic>t</italic></sub> and applies action primitive <italic>a</italic><sup><italic>i</italic></sup> while the following transitions recursively depart from the predicted state in the previous step and use any action that maximize the predicted reward. Finally, the mechanism selects the action that maximizes the cumulative predicted value: <inline-formula id="pcbi.1006316.e015"><alternatives><graphic id="pcbi.1006316.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g009" xlink:type="simple"/>
</fig>
<p>We optimized the efficiency of our sweep-based approach by taking into account a cost-benefit trade-off between the availability of more information and the computational costs required to execute long sweeps. From a theoretical perspective, forward sweeps can be stopped when there is sufficient discriminative evidence for action selection choice [<xref ref-type="bibr" rid="pcbi.1006316.ref023">23</xref>]. To model this, we defined an information-based measure of <italic>decision certainty</italic> that uses the transition- and value- models to decide which action to simulate and how far to deepen the sweeps (<xref ref-type="fig" rid="pcbi.1006316.g010">Fig 10</xref>). First, the value <italic>v</italic><sub><italic>s</italic></sub> of being in a state <italic>s</italic> was defined as the negative uncertainty of the probability to obtain reward in that state, i.e., <italic>v</italic><sub><italic>s</italic></sub> = log<sub>2</sub> <italic>P</italic><sub><italic>v</italic></sub>(<italic>r</italic> = 1|<italic>g</italic>,<italic>s</italic>). Then, the decision certainty <italic>d</italic><sub><italic>ij</italic></sub> of taking action <italic>a</italic><sup><italic>i</italic></sup> (assuming that it brings to state <inline-formula id="pcbi.1006316.e016"><alternatives><graphic id="pcbi.1006316.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>) instead of action <italic>a</italic><sup><italic>j</italic></sup> (expected that it brings to state <inline-formula id="pcbi.1006316.e017"><alternatives><graphic id="pcbi.1006316.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>) was defined as the difference <inline-formula id="pcbi.1006316.e018"><alternatives><graphic id="pcbi.1006316.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msub><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. Thus, to optimize sweep length, at each depth we calculated the decision certainty <italic>d</italic> relative to the two actions with greatest cumulative evidence for reward (see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1006316.s001">S1 File</xref>). Note that the log-difference used to calculate this value implicitly normalizes the accumulated evidence into probabilities. The sweeps were stopped when the decision certainty exceeded a threshold (here, 0.15) and the action with the greatest accumulated evidence for reward was selected.</p>
<fig id="pcbi.1006316.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Information-driven adaptive sweep-depth.</title>
<p>At each depth <italic>j</italic> is calculated discriminative certainty <inline-formula id="pcbi.1006316.e019"><alternatives><graphic id="pcbi.1006316.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for the two currently most valuable sweeps. Sweep depth increases until the selection certainty exceeds a given threshold: <italic>d</italic><sub><italic>j</italic></sub> &gt; <italic>d</italic><sub><italic>thr</italic></sub>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g010" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>MB-RL agent: Learning procedure</title>
<p>In general, the objective of the MB-RL agent is to obtain maximum reward by moving from a given start-position to a given goal-position, which may vary during the learning process (see later). Following a MB-RL approach, starting from an empty memory, the system has to learn a probabilistic model of the environment (i.e., the transitions from state to state in the maze) and the distribution of reward given the target (or goal), which varies from trial to trial (here, limited to 9 possible goals as in the rodent experiment). Learning thus consists in adjusting the probability distributions of the two (state-state and state-value) agent models on the basis of experience, using Bayes' rule [<xref ref-type="bibr" rid="pcbi.1006316.ref101">101</xref>]. Importantly, like other deep RL experiments, the model is not provided with a predefined state space, but also has to simultaneously learn to categorize the internal input signal <italic>x</italic> = <italic>G</italic>×<italic>h</italic> into behaviorally useful (spatial) categories <italic>s</italic> that represent the location of the agent in the maze. In sum, the agent has to simultaneously learn three things: state-state and state-value models, and state space; and learning is end-to-end (i.e., all parameters are trained jointly), see <xref ref-type="fig" rid="pcbi.1006316.g011">Fig 11</xref>.</p>
<fig id="pcbi.1006316.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006316.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Learning the latent state-space, the state-transition and state-value models.</title>
<p>Given (1) the last input <italic>x</italic><sub><italic>t</italic>−1</sub> and latent state <italic>s</italic><sub><italic>t</italic>−1</sub>, (2) performed action <italic>a</italic><sub><italic>t</italic>−1</sub>, (3) observed new input <italic>x</italic><sub><italic>t</italic></sub>, reward <italic>r</italic><sub><italic>t</italic></sub>, and inferred latent state <italic>s</italic><sub><italic>t</italic></sub>, learning consists of (5) adjusting the categorization model <inline-formula id="pcbi.1006316.e020"><alternatives><graphic id="pcbi.1006316.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> to make it more congruent with the state-transition model and updating the conjugate priors <inline-formula id="pcbi.1006316.e021"><alternatives><graphic id="pcbi.1006316.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006316.e022"><alternatives><graphic id="pcbi.1006316.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mo>:</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> of the state-transition and state-value models to accommodate the internal perception of the experienced behavioral evidence (see the text for details). Notably, the update of the state-value conjugate is a Bayesian analog of TD-learning using predicted discounted future value <inline-formula id="pcbi.1006316.e023"><alternatives><graphic id="pcbi.1006316.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mover accent="true"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> accumulated in (4) a forward sweep.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.g011" xlink:type="simple"/>
</fig>
<p>In our simulations, learning consisted of a sequence of trials, each of which started from a random position and had the goal to reach a randomly selected target within 32 time steps. Upon approaching the target within one unit distance and this time limit, the artificial agent received reward with a certain probability. Upon obtaining reward (or after the time limit had expired), a new trial began. Like our animal study [<xref ref-type="bibr" rid="pcbi.1006316.ref014">14</xref>], learning was divided into two phases that differed on reward distribution. In the first, <italic>Cue Conditioning</italic> phase that lasted for 2.000 trials, reaching a target within the time limit was rewarded with 100%. The starting position in the first 360 trials of this phase was the central area (within 4 units from the center). In the second, <italic>Contextual Conditioning</italic> phase that lasted for 700 trials, reward probability was 75% for the targets in a selected high-reward chamber (fixed for each agent) and 25% for the targets in the other two, low-reward chambers. The amount of reward obtained was increased to maintain the overall reward availability compatible to that in the first phase. The number of simulated learning trials matched those of the animal experiment [<xref ref-type="bibr" rid="pcbi.1006316.ref019">19</xref>].</p>
<p>In our simulations, we test two different learning procedures: a <italic>baseline</italic> procedure and a <italic>forward sweep</italic> procedure. The two procedures use the same methods for learning the state space and the state-state (transition) models. However, they differ in how they learn the state-value model. They both use the transition model to retrieve the value of possible future states and to update the value function on-line; however, the baseline learning procedure uses a short prediction (equivalent to a sweep of length 1) while the forward sweep procedure uses a longer look-ahead (i.e., sweeps of length 9—a length that is compatible with forward sweeps reported in the literature [<xref ref-type="bibr" rid="pcbi.1006316.ref007">7</xref>]).</p>
<p>Below we explain in detail how (the probability distributions of) the three components of the MB-RL are learned. All the learning procedures essentially consist in performing Bayesians update of the (conjugate priors of the) probability distributions, in the light of novel observed evidence. Note that hereafter we distinguish the prior from the posterior distributions with the help of additional time-index.</p>
<sec id="sec019">
<title>State space learning</title>
<p>State space learning uses Bayesian nonparametrics to categorize the input signal, with Chinese Restaurant Process (CRP) priors over the emergent categories [<xref ref-type="bibr" rid="pcbi.1006316.ref100">100</xref>]. This approach offers an unbounded latent space, but the CRP prior adapts the model complexity to the data by expanding it in a conservative way. Thus, any novel, or unexperienced so far input <italic>x</italic><sub><italic>n</italic>+1</sub> is “assigned” to a new category or state <italic>s</italic><sub><italic>new</italic></sub> with a small probability P<sub>c</sub>(s<sub>new</sub>|x<sub>n+1</sub>) = α/A (controlled by a concentration parameter α, here α = 20) or to previously initialized latent state <italic>s</italic><sub><italic>i</italic></sub> according to its popularity across all experienced input states P<sub>c</sub>(s<sub>i</sub>|x<sub>n+1</sub>) = ∑<sub>j</sub>P<sub>c</sub>(s<sub>i</sub>|x<sub>j</sub>)/A where A = α + ∑<sub>i,j</sub>P<sub>c</sub>(s<sub>i</sub>|x<sub>j</sub>) is a normalizing factor. At each time step <italic>t</italic>, an input signal <italic>x</italic><sub><italic>t</italic></sub> evokes the most probable input-specific latent state <inline-formula id="pcbi.1006316.e024"><alternatives><graphic id="pcbi.1006316.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> that is in turn used for action selection and learning. Critically, the belief in the category-assignment <inline-formula id="pcbi.1006316.e025"><alternatives><graphic id="pcbi.1006316.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> of the last input state <italic>x</italic><sub><italic>t</italic>−1</sub> is scaled for every state <italic>s</italic><sub><italic>i</italic></sub> with the state-transition contingencies <inline-formula id="pcbi.1006316.e026"><alternatives><graphic id="pcbi.1006316.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> (adapted from [<xref ref-type="bibr" rid="pcbi.1006316.ref056">56</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref102">102</xref>]):
<disp-formula id="pcbi.1006316.e027">
<alternatives>
<graphic id="pcbi.1006316.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>This method creates an implicit dependency between state space and state-state (transition) learning, in the sense that states assignment or categorization is retained with higher probability if it is congruent with the emergent transition model. Thus, a novel aspect of our MB-RL approach is that state-transition and state-value learning work in synergy with category- (or state-space-) learning to afford the acquisition of an effective internal model of the experienced interactions with the external environment, from scratch.</p>
<p>Two technical points are worth noticing to contextualize our approach. First, our latent-state model <italic>P</italic><sub><italic>c</italic></sub>(<italic>s</italic>|<italic>x</italic>) is a perceptual model, which permits inferring latent state s (e.g., the current location) corresponding to the current sensory state x. This is distinct from the SR approach [<xref ref-type="bibr" rid="pcbi.1006316.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref069">69</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref075">75</xref>,<xref ref-type="bibr" rid="pcbi.1006316.ref076">76</xref>], which encodes a predictive representation of future states (e.g., of future locations). The two learning methods may seem similar, especially because we use a part of the transition model <italic>P</italic><sub><italic>M</italic></sub>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) to learn a behaviorally relevant space of latent states, see Eq (<xref ref-type="disp-formula" rid="pcbi.1006316.e026">1</xref>). However, it is important to note that we use the probability of the "source" state <italic>s</italic>, not of the "outcome" <italic>s</italic>′, to adjust the perceptual model. This renders the state space and state transition model coherent, but does not yield predictive representations as in the SR approach. In other words, the predictive aspect of the MB-RL agent consists in using model-based, look-ahead prediction, as opposed to using predictive state representations as in the SR approach.</p>
<p>Second, in our simulations, the concentration (α) parameter plays a permissive role in the expansion of the latent state-space: the smaller the α, the smaller the probability that new latent states will be formed. This parameter operates only within the state space learning mechanism and is fully distinct from the parameters of the other model components, such as the (information-driven) mechanism that sets the depth of sweeps explained above, or the discounting factors.</p>
</sec>
<sec id="sec020">
<title>State-transition model</title>
<p>After executing an action <italic>a</italic><sub><italic>t</italic>−1</sub>, the agent obtains a new observation (sensory measurement <italic>x</italic><sub><italic>t</italic></sub>), infers the corresponding latent state <italic>s</italic><sub><italic>t</italic></sub>, and updates its transition distribution s<sub>t−1</sub> × a<sub>t−1</sub> → s<sub>t</sub>, i.e., learns the transition model (see <xref ref-type="fig" rid="pcbi.1006316.g011">Fig 11</xref>). To that aim, the conjugate Dirichlet prior of the conditional multinomial distribution is updated with the new evidence in the latent-space domain: <inline-formula id="pcbi.1006316.e028"><alternatives><graphic id="pcbi.1006316.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> and then normalized, to obtain the posterior of the state-transition multinomial <inline-formula id="pcbi.1006316.e029"><alternatives><graphic id="pcbi.1006316.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>′</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec021">
<title>State-value model</title>
<p>The state-value model <inline-formula id="pcbi.1006316.e030"><alternatives><graphic id="pcbi.1006316.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> is updated using a temporal-difference (TD) like manner that at each learning step accounts for the past experience, or conjugate prior <inline-formula id="pcbi.1006316.e031"><alternatives><graphic id="pcbi.1006316.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, observed true reward <italic>r</italic><sub><italic>t</italic></sub>, and predicted future value. The agent makes internal simulations (or sweeps) of length <italic>l</italic> (here, <italic>l</italic> = 1 for the baseline learning procedure and <italic>l</italic> = 9 for the forward sweep learning procedure) conditioned on the latent state <italic>s</italic><sub><italic>t</italic></sub>. During the sweeps, the agent accumulates expected future values in terms of parameters of the conjugate distribution of the model of reward, <inline-formula id="pcbi.1006316.e032"><alternatives><graphic id="pcbi.1006316.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">γ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, applying a discount factor (here, γ = 0.90). At each step <italic>i</italic> of the sweep, the BM-RL agent exploits a mechanism analogous to the forward sweeps used for control to collect predicted state values. As shown on <xref ref-type="fig" rid="pcbi.1006316.g011">Fig 11</xref>, the mechanism uses the state-transition and the state-value models, and local maximization to infer the latent states of the sweep, i.e., the states with the greatest reward expectancy among all the states that are achievable within one action from the previous state: <inline-formula id="pcbi.1006316.e033"><alternatives><graphic id="pcbi.1006316.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> where <inline-formula id="pcbi.1006316.e034"><alternatives><graphic id="pcbi.1006316.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. Then, the conjugate prior of the conditional binomial distribution of reward is updated in a way that is analogous to temporal difference (TD) learning [<xref ref-type="bibr" rid="pcbi.1006316.ref001">1</xref>] and which considers the (immediately) observed reward r<sub>t</sub>, and value φ<sub>Obs</sub> = [1 − r<sub>t</sub>,r<sub>t</sub>] and the (predicted) future value expectancy: <inline-formula id="pcbi.1006316.e035"><alternatives><graphic id="pcbi.1006316.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. The learning coefficient <italic>α</italic> decreases along with time on a log scale (<italic>α</italic> = <italic>α</italic><sub>0</sub>/log<sub>10</sub>(<italic>t</italic>), <italic>α</italic><sub>0</sub> = 1.5). This learning schedule gradually shifts the value-learning policy, from relying on new evidence to exploiting acquired knowledge. Finally, the posterior is obtained by normalizing the updated conjugate: <inline-formula id="pcbi.1006316.e036"><alternatives><graphic id="pcbi.1006316.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006316.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mo>:</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo>:</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
</sec>
<sec id="sec022">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006316.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006316.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Algorithm of sweep-based action selection.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank G. T. Meijer for help with the preparation of <xref ref-type="fig" rid="pcbi.1006316.g001">Fig 1A</xref> and Andrew Wikenheiser.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006316.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>Reinforcement Learning: An Introduction</chapter-title>. <publisher-loc>Cambridge MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Goals and habits in the brain</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>: <fpage>312</fpage>–<lpage>325</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.09.007" xlink:type="simple">10.1016/j.neuron.2013.09.007</ext-link></comment> <object-id pub-id-type="pmid">24139036</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1560" xlink:type="simple">10.1038/nn1560</ext-link></comment> <object-id pub-id-type="pmid">16286932</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Keefe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dostrovsky</surname> <given-names>J</given-names></name>. <article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Res Vol</source>. <year>1971</year>;<volume>34</volume>: <fpage>171</fpage>–<lpage>175</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Apicella</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Scarnati</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ljungberg</surname> <given-names>T</given-names></name>. <article-title>Neuronal activity in monkey ventral striatum related to the expectation of reward</article-title>. <source>J Neurosci</source>. <year>1992</year>;<volume>12</volume>: <fpage>4595</fpage>–<lpage>4610</lpage>. <object-id pub-id-type="pmid">1464759</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>12176</fpage>–<lpage>12189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3761-07.2007" xlink:type="simple">10.1523/JNEUROSCI.3761-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17989284</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Verschure</surname> <given-names>PFMJ</given-names></name>, <name name-style="western"><surname>Battaglia</surname> <given-names>FP</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>. <article-title>The hippocampal-striatal axis in learning, prediction and goal-directed behavior</article-title>. <source>Trends Neurosci</source>. <year>2011</year>;<volume>34</volume>: <fpage>548</fpage>–<lpage>559</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2011.08.001" xlink:type="simple">10.1016/j.tins.2011.08.001</ext-link></comment> <object-id pub-id-type="pmid">21889806</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Covert Expectation-of-Reward in Rat Ventral Striatum at Decision Points</article-title>. <source>Front Integr Neurosci</source>. <year>2009</year>;<volume>3</volume>: <fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.07.001.2009" xlink:type="simple">10.3389/neuro.07.001.2009</ext-link></comment> <object-id pub-id-type="pmid">19225578</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verschure</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>The why, what, where, when and how of goal-directed choice: neuronal and computational principles</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>: <volume>369</volume>: <fpage>20130483</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0483" xlink:type="simple">10.1098/rstb.2013.0483</ext-link></comment> <object-id pub-id-type="pmid">25267825</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDannald</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Lucantonio</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burke</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Ventral striatum and orbitofrontal cortex are both required for model-based, but not model-free, reinforcement learning</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>2700</fpage>–<lpage>2705</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5499-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5499-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21325538</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van der Meer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Information processing in decision-making systems</article-title>. <source>The Neuroscientist</source>. <year>2012</year>;<volume>18</volume>: <fpage>342</fpage>–<lpage>359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1073858411435128" xlink:type="simple">10.1177/1073858411435128</ext-link></comment> <object-id pub-id-type="pmid">22492194</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Lansink</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>. <article-title>Internally generated sequences in learning and executing goal-directed behavior</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>: <fpage>647</fpage>–<lpage>657</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.06.011" xlink:type="simple">10.1016/j.tics.2014.06.011</ext-link></comment> <object-id pub-id-type="pmid">25156191</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kemere</surname> <given-names>C</given-names></name>, <article-title>Meer M van der. Internally generated hippocampal sequences as a vantage point to probe future-oriented cognition</article-title>. <source>Ann N Y Acad Sci</source>. <year>2017</year>;<volume>1396</volume>: <fpage>144</fpage>–<lpage>165</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/nyas.13329" xlink:type="simple">10.1111/nyas.13329</ext-link></comment> <object-id pub-id-type="pmid">28548460</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penagos</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Varela</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Oscillations, neural computations and learning during wake and sleep</article-title>. <source>Curr Opin Neurobiol</source>. <year>2017</year>;<volume>44</volume>: <fpage>193</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2017.05.009" xlink:type="simple">10.1016/j.conb.2017.05.009</ext-link></comment> <object-id pub-id-type="pmid">28570953</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>. <article-title>Dorsal hippocampus contributes to model-based planning</article-title>. <source>Nat Neurosci</source>. <year>2017</year>;<volume>20</volume>: <fpage>1269</fpage>–<lpage>1276</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4613" xlink:type="simple">10.1038/nn.4613</ext-link></comment> <object-id pub-id-type="pmid">28758995</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Iodice</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maisto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stoianov</surname> <given-names>I</given-names></name>. <article-title>Model-Based Approaches to Active Perception and Control</article-title>. <source>Entropy</source>. <year>2017</year>;<volume>19</volume>: <fpage>266</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3390/e19060266" xlink:type="simple">10.3390/e19060266</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006316.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>The algorithmic anatomy of model-based evaluation</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130478</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lansink</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lankelma</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>BJ</given-names></name>, <etal>et al</etal>. <article-title>Reward cues in space: commonalities and differences in neural coding by hippocampal and ventral striatal ensembles</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>12444</fpage>–<lpage>12459</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0593-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0593-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22956836</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Vicarious trial and error</article-title>. <source>Nat Rev Neurosci</source>. <year>2016</year>;<volume>17</volume>: <fpage>147</fpage>–<lpage>159</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2015.30" xlink:type="simple">10.1038/nrn.2015.30</ext-link></comment> <object-id pub-id-type="pmid">26891625</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Expectancies in decision making, reinforcement learning, and ventral striatum</article-title>. <source>Front Neurosci</source>. <year>2010</year>;<volume>4</volume>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Zeidman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Burgess</surname> <given-names>N</given-names></name>. <article-title>Forward and Backward Inference in Spatial Cognition</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003383</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003383" xlink:type="simple">10.1371/journal.pcbi.1003383</ext-link></comment> <object-id pub-id-type="pmid">24348230</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Chersi</surname> <given-names>F</given-names></name>. <article-title>The Mixed Instrumental Controller: using Value of Information to combine habitual choice and mental simulation</article-title>. <source>Front Cogn</source>. <year>2013</year>;<volume>4</volume>: <fpage>92</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00092" xlink:type="simple">10.3389/fpsyg.2013.00092</ext-link></comment> <object-id pub-id-type="pmid">23459512</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref024"><label>24</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tesauro</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Galperin</surname> <given-names>GR</given-names></name>. <chapter-title>On-line Policy Improvement Using Monte-Carlo Search</chapter-title>. <source>Proceedings of the 9th International Conference on Neural Information Processing Systems</source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1996</year>. pp. <fpage>1068</fpage>–<lpage>1074</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=2998981.2999131" xlink:type="simple">http://dl.acm.org/citation.cfm?id=2998981.2999131</ext-link></mixed-citation></ref>
<ref id="pcbi.1006316.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lansink</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Goltstein</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Lankelma</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CM</given-names></name>. <article-title>Hippocampus leads ventral striatum in replay of place-reward information</article-title>. <source>PLoS Biol</source>. <year>2009</year>;<volume>7</volume>: <fpage>e1000173</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1000173" xlink:type="simple">10.1371/journal.pbio.1000173</ext-link></comment> <object-id pub-id-type="pmid">19688032</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfeiffer</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>. <article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>: <fpage>74</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12112" xlink:type="simple">10.1038/nature12112</ext-link></comment> <object-id pub-id-type="pmid">23594744</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>. <article-title>Reactivation of hippocampal ensemble memories during sleep</article-title>. <source>Science</source>. <year>1994</year>;<volume>265</volume>: <fpage>676</fpage>–<lpage>679</lpage>. <object-id pub-id-type="pmid">8036517</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diba</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>1241</fpage>–<lpage>1242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1961" xlink:type="simple">10.1038/nn1961</ext-link></comment> <object-id pub-id-type="pmid">17828259</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref029"><label>29</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>. <chapter-title>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</chapter-title>. <source>Proceedings of the Seventh International Conference on Machine Learning</source>. <publisher-loc>San Mateo, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>; <year>1990</year>. pp. <fpage>216</fpage>–<lpage>224</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mnih</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kavukcuoglu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rusu</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Veness</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bellemare</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>: <fpage>529</fpage>–<lpage>533</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14236" xlink:type="simple">10.1038/nature14236</ext-link></comment> <object-id pub-id-type="pmid">25719670</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>BJ</given-names></name>. <article-title>Selective excitotoxic lesions of the hippocampus and basolateral amygdala have dissociable effects on appetitive cue and place conditioning based on path integration in a novel Y-maze procedure</article-title>. <source>Eur J Neurosci</source>. <year>2006</year>;<volume>23</volume>: <fpage>3071</fpage>–<lpage>3080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2006.04883.x" xlink:type="simple">10.1111/j.1460-9568.2006.04883.x</ext-link></comment> <object-id pub-id-type="pmid">16819997</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>BJ</given-names></name>. <article-title>Functional interaction between the hippocampus and nucleus accumbens shell is necessary for the acquisition of appetitive spatial context conditioning</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>6950</fpage>–<lpage>6959</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1615-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1615-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18596169</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Wyatte</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Rohrlich</surname> <given-names>J</given-names></name>. <article-title>Deep Predictive Learning: A Comprehensive Model of Three Visual Streams</article-title>. <source>ArXiv170904654 Q-Bio</source>. <year>2017</year>; Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.04654" xlink:type="simple">http://arxiv.org/abs/1709.04654</ext-link></mixed-citation></ref>
<ref id="pcbi.1006316.ref034"><label>34</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <chapter-title>The Mind within the Brain: How We Make Decisions and How those Decisions Go Wrong</chapter-title>. <edition>1 edition</edition>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Improving generalization for temporal difference learning: The successor representation</article-title>. <source>Neural Comput</source>. <year>1993</year>;<volume>5</volume>: <fpage>613</fpage>–<lpage>624</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Russek</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Cheong</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The successor representation in human reinforcement learning</article-title>. <source>bioRxiv</source>. <year>2016</year>; <fpage>083824</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/083824" xlink:type="simple">10.1101/083824</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006316.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hok</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Save</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lenck-Santini</surname> <given-names>PP</given-names></name>, <name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>. <article-title>Coding for spatial goals in the prelimbic/infralimbic area of the rat frontal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>: <fpage>4602</fpage>–<lpage>4607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0407332102" xlink:type="simple">10.1073/pnas.0407332102</ext-link></comment> <object-id pub-id-type="pmid">15761059</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wikenheiser</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Hippocampal theta sequences reflect current goals</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>289</fpage>–<lpage>294</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3909" xlink:type="simple">10.1038/nn.3909</ext-link></comment> <object-id pub-id-type="pmid">25559082</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>. <object-id pub-id-type="pmid">9054347</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>A</given-names></name>. <article-title>Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective</article-title>. <source>Cognition</source>. <year>2008</year>; <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2008.08.011" xlink:type="simple">http://dx.doi.org/10.1016/j.cognition.2008.08.011</ext-link></mixed-citation></ref>
<ref id="pcbi.1006316.ref041"><label>41</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>A model of how the basal ganglia generates and uses neural signals that predict reinforcement</chapter-title>. In: <name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Beiser</surname> <given-names>D</given-names></name>, editors. <source>Models of Information Processing in the Basal Ganglia</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1995</year>. pp. <fpage>249</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solway</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Goal-directed decision making as probabilistic inference: A computational framework and potential neural correlates</article-title>. <source>Psychol Rev</source>. <year>2012</year>;<volume>119</volume>: <fpage>120</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0026435" xlink:type="simple">10.1037/a0026435</ext-link></comment> <object-id pub-id-type="pmid">22229491</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>. <chapter-title>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</chapter-title> [Internet]. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Henry Holt and Co., Inc</publisher-name>.; <year>1982</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1096911" xlink:type="simple">http://portal.acm.org/citation.cfm?id=1096911</ext-link></mixed-citation></ref>
<ref id="pcbi.1006316.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Prediction, Sequences and the Hippocampus</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2009</year>;<volume>364</volume>: <fpage>1193</fpage>–<lpage>1201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2008.0316" xlink:type="simple">10.1098/rstb.2008.0316</ext-link></comment> <object-id pub-id-type="pmid">19528000</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dragoi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Temporal encoding of place sequences by hippocampal cell assemblies</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>50</volume>: <fpage>145</fpage>–<lpage>157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.02.023" xlink:type="simple">10.1016/j.neuron.2006.02.023</ext-link></comment> <object-id pub-id-type="pmid">16600862</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erdem</surname> <given-names>UM</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>. <article-title>A biologically inspired hierarchical goal directed navigation model</article-title>. <source>J Physiol Paris</source>. <year>2014</year>;<volume>108</volume>: <fpage>28</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jphysparis.2013.07.002" xlink:type="simple">10.1016/j.jphysparis.2013.07.002</ext-link></comment> <object-id pub-id-type="pmid">23891644</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chersi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Using hippocampal-striatal loops for spatial navigation and goal-directed decision-making</article-title>. <source>Cogn Process</source>. <year>2012</year>;<volume>13</volume>: <fpage>125</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chersi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Mental imagery in the navigation domain: A computational model of sensory-motor simulation mechanisms</article-title>. <source>Adaptive Behavior</source>. <year>2013</year>: <fpage>251</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ognibene</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fitzgerald</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Active inference and epistemic value</article-title>. <source>Cogn Neurosci</source>. <year>2015</year>;0: <fpage>1</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/17588928.2015.1020053" xlink:type="simple">10.1080/17588928.2015.1020053</ext-link></comment> <object-id pub-id-type="pmid">25689102</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>FitzGerald</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schwartenbeck</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Active Inference: A Process Theory</article-title>. <source>Neural Comput</source>. <year>2016</year>; <fpage>1</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00912" xlink:type="simple">10.1162/NECO_a_00912</ext-link></comment> <object-id pub-id-type="pmid">27870614</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>FitzGerald</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schwartenbeck</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Active inference and learning</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2016</year>;<volume>68</volume>: <fpage>862</fpage>–<lpage>879</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2016.06.022" xlink:type="simple">10.1016/j.neubiorev.2016.06.022</ext-link></comment> <object-id pub-id-type="pmid">27375276</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cartoni</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pio-Lopez</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Active Inference, epistemic value, and vicarious trial and error</article-title>. <source>Learn Mem</source>. <year>2016</year>;<volume>23</volume>: <fpage>322</fpage>–<lpage>338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/lm.041780.116" xlink:type="simple">10.1101/lm.041780.116</ext-link></comment> <object-id pub-id-type="pmid">27317193</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Cognitive control over learning: creating, clustering, and generalizing task-set structure</article-title>. <source>Psychol Rev</source>. <year>2013</year>;<volume>120</volume>: <fpage>190</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0030852" xlink:type="simple">10.1037/a0030852</ext-link></comment> <object-id pub-id-type="pmid">23356780</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Koechlin</surname> <given-names>E</given-names></name>. <article-title>Reasoning, Learning, and Creativity: Frontal Lobe Function and Human Decision-Making</article-title>. <source>PLoS Biol</source>. <year>2012</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001293" xlink:type="simple">10.1371/journal.pbio.1001293</ext-link></comment> <object-id pub-id-type="pmid">22479152</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donoso</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Koechlin</surname> <given-names>E</given-names></name>. <article-title>Foundations of human reasoning in the prefrontal cortex</article-title>. <source>Science</source>. <year>2014</year>;<volume>344</volume>: <fpage>1481</fpage>–<lpage>1486</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1252254" xlink:type="simple">10.1126/science.1252254</ext-link></comment> <object-id pub-id-type="pmid">24876345</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stoianov</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Genovesio</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Prefrontal Goal Codes Emerge as Latent States in Probabilistic Value Learning</article-title>. <source>J Cogn Neurosci</source>. <year>2015</year>;<volume>28</volume>: <fpage>140</fpage>–<lpage>157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00886" xlink:type="simple">10.1162/jocn_a_00886</ext-link></comment> <object-id pub-id-type="pmid">26439267</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Hierarchical Active Inference: A Theory of Motivated Control</article-title>. <source>Trends Cogn Sci</source>. <year>2018</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2018.01.009" xlink:type="simple">10.1016/j.tics.2018.01.009</ext-link></comment> <object-id pub-id-type="pmid">29475638</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>. <article-title>The value of foresight: how prospection affects decision-making</article-title>. <source>Front Neurosci</source>. <year>2011</year>;<volume>5</volume>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name>. <article-title>Navigating the Affordance Landscape: Feedback Control as a Process Model of Behavior and Cognition</article-title>. <source>Trends Cogn Sci</source>. <year>2016</year>;<volume>20</volume>: <fpage>414</fpage>–<lpage>424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2016.03.013" xlink:type="simple">10.1016/j.tics.2016.03.013</ext-link></comment> <object-id pub-id-type="pmid">27118642</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hok</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Lenck-Santini</surname> <given-names>P-P</given-names></name>, <name name-style="western"><surname>Roux</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Save</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>RU</given-names></name>, <name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>. <article-title>Goal-Related Activity in Hippocampal Place Cells</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>472</fpage>–<lpage>482</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2864-06.2007" xlink:type="simple">10.1523/JNEUROSCI.2864-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17234580</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weinstein</surname> <given-names>A</given-names></name>. <article-title>Model-based hierarchical reinforcement learning and human action control</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130480</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0480" xlink:type="simple">10.1098/rstb.2013.0480</ext-link></comment> <object-id pub-id-type="pmid">25267822</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Maisto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Problem Solving as Probabilistic Inference with Subgoaling: Explaining Human Successes and Pitfalls in the Tower of Hanoi</article-title>. <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>, editor. <source>PLOS Comput Biol</source>. <year>2016</year>;<volume>12</volume>: <fpage>e1004864</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004864" xlink:type="simple">10.1371/journal.pcbi.1004864</ext-link></comment> <object-id pub-id-type="pmid">27074140</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solway</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diuk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Córdova</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Optimal Behavioral Hierarchy</article-title>. <source>PLOS Comput Biol</source>. <year>2014</year>;<volume>10</volume>: <fpage>e1003779</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003779" xlink:type="simple">10.1371/journal.pcbi.1003779</ext-link></comment> <object-id pub-id-type="pmid">25122479</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maisto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Divide et impera: subgoaling reduces the complexity of probabilistic inference and problem solving</article-title>. <source>J R Soc Interface</source>. <year>2015</year>;<volume>12</volume>: <fpage>20141335</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rsif.2014.1335" xlink:type="simple">10.1098/rsif.2014.1335</ext-link></comment> <object-id pub-id-type="pmid">25652466</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maisto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Nonparametric Problem-Space Clustering: Learning Efficient Codes for Cognitive Control Tasks</article-title>. <source>Entropy</source>. <year>2016</year>;<volume>18</volume>: <fpage>61</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3390/e18020061" xlink:type="simple">10.3390/e18020061</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006316.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Hierarchical models of behavior and prefrontal function</article-title>. <source>Trends Cogn Sci</source>. <year>2008</year>;<volume>12</volume>: <fpage>201</fpage>–<lpage>208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2008.02.009" xlink:type="simple">10.1016/j.tics.2008.02.009</ext-link></comment> <object-id pub-id-type="pmid">18420448</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Hierarchical Models in the Brain</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>: <fpage>e1000211</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment> <object-id pub-id-type="pmid">18989391</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Franzius</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wiskott</surname> <given-names>L</given-names></name>. <article-title>Slowness and sparseness lead to place, head-direction, and spatial-view cells</article-title>. <source>PLoS Comput Biol</source>. <year>2007</year>;<volume>3</volume>: <fpage>e166</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.0030166" xlink:type="simple">10.1371/journal.pcbi.0030166</ext-link></comment> <object-id pub-id-type="pmid">17784780</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stachenfeld</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The hippocampus as a predictive map</article-title>. <source>Nat Neurosci</source>. <year>2017</year>;<volume>20</volume>: <fpage>1643</fpage>–<lpage>1653</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4650" xlink:type="simple">10.1038/nn.4650</ext-link></comment> <object-id pub-id-type="pmid">28967910</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source>Psychol Rev</source>. <year>1995</year>;<volume>102</volume>: <fpage>419</fpage>–<lpage>457</lpage>. <object-id pub-id-type="pmid">7624455</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hobson</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Ondobaka</surname> <given-names>S</given-names></name>. <article-title>Active Inference, Curiosity and Insight</article-title>. <source>Neural Comput</source>. <year>2017</year>; <fpage>1</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco_a_00999" xlink:type="simple">10.1162/neco_a_00999</ext-link></comment> <object-id pub-id-type="pmid">28777724</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kumaran</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hassabis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>. <article-title>What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated</article-title>. <source>Trends Cogn Sci</source>. <year>2016</year>;<volume>20</volume>: <fpage>512</fpage>–<lpage>534</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2016.05.004" xlink:type="simple">10.1016/j.tics.2016.05.004</ext-link></comment> <object-id pub-id-type="pmid">27315762</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watkins</surname> <given-names>CJCH</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Q-learning</article-title>. <source>Mach Learn</source>. <year>1992</year>;<volume>8</volume>: <fpage>279</fpage>–<lpage>292</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spiers</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>SJ</given-names></name>. <article-title>Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions</article-title>. <source>Front Hum Neurosci</source>. <year>2015</year>;<volume>9</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2015.00125" xlink:type="simple">10.3389/fnhum.2015.00125</ext-link></comment> <object-id pub-id-type="pmid">25852515</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Russek</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Cheong</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The successor representation in human reinforcement learning</article-title>. <source>Nat Hum Behav</source>. <year>2017</year>;<volume>1</volume>: <fpage>680</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-017-0180-8" xlink:type="simple">10.1038/s41562-017-0180-8</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006316.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russek</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title>. <source>PLOS Comput Biol</source>. <year>2017</year>;<volume>13</volume>: <fpage>e1005768</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005768" xlink:type="simple">10.1371/journal.pcbi.1005768</ext-link></comment> <object-id pub-id-type="pmid">28945743</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hok</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Chah</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Save</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>. <article-title>Prefrontal cortex focally modulates hippocampal place cell firing patterns</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>3443</fpage>–<lpage>3451</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3427-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3427-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23426672</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hok</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Save</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lenck-Santini</surname> <given-names>PP</given-names></name>, <name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>. <article-title>Coding for spatial goals in the prelimbic/infralimbic area of the rat frontal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>: <fpage>4602</fpage>–<lpage>4607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0407332102" xlink:type="simple">10.1073/pnas.0407332102</ext-link></comment> <object-id pub-id-type="pmid">15761059</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Valentin</surname> <given-names>VV</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Determining the Neural Substrates of Goal-Directed Learning in the Human Brain</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>4019</fpage>–<lpage>4026</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0564-07.2007" xlink:type="simple">10.1523/JNEUROSCI.0564-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17428979</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stott</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>A functional difference in information processing between orbitofrontal cortex and ventral striatum during decision-making behaviour</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130472</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0472" xlink:type="simple">10.1098/rstb.2013.0472</ext-link></comment> <object-id pub-id-type="pmid">25267815</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terada</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fujisawa</surname> <given-names>S</given-names></name>. <article-title>Temporal and Rate Coding for Discrete Event Sequences in the Hippocampus</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>94</volume>: <fpage>1248</fpage>–<lpage>1262.e4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.05.024" xlink:type="simple">10.1016/j.neuron.2017.05.024</ext-link></comment> <object-id pub-id-type="pmid">28602691</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>. <article-title>Speed/accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>: <fpage>e1002055</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002055" xlink:type="simple">10.1371/journal.pcbi.1002055</ext-link></comment> <object-id pub-id-type="pmid">21637741</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Hippocampal theta sequences</article-title>. <source>Hippocampus</source>. <year>2007</year>;<volume>17</volume>: <fpage>1093</fpage>–<lpage>1099</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20345" xlink:type="simple">10.1002/hipo.20345</ext-link></comment> <object-id pub-id-type="pmid">17663452</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning</article-title>. <source>Hippocampus</source>. <year>2015</year>;<volume>25</volume>: <fpage>1073</fpage>–<lpage>1188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.22488" xlink:type="simple">10.1002/hipo.22488</ext-link></comment> <object-id pub-id-type="pmid">26135716</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ambrose</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>. <article-title>Reverse Replay of Hippocampal Place Cells Is Uniquely Modulated by Changing Reward</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>91</volume>: <fpage>1124</fpage>–<lpage>1136</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.07.047" xlink:type="simple">10.1016/j.neuron.2016.07.047</ext-link></comment> <object-id pub-id-type="pmid">27568518</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalenscher</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>. <article-title>Is a bird in the hand worth two in the future? The neuroeconomics of intertemporal decision-making</article-title>. <source>Prog Neurobiol</source>. <year>2008</year>;<volume>84</volume>: <fpage>284</fpage>–<lpage>315</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2007.11.004" xlink:type="simple">10.1016/j.pneurobio.2007.11.004</ext-link></comment> <object-id pub-id-type="pmid">18207301</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title>. <source>Nature</source>. <year>2006</year>;<volume>440</volume>: <fpage>680</fpage>–<lpage>683</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04587" xlink:type="simple">10.1038/nature04587</ext-link></comment> <object-id pub-id-type="pmid">16474382</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mattar</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Prioritized memory access explains planning and hippocampal replay</article-title>. <source>bioRxiv</source>. <year>2017</year>; <fpage>225664</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/225664" xlink:type="simple">10.1101/225664</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006316.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donnarumma</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Costantini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ambrosini</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>. <article-title>Action perception as hypothesis testing</article-title>. <source>Cortex</source>. <year>2017</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2017.01.016" xlink:type="simple">10.1016/j.cortex.2017.01.016</ext-link></comment> <object-id pub-id-type="pmid">28226255</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collin</surname> <given-names>SHP</given-names></name>, <name name-style="western"><surname>Milivojevic</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Doeller</surname> <given-names>CF</given-names></name>. <article-title>Memory hierarchies map onto the hippocampal long axis in humans</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>1562</fpage>–<lpage>1564</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4138" xlink:type="simple">10.1038/nn.4138</ext-link></comment> <object-id pub-id-type="pmid">26479587</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strange</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Witter</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Lein</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>. <article-title>Functional organization of the hippocampal longitudinal axis</article-title>. <source>Nat Rev Neurosci</source>. <year>2014</year>;<volume>15</volume>: <fpage>655</fpage>–<lpage>669</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3785" xlink:type="simple">10.1038/nrn3785</ext-link></comment> <object-id pub-id-type="pmid">25234264</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kjelstrup</surname> <given-names>KB</given-names></name>, <name name-style="western"><surname>Solstad</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Brun</surname> <given-names>VH</given-names></name>, <name name-style="western"><surname>Hafting</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Leutgeb</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Witter</surname> <given-names>MP</given-names></name>, <etal>et al</etal>. <article-title>Finite scale of spatial representation in the hippocampus</article-title>. <source>Science</source>. <year>2008</year>;<volume>321</volume>: <fpage>140</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1157086" xlink:type="simple">10.1126/science.1157086</ext-link></comment> <object-id pub-id-type="pmid">18599792</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fuhs</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Touretzky</surname> <given-names>DS</given-names></name>. <article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>4266</fpage>–<lpage>4276</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4353-05.2006" xlink:type="simple">10.1523/JNEUROSCI.4353-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16624947</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Battaglia</surname> <given-names>FP</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>M-B</given-names></name>. <article-title>Path integration and the neural basis of the “cognitive map.”</article-title> <source>Nat Rev Neurosci</source>. <year>2006</year>;<volume>7</volume>: <fpage>663</fpage>–<lpage>678</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1932" xlink:type="simple">10.1038/nrn1932</ext-link></comment> <object-id pub-id-type="pmid">16858394</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burgess</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Barry</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>O’Keefe</surname> <given-names>J</given-names></name>. <article-title>An oscillatory interference model of grid cell firing</article-title>. <source>Hippocampus</source>. <year>2007</year>;<volume>17</volume>: <fpage>801</fpage>–<lpage>812</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20327" xlink:type="simple">10.1002/hipo.20327</ext-link></comment> <object-id pub-id-type="pmid">17598147</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muller</surname> <given-names>RU</given-names></name>, <name name-style="western"><surname>Ranck</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Taube</surname> <given-names>JS</given-names></name>. <article-title>Head direction cells: properties and functional significance</article-title>. <source>Curr Opin Neurobiol</source>. <year>1996</year>;<volume>6</volume>: <fpage>196</fpage>–<lpage>206</lpage>. <object-id pub-id-type="pmid">8725961</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harland</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Grieves</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Bett</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stentiford</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Dudchenko</surname> <given-names>PA</given-names></name>. <article-title>Lesions of the Head Direction Cell System Increase Hippocampal Place Field Repetition</article-title>. <source>Curr Biol</source>. <year>2017</year>;<volume>27</volume>: <fpage>2706</fpage>–<lpage>2712.e2</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2017.07.071" xlink:type="simple">10.1016/j.cub.2017.07.071</ext-link></comment> <object-id pub-id-type="pmid">28867207</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giocomo</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>M-B</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>. <article-title>Computational Models of Grid Cells</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>71</volume>: <fpage>589</fpage>–<lpage>603</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.07.023" xlink:type="simple">10.1016/j.neuron.2011.07.023</ext-link></comment> <object-id pub-id-type="pmid">21867877</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solstad</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>, <name name-style="western"><surname>Einevoll</surname> <given-names>GT</given-names></name>. <article-title>From grid cells to place cells: a mathematical model</article-title>. <source>Hippocampus</source>. <year>2006</year>;<volume>16</volume>: <fpage>1026</fpage>–<lpage>1031</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20244" xlink:type="simple">10.1002/hipo.20244</ext-link></comment> <object-id pub-id-type="pmid">17094145</object-id></mixed-citation></ref>
<ref id="pcbi.1006316.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>. <article-title>A tutorial on Bayesian nonparametric models</article-title>. <source>J Math Psychol</source>. <year>2012</year>;<volume>56</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref101"><label>101</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <chapter-title>Pattern Recognition and Machine Learning</chapter-title>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1006316.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanborn</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Navarro</surname> <given-names>DJ</given-names></name>. <article-title>Rational approximations to rational models: alternative algorithms for category learning</article-title>. <source>Psychol Rev</source>. <year>2010</year>;<volume>117</volume>: <fpage>1144</fpage>–<lpage>1167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0020511" xlink:type="simple">10.1037/a0020511</ext-link></comment> <object-id pub-id-type="pmid">21038975</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>