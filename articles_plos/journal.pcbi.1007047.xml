<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007047</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00636</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject><subj-group><subject>Skewness</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical noise</subject><subj-group><subject>Gaussian noise</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Human representation of multimodal distributions as clusters of samples</article-title>
<alt-title alt-title-type="running-head">Clusters-of-samples representation of probability distributions</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Sun</surname>
<given-names>Jingwei</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3941-2622</contrib-id>
<name name-style="western">
<surname>Li</surname>
<given-names>Jian</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9771-0660</contrib-id>
<name name-style="western">
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Psychological and Cognitive Sciences and Beijing Key Laboratory of Behavior and Mental Health, Peking University, Beijing, China</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>PKU-IDG/McGovern Institute for Brain Research, Peking University, Beijing, China</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Peking-Tsinghua Center for Life Sciences, Peking University, Beijing, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">leekin@gmail.com</email> (JL); <email xlink:type="simple">hang.zhang@pku.edu.cn</email> (HZ)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>14</day>
<month>5</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>5</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>5</issue>
<elocation-id>e1007047</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>4</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Sun et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007047"/>
<abstract>
<p>Behavioral and neuroimaging evidence shows that human decisions are sensitive to the statistical regularities (mean, variance, skewness, etc.) of reward distributions. However, it is unclear what representations human observers form to approximate reward distributions, or probability distributions in general. When the possible values of a probability distribution are numerous, it is cognitively costly and perhaps unrealistic to maintain in mind the probability of each possible value. Here we propose a Clusters of Samples (CoS) representation model: The samples of the to-be-represented distribution are classified into a small number of clusters and only the centroids and relative weights of the clusters are retained for future use. We tested the behavioral relevance of CoS in four experiments. On each trial, human subjects reported the mean and mode of a sequentially presented multimodal distribution of spatial positions or orientations. By varying the global and local features of the distributions, we observed systematic errors in the reported mean and mode. We found that our CoS representation of probability distributions outperformed alternative models in accounting for subjects’ response patterns. The ostensible influence of positive/negative skewness on the over/under estimation of the reported mean, analogous to the “skewness preference” phenomenon in decisions, could be well explained by models based on CoS.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Life is full of uncertainties: An action may yield multiple possible consequences and a percept may imply multiple possible causes. To survive, humans and animals must compensate for the uncertainty in the environment and in their own perceptual and motor systems. However, how humans represent probability distributions to fulfill probabilistic computations for perception and action remains elusive. The number of possible values in a distribution is vast and grows exponentially with the dimension of the distribution. It would be costly, if not impossible, to maintain the probability of each possible value. Here we propose a sparse representation of probability distributions, which can reduce an arbitrary distribution to a small set of coefficients while still keeping important global and local features of the original distribution. Our experiments provide preliminary evidence for the use of such representations in human cognition.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31571117</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9771-0660</contrib-id>
<name name-style="western">
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31421003</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3941-2622</contrib-id>
<name name-style="western">
<surname>Li</surname>
<given-names>Jian</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002855</institution-id>
<institution>Ministry of Science and Technology of the People's Republic of China</institution>
</institution-wrap>
</funding-source>
<award-id>2015CB559200</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3941-2622</contrib-id>
<name name-style="western">
<surname>Li</surname>
<given-names>Jian</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31871101</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9771-0660</contrib-id>
<name name-style="western">
<surname>Zhang</surname>
<given-names>Hang</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
<institution>National Natural Science Foundation of China</institution>
</institution-wrap>
</funding-source>
<award-id>31871140</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3941-2622</contrib-id>
<name name-style="western">
<surname>Li</surname>
<given-names>Jian</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>HZ was supported by National Natural Science Foundation of China (<ext-link ext-link-type="uri" xlink:href="http://www.nsfc.gov.cn/" xlink:type="simple">http://www.nsfc.gov.cn/</ext-link>) grants 31571117 and 31871101, and funding from Peking-Tsinghua Center for Life Sciences (<ext-link ext-link-type="uri" xlink:href="http://cls.pku.edu.cn/" xlink:type="simple">http://cls.pku.edu.cn/</ext-link>). JL was supported by National Natural Science Foundation of China grants 31421003 and 31871140 and Ministry of Science and Technology of the People’s Republic of China (<ext-link ext-link-type="uri" xlink:href="http://www.most.gov.cn/" xlink:type="simple">http://www.most.gov.cn/</ext-link>) grant 2015CB559200. Part of the analysis was performed on the High Performance Computing Platform of the Center for Life Sciences at Peking University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-24</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available for download at <ext-link ext-link-type="uri" xlink:href="https://osf.io/6mxzw/" xlink:type="simple">https://osf.io/6mxzw/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>As Horace Barlow wrote, “the brain has to decide upon actions in a competitive, chance-driven world, and to do this well it must know about and exploit the non-random <italic>probabilities</italic> and interdependences of objects and events” [<xref ref-type="bibr" rid="pcbi.1007047.ref001">1</xref>]. In general, the probabilistic information the cognitive system needs to deal with lies in the form of probability distributions of varying kinds: distributions of sensory stimuli in the natural environment [<xref ref-type="bibr" rid="pcbi.1007047.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref004">4</xref>], distributions of sensorimotor errors for motor actions [<xref ref-type="bibr" rid="pcbi.1007047.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref007">7</xref>], and distributions of rewards and penalties for alternative choices [<xref ref-type="bibr" rid="pcbi.1007047.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref009">9</xref>]. There is evidence for close-to-optimal probabilistic inference in human perception [<xref ref-type="bibr" rid="pcbi.1007047.ref010">10</xref>], cognition [<xref ref-type="bibr" rid="pcbi.1007047.ref011">11</xref>], and motor control [<xref ref-type="bibr" rid="pcbi.1007047.ref006">6</xref>], suggesting that the cognitive system is capable of coding probability distributions to satisfactory precision. Given that an arbitrary distribution can have myriad possible values and render an exact representation unaffordable, what approximations may be used in human representation of probability distributions?</p>
<p>Three general approaches have been proposed for the internal coding of probability distributions [<xref ref-type="bibr" rid="pcbi.1007047.ref012">12</xref>]. The first is to represent the event probabilities separately [<xref ref-type="bibr" rid="pcbi.1007047.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref014">14</xref>]. However, such coding schemes would be practically impossible for continuous probability distributions where the number of potential events is infinite, unless additional discretization procedures are assumed. The second approach is sampling; that is, to represent the values of a set of samples from the underlying distribution [<xref ref-type="bibr" rid="pcbi.1007047.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref017">17</xref>]. Probabilistic inference or decision making, therefore, could be based upon samples harnessed from the underlying distribution [<xref ref-type="bibr" rid="pcbi.1007047.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref021">21</xref>], analogous to Monte Carlo methods. Indeed, there are circumstances where people seem to base their judgment or decision on a few [<xref ref-type="bibr" rid="pcbi.1007047.ref022">22</xref>] or even one [<xref ref-type="bibr" rid="pcbi.1007047.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref024">24</xref>] random sample taken from the distribution. Third, any probability distribution, in the form of probability density or probability mass functions, may be approximated by the linear combination of a set of basis functions [<xref ref-type="bibr" rid="pcbi.1007047.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref029">29</xref>], much like the fact that time series can be decomposed into the sum of sine and cosine functions in Fourier analysis. The idea of basis function representations is appealing, because it reduces the whole distribution into a set of coefficients and therefore alleviates the cognitive load subjects would have otherwise undertaken, given that the forms of the basis distributions are known [<xref ref-type="bibr" rid="pcbi.1007047.ref025">25</xref>]. Recently, Zhang, Daw, and Maloney [<xref ref-type="bibr" rid="pcbi.1007047.ref030">30</xref>] have provided preliminary behavioral evidence that people might represent their own sensorimotor error distributions with a small number of basis distributions.</p>
<p>A fourth approach, which has not been explicitly proposed but has been the foundation of statistical decision theory, is to encode the moments of probability distributions, such as mean (first), variance (second), skewness (third central moment), and so on [<xref ref-type="bibr" rid="pcbi.1007047.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref033">33</xref>]. Mathematically, the whole sequence of moments of a specific distribution contains all the information of the distribution. There have been a number of empirical studies of economic decision-making where efforts have been made to map brain regions dedicated to the calculation of the first three moments [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>], with the implicit assumption that different moments might be separately processed by different brain structures.</p>
<p>In the present study, we explore the basis function hypothesis where a probability distribution is represented as a set of coefficients of particular basis functions. What is often emphasized in previous theoretical work [<xref ref-type="bibr" rid="pcbi.1007047.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref028">28</xref>] is the flexibility of this approach. In theory, any probability distribution can be well approximated as long as enough basis distributions are used. Humans in practice, however, may not be able to afford a large number of basis distributions, and the coefficients they extract from the empirical distribution are error-prone. There is increasing evidence that human representations of prior distributions can deviate significantly from the empirical distribution [<xref ref-type="bibr" rid="pcbi.1007047.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref039">39</xref>] and such deviations prove to be an important source of suboptimality in human probabilistic inference [<xref ref-type="bibr" rid="pcbi.1007047.ref040">40</xref>]. If humans do not necessarily have a lossless representation of the encoded distribution, a natural question arises: What information is extracted from the empirical distribution? Here we propose the following representation of probability distributions (<xref ref-type="fig" rid="pcbi.1007047.g001">Fig 1</xref>). After a stochastic clustering process, samples from the distribution are classified into a small number of clusters and the centroids and relative weights of the clusters—<inline-formula id="pcbi.1007047.e001"><alternatives><graphic id="pcbi.1007047.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>—are maintained for future use. We call it Clusters-of-Samples (CoS) representation for which, as in the sampling approach, probabilistic information initially comes from samples and, as in the basis function approach, only a finite set of coefficients needs to be estimated and maintained to approximate the encoded distribution.</p>
<fig id="pcbi.1007047.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Scheme of the Clusters-of-Samples (CoS) representation.</title>
<p>The samples from the empirical distribution are partitioned into a few clusters after a stochastic clustering process and the centroids and relative weights of the clusters are internally maintained for future use (“Representation”). When asked to report the Mode and Mean of the distribution, subjects would report the centroid of the largest cluster as the Mode estimate, and a weighted average of all clusters (whose weights may be subject to additional transformations) as the Mean estimate.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g001" xlink:type="simple"/>
</fig>
<p>An idea akin to our proposal was Shelton et al.’s [<xref ref-type="bibr" rid="pcbi.1007047.ref041">41</xref>] select-and-sample approach, which assumes that a specific probability distribution is coded by samples but only a small set of pre-selected high-density areas of the distribution may be sampled. Similar to the CoS representation, the select-and-sample approach would reduce an arbitrary distribution to a small number of high-density centers. Shelton et al. [<xref ref-type="bibr" rid="pcbi.1007047.ref041">41</xref>] proved theoretically that such centers could efficiently approximate multimodal distributions in high-dimensional spaces while retaining the correlations between dimensions. Whereas in our hypothesis, by representing an arbitrary distribution with a set of cluster centers, we are essentially using the clusters as the basis functions to discretize the distribution and obtaining a mixture of Dirac delta functions. The CoS representation can also be extended to the usage of uniform or Gaussian distributions as the bases, with additional coefficients for the spread of the basis distributions.</p>
<p>On the one hand, even with just a small number of clusters, a CoS representation reflects the overall shape of the encoded distribution and is capable of capturing multiple modes if the encoded distribution is multimodal. On the other hand, the CoS representation is prone to information loss. With groups of samples summarized by the cluster centers, information about individual samples in each cluster and thus local details about the distribution are lost. Moreover, the stochasticity in the clustering process may cause variations in the weights as well as in the centroids of the clusters. Due to its characteristic lossy coding, CoS can lead to systematic errors in certain tasks, which would allow us to test CoS against candidate representations that predict no such errors or different patterns of errors. Following the reasoning above, a possible testbed for CoS would be processing multimodal distributions. The main goal of our study is to test how humans cope with the structure of multimodal distributions.</p>
<p>In Marr’s [<xref ref-type="bibr" rid="pcbi.1007047.ref042">42</xref>] term, our proposal of the CoS representation resides on the computational level, concerning what statistics for a probability distribution are internally maintained. How CoS is implemented algorithmically or biologically, however, is a separate question. In order to make our arguments concrete and testable, we specified certain computational procedures about the stochastic clustering process. In particular, we implemented the stochastic clustering process as a distance-dependent Chinese Restaurant Process (ddCRP) [<xref ref-type="bibr" rid="pcbi.1007047.ref043">43</xref>]. It has the desirable property that the number of clusters does not have to be specified in advance and is instead determined by a self-adaptive probabilistic process (see <xref ref-type="sec" rid="sec014">Methods</xref>). It should also be noted that not all the alternative representations we reviewed earlier are incompatible with CoS on the computational level. For example, a sampling-based representation following the select-and-sample approach [<xref ref-type="bibr" rid="pcbi.1007047.ref041">41</xref>] may have similar behavioral consequences as CoS.</p>
<p>In a series of behavioral experiments, we asked human subjects to report the summary statistics of visually presented distributions. On each trial (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2A</xref>), 70 vertical lines, whose horizontal coordinates were samples randomly drawn from a specific underlying probability distribution, were briefly and sequentially presented along the middle axis of the computer screen. Subjects’ task was to move a mouse pointer to locate (1) the <italic>Mean</italic> and (2) the <italic>Mode</italic> (location of the highest density) of the observed distribution of spatial positions. Subjects were not required to memorize the spatial positions of individual vertical lines but rather to report the ensemble statistics of spatial positions [<xref ref-type="bibr" rid="pcbi.1007047.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref048">48</xref>] (see [<xref ref-type="bibr" rid="pcbi.1007047.ref049">49</xref>] for a review of ensemble perception). The Mean and Mode estimation tasks were specifically chosen to test whether subjects’ representations captured both the global features and local details of the empirical distribution. The underlying distributions were generated as the weighted mixtures of multiple evenly-spaced beta-like distributions (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2B–2D</xref>). By varying the relative weight of different beta components, for example by assigning more weights to the left or to the right, we were able to manipulate the global distribution to be more positively or more negatively skewed. In contrast, by varying the shape of individual beta components, we modified the local asymmetry of the distribution. Subjects’ Mean and Mode estimates, therefore, afford a unique opportunity to test different representation hypotheses.</p>
<fig id="pcbi.1007047.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Task and design of the experiments.</title>
<p>(A) Time course of one trial. Following a fixation cross, subjects saw 70 red vertical lines sequentially presented on a white horizontal axis. Their task was to move a blue vertical line to indicate their estimate of the Mean (the average horizontal position of the red lines) and then a blue box to indicate the Mode (the area that would catch the largest number of red lines). (B) An example of a 3-beta mix (i.e. mixture of three beta distributions) underlying distribution used in Experiments 1, 3 and S1. This example consists of three negatively skewed beta components (“LocalNeg”), with the weights of the three components from left to right being 0.2, 0.3 and 0.5 (“weights = 235”). (C) An example of a 4-beta mix (i.e. mixture of four beta distributions) underlying distribution used in Experiments 2 and 3. This example consists of four positively skewed beta components (“LocalPos”), with the weights of the four components being 0.1, 0.2, 0.4 and 0.3 (“weights = 1243”). (D) All 3-beta mix underlying distributions used in Experiments 1, 3 and S1. “Pos”, “Sym”, and “Neg” are abbreviations respectively for positive, symmetric, and negative. Local skewness (LocalPos, LocalSym, and LocalNeg) refers to the skewness of the beta components and is controlled by the shape condition. Global skewness (GlobalPos, GlobalSym, and GlobalNeg) refers to the skewness of the whole distribution and is controlled by the weight condition.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g002" xlink:type="simple"/>
</fig>
<p>Had subjects represented the distribution exactly as it was observed, their Mean and Mode reports would be unbiased estimators about the true values of the Mean and Mode. However, in all four experiments that we tested, systematic deviations between subjects’ estimates and the ground truths of the empirical distributions were detected. We constructed computational models based on the CoS representation and alternative representations and compared different models’ performance in quantitatively predicting subjects’ Mean and Mode estimates. The CoS models outperformed the alternative models for both estimates.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Systematic errors in subjects’ estimates</title>
<p>In Experiment 1, the positions of the 70 vertical lines for each trial were randomly drawn from a mixture of three beta-like distributions that adjoined each other. We call this underlying distribution “3-beta mix”, for which both the shape (identical for all the components in the same distribution) and weights of the beta components were varied across trials (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2D</xref>). The shape of the beta components could be negatively skewed, symmetric, or positively skewed (see <xref ref-type="sec" rid="sec014">Methods</xref> for details). The weights for the three components, from left to right, could be (0.2, 0.3, 0.5), (0.3, 0.2, 0.5), (0.2, 0.5, 0.3), (0.3, 0.5, 0.2), (0.5, 0.2, 0.3), or (0.5, 0.3, 0.2). In what follows, “local skewness” refers to the skewness (shape) of the individual beta components in the 3-beta mix. We refer the skewness of the whole 3-beta mix distribution as “global skewness” here to differentiate from “local skewness”, which relies mainly on the weights of the beta components. Thus the effects of shape and weight conditions correspond to the local and global skewness effects, respectively. In the experiment, each of the 18 combinations of shape and weight conditions was repeated for 9 times, resulting in 162 trials.</p>
<p>“True mode” and “true mean” refer to the statistics computed from the empirical distribution (i.e. the 70 samples, see <xref ref-type="sec" rid="sec014">Methods</xref>). All 16 subjects’ Mode and Mean estimates were positively correlated with the true mode and mean (Pearson’s correlation, all <italic>p</italic> &lt; 0.001). Besides, subjects’ Mode estimate was closer to the true mode than to the true Mean (<italic>t</italic>(15) = -7.78, <italic>p</italic> &lt; 0.001), and their Mean estimate was closer to the true Mean than to the true Mode (<italic>t</italic>(15) = -26.36, <italic>p</italic> &lt; 0.001), indicating that subjects did report the two statistics as instructed instead of using the same estimates for the two tasks.</p>
<p>Meanwhile, the deviations of subjects’ Mode and Mean estimates from the ground truth varied systematically with the shape and weight conditions (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3A and 3C</xref>). For subjects’ errors in Mode estimates, a two-way (3 shapes × 6 weights) repeated-measures ANOVA showed significant main effects of shape (<italic>F</italic>(2, 150) = 71.52, <italic>p</italic> &lt; 0.001) and weight conditions (<italic>F</italic>(5, 150) = 199.96, <italic>p</italic> &lt; 0.001) as well as their interaction (<italic>F</italic>(10, 150) = 3.25, <italic>p</italic> = 0.001). Further post-hoc comparisons indicated that the three shape (local skewness) levels differed from each other (all <italic>p</italic> &lt; 0.001, Bonferroni corrected for three comparisons; <xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3B</xref>). A similar ANOVA on subjects’ errors in Mean estimates showed that the main effect of the weight condition (<italic>F</italic>(5, 150) = 39.13, <italic>p</italic> &lt; 0.001) was significant, and no other effects reached the 0.05 significance level.</p>
<fig id="pcbi.1007047.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Systematic errors of subjects’ estimates in Experiment 1.</title>
<p>(A, C) Subjects’ errors in Mode (A) and Mean (C) estimates varied with the weight condition (abscissa) and the shape condition (different colors). Shaded areas denote 1 SE. See the legend of <xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2</xref> or the text for the definition of global and local skewness. Different weight conditions are arranged from left to right in increasing global skewness. Note that subjects’ errors in Mode estimates changed non-monotonically with global skewness. See text for intuitions how this non-monotonicity can arise from the CoS representation. (B) The main effect of the shape condition (local skewness) on subjects’ errors in Mode estimates. ***: <italic>p</italic> &lt; 0.001 for Bonferroni corrected post-hoc comparisons. Error bars denote 1 SE.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g003" xlink:type="simple"/>
</fig>
<p>Three patterns emerged from the behavioral data. First, the Mode was overestimated for positive compared to negative local skewness (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3A and 3B</xref>, differences between the three colors). That is, though on average still falling within the beta component where the true mode resided, subjects’ Mode estimate was biased towards the center of the beta component. Second, the Mode was overestimated for positive and underestimated for negative global skewness (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3A</xref>, the ascending lines, but note the exceptions at 253 and 352). In other words, the Mode estimate was also biased towards the mean of the empirical distribution.</p>
<p>Third, the Mean was overestimated for positive and underestimated for negative global skewness (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3C</xref>, the ascending lines). When choosing among probability distributions of rewards, besides the well-known preference for higher mean (expected gain) and lower variance [<xref ref-type="bibr" rid="pcbi.1007047.ref050">50</xref>], it has been suggested that people tend to prefer positively skewed over symmetric, and symmetric over negatively skewed distributions. This phenomenon is known as “skewness preference” in economic decisions [<xref ref-type="bibr" rid="pcbi.1007047.ref051">51</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref055">55</xref>], which was considered to be associated with activities in dedicated brain structures devoted to the processing of skewness [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref057">57</xref>]. Our finding that the reported Mean was positively associated with distribution skewness, other things being equal, echoed previous literature on skewness preference and raised the possibility that skewness preference might be due to the mis-estimation of the mean of skewed reward distributions.</p>
<p>If subjects had an accurate representation of the empirical distribution and computed the required statistics properly, their Mode and Mean estimates would not systematically deviate from the true mode and mean. What representation of probability distributions could best account for the error patterns described above? Based on the different representations introduced earlier, we constructed a variety of models for the estimations of the Mode and Mean, and compared different models’ performance in explaining the data.</p>
</sec>
<sec id="sec004">
<title>Mode estimates: clusters-of-samples versus alternative models</title>
<p>In the CoS representation of a specific distribution—<inline-formula id="pcbi.1007047.e002"><alternatives><graphic id="pcbi.1007047.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, detailed information about the individual samples that constitute each cluster has been lost. As a result, the mode of the distribution cannot be exactly recovered from its CoS representation. In the CoS model for Mode estimates (see <xref ref-type="sec" rid="sec014">Methods</xref>), we assume that the subject simply reports the centroid of the cluster that is of the highest weight as the Mode estimate.</p>
<p>Intuitively, the CoS model can produce the observed two error patterns of Mode estimates (<xref ref-type="fig" rid="pcbi.1007047.g004">Fig 4</xref>): On the one hand, because the CoS representation is ignorant of the individual samples of each cluster and only identifies their means, the CoS model would predict that the mean—instead of the mode—of the largest beta component mainly drives subjects’ Mode estimates. On the other hand, due to the stochasticity of the clustering process, the largest cluster in the CoS representation occasionally does not correspond to the largest beta component. Thus, on average, the Mode estimate would deviate from the mean of the largest beta component towards the mean of the whole distribution. In addition, the occasional mismatch of the largest cluster in the CoS representation with the largest beta component in the underlying distribution gives rise to two more specific predictions for Mode estimates, both of which are supported by our data. First, it predicts that subjects’ Mode estimates across trials would be multimodally distributed, with the major peak centered at the largest beta component and a minor peak at the second largest beta component (see <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5C</xref> for our further specifications of the data and model predictions). Second, it can naturally predict the observed non-monotonic increase of errors with global skewness (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3A</xref>): Given that the 253 weight condition has its second largest beta component on the right of its largest beta component and the 352 condition has the reverse arrangement, the former is likely to incur a more positive error than the latter, though the former is associated with negative global skewness and the latter with positive global skewness.</p>
<fig id="pcbi.1007047.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Intuition of how CoS may explain the observed error patterns in Mode estimates.</title>
<p>Black or gray curves denote the empirical distribution. Yellow rectangles denote the clusters formed for the CoS representation. Orange dotted lines denote the means of the clusters. The red brown vertical line with circle arrowhead indicates the true Mode. The red brown arrow indicates the influence of local skewness. Given that subjects would report the mean of the largest cluster as the Mode estimate, the mode estimate would be overestimated for locally positively skewed distributions (“LocalPos”) and underestimated for the reverse (“LocalNeg”). Blue arrows indicate the influence of global skewness. Due to the stochasticity of the clustering process, occasionally the largest cluster in the CoS representation does not correspond to the largest beta component, leading to an overestimation of mode for globally positively skewed distributions (“GlobalPos”) and underestimation in the reverse case (“GlobalNeg”).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1007047.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Modeling results of Experiment 1.</title>
<p>(A, B) Summed ΔAICc (lower is better) for Mode estimates (A) and Mean estimates (B). GT: ground-truth model; IO: ideal observer model; MoMe: mode+mean model; MeSk: mean+skewness model; MoSk: mode+skewness model; MoMeSk: mode+mean+skewness model; CoS: clusters-of-samples model; SW: subjective weighting model. (C) Joint distributions of Mode and Mean estimates collapsed across subjects for data, the ideal observer models, the moment-based second-best models (mode+mean+skewness for Mode and mean+skewness for Mean) and the CoS models. (D) Relative frequency of different cluster sizes (number of samples per cluster) estimated for subjects’ CoS representations. For each subject, the relative frequency of each cluster size was averaged across trials and possible CoS representations on each trial. The statistics were then averaged across subjects. Error bars denote 1 SE. Overall, the clustering process had a tendency to generate clusters of small sizes. The frequencies for cluster sizes of 14, 21, and 35—which correspond to the three relative weights used in 3-beta mix—were much higher than those of their neighbors, indicating that the clustering process could partly recover the multimodal structure of the empirical distribution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g005" xlink:type="simple"/>
</fig>
<p>We considered several alternative models for Mode estimates. One model is the ground-truth model, where the Mode estimate is assumed to be the same as the true mode. Apparently the ground-truth model cannot explain any systematic biases from the true mode and therefore only serves as a baseline for the other models.</p>
<p>The second model is a Bayesian ideal observer model (see <xref ref-type="sec" rid="sec014">Methods</xref>), in the consideration that even an ideal observer may not be able to build an accurate representation of the empirical distribution from the available samples and thus may show certain biases. Following Orhan and Jacobs’ [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>] modeling of working memory, we used a Dirichlet Process Mixture Model (DPMM) as the generative model assumed by the ideal observer. That is, the ideal observer assumes a “bumpy” world: Each observed sample descends from a specific cluster and its value is generated from the Gaussian distribution associated with the cluster. The number of clusters and the number of samples in each cluster are assumed to follow a Dirichlet random process. DPMMs are commonly used to model cognitive processes [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref060">60</xref>], which have the desirable property that the number of clusters need not to be pre-specified but can be estimated from the data. By estimating the parameters of such a generative model from the observed samples, the ideal observer can approximate the empirical distribution with a Gaussian mixture distribution, whose mode would be reported as the Mode estimate. We found that the Gaussian mixture distribution obtained by the ideal observer closely matches the empirical distribution, even for beta mixtures that have skewed beta components (<xref ref-type="supplementary-material" rid="pcbi.1007047.s005">S4 Fig</xref>). That is, the behavior responses of the Bayesian ideal observer model would be almost equivalent to those of the ground-truth model and not show systematic errors.</p>
<p>The other alternative models are based on a moment-based representation of probability distributions, all of which can qualitatively reproduce, at least part of, the observed error patterns in Mode estimates. The moment representation does not necessarily suggest any biases: In theory, the mode of a distribution can be recovered from the set of moments of the distribution. However, if subjects only represented the first a few moments and used them to estimate the mode, or if subjects had an unbiased internal estimate of the true mode but their responses were biased by certain task-irrelevant moments, their responses might show systematic errors. To test these possibilities, we constructed a series of moment representation models where the Mode estimate is a weighted average of the true mode and moments of the distribution plus random noises. In particular, the models were mean+skewness, mode+mean, mode+skewness, and mode+mean+skewness. We did not include variance as a predictor in these models since all the distributions were generated with equal variance.</p>
<p>All models share a common assumption that the final Mode response undergoes an additional linear transformation and contains a Gaussian random noise, due to the imperfect mapping from perception to motor response. We fit each model to each subject’s Mode estimates using maximum likelihood estimates and computed the Akaike information criterion corrected for small sample-size (AICc) [<xref ref-type="bibr" rid="pcbi.1007047.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref062">62</xref>] as the metric of goodness-of-fit for model comparison. The ΔAICc of a specific model for a specific subject is defined as the difference between the AICc of the model and the lowest AICc for the subject. According to the summed ΔAICc across subjects (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5A</xref>), the CoS model was the best predictor of Mode estimates. A group-level Bayesian model selection [<xref ref-type="bibr" rid="pcbi.1007047.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref064">64</xref>] showed that the protected exceedance probability of the CoS model, i.e. the probability for the CoS model to outperform all the other models' predictions of Mode estimates, was close to 100%. The CoS model well predicted the systematic errors in subjects’ Mode estimates, including the non-monotonic increase of errors with global skewness and the effect of local skewness (<xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6A</xref>).</p>
<fig id="pcbi.1007047.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007047.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Model predictions and model lesion analysis for Experiment 1.</title>
<p>(A) Subjects’ errors in Mode and Mean estimates (left panels, the same as <xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3A &amp; 3C</xref>) versus the predictions of the CoS models (right panels). The CoS models could well predict subjects’ errors. Insets refer to the mean across the three local skewness conditions. (B) Summed ΔAICc of the CoS model for Mean estimates and its lesioned models. The lesioned models included models without Lateral Inhibition (“CoS w/o LI”), without Power Transformation (“CoS w/o PT”), without both the components (“CoS w/o LI&amp;PT”), and without clustering (“SW”, which is the subjective weighting model in <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5B</xref>). The unlesioned CoS model outperformed the lesioned models, implying that power transformation, lateral inhibition and the clustering process were all necessary for the CoS model to explain subjects’ Mean estimates. (C) Predictions of the four lesioned CoS models, which deviated from the data quantitatively or qualitatively. Given the models’ considerable differences in ΔAICc, their differences in the prediction plots might look counterintuitively small, which is partly due to the fact that these predictions are only about the mean trend of data whereas different models can also differ in their noise distributions. See text for the intuitions for these results.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Mean estimates: clusters-of-samples versus alternative models</title>
<p>On each trial, subjects reported both the mean and mode of the empirical distribution. It is reasonable to assume that the two estimates are based on a shared CoS representation. The exact CoS representation for a specific trial is not deterministic because the stochastic clustering process may end up with different clustering results and thus different CoS representations on different runs. However, subjects’ Mode estimates had allowed us to infer a probability distribution over different CoS representations for each trial (see <xref ref-type="sec" rid="sec014">Methods</xref>) and we used this information to model subjects’ Mean estimates. Given a specific CoS representation <inline-formula id="pcbi.1007047.e003"><alternatives><graphic id="pcbi.1007047.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, we assume that the Mean estimate is a weighted average of all <italic>c</italic><sub><italic>k</italic></sub>, where the subjective weight for <italic>c</italic><sub><italic>k</italic></sub> is a transformation of <italic>w</italic><sub><italic>k</italic></sub> that reflects probability distortion [<xref ref-type="bibr" rid="pcbi.1007047.ref065">65</xref>] and lateral inhibition [<xref ref-type="bibr" rid="pcbi.1007047.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref067">67</xref>].</p>
<p>We hypothesize that the systematic errors in subjects’ Mean estimates arise as a consequence of CoS representations followed by further transformations upon CoS. The CoS representation itself would not lead to any systematic errors in Mean estimates, because the relative weight of each sample in computing the mean is faithfully transferred to the cluster it is assigned to and thus effectively not altered by the clustering process. Probability distortion and lateral inhibition, as we specify later in a model lesion analysis, would cause overweighting or underweighting of different parts of the distribution and thus biases in Mean estimates. To exclude the possibility that the observed biases are solely induced by the additional transformations, we constructed an alternative model for Mean estimates with similar transformations but no clustering—the subjective weighting model (see <xref ref-type="sec" rid="sec014">Methods</xref>), and compared its performance with that of the CoS model (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5B</xref> &amp; <xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6B and 6C</xref>).</p>
<p>A Bayesian ideal observer model and several moment representation models for Mean estimates were defined in a similar way as their counterparts for Mode estimates (see <xref ref-type="sec" rid="sec014">Methods</xref>). Parallel to the models for Mode estimates, all models for Mean estimates include a linear transformation and Gaussian noise. The model fitting and comparison procedures were the same as those of Mode estimates.</p>
<p>The CoS model outperformed the other models for Mean estimates in the summed ΔAICc. According to a group-level Bayesian model selection, it had a 99.5% probability to excel all the other models (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5B</xref>). Note that the mean+skewness model—Mean estimate as a weighted average of the mean and skewness of the distribution—was among the models that were inferior to the CoS model, though it seems to provide a straightforward explanation for the “skewness preference” (<xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3C</xref>). Similarly, though assuming a lateral inhibition between samples can cause samples underweighted in dense areas and overweighted in sparse areas and thus explain the “skewness preference” (<xref ref-type="supplementary-material" rid="pcbi.1007047.s006">S5 Fig</xref>), the subjective weighting model without clustering fit worse to the data than the CoS model did.</p>
<p>The CoS model for Mean estimates (but not that for Mode estimates) includes additional transformations. In a model lesion analysis, we tested further how the additional transformations as well as the clustering process of the CoS model contributed to its performance in Mean estimates. When lateral inhibition, power transformation, both the transformations, or clustering were removed from the CoS model (the CoS model w/o clustering is equivalent to the subjective weighting model in <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5B</xref>), the resulting model fit worse to subjects’ Mean estimates than the CoS model did (<xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6B</xref>, also see <xref ref-type="supplementary-material" rid="pcbi.1007047.s008">S7 Fig</xref> for similar results of further experiments). Compared to its lesioned versions, CoS had a smaller summed ΔAICc and a protected exceedance probability of 95.6%. The CoS model well predicted subjects’ errors in Mean estimates (<xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6A</xref>), whereas the lesioned models failed quantitatively or qualitatively (<xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6C</xref>). The failure of the CoS model without both transformations (i.e. the CoS w/o both model) is straightforward since the CoS representation alone does not introduce biases into Mean estimates, as we mentioned earlier. The intuitions for the other two lesioned models are as follows. When there is no lateral inhibition (i.e. the CoS w/o LI model), power transformation leads to a re-distribution of cluster weights that depends only on the values of the weights but not on how the clustered are aligned. As we elaborate later, the major clusters in subjects’ CoS representations closely followed the beta components in the empirical distribution. Therefore, the weight re-distributions would be similar for the 235 and 325 weight conditions (weights moving from the rightmost cluster to the left two clusters) and for the 523 and 532 weight conditions (weights moving from the leftmost cluster to the right two clusters). As the result, the CoS w/o LI model predicted an <italic>S</italic>-shaped error pattern (left panel of <xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6C</xref>). In contrast, when there is lateral inhibition but no power transformation (i.e. the CoS w/o PT model), the re-distribution of cluster weights depends on the alignment of the clusters, for which 235 and 325, or 523 and 532 would be rather different. Consequently the CoS w/o PT model predicted a close-to-linear error pattern (central panel of <xref ref-type="fig" rid="pcbi.1007047.g006">Fig 6C</xref>). The error patterns predicted by the CoS model as well as that of the data lay between those of the two lesioned models.</p>
<p>To see whether subjects really, as we assumed, reported their Mode and Mean estimates based on a common CoS representation, we constructed an additional lesioned CoS model for Mean estimates that does not use the representational information inferred from the Mode estimate on the same trial. If two distinct CoS representations had been used for Mode and Mean estimates, the CoS representation inferred from the Mode estimate would be non-informative for predicting the Mean estimate and the lesioned model would perform equally well as the original model. Instead, we found that this lesioned model was inferior to the original CoS model in fitting subjects’ Mean estimates (<xref ref-type="supplementary-material" rid="pcbi.1007047.s007">S6 Fig</xref>), thus providing evidence for a shared CoS representation across the two estimation tasks.</p>
</sec>
<sec id="sec006">
<title>Multimodality and statistics of the clusters</title>
<p>Though more than one model can qualitatively predict the error patterns of Mode and Mean estimates in <xref ref-type="fig" rid="pcbi.1007047.g003">Fig 3</xref>, the CoS models outperformed the alternative models in predicting the full distributions of Mode and Mean estimates. <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5C</xref> shows the joint distributions of Mode and Mean estimates, collapsed across subjects and separately for the six weight conditions, compared between data and model predictions. In most of the data plots, the joint distributions appeared to be multimodal: We can see at least two separate peaks, one dominant and the other minor (see <xref ref-type="supplementary-material" rid="pcbi.1007047.s010">S9 Fig</xref> for statistical evidence). The major peak for each weight condition corresponds to the beta component of the largest weight, while the minor peak corresponds to the second-largest beta component. The CoS models predicted such multimodality, while the moment representation models predicted only unimodal distributions. The CoS representation could lead to the observed multimodality in Mode estimates because of the stochasticity inherent in its clustering process so that the same empirical distribution may be parsed into different partitions in different runs.</p>
<p>To illustrate how the clusters in subjects’ CoS representations can capture the statistics of the empirical distribution, we plot the relative frequencies of different cluster sizes (i.e. number of samples per cluster, see <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5D</xref>), which were averaged across trials and possible CoS representations on each trial. The frequencies for 14, 21, and 35 samples per cluster—which correspond to the relative weights of 0.2, 0.3, and 0.5 that were used in our stimuli—were much higher than those of their neighbors, indicating that the clustering process could partly recover the multimodal structure of the empirical distribution.</p>
<p>To test whether our findings in Experiment 1 can generalize to decision contexts other than spatial position judgments, we performed Experiment S1 (see <xref ref-type="sec" rid="sec014">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007047.s009">S8 Fig</xref>), which used the same design as Experiment 1 but replaced spatial positions with orientations—another widely used stimuli in ensemble perception [<xref ref-type="bibr" rid="pcbi.1007047.ref049">49</xref>]. The results of Experiment S1 (<xref ref-type="supplementary-material" rid="pcbi.1007047.s004">S3 Fig</xref>) replicated the major findings of Experiment 1, including the three error patterns in Mode and Mean estimates, the superiority of the CoS models over the alternative models, and the multimodal distributional structure captured in subjects’ CoS representations. We also noted a slight difference between the results of the orientation and the spatial position experiments: the CoS model for Mean estimates in Experiment S1 had a considerably lower protected exceedance probability compared to its counterpart in Experiment 1.</p>
<p>In Experiment 1, we found that subjects might be sensitive to the multimodality of the distribution and formed clusters accordingly. To further test this hypothesis, we conducted two new experiments, where the task structures were similar to Experiment 1 but the number of modes in the underlying distributions were varied. In Experiment 2, we increased the number of modes to four. In Experiment 3, we had separate mini-blocks where the numbers of modes were different (three or four).</p>
</sec>
<sec id="sec007">
<title>Experiment 2: 4-beta mix</title>
<p>Sixteen new subjects took part in Experiment 2, where the underlying distribution in each trial was a mixture of four beta-like distributions, abbreviated as 4-beta mix (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2C</xref>). As in Experiment 1, we found systematic deviations of the Mode and Mean estimates from the true mode and mean (<xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1 Fig</xref>): Both errors were significantly influenced by the weight condition, <italic>F</italic>(23, 644) = 53.27, <italic>p</italic> &lt; 0.001, and <italic>F</italic>(23, 644) = 13.58, <italic>p</italic> &lt; 0.001, according to repeated-measures ANOVAs. For Mode estimates, the main effect of the shape condition (<italic>F</italic>(2, 644) = 4.83, <italic>p</italic> = 0.008) and its interaction with the weight condition (<italic>F</italic>(46, 644) = 2.87, <italic>p</italic> &lt; 0.001) were also significant. Post-hoc comparisons (Bonferroni corrected for three comparisons) showed that the errors for locally-negatively-skewed distributions were significantly more negative than those of the locally-symmetric (<italic>t</italic>(14) = -3.684, <italic>p</italic> = 0.008) and locally-positively-skewed distributions (<italic>t</italic>(14) = -4.481, <italic>p</italic> &lt; 0.001), while the difference between the latter two conditions was insignificant (<italic>t</italic>(14) = 0.011, <italic>p</italic> = 1.000). There were no other significant effects for Mean estimates.</p>
<p>Again, the Bayesian ideal observer model was able to perfectly recover the underlying 4-beta mix distribution (<xref ref-type="supplementary-material" rid="pcbi.1007047.s005">S4B Fig</xref>), which could not explain the systematic errors in Experiment 2. Among the models developed for Experiment 1, the CoS models still performed the best in predicting subjects’ Mode and Mean estimates in Experiment 2, according to summed ΔAICc (<xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1 Fig</xref>). The probabilities for the CoS models to excel the alternative models (i.e. protected exceedance probability) were 100.0% and 93.6%, respectively for the Mode and Mean estimates.</p>
<p>Similar to Experiment 1, subjects’ CoS representations could capture the multimodal structure of the empirical distribution: Clusters of 7, 14, 21, and 28 samples, which correspond to the relative weights of 4-beta mix, stood out from the histogram (<xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1F Fig</xref>).</p>
</sec>
<sec id="sec008">
<title>Experiment 3: interleaved 3-beta and 4-beta mix</title>
<p>In Experiment 3, we tested whether our CoS model is flexible enough to adapt to a dynamic environment where 3-beta mix and 4-beta mix distributions were presented in alternating mini-blocks of 10–14 trials. As in Experiments 1 and 2, all subjects’ Mode and Mean estimates were significantly correlated with the true mode and mean (Pearson’s correlation, all <italic>p</italic> &lt; 0.001) but had systematic errors (<xref ref-type="supplementary-material" rid="pcbi.1007047.s003">S2 Fig</xref>), with the error patterns of the 3-beta and 4-beta trials respectively resembled those of Experiments 1 and 2. We performed 2 (shape conditions) × 2 (weight conditions) repeated-measures ANOVAs separately for 3-beta and 4-beta trials. For 3-beta trials, the main effects of the shape condition (<italic>F</italic>(2, 150) = 9.43, <italic>p</italic> &lt; 0.001) and the weight condition (<italic>F</italic>(5, 150) = 82.73, <italic>p</italic> &lt; 0.001) were significant for Mode estimates, while only the main effect of the weight condition was significant for Mean estimates (<italic>F</italic>(5, 150) = 22.78, <italic>p</italic> &lt; 0.001). For 4-beta trials, the main effects of the shape condition (<italic>F</italic>(2, 690) = 3.6, <italic>p</italic> = 0.028) and the weight condition (<italic>F</italic>(23, 690) = 32.48, <italic>p</italic> &lt; 0.001), and their interaction (<italic>F</italic>(46, 690) = 1.95, <italic>p</italic> &lt; 0.001) were significant for Mode estimates, while only the main effect of the weight condition (<italic>F</italic>(23, 690) = 4.36, <italic>p</italic> &lt; 0.001) was significant for Mean estimates.</p>
<p>The results of model comparisons replicated those of Experiments 1 and 2: The CoS models fit best to both the Mode and Mean estimates, according to summed ΔAICc (<xref ref-type="supplementary-material" rid="pcbi.1007047.s003">S2 Fig</xref>). On the group level, the probabilities for the CoS models to outperform the alternative models approached 100% and were 99.9%, respectively for the Mode and Mean estimates. The frequency statistics of cluster sizes of the 3-beta and 4-beta trials mimicked those of Experiments 1 and 2.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>We proposed clusters-of-samples (CoS) as an approximate representation of probability distributions and tested it against a variety of alternatives in four behavioral experiments. Human subjects were required to report the Mean and Mode of various multimodal probability distributions and systematic errors were observed in their estimates. We found that models based on the CoS representation accounted for subjects’ error patterns better than alternative models that implement more straightforward explanations based on moment representations.</p>
<sec id="sec010">
<title>The moment representation for probability distributions</title>
<p>The fact that probability distributions can be represented by their central moments—mean, variance, skewness, etc.—has gained increasing popularity in cognitive and decision neuroscience research. Though it was not among the theoretical possibilities formally proposed [<xref ref-type="bibr" rid="pcbi.1007047.ref012">12</xref>], the moment representation was implicitly assumed in the decision making studies that attempted to separate the brain regions for mean, variance, and skewness [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>]. These studies tested subjects’ preferences for different reward distributions to see how the choice-related neural activities may vary with specific moments of the reward distribution. However, concerning the coding of the moments higher than the mean (i.e. variance and skewness), the brain regions identified by recent neuroimaging studies were rather inconsistent: Variance representation was associated with anterior cingulate cortex [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>] or ventral striatum and anterior insula [<xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>]; skewness was associated with dorsal insula [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>], ventral striatum [<xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>], or anterior insula and dorsomedial prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1007047.ref037">37</xref>].</p>
<p>These conflicting findings raised the possibility that variance and skewness might not be the variables actually encoded in the brain. Instead, their influences on human decisions may come through another set of more basic variables humans adopt to appraise uncertainty. Indeed, we found that the moment-based models were inferior to the CoS models in fitting both the Mode and Mean estimates, even though the coding of task-irrelevant skewness seems to be a natural explanation for the increase of the Mode and Mean estimates with the skewness of the distribution. An obvious failure of the moment-based models was their inability to capture the multimodality of subjects’ responses.</p>
</sec>
<sec id="sec011">
<title>CoS and skewness preference</title>
<p>The phenomenon of skewness preference—positively skewed reward distributions are favored over symmetric, and symmetric over negatively skewed reward distributions—have been widely reported in animal studies [<xref ref-type="bibr" rid="pcbi.1007047.ref057">57</xref>] and in economics and finance [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref056">56</xref>]. Recent studies started to look into the neural basis of skewness preference, in an attempt to identify the brain regions dedicated to the processing of skewness [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>]. However, as discussed earlier, identification of brain regions dedicated in skewness processing is still actively debated and the proposition of skewness (or variance) preference was inconsistent with emerging behavioral results. For example, Strait and Hayden [<xref ref-type="bibr" rid="pcbi.1007047.ref035">35</xref>] reported a non-monotonic preference ranking of reward distributions in monkeys where weakly negatively skewed reward distributions were less preferable to weakly positively skewed reward distributions and the latter were further less preferable to strongly negatively skewed reward distributions.</p>
<p>Alternative explanations other than explicit representation of skewness have been proposed for skewness preference. For example, Genest, Stauffer, and Schultz [<xref ref-type="bibr" rid="pcbi.1007047.ref057">57</xref>] measured the utility function for individual monkeys and found that the skewness preference in monkeys’ choices can be accounted for by utility maximization, given that the monkeys’ utility functions are convex. This explanation, however, might have trouble generalizing to the skewness preference observed in human choices [<xref ref-type="bibr" rid="pcbi.1007047.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref056">56</xref>], because humans’ utility functions are typically concave [<xref ref-type="bibr" rid="pcbi.1007047.ref068">68</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref069">69</xref>]) and would predict the opposite behavior. Indeed, a recent empirical study of decision under risk in humans [<xref ref-type="bibr" rid="pcbi.1007047.ref070">70</xref>] reported the coexistence of skewness preference with concave utility functions.</p>
<p>The findings of the present study suggest a new possibility: skewness preference can be the epi-phenomenon of mis-estimating the mean—the expected value of the distribution. We found that subjects overestimated the mean of positively skewed distributions and underestimated the mean of negatively skewed distributions. This would appear to be skewness preference if subjects had been asked to choose between two distributions to maximize the expected value of their choice. In accordance with our conjecture, one recent neuroimaging study showed that higher skewness of reward distributions would lead to stronger activation in ventral striatum [<xref ref-type="bibr" rid="pcbi.1007047.ref038">38</xref>], a brain region involved in the representation of expected value [<xref ref-type="bibr" rid="pcbi.1007047.ref071">71</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref072">72</xref>].</p>
<p>Since no preference judgment was involved in our task, the patterned errors we found in subjects’ Mean estimates cannot be an effect of non-linear utility functions. Instead, we showed that an approximate representation of probability distributions along with subsequent distortions of probabilities, as implemented in the CoS model for Mean estimates, may give rise to the “skewness preference”.</p>
</sec>
<sec id="sec012">
<title>CoS as a simplified representation of probability distributions</title>
<p>Representing probability distributions in the real world, which are often high-dimensional and multimodal, confronts human cognition with potential problems such as the curse of dimensionality [<xref ref-type="bibr" rid="pcbi.1007047.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref073">73</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref076">76</xref>]. The CoS representation proposed here provides a simplified representation of probability distributions by reducing an arbitrary distribution to a few pairs of summary statistics <inline-formula id="pcbi.1007047.e004"><alternatives><graphic id="pcbi.1007047.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Though coming at the cost of information loss, such simplification is likely to alleviate the mnemonic and computational load in probabilistic inference and decision making.</p>
<p>As a basis-function representation [<xref ref-type="bibr" rid="pcbi.1007047.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref030">30</xref>], CoS adds to extant sparse-coding approaches to simplifying the representation and computation of probabilistic information [<xref ref-type="bibr" rid="pcbi.1007047.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref077">77</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref078">78</xref>]. Zhang, Daw, and Maloney [<xref ref-type="bibr" rid="pcbi.1007047.ref030">30</xref>] investigated subjects’ internal representation of their own visuo-motor error distributions in a motor choice task and found empirical evidence for the basis-function representation. Though the objective distribution was unimodal and close to Gaussian, they found that subjects’ internal representation was multimodal and, among a variety of distribution families, was best fit by the mixture of a small number of non-overlapping basis functions. What remains unknown, however, is how subjects’ internal representation arises from the empirical distribution. Here, with the CoS representation, we attempted to bridge the gap between an empirical distribution and its internal representation through a stochastic clustering process.</p>
<p>We showed that a Bayesian ideal observer that is unaware of the generative process of the distribution stimuli used in our experiments can form an accurate representation of the empirical distribution based on Gaussian mixtures (<xref ref-type="supplementary-material" rid="pcbi.1007047.s005">S4 Fig</xref>). As a result, the ideal observer model failed to account for the systematic errors in subjects’ Mode and Mean estimations, but rather performed close to the ground-truth model (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5A and 5B</xref>). The CoS model differs from the ideal observer model in several aspects. First, CoS does not involve Bayesian inference and the stochasticity inherent in clustering may not be eliminated even with a large number of samples observed. Second, by only keeping the cluster centers, CoS loses higher-order information about each cluster such as the local skewness. As we reasoned earlier, these characteristics allow CoS to explain the specific error patterns in subjects’ Mode estimates. The combination of CoS and additional transformations but not the transformations alone can also explain subjects’ Mean estimates. By comparing CoS with the ideal observer model in predicting human data, we have obtained evidence for the two key assumptions of the CoS representation: stochastic clustering and the loss of local information.</p>
<p>Our results raise the possibility that inferring the generative process from the observed samples might not occur for a complicated generative process. Even when the form of the generative model is simple and known, a recent study [<xref ref-type="bibr" rid="pcbi.1007047.ref079">79</xref>] found that people tend not to estimate the generative model. On each trial of their task, subjects saw an array of four dots distributed around a vertical line and were explicitly informed that the horizontal coordinates of the dots were generated from a Gaussian distribution centered at the line. Subjects were required to locate the range that would include 65% of the distribution. Despite comprehensive feedbacks after each trial, subjects’ behavioral patterns systematically deviated from those predicted by a Gaussian internal model. Instead, subjects’ internal model was well approximated by a kernel density estimation based on the four samples, which was consequently multimodal. Though there is doubt whether the kernel density representation holds for distributions presented by more than four samples (see [<xref ref-type="bibr" rid="pcbi.1007047.ref030">30</xref>] for opposing evidence), the results of [<xref ref-type="bibr" rid="pcbi.1007047.ref079">79</xref>] as well as our own study suggest that humans may not function as Bayesian ideal observers in probability density estimation.</p>
<p>The CoS representation we proposed also echoes the spontaneous clustering process theorized in the memory literature, for both working memory [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref080">80</xref>] and long-term memory [<xref ref-type="bibr" rid="pcbi.1007047.ref081">81</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref082">82</xref>]. According to Orhan and Jacobs [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>], people assume the world is “bumpy” and try to infer the clusters from which individual items have been generated. Though their task was to memorize individual items, subjects’ biases during retrieval suggest that the inferred clusters were also maintained and used to compensate for perceptual and mnemonic noises. What we considered here is a different situation, where the task was not to memorize individual items but to report the summary statistics of a distribution. That is, subjects were nudged to extract a representation of the distribution from the samples. While an ideal observer can almost perfectly recover the underlying distribution used in our experiments (<xref ref-type="supplementary-material" rid="pcbi.1007047.s005">S4 Fig</xref>), human behavioral data suggested that the local features of the distribution were lost.</p>
</sec>
<sec id="sec013">
<title>Limitations and future directions</title>
<p>For Gaussian or any symmetric unimodal distributions, mean, median, and mode are all the same. In such cases, it would be theoretically difficult to tell apart different hypotheses about human representations of probability distributions. That is why highly skewed [<xref ref-type="bibr" rid="pcbi.1007047.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref083">83</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref084">84</xref>] or multi-modal [<xref ref-type="bibr" rid="pcbi.1007047.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref085">85</xref>] distributions have been used to investigate how people represent probability distributions. Indeed, the specific one-dimensional multimodal distributions we used in the present study revealed diagnostic error patterns, which provided preliminary evidence in support of the CoS representation. But it is still an empirical question whether CoS well describes human representations of probability distributions that are beyond multimodal distributions.</p>
<p>In theory, the CoS representation is applicable to distributions of a higher dimension. It can also be extended to accommodate modulations from top-down cognitive processes by assuming that the parameters controlling the clustering process may be modulated by prior knowledge. Further empirical tests in a broader range of tasks would be required to establish the CoS representation as a general heuristic in representing probability distributions.</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Ethics statement</title>
<p>The experiments had been approved by the Institutional Review Board of School of Psychological and Cognitive Sciences at Peking University. Informed consent was given by all subjects prior to the experiments.</p>
</sec>
<sec id="sec016">
<title>Experiments</title>
<p>Sixty-four paid subjects (19–25 years old, 36 females) participated in the four experiments, with 16 subjects for each experiment. All of them were naïve to the goal of our study. Subjects whose Mode or Mean estimates failed to show significant correlations with the corresponding true statistics were excluded from further data analysis. Only one subject from Experiment 2 was excluded.</p>
<p>Stimuli were presented on black backgrounds on a 52.0×32.5-cm computer screen (1920×1200 px, refresh rate 60 Hz) controlled by Matlab and Psychophysics Toolbox [<xref ref-type="bibr" rid="pcbi.1007047.ref086">86</xref>–<xref ref-type="bibr" rid="pcbi.1007047.ref088">88</xref>] and were viewed by subjects from a distance of approximately 65 cm. The experimental procedure was the same for all the experiments, with spatial positions used as stimuli for Experiments 1–3 and orientations used for Experiment S1. On each trial of Experiments 1–3 (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2A</xref>), following a 1-s fixation cross, a 40-cm white horizontal axis appeared in the middle of the screen. In the subsequent 25 seconds, 70 red vertical lines were sequentially presented on the axis at different horizontal positions, each for 0.18-s and separated by 0.18-s intervals. The tasks were to report the Mean and Mode of the observed horizontal distribution of the red lines: Subjects first saw a blue vertical line and were required to move it along the axis to locate the Mean (the average horizontal position of the red lines); after completing the estimation of the Mean, they saw a blue box and were instructed to locate it at the position where it would catch the largest number of red lines. To help subjects understand the task, graphed illustrations of mean and mode were given during the instructions. The initial position of the blue line or box was randomly chosen. Subjects used the mouse cursor to move it and left clicked to confirm. No time limit was imposed on either task.</p>
<p>The horizontal coordinates of the 70 red lines on each trial were sampled from a specific underlying distribution, which was a linear combination of multiple beta-like distributions (<xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2B–2D</xref>). Experiments 1–3 differed in the number of components that consisted of the beta-mix distributions: three components for Experiment 1 (“3-beta mix”), four components for Experiment 2 (“4-beta mix”), and a combination of 3-beta and 4-beta mix for Experiment 3.</p>
<p>Each beta-mix distribution had two sets of parameters: shape and weight. The shape parameters, (α, β), controlled the shape of individual beta components and were the same for all the components in the same distribution. The (α, β) could be (3.1, 1.1), (2.9, 2.9) or (1.1, 3.1), respectively corresponding to negatively-skewed, symmetric, and positively-skewed local components of equal variance. In contrast, the weight parameters, (<italic>φ</italic><sub>1</sub>,<italic>φ</italic><sub>2</sub>,…,<italic>φ</italic><sub><italic>m</italic></sub>), with <inline-formula id="pcbi.1007047.e005"><alternatives><graphic id="pcbi.1007047.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, referred to the relative weight of each component in the mixture distribution, ordered from left to right. The different beta components of a beta-mix distribution had equal widths and joined each other’s ends, whose standard deviations were 0.19 times of their widths.</p>
<p>In Experiment 1, where the distributions were 3-beta mix, the relative weights could be (0.2, 0.3, 0.5), (0.3, 0.2, 0.5), (0.2, 0.5, 0.3), (0.3, 0.5, 0.2), (0.5, 0.2, 0.3), or (0.5, 0.3, 0.2), that is, the full permutation of (0.2, 0.3, 0.5). Each combination of the 3 shape and 6 weight conditions was repeated for 9 times, resulting in 3×6×9 = 162 trials. In Experiment 2, the relative weights for the 4-beta mix were the full permutation of (0.1, 0.2, 0.3, 0.4). Each combination of the 3 shape and 24 weight conditions was repeated twice, resulting in 3×24×2 = 144 trials. Experiment 3 was a combination of 72 trials of 3-beta mix from Experiment 1 (3 shapes × 6 weights × 4 repetitions) and 72 trials of 4-beta mix from Experiment 2 (3 shapes × 24 weights × 1 repetition). The 144 trials were divided into 12 mini-blocks of 10–14 trials, with each mini-block devoted to either 3-beta or 4-beta mix and the two types of mini-blocks interleaved. The existence of mini-blocks was unbeknown to the subject.</p>
<p>The 70 samples for each trial (i.e. the horizontal coordinates of the red lines) were first generated by random and independent draws from its underlying beta-mix distribution, and subsequently scaled to a specific standard deviation and jittered around the center of the screen. The standard deviations of the samples for Experiments 1, 2, and 3 were respectively set to be 7.27, 7.20 and 7.55 cm. Given the screen center as the origin, the samples were allowed to range from –20 to 20 cm and the mean of the samples was within the range of –3.8 to 3.8 cm. In a specific experiment, the same set of samples was used for all subjects, but the order of samples within each trial and the order of the trials were randomized for each subject.</p>
<p>Experiment S1 was a conceptual replication of Experiment 1 where orientations instead of spatial positions were used as stimuli (<xref ref-type="supplementary-material" rid="pcbi.1007047.s009">S8 Fig</xref>). Samples were lines of 8 cm long, starting from the center of the screen and pointing to various directions. For a specific subject, all sample lines pointed towards either the upper or lower half of the screen so that the whole range of the stimuli was within 180 degrees. Subjects rotated a line or bar around the origin to report the Mean or Mode of the orientations, analogous to the responding procedures in Experiments 1–3. Half of the subjects reported the Mean first and half reported the Mode first. We did not find any significant differences between these two task orders in subjects’ error patterns, no matter for Mode or Mean estimates.</p>
<p>There were three practice trials preceding the formal experiment. Each experiment took approximately 1.5 hours.</p>
</sec>
<sec id="sec017">
<title>Data analysis and modeling</title>
<sec id="sec018">
<title>True mode, mean and skewness</title>
<p>The true mode, mean, and skewness of the distribution were defined based on the 70 samples subjects actually saw, which might be different from those of the underlying distribution. The true mean and skewness were simply the corresponding statistics of the samples. To compute the true mode, we applied the diffusion algorithm of kernel density estimation [<xref ref-type="bibr" rid="pcbi.1007047.ref089">89</xref>]—a smoothing method performing well for multi-modal distributions—to the samples, with bandwidth parameters chosen automatically by the algorithm. The true mode was defined as the highest peak of the resulting density curve.</p>
</sec>
<sec id="sec019">
<title>Clusters-of-samples model for mode estimates</title>
<p>According to the clusters-of-samples (CoS) representation, on each trial, subjects would classify the 70 samples into clusters and maintain the centroids and relative weights of the clusters—<inline-formula id="pcbi.1007047.e006"><alternatives><graphic id="pcbi.1007047.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>—as the representation for the empirical distribution. In particular, we implemented the clustering process as a distance-dependent Chinese Restaurant Process (ddCRP, [<xref ref-type="bibr" rid="pcbi.1007047.ref043">43</xref>]) where the number of clusters is controlled by the random process and samples that are close to each other are more likely to be assigned to the same cluster. It should be noted that we used the ddCRP not as a prior for Bayesian inference but as a clustering algorithm.</p>
<p>In the original ddCRP, each sample is attracted by all other samples and may join one of them, otherwise staying alone. Given the large number of samples in our case, for simplicity, we assumed that the probability for a sample to stay alone approaches zero. The probability for the <italic>j</italic>-th sample to join the <italic>l</italic>-th sample (<italic>j</italic>≠<italic>l</italic>) is a function of their distance <italic>d</italic><sub><italic>jl</italic></sub>:
<disp-formula id="pcbi.1007047.e007">
<alternatives>
<graphic id="pcbi.1007047.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>γ</italic> is a scaling parameter controlling how quickly the joining probability would decay with distance, and <inline-formula id="pcbi.1007047.e008"><alternatives><graphic id="pcbi.1007047.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Each sample goes to the same cluster as the sample it joins.</p>
<p>For a specific cluster <italic>k</italic> following the ddCRP, denote the centroid (i.e. mean value) of its samples by <italic>c</italic><sub><italic>k</italic></sub> and its relative weight (i.e. number of its samples divided by the total number of samples) by <italic>w</italic><sub><italic>k</italic></sub>, where <italic>k</italic> = 1,2,…,<italic>K</italic>. In the CoS model for Mode estimates, we assumed that subjects would report the centroid of the cluster with the highest relative weight, denoted <italic>C</italic>*.</p>
<p>The ddCRP is a stochastic clustering process, which may lead to different sets of <inline-formula id="pcbi.1007047.e009"><alternatives><graphic id="pcbi.1007047.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and thus different <italic>C</italic>* in different runs. In other words, <italic>C</italic>* is a random variable whose distribution is determined by the empirical distribution of the samples and the distance-scaling parameter <italic>γ</italic>. The distribution of <italic>C</italic>* has no closed forms and was estimated through Monte Carlo methods: For each specific trial and choice of <italic>γ</italic>, we simulated the clustering process for 1000 times, obtained 1000 sets of <inline-formula id="pcbi.1007047.e010"><alternatives><graphic id="pcbi.1007047.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and computed the value of <italic>C</italic>* for each set.</p>
<p>The response in a magnitude estimation task often has a tendency of regression to mean [<xref ref-type="bibr" rid="pcbi.1007047.ref090">90</xref>]. In all the models, we assume that subjects’ final estimate <italic>Y</italic> undergoes a linear transformation and is contaminated by a Gaussian noise. That is, for the CoS model for Mode estimates,
<disp-formula id="pcbi.1007047.e011">
<alternatives>
<graphic id="pcbi.1007047.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi>C</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">mode</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, and <italic>σ</italic><sub>mode</sub> are free parameters.</p>
<p>In sum, the CoS model for Mode estimates has four free parameters: <italic>γ</italic>, <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, <italic>σ</italic><sub>mode</sub>.</p>
</sec>
<sec id="sec020">
<title>Clusters-of-samples model for Mean estimates</title>
<p>In the CoS model for Mean estimates, we assume that subjects would report a weighted average of the centroids of all the clusters. Similar to that of Mode estimates, the modeling of Mean estimates needs to take into account the randomness in clustering.</p>
<p>As elaborated in the Results, we assume that subjects used the same CoS representation—a specific set <inline-formula id="pcbi.1007047.e012"><alternatives><graphic id="pcbi.1007047.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> —for both Mean and Mode estimates. When fitting the CoS model for Mode estimate, we had run the stochastic clustering process for 1000 times and obtained 1000 sets of <inline-formula id="pcbi.1007047.e013"><alternatives><graphic id="pcbi.1007047.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> for each trial and each <italic>γ</italic>. For the best-fit <italic>γ</italic>, we could compute the likelihood that the observed Mode estimate, <italic>y</italic><sub>mode</sub>, resulted from the <italic>j</italic>-th <inline-formula id="pcbi.1007047.e014"><alternatives><graphic id="pcbi.1007047.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1007047.e015">
<alternatives>
<graphic id="pcbi.1007047.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">mode</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="normal">mode</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">mode</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <inline-formula id="pcbi.1007047.e016"><alternatives><graphic id="pcbi.1007047.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> denotes the centroid of the cluster of the highest weight for the <italic>j</italic>-th <inline-formula id="pcbi.1007047.e017"><alternatives><graphic id="pcbi.1007047.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. That is, given the <italic>y</italic><sub>mode</sub> on a specific trial, we could compute the posterior probability of the <italic>j</italic>-th <inline-formula id="pcbi.1007047.e018"><alternatives><graphic id="pcbi.1007047.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> being the CoS representation in use:
<disp-formula id="pcbi.1007047.e019">
<alternatives>
<graphic id="pcbi.1007047.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
with the normalization <inline-formula id="pcbi.1007047.e020"><alternatives><graphic id="pcbi.1007047.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1000</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Mode is a local feature of a probability distribution, whose value is determined only by the largest cluster, while mean is a global feature that involves the integration of multiple clusters. We assume that such integration would introduce two additional transformations on the relative weight <italic>w</italic><sub><italic>k</italic></sub>. One transformation maps <italic>w</italic><sub><italic>k</italic></sub> to <inline-formula id="pcbi.1007047.e021"><alternatives><graphic id="pcbi.1007047.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, which is partly motivated by Stevens’ power law [<xref ref-type="bibr" rid="pcbi.1007047.ref091">91</xref>]. The power transformation would lead to a subjective weighting of probability that is mathematically equivalent to the form of linear in log-odds [<xref ref-type="bibr" rid="pcbi.1007047.ref069">69</xref>] widely observed in human judgment and decision-making [<xref ref-type="bibr" rid="pcbi.1007047.ref065">65</xref>]. Depending on the exponent <italic>α</italic>, small <italic>w</italic><sub><italic>k</italic></sub> is overweighted and large <italic>w</italic><sub><italic>k</italic></sub> underweighted, or the reverse. A second transformation on <italic>w</italic><sub><italic>k</italic></sub> works analogous to the lateral inhibition in perception [<xref ref-type="bibr" rid="pcbi.1007047.ref092">92</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref093">93</xref>]: The subjective weight of any cluster is reduced by the existence of any other clusters; the closer or the larger the weight of the other cluster, the larger the influence. We assume that the lateral inhibition influences the subjective weight in the form of a shunting inhibition [<xref ref-type="bibr" rid="pcbi.1007047.ref066">66</xref>], with the inhibitory forces between two clusters decreasing as a Gaussian function of their distance [<xref ref-type="bibr" rid="pcbi.1007047.ref067">67</xref>]. Combining the two transformations, the subjective weight of <italic>w</italic><sub><italic>k</italic></sub> has the form:
<disp-formula id="pcbi.1007047.e022">
<alternatives>
<graphic id="pcbi.1007047.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where <italic>α</italic> and <italic>σ</italic><sub><italic>LI</italic></sub> are free parameters, and <italic>A</italic> is a constant to normalize <inline-formula id="pcbi.1007047.e023"><alternatives><graphic id="pcbi.1007047.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>For a specific <inline-formula id="pcbi.1007047.e024"><alternatives><graphic id="pcbi.1007047.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> representation, the Mean estimate is thus
<disp-formula id="pcbi.1007047.e025">
<alternatives>
<graphic id="pcbi.1007047.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
The final report of the mean is a random mixture of different <italic>M</italic>* from different <inline-formula id="pcbi.1007047.e026"><alternatives><graphic id="pcbi.1007047.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, weighted by their posterior probabilities (<xref ref-type="disp-formula" rid="pcbi.1007047.e019">Eq 4</xref>), linearly transformed and contaminated by Gaussian noise. The CoS model for Mean estimates has five free parameters: <italic>α</italic>, <italic>σ</italic><sub><italic>LI</italic></sub>, <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, <italic>σ</italic><sub>mean</sub>. See <xref ref-type="supplementary-material" rid="pcbi.1007047.s001">S1 Table</xref> for the estimated parameters of the CoS models.</p>
<p>In our modeling, we first used subjects’ Mode estimates to infer their CoS representations and then applied this information to model their Mean estimates. We chose not to do the other way around because mean is a statistic that integrates over multiple clusters and thus provides less diagnostic information for individual clusters than mode does (global versus local). The additional transformations applied to the CoS model for Mean estimates also increases the computational intractability of using Mean estimates to infer subjects’ CoS representations.</p>
</sec>
<sec id="sec021">
<title>Subjective weighting model for Mean estimates</title>
<p>The subjective weighting model for Mean estimates assumes that subjects would report the weighted average of all the samples (<italic>N</italic> = 70), with the relative weight of each sample undergoing similar power and lateral inhibition transformations as those of the CoS model. Given that the original weight of each sample is equal (i.e. <italic>ϕ</italic><sub><italic>i</italic></sub> = 1/<italic>N</italic>, <italic>i</italic> = 1,2,…,<italic>N</italic>), the subjective weight has a similar form as <xref ref-type="disp-formula" rid="pcbi.1007047.e022">Eq 5</xref> except that the exponent <italic>α</italic> is naturally dropped:
<disp-formula id="pcbi.1007047.e027">
<alternatives>
<graphic id="pcbi.1007047.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi><mml:mi>α</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub> denotes the location of the <italic>i</italic>-th sample, <italic>σ</italic><sub><italic>LI</italic></sub> is a free parameter, and B is a constant to normalize <inline-formula id="pcbi.1007047.e028"><alternatives><graphic id="pcbi.1007047.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The final estimate is <inline-formula id="pcbi.1007047.e029"><alternatives><graphic id="pcbi.1007047.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>, linearly transformed and contaminated by Gaussian noise. In sum, the model has four free parameters: <italic>σ</italic><sub><italic>LI</italic></sub>, <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, <italic>σ</italic><sub>mean</sub>.</p>
</sec>
<sec id="sec022">
<title>Ideal observer models for mode and mean estimates</title>
<p>In the ideal observer models, we applied the Dirichlet Process Mixture Model (DPMM) to estimate the generative distribution underlying a set of samples. Denote the value of the <italic>i</italic>-th sample by <italic>x</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1,2,…,<italic>N</italic>. Similar to Orhan and Jacobs [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>], the ideal observer’s internal model of the generative process is specified by the following equations:
<disp-formula id="pcbi.1007047.e030">
<alternatives>
<graphic id="pcbi.1007047.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mrow><mml:mi>G</mml:mi><mml:mo>∼</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
<disp-formula id="pcbi.1007047.e031">
<alternatives>
<graphic id="pcbi.1007047.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="fraktur">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
<disp-formula id="pcbi.1007047.e032">
<alternatives>
<graphic id="pcbi.1007047.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e032" xlink:type="simple"/>
<mml:math display="block" id="M32">
<mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
<disp-formula id="pcbi.1007047.e033">
<alternatives>
<graphic id="pcbi.1007047.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e033" xlink:type="simple"/>
<mml:math display="block" id="M33">
<mml:mrow><mml:mi>τ</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>τ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
<disp-formula id="pcbi.1007047.e034">
<alternatives>
<graphic id="pcbi.1007047.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e034" xlink:type="simple"/>
<mml:math display="block" id="M34">
<mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
Here <italic>G</italic> is a distribution of clusters that is itself distributed following a Dirichlet process (DP) with base distribution <italic>G</italic><sub>0</sub> and concentration parameter <italic>α</italic>. <italic>μ</italic><sub><italic>i</italic></sub> denotes the mean of the cluster to which the <italic>i</italic>-th sample belongs. The base distribution <italic>G</italic><sub>0</sub>(<italic>μ</italic><sub><italic>i</italic></sub>) is a uniform distribution over the interval [<italic>a</italic>,<italic>b</italic>]. The precision <italic>τ</italic> is identical for all clusters and distributed according to a gamma distribution with scale parameter <italic>α</italic><sub><italic>τ</italic></sub> and shape parameter <italic>β</italic><sub><italic>τ</italic></sub>. The value of the <italic>i</italic>-th sample <italic>x</italic><sub><italic>i</italic></sub> is generated from a Gaussian distribution with mean <italic>μ</italic><sub><italic>i</italic></sub> and precision <italic>τ</italic>. The values or priors for the parameters in the generative model had similar settings as those of Orhan and Jacobs[<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>]: The interval [<italic>a</italic>,<italic>b</italic>] was set to be sufficiently large to include the minimum and maximum sample values in each experiment. <italic>α</italic><sub><italic>τ</italic></sub> = 1 and a gamma <inline-formula id="pcbi.1007047.e035"><alternatives><graphic id="pcbi.1007047.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> prior was put on <italic>β</italic><sub><italic>τ</italic></sub>. A <inline-formula id="pcbi.1007047.e036"><alternatives><graphic id="pcbi.1007047.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> prior was put on the Dirichlet process concentration parameter <italic>α</italic>, where <italic>α</italic><sub><italic>c</italic></sub> was treated as a free parameter.</p>
<p>On each trial, the ideal observer would use the observed <inline-formula id="pcbi.1007047.e037"><alternatives><graphic id="pcbi.1007047.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to infer the latent variables of the generative model and thus the Gaussian mixture distribution that has generated the observed samples. The mean and mode of the Gaussian mixture distribution can then be calculated. The Bayesian inference was performed using a Markov chain Monte Carlo (MCMC) algorithm of Neal [<xref ref-type="bibr" rid="pcbi.1007047.ref094">94</xref>], implemented by Matlab codes adapted from Orhan and Jacobs [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>]. For each trial, we obtained 1000 samples from four MCMC chains of 500 samples after burning-in the first 250 samples of each chain.</p>
<p>Following Orhan and Jacobs [<xref ref-type="bibr" rid="pcbi.1007047.ref058">58</xref>] and others (e.g. [<xref ref-type="bibr" rid="pcbi.1007047.ref095">95</xref>]), we assume that the ideal observer would average over all posterior samples of the mean and mode as their Mean and Mode estimates. Parallel to the CoS models, we used subjects’ Mode estimates to infer the Gaussian mixture distributions and applied the results to fit subjects’ Mean estimates. In sum, the ideal observer model for Mode estimates has four free parameters: <italic>α</italic><sub><italic>c</italic></sub>, <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, <italic>σ</italic><sub>mode</sub>. The ideal observer model for Mean estimates has three free parameters: <italic>β</italic><sub>0</sub>, <italic>β</italic><sub>1</sub>, <italic>σ</italic><sub>mean</sub>.</p>
</sec>
<sec id="sec023">
<title>Other models for mode and mean estimates</title>
<p>The ground-truth and all the moment-based models we considered for Mode and Mean estimates have the general form:
<disp-formula id="pcbi.1007047.e038">
<alternatives>
<graphic id="pcbi.1007047.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007047.e038" xlink:type="simple"/>
<mml:math display="block" id="M38">
<mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
where <italic>Y</italic> denotes the response (Mode or Mean estimate), <italic>Z</italic><sub><italic>j</italic></sub> denotes the <italic>j</italic>-th predictor among a total of <italic>q</italic> different predictors that may include the true mode, mean, and skewness of the empirical distribution, <italic>β</italic><sub>0</sub>, <italic>β</italic><sub><italic>j</italic></sub>, and <italic>σ</italic> are free parameters. The predictors of each model, as suggested by the model name, are described in the Results.</p>
</sec>
<sec id="sec024">
<title>Model fitting and comparison procedures</title>
<p>All the models were fitted on the individual level using the maximum likelihood estimate. We used <italic>fminsearchbnd</italic> (J. D’Errico)—a function that implements the Nelder-Mead method and extends the standard Matlab function <italic>fminsearch</italic> to bounded parameters—to search for the parameters that minimized negative log likelihood. To verify that we had found the global minimum, we repeated the search process with different starting points. For the distance-scaling parameter <italic>γ</italic> in the CoS model of Mode estimates, whose change would introduce stochasticity that the Nelder-Mead method cannot handle, we grid searched <italic>γ</italic> ranging from 0.02 to 2.00, optimized the other parameters of the model for each value of <italic>γ</italic>, and chose the combination of <italic>γ</italic> and other parameters that minimized negative log likelihood. We used a similar grid search procedure for the concentration parameter <italic>α</italic><sub><italic>c</italic></sub> (ranging from 0.10 to 3.00) in the ideal observer model.</p>
<p>For model comparison, we applied AICc—the Akaike information criterion with a correction for finite sample size [<xref ref-type="bibr" rid="pcbi.1007047.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref062">62</xref>]—to each subject and model as the information criterion for goodness-of-fit. We further calculated the protected exceedance probability based on the group-level Bayesian model selection method [<xref ref-type="bibr" rid="pcbi.1007047.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1007047.ref064">64</xref>], which is an omnibus measure across subjects to indicate the probability that a specific model is the best model in the comparison set.</p>
</sec>
</sec>
</sec>
<sec id="sec025">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007047.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Estimated parameters of the CoS models for mode and mean estimates.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Results of Experiment 2 (4-beta mix).</title>
<p>(A, C) Subjects’ errors in Mode (A) and Mean (C) estimates varied with the weight condition (abscissa) and the shape condition (different colors). Different weight conditions are arranged from left to right in increasing global skewness. Shaded areas denote 1 SE. (B) The main effect of the shape condition (local skewness) on subjects’ errors in Mode estimates. ns: non-significant, *: <italic>p</italic> &lt; 0.05, **: <italic>p</italic> &lt; 0.01, ***: <italic>p</italic> &lt; 0.001, for Bonferroni corrected post-hoc comparisons. Error bars denote 1 SE.</p>
<p>(D-E) Model comparison results: summed ΔAICc (left, the lower the better) and protected exceedance probability (right, the higher the better). The black bar refers to the protected exceedance probability for the CoS model, and the red bar on the top of the black bar refers to the sum of protected exceedance probabilities of all other models (which is invisible in the Mode plot). For both Mode (D) and Mean (E) estimates, the CoS models fit best to data. GT: ground-truth model; IO: ideal observer model; MoMe: mode+mean model; MeSk: mean+skewness model; MoSk: mode+skewness model; MoMeSk: mode+mean+skewness model; CoS: clusters-of-samples model; SW: subjective weighting model.</p>
<p>(F) Relative frequency of different cluster sizes (number of samples per cluster) estimated for subjects’ CoS representations. For each subject, the relative frequency of each cluster size was averaged across trials and possible CoS representations on each trial. The statistics were then averaged across subjects. Error bars denote 1 SE. Overall, the clustering process tended to generate clusters of small sizes. The frequencies for cluster sizes of 7, 14, 21, and 28—which correspond to the four relative weights used in 4-beta mix—were much higher than those of their neighbors, indicating that the clustering process could partly recover the multimodal structure of the empirical distribution.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Results of Experiment 3 (interleaved 3-beta and 4-beta mix).</title>
<p>Plotted in the same format as the results of Experiment 2 (<xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1 Fig</xref>).</p>
<p>(A-C) Subjects’ errors in Mode and Mean estimates for 3-beta trials.</p>
<p>(D-F) Subjects’ errors in Mode and Mean estimates for 4-beta trials.</p>
<p>(G, H) Model comparison results for Mode and Mean estimates.</p>
<p>(I) Relative frequency of different cluster sizes estimated for subjects’ CoS representations, separately for 3-beta (gray bars) and 4-beta (white bars) trials.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Results of Experiment S1 (3-beta mix, orientation).</title>
<p>Plotted in the same format as the results of Experiment 2 (<xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1 Fig</xref>).</p>
<p>(A-C) Subjects’ errors in Mode and Mean estimates.</p>
<p>(D, E) Model comparison results for Mode and Mean estimates.</p>
<p>(F) Relative frequency of different cluster sizes estimated for subjects’ CoS representations.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s005" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Distributions recovered by ideal observers versus the empirical distributions.</title>
<p>(A) Examples for 3-beta distributions. (B) Examples for 4-beta distributions. The gray areas in each panel denote the histogram of the samples presented on a single trial. The generative distribution of the trial is described in the title of the panel (see the legend of <xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2</xref> for the notations). For example, the generative distribution “LocalNeg, 532” (top left panel) consists of three negatively skewed beta components (“LocalNeg”), with the weights of the three components from left to right being 0.5, 0.3 and 0.2 (“532”). The (α, β) parameters of the LocalNeg, LocalSym and LocalPos beta components were respectively (3.1, 1.1), (2.9, 2.9) and (1.1, 3.1). Dashed curves denote the kernel density of samples. Solid curves denote the posterior density estimation of the Bayesian ideal observer modeled by a Dirichlet Process Mixture Model (DPMM, see the main text). Note that the DPMM posterior density closely matches the empirical kernel density, even for beta mixtures that have skewed beta components.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s006" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Intuition of lateral inhibition leading to “skewness preference”.</title>
<p>Suppose there are three samples (black lines), with the left two samples closer to each other than to the third, thus forming a positively skewed distribution. If we assume lateral inhibition decreases with distance, the left two samples would exert strong inhibitions on each other, while the inhibitions between them and the third sample would be weaker. Therefore, the left two samples would be underweighted, leading to an overestimation of Mean for positively skewed distributions, and vice versa.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s007" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Evidence for shared CoS representations across Mode and Mean estimations.</title>
<p>Model comparison results for subjects’ Mean estimates between the original CoS model (“CoS”) and a lesioned model (“CoS w/o Prior”) that does not use the representational information inferred from the Mode estimates on the same trial. (A) Experiment 1. (B) Experiment 2. (C) Experiment 3. (D) Experiment S1. The left and right plots are respectively for summed ΔAICc (the lower the better) and protected exceedance probability (the higher the better). If two distinct CoS representations had been used for Mode and Mean estimates, the CoS representation inferred from the Mode estimate would be non-informative for predicting the Mean estimate and the lesioned model would perform equally well as the original model. In all the experiments, however, the CoS model outperformed the CoS w/o Prior model, providing evidence for a shared CoS representation across the two estimation tasks.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s008" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title/>
<p><bold>Model lesion analysis for Experiment 2 (A), Experiment 3 (B) and Experiment S1 (C).</bold> Model comparisons between the CoS model for Mean estimates and its lesioned models: summed ΔAICc (left) and protected exceedance probability (right). The lesioned models included models without Lateral Inhibition (“CoS w/o LI”), without Power Transformation (“CoS w/o PT”), without both the components (“CoS w/o LI&amp;PT”), and without clustering (“SW”, i.e. the subjective weighting model presented in <xref ref-type="supplementary-material" rid="pcbi.1007047.s002">S1E</xref>, <xref ref-type="supplementary-material" rid="pcbi.1007047.s003">S2H</xref> &amp; <xref ref-type="supplementary-material" rid="pcbi.1007047.s004">S3E</xref> Figs). In the protected exceedance probability plots of Experiments 2 and 3, the black bar refers to the protected exceedance probability for the CoS model, and the red bar on the top of the black bar refers to the sum of protected exceedance probabilities of all other models (which is almost invisible for Experiment 3). For Experiment S1, “1” refers to the CoS w/o PT model, “2” refers to the CoS w/o LI model, “3” refers to the CoS w/o LI&amp;PT model. In all the experiments, the original CoS model outperformed the lesioned models, implying that power transformation, lateral inhibition and clustering were all necessary for the CoS model to explain subjects’ Mean estimates.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s009" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Stimuli of Experiment S1.</title>
<p>Experiment S1 was a conceptual replication of Experiment 1 where orientations instead of spatial positions were used as stimuli. Samples were lines of 8 cm long, starting from the center of the screen and pointing to various directions. For a specific subject, all sample lines pointed towards either the upper or lower half of the screen so that the whole range of the orientations was less than 180 degrees. Subjects rotated a line or bar around the origin to report the Mean or Mode of the orientations. Half of the subjects were required to report the Mean estimate first and the other half the Mode estimate first. Otherwise, the procedure and design were the same as those of Experiment 1 (see <xref ref-type="fig" rid="pcbi.1007047.g002">Fig 2</xref>).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007047.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007047.s010" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Multimodality of subjects’ Mode and Mean estimates in Experiment 1.</title>
<p>To investigate whether the joint distribution of subjects’ Mode and Mean estimates (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5C</xref>) was multimodal, we adopted the mixture model clustering method with the integrated completed likelihood criterion (ICL). The “Rmixmod” R package implementing the method was used to evaluate the number of clusters in subjects’ Mode and Mean estimates in different weight conditions. Each panel is for one weight condition, corresponding to that of <xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5C</xref>. Each data point denotes one subject’s Mode and Mean estimates on one trial. The clustering results are presented using symbols of different colors and superimposed ellipses. When a maximum of three clusters were allowed, 2–3 clusters were formed for each weight condition, whose positions agreed with the CoS predictions (<xref ref-type="fig" rid="pcbi.1007047.g005">Fig 5C</xref>, see the main text). ICL: the ICL value of the clustering results presented. ICL<sub>1</sub>: the ICL value for one cluster. That ICL &lt; ICL<sub>1</sub> indicated that subjects’ Mode and Mean estimates were better fit by multiple clusters than by one cluster and were thus multimodally distributed.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007047.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname> <given-names>H</given-names></name>. <article-title>Redundancy reduction revisited</article-title>. <source>Network: Comput Neural Syst</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>241</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. <chapter-title>Possible principles underlying the transformations of sensory messages</chapter-title>. In: <name name-style="western"><surname>Rosenblith</surname> <given-names>WA</given-names></name>, editor. <source>Sensory Communication</source>. <publisher-loc>Cambridge,Mass</publisher-loc>: <publisher-name>M.I.T. Press</publisher-name>; <year>1961</year>. p. <fpage>217</fpage>–<lpage>34</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Natural image statistics and neural representation</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>(<issue>1</issue>):<fpage>1193</fpage>–<lpage>216</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>. <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source>. <year>2011</year>;<volume>331</volume>(<issue>6013</issue>):<fpage>83</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1195870" xlink:type="simple">10.1126/science.1195870</ext-link></comment> <object-id pub-id-type="pmid">21212356</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Battaglia</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Schrater</surname> <given-names>PR</given-names></name>. <article-title>Humans trade off viewing time and movement duration to improve visuomotor accuracy in a fast reaching task</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>(<issue>26</issue>):<fpage>6984</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1309-07.2007" xlink:type="simple">10.1523/JNEUROSCI.1309-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17596447</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source>. <year>2004</year>;<volume>427</volume>(<issue>6971</issue>):<fpage>244</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02169" xlink:type="simple">10.1038/nature02169</ext-link></comment> <object-id pub-id-type="pmid">14724638</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Decision making, movement planning and statistical decision theory</article-title>. <source>Trends Cogn Sci</source>. <year>2008</year>;<volume>12</volume>(<issue>8</issue>):<fpage>291</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2008.04.010" xlink:type="simple">10.1016/j.tics.2008.04.010</ext-link></comment> <object-id pub-id-type="pmid">18614390</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">9054347</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. D<article-title>opamine reward prediction-error signalling: a two-component response</article-title>. <source>Nat Rev Neurosci</source>. <year>2016</year>;<volume>17</volume>(<issue>3</issue>):<fpage>183</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2015.26" xlink:type="simple">10.1038/nrn.2015.26</ext-link></comment> <object-id pub-id-type="pmid">26865020</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>:<fpage>429</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Optimal predictions in everyday cognition</article-title>. <source>Psychological Science</source>. <year>2006</year>;<volume>17</volume>(<issue>9</issue>):<fpage>767</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2006.01780.x" xlink:type="simple">10.1111/j.1467-9280.2006.01780.x</ext-link></comment> <object-id pub-id-type="pmid">16984293</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>(<issue>9</issue>):<fpage>1170</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Optimal representation of sensory information by neural populations</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>5</issue>):<fpage>690</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1691" xlink:type="simple">10.1038/nn1691</ext-link></comment> <object-id pub-id-type="pmid">16617339</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>. <article-title>The basal ganglia and cortex implement optimal decision making between alternative actions</article-title>. <source>Neural Comput</source>. <year>2007</year>;<volume>19</volume>(<issue>2</issue>):<fpage>442</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2007.19.2.442" xlink:type="simple">10.1162/neco.2007.19.2.442</ext-link></comment> <object-id pub-id-type="pmid">17206871</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoyer</surname> <given-names>PO</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>, editors. <article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior</article-title>. <source>Advances in neural information processing systems</source>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Bayesian sampling in visual perception</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2011</year>;<volume>108</volume>(<issue>30</issue>):<fpage>12491</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1101430108" xlink:type="simple">10.1073/pnas.1101430108</ext-link></comment> <object-id pub-id-type="pmid">21742982</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>(<issue>3</issue>):<fpage>119</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2010.01.003" xlink:type="simple">10.1016/j.tics.2010.01.003</ext-link></comment> <object-id pub-id-type="pmid">20153683</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bill</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons</article-title>. <source>PLoS Comp Biol</source>. <year>2011</year>;<volume>7</volume>(<issue>11</issue>):<fpage>e1002211</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Vul</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Multistability and perceptual inference</article-title>. <source>Neural Comput</source>. <year>2012</year>;<volume>24</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00226" xlink:type="simple">10.1162/NECO_a_00226</ext-link></comment> <object-id pub-id-type="pmid">22023198</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Neural variability and sampling-based probabilistic representations in the visual cortex</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>92</volume>(<issue>2</issue>):<fpage>530</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.09.038" xlink:type="simple">10.1016/j.neuron.2016.09.038</ext-link></comment> <object-id pub-id-type="pmid">27764674</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vandormael</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Castañón</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Balaguer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>. <article-title>Robust sampling of decision information during perceptual choice</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2017</year>;<volume>114</volume>(<issue>10</issue>):<fpage>2771</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1613950114" xlink:type="simple">10.1073/pnas.1613950114</ext-link></comment> <object-id pub-id-type="pmid">28223519</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erev</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ert</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Haruvy</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Herzog</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Hau</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>A choice prediction competition: Choices from experience and from description</article-title>. <source>J Behav Decis Mak</source>. <year>2010</year>;<volume>23</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>47</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vul</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>One and done? Optimal decisions from very few samples</article-title>. <source>Cogn Sci</source>. <year>2014</year>;<volume>38</volume>(<issue>4</issue>):<fpage>599</fpage>–<lpage>637</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/cogs.12101" xlink:type="simple">10.1111/cogs.12101</ext-link></comment> <object-id pub-id-type="pmid">24467492</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornstein</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Khaw</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Shohamy</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Reminders of past choices bias decisions for reward in humans</article-title>. <source>Nat Commun</source>. <year>2017</year>;<volume>8</volume>:<fpage>15958</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms15958" xlink:type="simple">10.1038/ncomms15958</ext-link></comment> <object-id pub-id-type="pmid">28653668</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A theory of how the brain might work</article-title>. <source>Cold Spring Harbor Symp Quant Biol</source>. <year>1990</year>;<volume>55</volume>:<fpage>899</fpage>–<lpage>910</lpage>. <object-id pub-id-type="pmid">2132866</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>11</issue>):<fpage>1432</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1790" xlink:type="simple">10.1038/nn1790</ext-link></comment> <object-id pub-id-type="pmid">17057707</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graf</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>2</issue>):<fpage>239</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2733" xlink:type="simple">10.1038/nn.2733</ext-link></comment> <object-id pub-id-type="pmid">21217762</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>. <article-title>Neurobiological computational systems</article-title>. <source>Computational intelligence imitating life</source>. <year>1994</year>;<fpage>213222</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zemel</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Probabilistic interpretation of population codes</article-title>. <source>Neural Comput</source>. <year>1998</year>;<volume>10</volume>(<issue>2</issue>):<fpage>403</fpage>–<lpage>30</lpage>. <object-id pub-id-type="pmid">9472488</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>(<issue>8</issue>):<fpage>1152</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4055" xlink:type="simple">10.1038/nn.4055</ext-link></comment> <object-id pub-id-type="pmid">26120962</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markowitz</surname> <given-names>H</given-names></name>. <article-title>Portfolio selection</article-title>. <source>J Finance</source>. <year>1952</year>;<volume>7</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>91</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref032"><label>32</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>EU</given-names></name>. <chapter-title>Valuation for risky and uncertain choices</chapter-title>. In: <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Fehr</surname> <given-names>E</given-names></name>, editors. <source>Neuroeconomics (Second Edition)</source>: <publisher-name>Elsevier</publisher-name>; <year>2014</year>. p. <fpage>149</fpage>–<lpage>72</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref033"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Johnson J, Wilke A, Weber EU. Beyond a trait view of risk taking: A domain-specific scale measuring risk perceptions, expected benefits, and perceived-risk attitudes in German-speaking populations. 2004.</mixed-citation></ref>
<ref id="pcbi.1007047.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burke</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>. <article-title>Reward skewness coding in the insula independent of probability and loss</article-title>. <source>J Neurophysiol</source>. <year>2011</year>;<volume>106</volume>(<issue>5</issue>):<fpage>2415</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00471.2011" xlink:type="simple">10.1152/jn.00471.2011</ext-link></comment> <object-id pub-id-type="pmid">21849610</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strait</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Hayden</surname> <given-names>BY</given-names></name>. <article-title>Preference patterns for skewed gambles in rhesus monkeys</article-title>. <source>Biol Lett</source>. <year>2013</year>;<volume>9</volume>(<issue>6</issue>):<fpage>20130902</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rsbl.2013.0902" xlink:type="simple">10.1098/rsbl.2013.0902</ext-link></comment> <object-id pub-id-type="pmid">24335272</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Symmonds</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Bach</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Deconstructing risk: separable encoding of variance and skewness in the brain</article-title>. <source>NeuroImage</source>. <year>2011</year>;<volume>58</volume>(<issue>4</issue>):<fpage>1139</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.06.087" xlink:type="simple">10.1016/j.neuroimage.2011.06.087</ext-link></comment> <object-id pub-id-type="pmid">21763444</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wright</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Symmonds</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dissociable influences of skewness and valence on economic choice and neural activity</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>12</issue>):<fpage>e83454</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0083454" xlink:type="simple">10.1371/journal.pone.0083454</ext-link></comment> <object-id pub-id-type="pmid">24376705</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Sacchet</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Knutson</surname> <given-names>B</given-names></name>. <article-title>Toward an affective neuroscience account of financial risk taking</article-title>. <source>Front Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>159</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2012.00159" xlink:type="simple">10.3389/fnins.2012.00159</ext-link></comment> <object-id pub-id-type="pmid">23129993</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Testing whether humans have an accurate model of their own motor uncertainty in a speeded reaching task</article-title>. <source>PLoS Comp Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>5</issue>):<fpage>e1003080</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Acerbi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vijayakumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>On the origins of suboptimality in human probabilistic inference</article-title>. <source>PLoS Comp Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003661</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shelton</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Sheikh</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bornschein</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lücke</surname> <given-names>J</given-names></name>. <article-title>Select and sample-a model of efficient neural inference and learning</article-title>. <source>Advances in Neural Information Processing Systems</source> <year>2011</year>. p. <fpage>2618</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref042"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Marr D. Vision: A computational approach. Freeman.[aAC] San Francisco; 1982.</mixed-citation></ref>
<ref id="pcbi.1007047.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Frazier</surname> <given-names>PI</given-names></name>. <article-title>Distance dependent Chinese restaurant processes</article-title>. <source>Journal of Machine Learning Research</source>. <year>2011</year>;<volume>12</volume>(<month>Aug</month>):<fpage>2461</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alvarez</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>The representation of simple ensemble visual features outside the focus of attention</article-title>. <source>Psychol Sci</source>. <year>2008</year>;<volume>19</volume>(<issue>4</issue>):<fpage>392</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2008.02098.x" xlink:type="simple">10.1111/j.1467-9280.2008.02098.x</ext-link></comment> <object-id pub-id-type="pmid">18399893</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubert-Wallander</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>Not all summary statistics are made equal: Evidence from extracting summaries across time</article-title>. <source>J Vis</source>. <year>2015</year>;<volume>15</volume>(<issue>4</issue>):<fpage>5</fpage>–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.4.5" xlink:type="simple">10.1167/15.4.5</ext-link></comment> <object-id pub-id-type="pmid">26053144</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Melcher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kowler</surname> <given-names>E</given-names></name>. <article-title>Shapes, surfaces and saccades</article-title>. <source>Vision Res</source>. <year>1999</year>;<volume>39</volume>(<issue>17</issue>):<fpage>2929</fpage>–<lpage>46</lpage>. <object-id pub-id-type="pmid">10492819</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vishwanath</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kowler</surname> <given-names>E</given-names></name>. <article-title>Localization of shapes: Eye movements and perception compared</article-title>. <source>Vision Res</source>. <year>2003</year>;<volume>43</volume>(<issue>15</issue>):<fpage>1637</fpage>–<lpage>53</lpage>. <object-id pub-id-type="pmid">12798146</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Juni</surname> <given-names>MZ</given-names></name>, <name name-style="western"><surname>Gureckis</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Effective integration of serially presented stochastic cues</article-title>. <source>J Vis</source>. <year>2012</year>;<volume>12</volume>(<issue>8</issue>):<fpage>12</fpage>–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/12.8.12" xlink:type="simple">10.1167/12.8.12</ext-link></comment> <object-id pub-id-type="pmid">22911906</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whitney</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yamanashi Leib</surname> <given-names>A</given-names></name>. <article-title>Ensemble perception</article-title>. <source>Annual review of psychology</source>. <year>2018</year>;<volume>69</volume>:<fpage>105</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-psych-010416-044232" xlink:type="simple">10.1146/annurev-psych-010416-044232</ext-link></comment> <object-id pub-id-type="pmid">28892638</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levy</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Markowitz</surname> <given-names>HM</given-names></name>. <article-title>Approximating expected utility by a function of mean and variance</article-title>. <source>The American Economic Review</source>. <year>1979</year>:<fpage>308</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hamilton</surname> <given-names>BH</given-names></name>. <article-title>Does entrepreneurship pay? An empirical analysis of the returns to self-employment</article-title>. <source>J Polit Economy</source>. <year>2000</year>;<volume>108</volume>(<issue>3</issue>):<fpage>604</fpage>–<lpage>31</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Siddique</surname> <given-names>A</given-names></name>. <article-title>Conditional skewness in asset pricing tests</article-title>. <source>J Finance</source>. <year>2000</year>;<volume>55</volume>(<issue>3</issue>):<fpage>1263</fpage>–<lpage>95</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kraus</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litzenberger</surname> <given-names>RH</given-names></name>. <article-title>Skewness preference and the valuation of risk assets</article-title>. <source>J Finance</source>. <year>1976</year>;<volume>31</volume>(<issue>4</issue>):<fpage>1085</fpage>–<lpage>100</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moskowitz</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Vissing-Jørgensen</surname> <given-names>A</given-names></name>. <article-title>The returns to entrepreneurial investment: A private equity premium puzzle?</article-title> <source>Amer Econ Rev</source>. <year>2002</year>;<volume>92</volume>(<issue>4</issue>):<fpage>745</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chiu</surname> <given-names>WH</given-names></name>. <article-title>Skewness preference, risk taking and expected utility maximisation</article-title>. <source>Geneva Risk Insur Rev</source>. <year>2010</year>;<volume>35</volume>(<issue>2</issue>):<fpage>108</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Knutson</surname> <given-names>B</given-names></name>. <article-title>The affective impact of financial skewness on neural activity and choice</article-title>. <source>PloS one</source>. <year>2011</year>;<volume>6</volume>(<issue>2</issue>):<fpage>e16838</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0016838" xlink:type="simple">10.1371/journal.pone.0016838</ext-link></comment> <object-id pub-id-type="pmid">21347239</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Genest</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Stauffer</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Utility functions predict variance and skewness risk preferences in monkeys</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2016</year>;<volume>113</volume>(<issue>30</issue>):<fpage>8402</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1602217113" xlink:type="simple">10.1073/pnas.1602217113</ext-link></comment> <object-id pub-id-type="pmid">27402743</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orhan</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>RA</given-names></name>. <article-title>A probabilistic clustering theory of the organization of visual short-term memory</article-title>. <source>Psychol Rev</source>. <year>2013</year>;<volume>120</volume>(<issue>2</issue>):<fpage>297</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0031541" xlink:type="simple">10.1037/a0031541</ext-link></comment> <object-id pub-id-type="pmid">23356778</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Monfils</surname> <given-names>M-H</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>The computational nature of memory modification</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>:<fpage>e23763</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.23763" xlink:type="simple">10.7554/eLife.23763</ext-link></comment> <object-id pub-id-type="pmid">28294944</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Navarro</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Steyvers</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>MD</given-names></name>. <article-title>Modeling individual differences using Dirichlet processes</article-title>. <source>J Math Psychol</source>. <year>2006</year>;<volume>50</volume>(<issue>2</issue>):<fpage>101</fpage>–<lpage>22</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>A new look at the statistical model identification</article-title>. <source>IEEE Trans Automat Contr</source>. <year>1974</year>;<volume>19</volume>(<issue>6</issue>):<fpage>716</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hurvich</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Tsai</surname> <given-names>C-L</given-names></name>. <article-title>Regression and time series model selection in small samples</article-title>. <source>Biometrika</source>. <year>1989</year>;<volume>76</volume>(<issue>2</issue>):<fpage>297</fpage>–<lpage>307</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Bayesian model selection for group studies</article-title>. <source>NeuroImage</source>. <year>2009</year>;<volume>46</volume>(<issue>4</issue>):<fpage>1004</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.03.025" xlink:type="simple">10.1016/j.neuroimage.2009.03.025</ext-link></comment> <object-id pub-id-type="pmid">19306932</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>. <article-title>Bayesian model selection for group studies—revisited</article-title>. <source>NeuroImage</source>. <year>2014</year>;<volume>84</volume>:<fpage>971</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2013.08.065" xlink:type="simple">10.1016/j.neuroimage.2013.08.065</ext-link></comment> <object-id pub-id-type="pmid">24018303</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Ubiquitous log odds: a common representation of probability and frequency distortion in perception, action, and cognition</article-title>. <source>Front Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2012.00001" xlink:type="simple">10.3389/fnins.2012.00001</ext-link></comment> <object-id pub-id-type="pmid">22294978</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mach</surname> <given-names>E</given-names></name>. <article-title>On the effect of the spatial distribution of the light stimulus on the retina</article-title>. <source>Mach bands</source>. <year>1965</year>:<fpage>253</fpage>–<lpage>71</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Almeida</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Barbosa</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Compte</surname> <given-names>A</given-names></name>. <article-title>Neural circuit basis of visuo-spatial working memory precision: a computational and behavioral study</article-title>. <source>J Neurophysiol</source>. <year>2015</year>;<volume>114</volume>(<issue>3</issue>):<fpage>1806</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00362.2015" xlink:type="simple">10.1152/jn.00362.2015</ext-link></comment> <object-id pub-id-type="pmid">26180122</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tversky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>. <article-title>Advances in prospect theory: Cumulative representation of uncertainty</article-title>. <source>J Risk Uncertain</source>. <year>1992</year>;<volume>5</volume>(<issue>4</issue>):<fpage>297</fpage>–<lpage>323</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gonzalez</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>G</given-names></name>. <article-title>On the shape of the probability weighting function</article-title>. <source>Cogn Psychol</source>. <year>1999</year>;<volume>38</volume>(<issue>1</issue>):<fpage>129</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/cogp.1998.0710" xlink:type="simple">10.1006/cogp.1998.0710</ext-link></comment> <object-id pub-id-type="pmid">10090801</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Åstebro</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mata</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Santos-Pinto</surname> <given-names>L</given-names></name>. <article-title>Skewness seeking: risk loving, optimism or overweighting of small probabilities?</article-title> <source>Theory</source> <month>Dec</month>. <year>2015</year>;<volume>78</volume>(<issue>2</issue>):<fpage>189</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Apicella</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Scarnati</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ljungberg</surname> <given-names>T</given-names></name>. <article-title>Neuronal activity in monkey ventral striatum related to the expectation of reward</article-title>. <source>J Neurosci</source>. <year>1992</year>;<volume>12</volume>(<issue>12</issue>):<fpage>4595</fpage>–<lpage>610</lpage>. <object-id pub-id-type="pmid">1464759</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'doherty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Deichmann</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>Science</source>. <year>2004</year>;<volume>304</volume>(<issue>5669</issue>):<fpage>452</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1094285" xlink:type="simple">10.1126/science.1094285</ext-link></comment> <object-id pub-id-type="pmid">15087550</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref073"><label>73</label><mixed-citation publication-type="other" xlink:type="simple">Köppen M. The curse of dimensionality. 5th Online World Conference on Soft Computing in Industrial Applications (WSC5); 4–8 September; IEEE Finland Section2000.</mixed-citation></ref>
<ref id="pcbi.1007047.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitzgerald</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Rowekamp</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Sincich</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title>. <source>PLoS Comp Biol</source>. <year>2011</year>;<volume>7</volume>(<issue>10</issue>):<fpage>e1002249</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref075"><label>75</label><mixed-citation publication-type="other" xlink:type="simple">Gershman S, Wilson R. The neural costs of optimal control. Advances in neural information processing systems, 23, Neural Information Processing Systems Foundation2010. p. 712–20.</mixed-citation></ref>
<ref id="pcbi.1007047.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mnih</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kavukcuoglu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rusu</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Veness</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bellemare</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>(<issue>7540</issue>):<fpage>529</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14236" xlink:type="simple">10.1038/nature14236</ext-link></comment> <object-id pub-id-type="pmid">25719670</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref077"><label>77</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <chapter-title>Semi-rational models of conditioning: The case of trial order</chapter-title>. In: <name name-style="western"><surname>Chater</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Oaksford</surname> <given-names>M</given-names></name>, editors. <source>The Probabilistic Mind: Prospects for Bayesian Cognitive Science</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2008</year>. p. <fpage>431</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanborn</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Navarro</surname> <given-names>DJ</given-names></name>. <article-title>Rational approximations to rational models: alternative algorithms for category learning</article-title>. <source>Psychol Rev</source>. <year>2010</year>;<volume>117</volume>(<issue>4</issue>):<fpage>1144</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0020511" xlink:type="simple">10.1037/a0020511</ext-link></comment> <object-id pub-id-type="pmid">21038975</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schustek</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>. <article-title>Instance-based generalization for human judgments about uncertainty</article-title>. <source>PLoS Comp Biol</source>. <year>2018</year>;<volume>14</volume>(<issue>6</issue>):<fpage>e1006205</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Helmers</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Chunking as a rational strategy for lossy data compression in visual working memory</article-title>. <source>bioRxiv</source>. <year>2017</year>:<fpage>098939</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Lazarus</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Polyn</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kahana</surname> <given-names>MJ</given-names></name>. <article-title>Spatial clustering during memory search</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>. <year>2013</year>;<volume>39</volume>(<issue>3</issue>):<fpage>773</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0029684" xlink:type="simple">10.1037/a0029684</ext-link></comment> <object-id pub-id-type="pmid">22905933</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farrell</surname> <given-names>S</given-names></name>. <article-title>Temporal clustering and sequencing in short-term memory and episodic memory</article-title>. <source>Psychol Rev</source>. <year>2012</year>;<volume>119</volume>(<issue>2</issue>):<fpage>223</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0027371" xlink:type="simple">10.1037/a0027371</ext-link></comment> <object-id pub-id-type="pmid">22506678</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>The loss function of sensorimotor learning</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2004</year>;<volume>101</volume>(<issue>26</issue>):<fpage>9839</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0308394101" xlink:type="simple">10.1073/pnas.0308394101</ext-link></comment> <object-id pub-id-type="pmid">15210973</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ting</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>S-W</given-names></name>. <article-title>Neural mechanisms for integrating prior knowledge and likelihood in value-based probabilistic inference</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>4</issue>):<fpage>1792</fpage>–<lpage>805</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3161-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3161-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25632152</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yeung</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Whalen</surname> <given-names>A</given-names></name>. <article-title>Learning of bimodally distributed quantities. Proceedings of the 37th</article-title> <source>Annual Conference of the Cognitive Science Society</source><year>2015</year>. p. <fpage>2745</fpage>–<lpage>50</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Vision</surname> <given-names>S</given-names></name>. <article-title>The psychophysics toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>:<fpage>433</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname> <given-names>DG</given-names></name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">9176953</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleiner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brainard</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pelli</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ingling</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Broussard</surname> <given-names>C</given-names></name>. <article-title>What’s new in Psychtoolbox-3</article-title>. <source>Perception</source>. <year>2007</year>;<volume>36</volume>(<issue>14</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botev</surname> <given-names>ZI</given-names></name>, <name name-style="western"><surname>Grotowski</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Kroese</surname> <given-names>DP</given-names></name>. <article-title>Kernel density estimation via diffusion</article-title>. <source>The annals of Statistics</source>. <year>2010</year>;<volume>38</volume>(<issue>5</issue>):<fpage>2916</fpage>–<lpage>57</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petzschner</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Glasauer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>. <article-title>A Bayesian perspective on magnitude estimation</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year>;<volume>19</volume>(<issue>5</issue>):<fpage>285</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2015.03.002" xlink:type="simple">10.1016/j.tics.2015.03.002</ext-link></comment> <object-id pub-id-type="pmid">25843543</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevens</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Galanter</surname> <given-names>EH</given-names></name>. <article-title>Ratio scales and category scales for a dozen perceptual continua</article-title>. <source>J Exp Psychol</source>. <year>1957</year>;<volume>54</volume>(<issue>6</issue>):<fpage>377</fpage>–<lpage>411</lpage>. <object-id pub-id-type="pmid">13491766</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref092"><label>92</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <source>Sensation and perception</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Worth Publisher</publisher-name>; <year>2014</year>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blakemore</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Georgeson</surname> <given-names>MA</given-names></name>. <article-title>Lateral inhibition between orientation detectors in the human visual system</article-title>. <source>Nature</source>. <year>1970</year>;<volume>228</volume>(<issue>5266</issue>):<fpage>37</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">5456209</object-id></mixed-citation></ref>
<ref id="pcbi.1007047.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neal</surname> <given-names>RM</given-names></name>. <article-title>Markov chain sampling methods for Dirichlet process mixture models</article-title>. <source>Journal of computational and graphical statistics</source>. <year>2000</year>;<volume>9</volume>(<issue>2</issue>):<fpage>249</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007047.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Temporal context calibrates interval timing</article-title>. <source>Nature neuroscience</source>. <year>2010</year>;<volume>13</volume>(<issue>8</issue>):<fpage>1020</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2590" xlink:type="simple">10.1038/nn.2590</ext-link></comment> <object-id pub-id-type="pmid">20581842</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>