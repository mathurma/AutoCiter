<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007263</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-00289</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>System instability</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>System instability</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI</article-title>
<alt-title alt-title-type="running-head">Identifying nonlinear dynamical systems via generative RNNs with applications to fMRI</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2941-9238</contrib-id>
<name name-style="western">
<surname>Koppe</surname>
<given-names>Georgia</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2655-3071</contrib-id>
<name name-style="western">
<surname>Toutounji</surname>
<given-names>Hazem</given-names>
</name>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kirsch</surname>
<given-names>Peter</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8051-2756</contrib-id>
<name name-style="western">
<surname>Lis</surname>
<given-names>Stefanie</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Clinical Psychology, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Institute for Psychiatric and Psychosomatic Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Isik</surname>
<given-names>Leyla</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>MIT, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">georgia.koppe@zi-mannheim.de</email> (GK); <email xlink:type="simple">daniel.durstewitz@zi-mannheim.de</email> (DD)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>21</day>
<month>8</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>8</issue>
<elocation-id>e1007263</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>2</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>7</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Koppe et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007263"/>
<abstract>
<p>A major tenet in theoretical neuroscience is that cognitive and behavioral processes are ultimately implemented in terms of the neural system dynamics. Accordingly, a major aim for the analysis of neurophysiological measurements should lie in the identification of the computational dynamics underlying task processing. Here we advance a state space model (SSM) based on generative piecewise-linear recurrent neural networks (PLRNN) to assess dynamics from neuroimaging data. In contrast to many other nonlinear time series models which have been proposed for reconstructing latent dynamics, our model is easily interpretable in neural terms, amenable to systematic dynamical systems analysis of the resulting set of equations, and can straightforwardly be transformed into an equivalent continuous-time dynamical system. The major contributions of this paper are the introduction of a new observation model suitable for functional magnetic resonance imaging (fMRI) coupled to the latent PLRNN, an efficient stepwise training procedure that forces the latent model to capture the ‘true’ underlying dynamics rather than just fitting (or predicting) the observations, and of an empirical measure based on the Kullback-Leibler divergence to evaluate from empirical time series how well this goal of approximating the underlying dynamics has been achieved. We validate and illustrate the power of our approach on simulated ‘ground-truth’ dynamical systems as well as on experimental fMRI time series, and demonstrate that the learnt dynamics harbors task-related nonlinear structure that a linear dynamical model fails to capture. Given that fMRI is one of the most common techniques for measuring brain activity non-invasively in human subjects, this approach may provide a novel step toward analyzing aberrant (nonlinear) dynamics for clinical assessment or neuroscientific research.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computational processes in the brain are often assumed to be implemented in terms of nonlinear neural network dynamics. However, experimentally we usually do not have direct access to this underlying dynamical process that generated the observed time series, but have to infer it from a sample of noisy and mixed measurements like fMRI data. Here we combine a dynamically universal recurrent neural network (RNN) model for approximating the unknown system dynamics with an observation model that links this dynamics to experimental measurements, taking fMRI data as an example. We develop a new stepwise optimization algorithm, within the statistical framework of state space models, that forces the latent RNN model toward the true data-generating dynamical process, and demonstrate its power on benchmark systems like the chaotic Lorenz attractor. We also introduce a novel, fast-to-compute measure for assessing how well this worked out in any empirical situation for which the ground truth dynamical system is not known. RNN models trained on human fMRI data this way can generate new data with the same temporal structure and properties, and exhibit interesting nonlinear dynamical phenomena related to experimental task conditions and behavioral performance. This approach can easily be generalized to many other recording modalities.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>Du 354/8-2</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>Du 354/10-1</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002347</institution-id>
<institution>Bundesministerium für Bildung und Forschung</institution>
</institution-wrap>
</funding-source>
<award-id>01ZX1311A</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002347</institution-id>
<institution>Bundesministerium für Bildung und Forschung</institution>
</institution-wrap>
</funding-source>
<award-id>01ZX1314G</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded by grants from the German Research Foundation (DFG, Du 354/8-2, Du 354/10-1, <ext-link ext-link-type="uri" xlink:href="http://www.dfg.de" xlink:type="simple">www.dfg.de</ext-link>) and the German Federal Ministry of Education and Research (BMBF, e:Med 01ZX1311A and 01ZX1314G, <ext-link ext-link-type="uri" xlink:href="http://www.bmbf.de" xlink:type="simple">www.bmbf.de</ext-link>) to DD. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="35"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-09-03</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Code for the developed PLRNN-SSM methods and data analysed in the current manuscript may be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/DurstewitzLab/PLRNN_SSM" xlink:type="simple">https://github.com/DurstewitzLab/PLRNN_SSM</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/GKoppe/PLRNN_SSM" xlink:type="simple">https://github.com/GKoppe/PLRNN_SSM</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A central tenet in computational neuroscience is that computational processes in the brain are ultimately implemented through (stochastic) nonlinear neural system dynamics [<xref ref-type="bibr" rid="pcbi.1007263.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref003">3</xref>]. This idea reaches from Hopfield’s [<xref ref-type="bibr" rid="pcbi.1007263.ref004">4</xref>] early proposal on memory patterns as fixed point attractors in recurrent neural networks, working memory as rate attractors [<xref ref-type="bibr" rid="pcbi.1007263.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref006">6</xref>], decision making as stochastic transitions among competing attractor states [<xref ref-type="bibr" rid="pcbi.1007263.ref007">7</xref>], motor or thought sequences as limit cycles or heteroclinic chains of saddle nodes [<xref ref-type="bibr" rid="pcbi.1007263.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref009">9</xref>], to the role of line attractors in parametric working memory [<xref ref-type="bibr" rid="pcbi.1007263.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref012">12</xref>], neural integration [<xref ref-type="bibr" rid="pcbi.1007263.ref013">13</xref>], interval timing [<xref ref-type="bibr" rid="pcbi.1007263.ref014">14</xref>], and more recent thoughts on the role of <italic>transient</italic> dynamics in cognitive processing [<xref ref-type="bibr" rid="pcbi.1007263.ref015">15</xref>]. To test and further develop such theories, methods for directly assessing system dynamics from neural measurements would be of great value.</p>
<p>Traditionally, mostly linear approaches like linear (Gaussian or Gaussian-Poisson) state space models [<xref ref-type="bibr" rid="pcbi.1007263.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref019">19</xref>], Gaussian Process Factor Analysis [GPFA; <xref ref-type="bibr" rid="pcbi.1007263.ref020">20</xref>], Dynamic Causal Modeling [DCM; <xref ref-type="bibr" rid="pcbi.1007263.ref021">21</xref>], or (nonlinear, but model-free) delay embedding techniques [<xref ref-type="bibr" rid="pcbi.1007263.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref023">23</xref>], have been used for reconstructing state space trajectories from <italic>experimental</italic> recordings. While these are powerful visualization tools that may also give some insight into system parameters, like connectivity [<xref ref-type="bibr" rid="pcbi.1007263.ref021">21</xref>], linear dynamical systems (DS) are <italic>inherently</italic> very limited with regards to the range of dynamical phenomena they can produce [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref024">24</xref>]. The representation of smoothed trajectories in the latent space may still inform the researcher about interesting aspects of the dynamics, but the inferred latent model on its own is not powerful enough to reproduce many interesting and computationally important phenomena like multi-stability, complex limit cycles, or chaos [<xref ref-type="bibr" rid="pcbi.1007263.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref025">25</xref>]. More formally, given experimental observations <bold>X</bold> = {<bold>x</bold><sub><italic>t</italic></sub>} supposedly generated by some underlying latent dynamical process <bold>Z</bold> = {<bold>z</bold><sub><italic>t</italic></sub>} (<xref ref-type="fig" rid="pcbi.1007263.g001">Fig 1</xref>), linear state space models may yield a useful approximation to the posterior <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), but–due to their linear limitations–they will not produce an adequate mathematical model of the prior dynamics <italic>p</italic>(<bold>Z</bold>) that could generate the actual observations via <italic>p</italic>(<bold>X</bold>|<bold>Z</bold>).</p>
<fig id="pcbi.1007263.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Analysis pipeline.</title>
<p>Top: Analysis pipeline for simulated data. From the two benchmark systems (van der Pol and Lorenz systems), noisy trajectories were drawn and handed over to the PLRNN-SSM inference algorithm. With the inferred model parameters, completely new trajectories were generated and compared to the state space distribution over true trajectories via the Kullback-Leibler divergence <italic>KL</italic><sub><bold>x</bold></sub> (see <xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>). Bottom: analysis pipeline for experimental data. We used preprocessed fMRI data from human subjects undergoing a classic working memory n-back paradigm. First, nuisance variables, in this case related to movement, were collected. Then, time series obtained from regions of interest (ROI) were extracted, standardized, and filtered (in agreement with the study design). From these preprocessed time series, we derived the first principle components and handed them to the inference algorithm (once including and once excluding variables indicating external stimulus presentations during the experiment). With the inferred parameters, the system was then run freely to produce new trajectories which were compared to the state space distribution from the inferred trajectories via the Kullback-Leibler divergence <italic>KL</italic><sub>z</sub> (see <xref ref-type="disp-formula" rid="pcbi.1007263.e078">Eq 11</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g001" xlink:type="simple"/>
</fig>
<p>In contrast, recurrent neural networks (RNNs) represent a class of nonlinear DS models which are universal in the sense that they can approximate arbitrarily closely the flow of any other dynamical system [<xref ref-type="bibr" rid="pcbi.1007263.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref028">28</xref>]. Hence, RNNs are, in theory, sufficiently powerful to emulate any type of brain dynamics. Based on previous work embedding RNNs into a statistical inference framework [<xref ref-type="bibr" rid="pcbi.1007263.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>], we have recently developed a nonlinear state space model utilizing piecewise-linear RNNs (PLRNNs) for the latent dynamical process [<xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>]. In state space models, similar to sequential variational auto-encoders (VAE) [<xref ref-type="bibr" rid="pcbi.1007263.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref034">34</xref>], one attempts to infer the system parameters <bold>θ</bold> by maximizing a lower bound on the log-likelihood log <italic>p</italic>(<bold>X</bold>|<bold>θ</bold>). In contrast to many other RNN-based approaches [<xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref035">35</xref>], including current sequential VAE implementations [<xref ref-type="bibr" rid="pcbi.1007263.ref036">36</xref>], our method returns a set of neuronally interpretable and partly analytically tractable dynamical equations that could be used to gain further insight into the generating system.</p>
<p>The present work further advances this powerful methodology along three major directions: First, we develop a stepwise initialization and training scheme that forces the latent PLRNN model toward the correct underlying dynamics: Good prediction of the time series observations and informative smooth latent trajectories may be achieved even without recreating a sufficiently good approximation to the underlying DS (as evidenced by the success of linear state space models). Through a kind of annealing protocol that places increasingly more burden of predicting the observations onto the latent process model, we enforce the correct dynamics. Second, we show that a Kullback-Leibler divergence defined across state space between the prior generative model dynamics <italic>p</italic>(<bold>Z</bold>) (independent of the observations) and the inferred latent states given the observations, <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), provides a good measure for how well the reconstructed DS (emulated by the PLRNN) can be expected to have captured the correct underlying system. Hence, our approach, rather than just inferring the latent space underlying the observations, attempts to force the system to capture the correct dynamics in its governing equations, and provides a quantitative sense of how well this worked for any empirically observed system for which the ground truth is not known. Third, given that fMRI is likely the most important non-invasive technique for gaining insight into human brain function in healthy subjects and psychiatric illness, we provide an observation (‘decoder’) model for the PLRNN that takes the hemodynamic response filtering into account.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>PLRNN-based state space model (PLRNN-SSM)</title>
<p>We start by introducing our nonlinear state space model (SSM) and statistical inference framework [originally developed in <xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>]. Within a SSM, one aims to predict observed experimental time series <inline-formula id="pcbi.1007263.e001"><alternatives><graphic id="pcbi.1007263.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> from a set of latent variables <inline-formula id="pcbi.1007263.e002"><alternatives><graphic id="pcbi.1007263.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> (where usually <italic>M</italic>≠<italic>N</italic>) and their temporal dynamics. Here we use a piecewise-linear (or, strictly, piecewise-affine) recurrent neural network (PLRNN) (i.e., a RNN composed of rectified-linear units [ReLUs]) for modeling the unknown latent dynamics:
<disp-formula id="pcbi.1007263.e003">
<alternatives>
<graphic id="pcbi.1007263.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi>φ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">ε</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.50em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">ε</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1007263.e004">
<alternatives>
<graphic id="pcbi.1007263.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <bold>z</bold><sub><italic>t</italic></sub> is the latent state vector at time <italic>t</italic> = 1…<italic>T</italic>, <inline-formula id="pcbi.1007263.e005"><alternatives><graphic id="pcbi.1007263.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is a diagonal matrix of (linear) auto-regression weights, <inline-formula id="pcbi.1007263.e006"><alternatives><graphic id="pcbi.1007263.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> an off-diagonal matrix of connection weights, and <italic>φ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) = max(<bold>z</bold><sub><italic>t</italic></sub>,0) is an (element-wise) ReLU transfer function. <inline-formula id="pcbi.1007263.e007"><alternatives><graphic id="pcbi.1007263.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> denotes time-dependent external inputs that influence latent states through coefficient matrix <inline-formula id="pcbi.1007263.e008"><alternatives><graphic id="pcbi.1007263.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mi mathvariant="bold">C</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, and <bold>ε</bold><sub><italic>t</italic></sub> is a Gaussian white noise process with diagonal covariance matrix <bold>Σ</bold>. (The basic model was modified from Durstewitz [<xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>] to enable efficient estimation of bias parameters <bold>h</bold> and speeding up the inference algorithm by orders of magnitude.) The diagonal and off-diagonal structure of <bold>A</bold> and <bold>W</bold>, respectively, help to ensure that system parameters remain identifiable. Although here we advance model (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) mainly as a tool for approximating unknown dynamical systems, it may be interpreted as a neural rate model [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref038">38</xref>], with <bold>A</bold> the units’ passive time constants, <bold>W</bold> the synaptic coupling matrix, and <italic>φ</italic>(<bold>z</bold>) a current/voltage to spike rate transfer function which for cortical pyramidal cells is often non-saturating and close to a ReLU within the physiologically relevant regime [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref039">39</xref>].</p>
<p>The observed time series are generated from the ReLU-transformed latent states (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) through a linear-Gaussian model:
<disp-formula id="pcbi.1007263.e009">
<alternatives>
<graphic id="pcbi.1007263.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>φ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.50em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <bold>x</bold><sub><italic>t</italic></sub> are the observed <italic>N</italic>-dimensional measurements at time <italic>t</italic> generated from <bold>z</bold><sub><italic>t</italic></sub>, <inline-formula id="pcbi.1007263.e010"><alternatives><graphic id="pcbi.1007263.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi mathvariant="bold">B</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is a matrix of regression weights (factor loadings), and <bold>η</bold><sub><italic>t</italic></sub> denotes a Gaussian white observation noise process with diagonal covariance matrix <bold>Γ</bold>.</p>
<p>Thus, the model is specified by the set of parameters <bold>θ</bold> = {<bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>C</bold>,<bold>h</bold>,<bold>B</bold>,<bold>Γ</bold>,<bold>Σ</bold>}, and we are interested in recovering <bold>θ</bold> as well as the posterior distribution <italic>p</italic>(<bold>Z</bold>|<bold>X)</bold> over the latent state path <bold>Z</bold> = {<bold>z</bold><sub>1:<italic>T</italic></sub>} from the experimentally observed time series <bold>X</bold> = {<bold>x</bold><sub>1:<italic>T</italic></sub>} and experimental inputs <bold>S</bold> = {<bold>s</bold><sub>1:<italic>T</italic></sub>}. In the following, we will sometimes use the notation <bold>θ</bold><sub><italic>lat</italic></sub> = {<bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>C</bold>,<bold>h</bold>,<bold>Σ</bold>} and <bold>θ</bold><sub><italic>obs</italic></sub> = {<bold>B</bold>,<bold>Γ</bold>} to exclusively refer to parameters in the evolution or observation equation, respectively.</p>
</sec>
<sec id="sec004">
<title>Observation model for BOLD time series</title>
<p>An appealing feature of the SSM framework is that different measurement modalities and properties can be accommodated by connecting different observation models to the same latent model. In order to apply our model to fMRI time series, we need only to adapt observation <xref ref-type="disp-formula" rid="pcbi.1007263.e009">Eq 2</xref> to meet the distributional assumptions and temporal filtering of the blood-oxygen-level dependent (BOLD) signal, while retaining process <xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref> with its universal approximation capabilities. In contrast to electrophysiological measurements, BOLD time-series are a strongly filtered, highly smoothed version of some underlying neural process, only accessible through the hemodynamic response function (HRF) [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref040">40</xref>]. Hence, we modified the observation model (<xref ref-type="disp-formula" rid="pcbi.1007263.e009">Eq 2</xref>) such that the observed BOLD signal is generated from the latent states (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) through a linear-Gaussian model with HRF convolution:
<disp-formula id="pcbi.1007263.e011">
<alternatives>
<graphic id="pcbi.1007263.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.50em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">η</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <bold>x</bold><sub><italic>t</italic></sub> are the observed BOLD responses in <italic>N</italic> voxels at time <italic>t</italic> generated from <bold>z</bold><sub><italic>τ</italic>:<italic>t</italic></sub> (concatenated into one vector and convolved with the hemodynamic response function). We also added nuisance predictors <inline-formula id="pcbi.1007263.e012"><alternatives><graphic id="pcbi.1007263.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, which account for artifacts caused, e.g., by movements. <inline-formula id="pcbi.1007263.e013"><alternatives><graphic id="pcbi.1007263.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi mathvariant="bold">J</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is the coefficient matrix of these nuisance variables, and <bold>B</bold>,<bold>Γ</bold> and <bold>η</bold><sub><italic>t</italic></sub> are the same as in <xref ref-type="disp-formula" rid="pcbi.1007263.e009">Eq 2</xref>. Hence, the observation model takes the typical form of a General Linear Model for BOLD signal analysis as, e.g., implemented in the statistical parametric mapping (SPM) framework [<xref ref-type="bibr" rid="pcbi.1007263.ref040">40</xref>]. Note that while nuisance variables are assumed to directly blur into the observed signals (they do not affect the neural dynamics but rather the recording process), external stimuli presented to the subjects are, in contrast, assumed to exert their effects through the underlying neuronal dynamics (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>). Thus, the fMRI PLRNN-SSM (termed ‘PLRNN-BOLD-SSM’) is now specified by the set of parameters <bold>θ</bold> = {<bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>C</bold>,<bold>h</bold>,<bold>B</bold>,<bold>J</bold>,<bold>Γ</bold>,<bold>Σ</bold>}. Model inference is performed through a type of Expectation-Maximization (EM) algorithm (see <xref ref-type="sec" rid="sec014">Methods</xref> and full derivations in supporting file <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref>).</p>
<p>One complication here is that the observations in <xref ref-type="disp-formula" rid="pcbi.1007263.e011">Eq 3</xref> do not just depend on the current state <bold>z</bold><sub><italic>t</italic></sub> as in a conventional SSM, but on a set of states <bold>z</bold><sub><italic>τ</italic>:<italic>t</italic></sub> across several previous time steps. This severely complicates standard solution techniques for the E-step like extended or unscented Kalman filtering [<xref ref-type="bibr" rid="pcbi.1007263.ref041">41</xref>]. Our E-step procedure [cf. <xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>], however, combines a global Laplace approximation with an efficient iterative (fixed point-type) mode search algorithm that exploits the sparse, block-banded structure of the involved covariance (inverse Hessian) matrices, which is more easily adapted for the current situation with longer-term temporal dependencies (see <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Model specification and inference’ &amp; <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref> for further details).</p>
</sec>
<sec id="sec005">
<title>Stepwise initialization and training protocol</title>
<p>The EM-algorithm aims to compute (in the linear case) or approximate the posterior distribution <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) of the latent states given the observations in the E-step, in order to maximize the expected joint log-likelihood E<sub><italic>q</italic>(<bold>Z</bold>|<bold>X</bold>)</sub>[log <italic>p</italic><sub><bold>θ</bold></sub>(<bold>Z</bold>,<bold>X</bold>)] with respect to the unknown model parameters <bold>θ</bold> under this approximate posterior <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>)≈<italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) in the M-step (by doing so, a lower bound of the log-likelihood log <italic>p</italic>(<bold>X</bold>|<bold>θ</bold>)≥E<sub><italic>q</italic></sub>[log <italic>p</italic>(<bold>Z</bold>,<bold>X</bold>)]−E<sub><italic>q</italic></sub>[log <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>)] is maximized, see <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Parameter estimation’ &amp; <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref>). This does not by itself guarantee that the latent system on its own, as represented by the prior distribution <inline-formula id="pcbi.1007263.e014"><alternatives><graphic id="pcbi.1007263.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, provides a good incarnation of the true but unobserved DS that generated the observations <bold>X</bold>. As for any nonlinear neural network model, the log-likelihood landscape for our model is complicated and usually contains many local modes, very flat and saddle regions [<xref ref-type="bibr" rid="pcbi.1007263.ref042">42</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref045">45</xref>]. Since E<sub><italic>q</italic></sub>[log <italic>p</italic>(<bold>Z</bold>,<bold>X</bold>)] = E<sub><italic>q</italic></sub>[log <italic>p</italic>(<bold>X</bold>|<bold>Z</bold>)]+ E<sub><italic>q</italic></sub>[log <italic>p</italic>(<bold>Z</bold>)], with the expectation taken across <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>)≈<italic>p</italic>(<bold>Z</bold>|<bold>X</bold>)∝<italic>p</italic>(<bold>X</bold>|<bold>Z</bold>)<italic>p</italic>(<bold>Z</bold>), the inference procedure may easily get stuck in local maxima in which high likelihood values are attained by finding parameter and state configurations which overemphasize fitting the observations, <italic>p</italic>(<bold>X</bold>|<bold>Z</bold>), rather than capturing the underlying dynamics in <italic>p</italic>(<bold>Z</bold>) (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>; see <xref ref-type="sec" rid="sec014">Methods</xref> for more details). To address this issue, we here propose a step-wise training by annealing protocol (termed ‘PLRNN-SSM-anneal’, Algorithm-1 in Methods) which systematically varies the trade-off between fitting the observations (maximizing <italic>p</italic>(<bold>X</bold>|<bold>Z</bold>); Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e011">3</xref>) as compared to fitting the dynamics (<italic>p</italic>(<bold>Z</bold>); <xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) in successive optimization steps [see also <xref ref-type="bibr" rid="pcbi.1007263.ref046">46</xref>]. In brief, while early steps of the training scheme prioritize the fit to the observed measurements through the observation (or ‘decoder’) model <italic>p</italic>(<bold>X</bold>|<bold>Z</bold>) (Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e011">3</xref>), subsequent annealing steps shift the burden of reproducing the observations onto the latent model <italic>p</italic>(<bold>Z</bold>) (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) by, at some point, fixing the observation parameters <bold>θ</bold><sub><italic>obs</italic></sub>, and then enforcing the temporal consistency within the latent model equations (as demanded by <xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>) by gradually boosting the contribution of this term to the log-likelihood (see <xref ref-type="sec" rid="sec014">Methods</xref>).</p>
</sec>
<sec id="sec006">
<title>Evaluation of training protocol</title>
<p>We examined the performance of this annealing protocol in terms of how well the inferred model was capable of recovering the true underlying dynamics of the Lorenz system. This 3-dimensional benchmark system (equations and parameter values used given in <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref> legend), conceived by Edward Lorenz in 1963 to describe atmospheric convection [<xref ref-type="bibr" rid="pcbi.1007263.ref047">47</xref>], exhibits chaotic behavior in certain regimes (see, e.g., <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4A</xref>). We measured the quality of DS reconstruction by the Kullback-Leibler divergence <italic>KL</italic><sub><bold>x</bold></sub>(<italic>p</italic><sub><italic>true</italic></sub>(<bold>x</bold>),<italic>p</italic><sub><italic>gen</italic></sub>(<bold>x</bold>|<bold>z</bold>)) between the spatial probability distributions <italic>p</italic><sub><italic>true</italic></sub>(<bold>x</bold>) over observed system states in <bold>x</bold>-space from trajectories produced by the (true) Lorenz system and <italic>p</italic><sub><italic>gen</italic></sub>(<bold>x</bold>|<bold>z</bold>) from trajectories generated by the trained PLRNN-SSM (<italic>KL</italic><sub><bold>x</bold></sub>, in the following refers to this divergence evaluated in observation space, see (Eq 9) in <xref ref-type="sec" rid="sec014">Methods</xref>, where <inline-formula id="pcbi.1007263.e015"><alternatives><graphic id="pcbi.1007263.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> denotes a normalized version of this measure; see <xref ref-type="fig" rid="pcbi.1007263.g001">Fig 1</xref> and <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Reconstruction of benchmark dynamical systems’ for details). Hence, importantly, our measure compares the dynamical behavior in <italic>state space</italic>, i.e. focuses on the agreement between attractor (or, more generally trajectory) <italic>geometries</italic>, similar in spirit to the delay embedding theorems (which ensure topological equivalence) [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref050">50</xref>], instead of comparing the fit directly on the time series themselves which can be highly misleading for chaotic systems because of the exponential divergence of nearby trajectories [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref051">51</xref>], as illustrated in <xref ref-type="fig" rid="pcbi.1007263.g002">Fig 2A</xref>. Note that for a (deterministic, autonomous) dynamical system the flow at each point in state space is uniquely determined [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref024">24</xref>] and induces a specific spatial distribution of states, in this sense translates aspects of the temporal dynamics into a specific spatial geometry. <xref ref-type="fig" rid="pcbi.1007263.g002">Fig 2B</xref> gives examples where our measure <inline-formula id="pcbi.1007263.e016"><alternatives><graphic id="pcbi.1007263.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> correctly indicates whether the Lorenz attractor geometry was properly mapped by a trained PLRNN, while a direct evaluation of the time series fit (incorrectly) indicated the contrary.</p>
<fig id="pcbi.1007263.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Illustration of DS reconstruction measures defined in state space (<inline-formula id="pcbi.1007263.e017"><alternatives><graphic id="pcbi.1007263.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) vs. on the time series (mean squared error; MSE).</title>
<p>A. Two noise-free time series from the Lorenz equations started from slightly different initial conditions. Although initially the two time series (blue and yellow) stay closely together (low MSE), they then quickly diverge yielding a very large discrepancy in terms of the MSE, although truly they come from the very same system with the very same parameters. These problems will be aggravated once noise is added to the system and initial conditions are not tightly matched (as almost impossible for systems observed empirically), rendering any measure based on direct matching between time series a relatively poor choice for assessing dynamical systems reconstruction except for a couple of initial time steps. B. Example time series and state spaces from trained PLRNN-SSMs which capture the chaotic structure of the Lorenz attractor quite well (left) or produce rather a simple limit cycle but not chaos (right). The dynamical reconstruction quality is correctly indicated by <inline-formula id="pcbi.1007263.e018"><alternatives><graphic id="pcbi.1007263.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (low on the left but high on the right), while the MSE between true (grey) and generated (orange) time series, on the contrary, would wrongly suggest that the right reconstruction (MSE = 1.4) is better than the one on the left (MSE = 2.48).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g002" xlink:type="simple"/>
</fig>
<p>For evaluating our specific training protocol (termed ‘PLRNN-SSM-anneal’, Algorithm-1 in Methods), trajectories of length <italic>T</italic> = 1000 were drawn with process noise (<italic>σ</italic><sup>2</sup> = .3) from the Lorenz system and handed to the inference algorithm (for statistics, a total of 100 such trajectories were simulated and model fits carried out on each, and a range of different numbers of latent states, <italic>M</italic> = {8, 10, 12, 14}, was explored). Models were trained through ‘PLRNN-SSM-anneal’ and compared to models trained from random initial conditions (termed ‘PLRNN-SSM-random’) in which parameters were randomly initialized (see <xref ref-type="fig" rid="pcbi.1007263.g003">Fig 3</xref>).</p>
<fig id="pcbi.1007263.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Evaluation of stepwise training protocol on chaotic Lorenz attractor.</title>
<p>A. Relative frequency of normalized KL divergences evaluated on the observation space (<inline-formula id="pcbi.1007263.e019"><alternatives><graphic id="pcbi.1007263.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) after running the EM algorithm with the PLRNN-SSM-anneal (blue) and PLRNN-SSM-random (red) protocols on 100 distinct trajectories drawn from the Lorenz system (with <italic>T</italic> = 1000, and <italic>M</italic> = 8, 10, 12, 14). B. Same as A for normalized expected joint log-likelihood E<sub><italic>q</italic>(<bold>z</bold>|<bold>x</bold>)</sub>[log <italic>p</italic>(<bold>X</bold>,<bold>Z</bold>|<bold>θ</bold>)] (see <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref> <xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>). C. Decrease in <italic>KL</italic><sub><bold>x</bold></sub> over the distinct training steps of ‘PLRNN-SSM-anneal’ (see Algorithm-1; the first step refers to a LDS initialization and was removed). D. Increase in (rescaled) expected joint log-likelihood across training steps 2−3<sub>1−3</sub> in ‘PLRNN-SSM-anneal’. Since the protocol partly works by systematically scaling down <bold>Σ</bold>, for comparability the log-likelihood after each step was recomputed (rescaled) by setting <bold>Σ</bold> to the identity matrix. E. Representative example of joint log-likelihood increase during the EM iterations of the individual training steps 2−3<sub>1−3</sub> for a single Lorenz trajectory. Unstable system estimates and likelihood values&lt;-10<sup>3</sup> were removed from all figures for visualization purposes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g003" xlink:type="simple"/>
</fig>
<p>In general, the PLRNN-SSM-anneal protocol significantly decreased the normalized KL divergence <inline-formula id="pcbi.1007263.e020"><alternatives><graphic id="pcbi.1007263.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>) and increased the joint log-likelihood when compared to the PLRNN-SSM-random initialization scheme (see <xref ref-type="fig" rid="pcbi.1007263.g003">Fig 3A and 3B</xref>, independent <italic>t</italic>-test on <inline-formula id="pcbi.1007263.e021"><alternatives><graphic id="pcbi.1007263.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>: <italic>t</italic>(686) = -16.3, <italic>p</italic> &lt; .001, and on the expected joint log-likelihood: <italic>t</italic>(640) = 11.32, <italic>p</italic> &lt; .001). More importantly though, the PLRNN-SSM-anneal protocol produced more estimates for which <inline-formula id="pcbi.1007263.e022"><alternatives><graphic id="pcbi.1007263.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> was in a regime in which the chaotic attractor could be well reconstructed (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref>, grey shaded area indicates <italic>KL</italic><sub>x</sub> values for which the chaotic attractor was reproduced). Furthermore, the expected joint log-likelihood increased (<xref ref-type="fig" rid="pcbi.1007263.g003">Fig 3D</xref>) while <italic>KL</italic><sub><bold>x</bold></sub> decreased (<xref ref-type="fig" rid="pcbi.1007263.g003">Fig 3C</xref>) over the distinct training steps of the PLRNN-SSM-anneal protocol, indicating that each step further enhances the solution quality. <italic>KL</italic><sub>x</sub> and the normalized log-likelihood were, however, only moderately correlated (<italic>r</italic> = -.27, <italic>p</italic> &lt; .001), as expected based on the formal considerations above (sect. ‘Stepwise initialization and training protocol’).</p>
<fig id="pcbi.1007263.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Evaluation of training protocol and KL measure on dynamical systems benchmarks.</title>
<p>A. True trajectory from chaotic Lorenz attractor (with parameters s = 10, r = 28, b = 8/3). B. Distribution of <inline-formula id="pcbi.1007263.e023"><alternatives><graphic id="pcbi.1007263.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>) across all samples, binned at .05, for PLRNN-SSM (black) and LDS-SSM (red). For the PLRNN-SSM, around 26% of these samples (grey shaded area, pooled across different numbers of latent states <italic>M</italic>) captured the butterfly structure of the Lorenz attractor well (see also D). Unsurprisingly, the LDS completely failed to reconstruct the Lorenz attractor. C. Estimated Lyapunov exponents for reconstructed Lorenz systems for PLRNN-SSM (black) and LDS-SSM (red) (estimated exponent for true Lorenz system ≈.9, cyan line). A significant positive correlation between the absolute deviation in Lyapunov exponents for true and reconstructed systems with <inline-formula id="pcbi.1007263.e024"><alternatives><graphic id="pcbi.1007263.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (<italic>r</italic> = .27, <italic>p</italic> &lt; .001) further supports that the latter measures salient aspects of the nonlinear dynamics in the PLRNN-SSM (for the LDS-SSM, all of these empirically determined Lyapunov exponents were either &lt; 0, as indicative of convergence to a fixed point, or at least very close to 0, light-gray line). D. Samples of PLRNN-generated trajectories for different <inline-formula id="pcbi.1007263.e025"><alternatives><graphic id="pcbi.1007263.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values. The grey shaded area indicates successful estimates. E. True van der Pol system trajectories (with μ = 2 and ω = 1). F. Same as in B but for van der Pol system. G. Correlation of the spectral density between true and reconstructed van der Pol systems for the PLRNN-SSM (black) and LDS-SSM (red). A significant negative correlation for the PLRNN-SSM between the agreement in the power spectrum (high values on y-axis) and <inline-formula id="pcbi.1007263.e026"><alternatives><graphic id="pcbi.1007263.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> again supports that the normalized KL divergence defined across state space (<xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>) captures the dynamics (we note that measuring the correlation between power spectra comes with its own problems, however). For the LDS-SSM, in contrast, all power-spectra correlations and <inline-formula id="pcbi.1007263.e027"><alternatives><graphic id="pcbi.1007263.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> measures were poor. H. Same as in D for van der Pol system. Note that even reconstructed systems with high <inline-formula id="pcbi.1007263.e028"><alternatives><graphic id="pcbi.1007263.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values may capture the limit cycle behavior and thus the basic topological structure of the underlying true system (in general, the 2-dimensional vdP system is likely easier to reconstruct than the chaotic Lorenz system; vice versa, low <inline-formula id="pcbi.1007263.e029"><alternatives><graphic id="pcbi.1007263.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values do not ascertain that the reconstructed system exhibits the same frequencies).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Reconstruction of benchmark dynamical systems</title>
<p>After establishing an efficient training procedure designed to enforce recovery of the underlying DS by the prior model (<xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>), we more formally evaluated dynamical reconstructions on the chaotic Lorenz system and on the van der Pol (vdP) nonlinear oscillator. The vdP oscillator with nonlinear dampening is a simple 2-dimensional model for electrical circuits consisting of vacuum tubes [<xref ref-type="bibr" rid="pcbi.1007263.ref052">52</xref>] (equations given in <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref>). <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref> illustrates its flow field in the plane, together with several trajectories converging to the system’s limit cycle (note that training was always performed on samples of the time series, not on the generally unknown flow field!).</p>
<p>As for the Lorenz system, we drew 100 time series samples of length <italic>T</italic> = 1000 with process noise (<italic>σ</italic><sup>2</sup> = .1) using Runge-Kutta numerical integration, and handed each of those over to a separate PLRNN-SSM inference run, testing with a range <italic>M</italic> = {8, 10, 12, 14} of latent states (see below and Discussion for how to determine a suitable latent space dimensionality <italic>M</italic>). As above, reconstruction performance was assessed in terms of the (normalized) KL divergence <inline-formula id="pcbi.1007263.e030"><alternatives><graphic id="pcbi.1007263.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>) between the distributions over true and generated states in state space. In addition, for the chaotic attractor, the absolute difference between Lyapunov exponents [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref050">50</xref>] from the true vs. the PLRNN-SSM-generated trajectories was assessed, as another measure of how well hallmark dynamical characteristics of the chaotic Lorenz system had been captured. For the vdP (non-chaotic) oscillator, we instead assessed the correlation between the power spectrum of the true and the generated trajectories (see <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Reconstruction of benchmark dynamical systems’).</p>
<p>Overall, our PLRNN-SSM-anneal algorithm managed to recover the nonlinear dynamics of these two benchmark systems (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref>). The inferred PLRNN-SSM equations reproduced the ‘butterfly’ structure of the somewhat challenging chaotic attractor very well (<xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4D</xref>). The <inline-formula id="pcbi.1007263.e031"><alternatives><graphic id="pcbi.1007263.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> measure effectively captured this reconstruction quality, with PLRNN reconstructions achieving values below <inline-formula id="pcbi.1007263.e032"><alternatives><graphic id="pcbi.1007263.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo>.</mml:mo><mml:mn>4</mml:mn></mml:math></alternatives></inline-formula> agreeing well with the Lorenz attractor’s ‘butterfly’ structure as assessed by visual inspection (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4B</xref>). At the same time, for this range of <inline-formula id="pcbi.1007263.e033"><alternatives><graphic id="pcbi.1007263.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values the deviation between Lyapunov exponents of the true and generated Lorenz system was generally very low (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4C</xref>, grey shaded area). If we accept this value as an indicator for successful reconstruction, our algorithm was successful in 15%, 24%, 35%, and 28% of all samples for <italic>M</italic> = 8, 10, 12, and 14 states, respectively. Note that our algorithm had access only to rather short time series of <italic>T = 1000</italic>, to create a situation comparable to that for fMRI data. When examining the dependence of <inline-formula id="pcbi.1007263.e034"><alternatives><graphic id="pcbi.1007263.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> on the number of latent states across a larger range in more detail, <italic>M</italic> ≈ 16 turned out to be optimal for this setting (<xref ref-type="supplementary-material" rid="pcbi.1007263.s002">S1 Fig</xref>), as for <italic>M</italic> &gt; 16 no further decrease in <inline-formula id="pcbi.1007263.e035"><alternatives><graphic id="pcbi.1007263.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (hence no further improvement in approximating the true attractor geometry) was observed.</p>
<p>Importantly and in contrast to most previous studies, note we requested full independent generation of the original attractor object from the once trained PLRNN. That is, we neither ‘just’ evaluated the posterior <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) conditioned on the actual observations (as e.g. in [<xref ref-type="bibr" rid="pcbi.1007263.ref053">53</xref>], or [<xref ref-type="bibr" rid="pcbi.1007263.ref036">36</xref>]) , nor did we ‘just’ assess predictions a couple of time steps ahead (as, e.g., in [<xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>]), but rather defined a much more ambitious goal for our algorithm.</p>
<p>For the vdP system, our inference procedure yielded agreeable results in 20%, 31%, 25%, and 35% of all samples for <italic>M</italic> = 8, 10, 12, and 14 states, respectively (grey shaded area in <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4F</xref>), with <italic>M</italic> = 14 about optimal for this setting (<xref ref-type="supplementary-material" rid="pcbi.1007263.s002">S1 Fig</xref>). Furthermore, around 50% of all estimates generated stable limit cycles and hence a topologically equivalent attractor object in state space, although these limit cycles varied a lot in frequency and amplitude compared to the true oscillator. Like for the Lorenz system, the <inline-formula id="pcbi.1007263.e036"><alternatives><graphic id="pcbi.1007263.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> measure generally served as a good indicator of reconstruction quality (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4H</xref>), particularly when combined with the power spectrum correlation (<xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4G</xref>), although low <inline-formula id="pcbi.1007263.e037"><alternatives><graphic id="pcbi.1007263.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values did not always guarantee and high values did not exclude the retrieval of a stable limit cycle.</p>
<p>As noted in the Introduction, a linear dynamical system (LDS) is inherently (mathematically) incapable of producing more complex dynamical phenomena like limit cycles or chaos. To explicitly illustrate this, we ran the same training procedure (Algorithm-1) on a <italic>linear</italic> state space model (LDS-SSM) which we created by simply swapping the ReLU nonlinearity <italic>φ</italic>(<bold>z</bold>) = max(<bold>z</bold>,0) with the linear function <italic>φ</italic>(<bold>z</bold>) = <bold>z</bold> in Eq <xref ref-type="disp-formula" rid="pcbi.1007263.e003">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref>. As expected, this had a dramatic effect on the system’s capability to capture the true underlying dynamics, with <inline-formula id="pcbi.1007263.e038"><alternatives><graphic id="pcbi.1007263.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> close to 1 in most cases for both the Lorenz (<xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4B and 4C</xref>) and the vdP (<xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4F and 4G</xref>) equations. Even for the simpler (but nonlinear) oscillatory vdP system, LDS-SSM would at most produce damped (and linear, harmonic) oscillations which decay to a fixed point over time (<xref ref-type="fig" rid="pcbi.1007263.g005">Fig 5A</xref>).</p>
<fig id="pcbi.1007263.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Example time series from an LDS-SSM and a PLRNN-SSM trained on the vdP system.</title>
<p>A. Example time graph (left) and state space (right) for a trajectory generated by an LDS-SSM (red) trained on the vdP system (true vdP trajectories in green). Trajectories from a LDS will almost inevitably decay toward a fixed point over time (or diverge). B. Trajectories generated by a trained PLRNN-SSM, in contrast, closely follow the vdP-system’s original limit cycle.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Reconstruction of experimental data</title>
<p>We next tested our PLRNN inference scheme, with a modified observation model that takes the hemodynamic response filtering into account (PLRNN-BOLD-SSM; see sect. ‘Observation model for BOLD time series’), on a previously published experimental fMRI data set [<xref ref-type="bibr" rid="pcbi.1007263.ref054">54</xref>]. In brief, the experimental paradigm assessed three cognitive tasks presented within repeated blocks, two variants of the well-established working memory (WM) n-back task: a 1-back continuous delayed response task (CDRT), a 1-back continuous matching task (CMT), and a (0-back control) choice reaction task (CRT). Exact details on the experimental paradigm, fMRI data acquisition, preprocessing, and sample information can be found in [<xref ref-type="bibr" rid="pcbi.1007263.ref054">54</xref>]. From these data obtained from 26 subjects, we preselected as time series the first principle component from each of 10 bilateral regions identified as relevant to the n-back task in a previous meta-analysis [<xref ref-type="bibr" rid="pcbi.1007263.ref055">55</xref>]. These time series along with the individual movement vectors obtained from the SPM realignment procedure (see also <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Data acquisition and preprocessing’) were given to the inference algorithm for each subject: Models with <italic>M</italic> = {1,…,10} latent states were inferred twice: once explicitly including, and once excluding external (experimental) inputs (i.e., in the latter analysis, the model had to account for fluctuations in the BOLD signal all by itself, without information about changes in the environment).</p>
<p>For experimentally observed time series, unlike for the benchmark systems, we do not know the ground truth (i.e., the true data generating process), and generally do not have access to the complete true state space either (but only to some possibly incomplete, nonlinear projection of it). Thus, we cannot determine the agreement between generated and true distributions directly in the space of observables, as we could for the benchmark systems. Therefore we use a proxy: If the prior dynamics is close to the true system which generated the experimental observations, and those represent the true dynamics well (at the very least, they are the best information we have), then the distribution of latent states constrained by the data, i.e. <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), should be a good representative of the distribution over latent states generated by the prior model on its own, i.e. <italic>p</italic>(<bold>Z</bold>). Hence, our proxy for the reconstruction quality is the KL divergence <italic>KL</italic><sub><bold>z</bold></sub>(<italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>),<italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>)) (<italic>KL</italic><sub><bold>z</bold></sub> for short, or, when normalized, <inline-formula id="pcbi.1007263.e039"><alternatives><graphic id="pcbi.1007263.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>; see (Eq 11) in <xref ref-type="sec" rid="sec014">Methods</xref>) between the posterior (inferred) distribution <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>) over latent states <bold>z</bold> conditioned on the experimental data <bold>x</bold>, and the spatial distribution <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>) over latent states as generated by the model’s prior (governing the free-running model dynamics; we use capital letters, <bold>Z</bold>, and lowercase letters, <bold>z</bold>, to distinguish between full trajectories and single vector points in state space, respectively). Note that the latent space defines a complete state space as we have that complete model available (also note that our measure, as before, assesses the agreement in <italic>state space</italic>, not the agreement between time series).</p>
<p>For the benchmark systems, our proposed proxy <italic>KL</italic><sub><bold>z</bold></sub> was well correlated with the KL divergence <italic>KL</italic><sub><bold>x</bold></sub> assessed directly in the complete observation space, i.e., between true and generated distributions (<xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6A</xref>, <italic>r</italic> = .72 on a logarithmic scale, <italic>p</italic> &lt; .001; likewise, <italic>KL</italic><sub><bold>z</bold></sub>(<italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>),<italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>)) and <italic>KL</italic><sub><bold>z</bold></sub>(<italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>),<italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>)) were generally correlated highly; <italic>r</italic>&gt;.9, <italic>p</italic> &lt; .001). Moreover, although especially for chaotic systems we would not necessarily expect a good fit between observed or inferred and generated time series [c.f. <xref ref-type="bibr" rid="pcbi.1007263.ref051">51</xref>], <inline-formula id="pcbi.1007263.e040"><alternatives><graphic id="pcbi.1007263.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> on the latent space turned out to be significantly related to the correlation between inferred and generated latent state <italic>series</italic> in our case (on a logarithmic scale, see <xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6B</xref>). That is, lower <inline-formula id="pcbi.1007263.e041"><alternatives><graphic id="pcbi.1007263.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> values were associated with a better match of inferred and generated state trajectories.</p>
<fig id="pcbi.1007263.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Model evaluation on experimental data.</title>
<p>A. Association between KL divergence measures on observation (<italic>KL</italic><sub><bold>x</bold></sub>) vs. latent space (<italic>KL</italic><sub><bold>z</bold></sub>) for the Lorenz system; y-axis displayed in log-scale. B. Association between <inline-formula id="pcbi.1007263.e042"><alternatives><graphic id="pcbi.1007263.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1007263.e078">Eq 11</xref>; in log scale) and correlation between generated and inferred state series for models with inputs (top, displayed in shades of blue for <italic>M</italic> = 1…10), and models without inputs (bottom, displayed in shades of red for <italic>M</italic> = 1…10). C. Distributions of <inline-formula id="pcbi.1007263.e043"><alternatives><graphic id="pcbi.1007263.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (y-axis) in an experimental sample of n = 26 subjects for different latent state dimensions (x-axis), for models including (top) or excluding (bottom) external inputs. D. Mean squared error (MSE) between generated and true observations for the PLRNN-BOLD-SSM (squares) and the LDS-BOLD-SSM (triangles) as a function of ahead-prediction step for models including (left) or excluding (right) external inputs. The PLRNN-BOLD-SSM starts to robustly outperform the LDS-BOLD-SSM for predictions of observations more than about 3 time steps ahead, the latter in contrast to the former exhibiting a strongly nonlinear rise in prediction errors from that time step onward. The LDS-BOLD-SSM also does not seem to profit as much from increasing the latent state dimensionality. E. Same as D for the MSE between generated and inferred states as a function of ahead-prediction step, showing that the comparatively sharp rise in prediction errors for the LDS-BOLD-SSM in contrast to the PLRNN-BOLD-SSM is accompanied by a sharp increase in the discrepancy between generated and inferred state trajectories after the 3<sup>rd</sup> prediction step. Globally unstable system estimates were removed from D and E.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g006" xlink:type="simple"/>
</fig>
<p>This tight relation was particularly pronounced in models including external inputs (<xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6B</xref> blue, top). This is expected, as in this case the internal dynamics are reset or partly driven by the external inputs, which will therefore induce correlations between directly inferred and freely generated trajectories. Thus, overall, <italic>KL</italic><sub><bold>z</bold></sub> was slightly lower for models including external inputs as compared to autonomous models (see also <xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6C</xref>). One simple but important conclusion from this is that knowledge about additional external inputs and the experimental task structure may (strongly) help to recover the true underlying DS. This was also evident in the mean squared error on <italic>n</italic>-step ahead predictions of generated as compared to true data (<xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6D</xref>), i.e. when comparing predicted observations from the PLRNN-BOLD-SSM run freely for <italic>n</italic> time steps to the true observations (once again we stress, however, that a measure evaluated directly on the time series may not necessarily give a good intuition about whether the underlying DS has been captured well; see also <xref ref-type="fig" rid="pcbi.1007263.g002">Fig 2</xref>). Accuracy of <italic>n</italic>-step-ahead predictions also generally improved with increasing number of latent state dimensions, that is, adding latent states to the model appeared to enhance the dynamical reconstruction within the range studied here.</p>
<p>In contrast to the PLRNN-BOLD-SSM, the performance of the LDS-SSM with the same BOLD observation model (termed LDS-BOLD-SSM), and trained according to the same protocol (Algorithm-1, see also previous section), quickly decayed after about only three prediction time steps (<xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6D</xref>), clearly below the prediction accuracy achieved by the PLRNN-BOLD-SSM for which the decay was much more linear. Interestingly, this comparatively sharp drop in prediction accuracy for the LDS-BOLD-SSM, unlike the PLRNN-BOLD-SSM, was accompanied by a similarly sharp rise in the discrepancy between generated and inferred latent state trajectories (<xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6E</xref>), which was not apparent for the PLRNN-BOLD-SSM. This suggests that the rise in LDS-BOLD-SSM prediction errors is directly related to the model’s inability to capture the underlying system in its <italic>generative</italic> dynamics (while the inferred latent states may still provide reasonable fits), and–moreover–that the agreement between inferred and generated latent states is indeed a good indicator of how well this goal of reconstructing dynamics has been achieved. The linear model’s failure to capture the underlying dynamics was also evident from the fact that its generated trajectories often quickly converged to fixed points (<xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7C</xref>), while the trained PLRNNs often mimicked the oscillatory activity found in the real data in their generative behavior (<xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7B</xref>, see also <xref ref-type="supplementary-material" rid="pcbi.1007263.s006">S1 Video</xref>).</p>
<fig id="pcbi.1007263.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Decoding task conditions from model trajectories.</title>
<p>A. Relative LDA classification error on different task phases based on the inferred states (top) and freely generated states (bottom) from the PLRNN-BOLD-SSM (solid lines) and LDS-BOLD-SSM (dashed lines), for models including (blue) or excluding (red) stimulus inputs. Black lines indicate classification results for random state permutations. Except for <italic>M</italic> = 2, the classification error for the PLRNN-BOLD-SSM based on generated states, drawn from the prior model <italic>p</italic><sub><italic>gen</italic></sub>(<bold>Z</bold>), is significantly lower than for the permutation bootstraps (all <italic>p</italic> &lt; .01), indicating that the prior dynamics contains task-related information. In contrast, the LDS-BOLD-SSM produced substantially higher discrimination errors for the generated trajectories (which were close to chance level when stimulus information was excluded), and even on the inferred trajectories. Globally unstable system estimates were removed from analysis. B. Typical example of inferred (left) and generated (right) state space trajectories from a PLRNN-BOLD-SSM, projected down to the first 3 principle components for visualization purposes, color-coded according to task phases (see legend). C. Same as in B for example from trained LDS-BOLD-SSM. The simulated (generated) states usually converged to a fixed point in this case.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g007" xlink:type="simple"/>
</fig>
<p>Moreover, we observed that a PLRNN-BOLD model fit directly to the observations (as one would, e.g., do for an ARMA model; see <xref ref-type="sec" rid="sec014">Methods</xref>), i.e. essentially lacking latent states, was much worse in forecasting the time series than either the PLRNN-BOLD-SSM or the LDS-BOLD-SSM, with predictions errors on average above 3.28 even for just a single time step ahead, either when external inputs were absent (MSE &gt; 2.79 for 1-step) or present (MSE &gt; 3.77 for 1-step), as compared to the results for the latent variable models in <xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6D</xref>. On top, they produced a large number of globally unstable solutions (35% and 46%, respectively). This suggests that the latent state structure is absolutely necessary for reconstructing the dynamics, perhaps not surprisingly so given that the whole motivation behind delay embedding techniques in nonlinear dynamics is that the true attractor geometries are almost never accessible directly in the observation space [<xref ref-type="bibr" rid="pcbi.1007263.ref050">50</xref>].</p>
<p>To ensure that the retrieved dynamics did not simply capture data variation related to background fluctuations in blood flow (or other systematic effects of no interest), we examined whether the generated trajectories carried task-specific information. For this purpose, we assessed how well we could classify the three experimental tasks (which demanded distinct cognitive processes) via linear discriminant analysis (LDA) based on the <italic>generated</italic> (through the prior model) latent state trajectories. (We exclusively focused on classifying task phases, as these were pseudo-randomized across subjects, while ‘resting’ and ‘instruction’ phases occurred at fixed times, and we wanted to prevent significant classification differences which may occur either due to a fixed temporal order, or due to differences in presentation of experimental inputs during resting/instruction vs. proper task phases.) <xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7A</xref> shows the relative classification error obtained when classifying the three tasks by the generated trajectories (bottom) as compared to that from the directly inferred trajectories (top), and to bootstrap permutations of these trajectories (black solid lines).</p>
<p>Overall, for <italic>M</italic>&gt;2 latent states, generated trajectories significantly reduced the relative classification error, even in the absence of any external stimulus information, suggesting that distinct cognitive processes were associated with distinct regions in the latent space, and that this cognitive aspect was captured by the PLRNN-BOLD-SSM prior model (see also <xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7B</xref> for an example of a generated state space for a sample subject, and <xref ref-type="fig" rid="pcbi.1007263.g008">Fig 8</xref>). As observed for the ahead-prediction error above, performance improved with increasing latent state dimensionality. While adding dimensions will boost LDA classifications in general, as it becomes easier to find well separating linear discriminant surfaces in higher dimensions, we did not observe as strong a reduction in classification error for the permutation bootstraps, suggesting that at least part of the observed improvement was related to better reconstruction of the underlying dynamics. Of note, models which included external inputs enabled almost perfect classifications with as few as <italic>M</italic> = 8 states. These results are not solely attributable to the model receiving external inputs, as these did not differentiate between cognitive tasks (i.e., number and type of inputs were the same for all tasks, see <xref ref-type="sec" rid="sec014">Methods</xref> sect. ‘Experimental paradigm’).</p>
<fig id="pcbi.1007263.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Exemplary DS reconstruction in a sample subject.</title>
<p>A. Top: Latent trajectories generated by the prior model projected down to the first 3 principle components for visualization purposes in a model including external inputs and <italic>M = 6</italic> latent states. Task separation is clearly visible in the generated state space (color-coded as in the legend), i.e. different cognitive demands are associated with different regions of state space (hard step-like changes in state are caused by the external inputs). Bottom: Observed time series (black) and their predictions based on the <italic>generated</italic> trajectories (red, with 90% CI in grey) for the same subject. See also <xref ref-type="supplementary-material" rid="pcbi.1007263.s006">S1 Video</xref>. B. Same as A for the same subject in a PLRNN without external inputs. *BA = Brodmann area, Le/Re = left/right, CRT = choice reaction task, CDRT = continuous delayed response task, CMT = continuous matching task.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g008" xlink:type="simple"/>
</fig>
<p>This is further supported by the observation that the LDS-BOLD-SSM produced much higher classification errors than the PLRNN-BOLD-SSM when either external inputs were present or absent (<xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7A</xref>, dashed lines). Hence, not only does the LDS fail to capture the underlying dynamics and fares worse in ahead predictions (cf. <xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6D and 6E</xref>), but it also seems to contain less information about the actual task structure, even in the inferred trajectories. This was particularly evident in the situation where trajectories were simulated (generated) and information about external stimuli was not provided to the models, where LDS-BOLD-SSM-based classification performance was close to chance level across all latent state dimensionalities (<xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7A</xref> bottom, red dashed line), consistent with the fact that simulated LDS quickly converged to fixed points (cf. <xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7C</xref>).</p>
<p>Lastly, we observed that trained PLRNN-BOLD-SSMs in many cases produced interesting nonlinear dynamics, including stable limit cycles, chaotic attractors, and multi-stability between various attractor objects (<xref ref-type="fig" rid="pcbi.1007263.g009">Fig 9</xref>). This indicates that the fMRI data may indeed harbor interesting dynamical structure that one would not have been able to reveal with linear state space models like classical DCMs, at least not within the retrieved system of equations (as argued above, the inferred posterior <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) may still reflect this structure, but the model itself would not reproduce it).</p>
<fig id="pcbi.1007263.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Examples of highly nonlinear phenomena extracted from fMRI data (in systems with <italic>M = 10</italic> states, no external inputs).</title>
<p>A. PLRNN-BOLD-SSM with 3 stable limit cycles (LC) estimated from one subject (top: subspace of state space for 3 selected states; bottom: time graphs). B. PLRNN with 2 stable limit cycles and one chaotic attractor, estimated from another subject. C. PLRNN with one stable limit cycle and one stable fixed point. D. Increase in average (log Euclidean) distance between initially infinitesimally close trajectories with time for chaotic attractor in B. (In A and B states diverging towards–∞ were removed, as by virtue of the ReLU transformation they would not affect the other states and hence overall dynamics).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g009" xlink:type="simple"/>
</fig>
<p>Furthermore, some of this structure clearly appeared to be linked to task properties: A power spectral analysis of time series generated by the trained PLRNNs revealed that the oscillations exhibited by these models had dominant periods in the same range as the durations of different task phases, as well as periods on the order of the duration of all three different tasks which were delivered in a repetitive manner (<xref ref-type="fig" rid="pcbi.1007263.g010">Fig 10A</xref>). Hence the PLRNN-BOLD-SSM has captured the periodic nature of the experimental design and associated cognitive demands within its limit cycle behavior, even when it was provided with no other source of information than the recorded BOLD activity itself (<xref ref-type="fig" rid="pcbi.1007263.g010">Fig 10A</xref>, left). Moreover, it appeared that the total number of stable objects and unstable fixed points in state space was related to task performance, with better performance (in terms of % correct choices) associated with a larger difference in the number of unstable relative to that of stable objects in the CMT (<xref ref-type="fig" rid="pcbi.1007263.g010">Fig 10B</xref>). From a dynamical systems perspective, one may speculate that these changes in state space structure are associated with a richer and more complex system dynamics [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref056">56</xref>], which in turn may imply better and more flexible cognitive performance (note that by ‘unstable objects’ we are referring to unstable fixed points of the system dynamics, not to single latent states; unstable fixed points are as physiological as stable fixed points, only that they are hardly accessible experimentally since activity diverges from them, while our method by inferring the generating equations makes them ‘visible’).</p>
<p>While these observations serve to illustrate the new possibilities for analyzing links between system dynamics and computational properties provided by our approach, and the new types of questions about neural systems one may be able to ask, we caution that more detailed analyses (and possibly purpose-tailored task designs), beyond the scope of the present study, would be required to establish a stronger link. For instance, <italic>unstable</italic> limit cycles or chaotic objects were not considered here (for reasons of computational tractability), ceiling effects in percent of correct choices, and an increase in the proportion of globally unstable system estimates for <italic>M</italic>&gt;9 (partly possibly due to the limited length of the time series) made a more systematic evaluation difficult in the present experimental data set.</p>
<fig id="pcbi.1007263.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007263.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Links between properties of system dynamics captured by the PLRNN-BOLD-SSM and behavioral task performance.</title>
<p>A. Average power spectra for PLRNN-generated time series when external inputs were excluded (left) and included (right), and for the original BOLD traces (yellow). <italic>M</italic> = 9 latent states were used in this analysis, as at this <italic>M</italic> the number of stable and unstable objects appeared to roughly plateau (<xref ref-type="supplementary-material" rid="pcbi.1007263.s003">S2A Fig</xref>). The left grey line marks the frequency of one entire task sequence cycle (3⋅72s = 216s = .0046Hz) and the right grey line the frequency of one task and resting block (36s+36s = 72s = .0139 Hz). The peaks in the power spectra of the model-generated time series at these points indicate that the PLRNN has captured the periodic reoccurrence of single task blocks as well as that of the whole task block sequence in its limit cycle activity. B. Relation of the number of stable and unstable dynamical objects (see <xref ref-type="sec" rid="sec014">Methods</xref>) to behavioral performance for models without external inputs (<italic>M</italic> = 9; see <xref ref-type="supplementary-material" rid="pcbi.1007263.s003">S2B Fig</xref> for data pooled across <italic>M</italic> = 2…10). Low and high performance groups were formed according to median splits over correct responses during the CMT. A repeated measures ANOVA with between-subject factor ‘performance’ (‘low’ vs. ‘high’ percentage of correct responses) and within-subject factor ‘stability’ (‘stable’ vs. ‘unstable’ objects) revealed a significant 2-way ‘performance x stability’ interaction (<italic>F</italic>(1,24) = 5.28, <italic>p</italic> = .031). We focused on the CMT for this analysis since for the other two tasks performance was close to a ceiling effect (although results still hold when averaging across tasks, <italic>p</italic> = .012).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.g010" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>Theories about neural computation and information processing are often formulated in terms of nonlinear DS models, i.e. in terms of attractor states, transitions among these, or transient dynamics still under the influence of attractors or other salient geometrical properties of the state space [<xref ref-type="bibr" rid="pcbi.1007263.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref057">57</xref>]. Given the success of DS theory in neuroscience, and the recent surge in interest in reconstructing trajectory flows and state spaces from experimental recordings [<xref ref-type="bibr" rid="pcbi.1007263.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref061">61</xref>], methodological tools which would return not only state space representations, but actually a model of the governing equations, would be of great benefit. Here we suggested a novel algorithm within an SSM framework that specifically forces the latent model, represented by a PLRNN, to capture the underlying dynamics in its <italic>intrinsic</italic> behavior, such that it can produce on its own time series of ‘fake observations’ that closely match the real ones (see also <xref ref-type="supplementary-material" rid="pcbi.1007263.s006">S1 Video</xref>). We also evaluated a measure, the KL divergence defined across state space (not time) between the inferred (posterior) and intrinsically generated (prior) distribution of latent states, which would give us a quantitative sense of how well the underlying state space geometry has been captured in empirical situations where no ground truth is available. Finally, given that fMRI is the most common non-invasive technique to study human cognition in health and psychiatric illness, we derived a new observation model specifically for fMRI data that takes the HRF into account. Using this, we demonstrated that our approach could recover nonlinear dynamics and trajectory flows from human fMRI recordings that were related to task structure and behavioral performance in a working memory paradigm. This, to our knowledge, has not been shown before.</p>
<sec id="sec010">
<title>Choice of model formalism and latent space dimensionality</title>
<p>Our major goal here was to establish an efficient methodological approach for recovering dynamical systems from empirical data in a truly generative sense, i.e. such that the trained models exhibit an intrinsic, standalone dynamics that mimics the underlying dynamics of the unknown real system, and to provide a specific measure based on attractor geometries for how well this aim has been achieved. We chose RNNs for the latent model because they are universal approximators of dynamical systems [<xref ref-type="bibr" rid="pcbi.1007263.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref028">28</xref>] and can emulate any Turing machine [<xref ref-type="bibr" rid="pcbi.1007263.ref062">62</xref>]. Just like the computations performed by a Turing machine can be implemented in many different substrates and algorithmic environments [see, e.g., discussion in <xref ref-type="bibr" rid="pcbi.1007263.ref063">63</xref>], the same nonlinear dynamical system and behavior can be implemented in numerous different ways [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref062">62</xref>]. Note, for instance, that the PLRNN can reproduce the chaotic Lorenz attractor although its set of equations is quite different from the original Lorenz equations. Hence, from a pure dynamical systems perspective, the functional form of the nonlinear model, and how close it is to biology, may be largely irrelevant as long as it is powerful enough to approximate any kind of dynamics sufficiently well, i.e. has the required representational expressiveness.</p>
<p>Nevertheless, we would like to repeat that our PLRNN does in fact have the mathematical form of a typical neural rate model as indicated in the first Results section [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref038">38</xref>], and that its ReLU nonlinearity compares quite well to I/O functions of cortical pyramidal cells within the physiologically relevant regime [<xref ref-type="bibr" rid="pcbi.1007263.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref064">64</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref065">65</xref>], making the model neuronally directly interpretable in principle.</p>
<p>The major reason for settling on a ReLU nonlinearity was, however, that it allows for highly efficient optimization approaches, which also made ReLUs the de-facto standard in modern deep learning applications [<xref ref-type="bibr" rid="pcbi.1007263.ref044">44</xref>]. In our case, the ReLUs are centerpiece to an efficient fixed-point-iteration-type algorithm for the E-step and enable to compute most expectations analytically and fast (see <xref ref-type="sec" rid="sec014">Methods</xref> ‘State Estimation’). We believe that this efficiency of optimization, assuring that, in probability, we achieve better approximations to the underlying (biological or physical) system, is more important for capturing biology than the precise functional form of the latent model.</p>
<p>Although this was not a goal here, we further would like to point out that of course also task-specific coupling matrices <bold>W</bold> could be estimated, with subsets of latent states strictly assigned to only certain brain regions (via restrictions on <bold>B</bold>, Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e011">3</xref>). From a DS perspective, however, one might rather want to think about the same DS (with same parameters) producing different types of tasks (e.g., [<xref ref-type="bibr" rid="pcbi.1007263.ref038">38</xref>]), 2019), where the different tasks are more reflected by different local dynamics in possibly different regions of state space (cf. <xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7B</xref>) rather than by differences in coupling parameters.</p>
<p>Finally, so far we have touched only briefly on the important question of how to determine the latent space dimensionality <italic>M</italic> in any practical setting. In our presentation we have deliberately explored a larger range of <italic>M</italic> values for testing and illustrating our algorithm, and mostly demonstrated that results were consistent across this larger range. While one may hope that reconstructing the underlying dynamical system involves a dimensionality reduction (<italic>M</italic> &lt; <italic>N</italic>), i.e. that the effective dynamics lives in a lower-dimensional space than occupied by the observed measurements, the delay embedding theorems [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref049">49</xref>] as well as the universal approximation theorems for RNN [<xref ref-type="bibr" rid="pcbi.1007263.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref027">27</xref>] imply that we may instead have to move to (much) higher-dimensional spaces for achieving a good approximation to the underlying system and disentanglement of trajectories (an RNN approximates the underlying system through a type of basis expansion, and for, e.g., the Lorenz attractor, a set of just <italic>M</italic> = 3 piecewise linear functions cannot be expected to yield a reasonable representation). This implies that <italic>M</italic> should not be too low, but on other hand, for obtaining a well tractable and parsimonious system, we would not want to increase the latent space dimensionality more than absolutely necessary. Based on <xref ref-type="supplementary-material" rid="pcbi.1007263.s002">S1 Fig</xref> we had suggested that <italic>M</italic> ≈14 and 16 may be optimal for the vdP and Lorenz systems, respectively, based on the observation that from these points onwards no further improvement in geometry reconstruction according to <inline-formula id="pcbi.1007263.e044"><alternatives><graphic id="pcbi.1007263.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> was observed. For <xref ref-type="fig" rid="pcbi.1007263.g010">Fig 10B</xref>, which analyzes the number of stable and unstable dynamical objects, <italic>M</italic>≈9 was chosen based on the fact that the number of retrieved dynamical objects roughly plateau-ed at this level (<xref ref-type="supplementary-material" rid="pcbi.1007263.s003">S2A Fig</xref>). Moreover, the finite length of the time series (which are very short in fMRI) will also place an upper bound on the system size for which reliable estimates could still be achieved. In our case, for <italic>M</italic> &gt;9 we obtained more globally unstable model estimates which curtails the possibilities for analysis. More generally, in practice, one could try to devise a type of cross-validation procedure [<xref ref-type="bibr" rid="pcbi.1007263.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref066">66</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref067">67</xref>] based on <inline-formula id="pcbi.1007263.e045"><alternatives><graphic id="pcbi.1007263.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, but cross-validation for latent-variable time series models is notoriously difficult [<xref ref-type="bibr" rid="pcbi.1007263.ref068">68</xref>] and for <italic>M</italic>≥4 a clear dip in the <inline-formula id="pcbi.1007263.e046"><alternatives><graphic id="pcbi.1007263.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> curve (see <xref ref-type="fig" rid="pcbi.1007263.g006">Fig 6C</xref>, bottom) was hard to discern in our case. Hence, beyond the empirical guidelines given here, this certainly remains a topic for future investigation.</p>
</sec>
<sec id="sec011">
<title>Comparison to other approaches for identifying dynamical systems</title>
<p>The ‘classical’ technique for reconstructing attractor dynamics from experimental time series is delay embedding, based on the delay embedding theorems by Takens [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>] and Sauer et al. [<xref ref-type="bibr" rid="pcbi.1007263.ref049">49</xref>]. It has been used to disentangle task-related trajectory flows and attractor-like properties in experimentally assessed neuronal time series [<xref ref-type="bibr" rid="pcbi.1007263.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref023">23</xref>]. However, as a completely non-parametric technique, delay embedding will not give a complete picture of the system’s flow field, nor access to the governing equations. Linear dynamical systems, coupled to Gaussian or Poisson observation equations [<xref ref-type="bibr" rid="pcbi.1007263.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref019">19</xref>], and related approaches like GPFA [<xref ref-type="bibr" rid="pcbi.1007263.ref020">20</xref>], are quite popular in neurophysiology for obtaining smoothed trajectories and state spaces, but–due to their linear latent dynamics–are inherently unsuitable for reconstructing the underlying DS itself in most cases (as explained above, they may still yield a good approximation to the posterior <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), thus still useful, but they would fail to capture the generative dynamics itself as explicitly shown in <xref ref-type="fig" rid="pcbi.1007263.g005">Fig 5</xref> and <xref ref-type="fig" rid="pcbi.1007263.g007">Fig 7</xref>). In consequence, unlike the PLRNN-based models, LDS models were not able to pick up the nonlinear structure present in the BOLD signals in their generative dynamics (but mostly converged to simple fixed points), and probably as a result thereof produced worse forward predictions and contained less information about the cognitive tasks than the PLRNN.</p>
<p>To our knowledge, Roweis and Ghahramani [<xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>], and somewhat later Yu et al. [<xref ref-type="bibr" rid="pcbi.1007263.ref029">29</xref>], were among the first to suggest an RNN for the latent model in order to reconstruct dynamics. These earlier contributions still focused more on in the inferred space <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), rather than on the fully generative capabilities of their models (at least were these not systematically analyzed), perhaps partly due to the fact that numerically less stable and efficient inference methods like the extended Kalman filter were employed at the time. Very recent work by Zhao and Park [<xref ref-type="bibr" rid="pcbi.1007263.ref035">35</xref>] built on the radial basis function networks suggested by Roweis and Ghahramani [<xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>] for the latent model, and combined it with variational inference. They showed ahead predictions of their model for up to 1000 time steps. Similarly, Pandarinath et al. [<xref ref-type="bibr" rid="pcbi.1007263.ref036">36</xref>] recently proposed a sequential variational auto-encoder framework for inferring dynamics from neural recordings (although here as well the focus was more on the posterior encoding in the latent states, and on inference of initial conditions and perturbations). Both these models, however, are fairly complex and not directly interpretable in neural terms, and, moreover, hard to analyze with respect to their intrinsic dynamics.</p>
<p>The PLRNN framework offers several distinct advantages compared to other approaches: The equations have a fairly direct neural interpretation [<xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>], in fact have the general form of neural rate equations that have been used to model various neural and cognitive phenomena [<xref ref-type="bibr" rid="pcbi.1007263.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref038">38</xref>], and–due to their piecewise-linear structure–can also be easily translated into an equivalent <italic>continuous-time</italic> neural rate model [see <xref ref-type="bibr" rid="pcbi.1007263.ref069">69</xref>]. Dynamical phenomena can be analyzed more easily in PLRNNs than in other frameworks, e.g. fixed points and their stability can be determined analytically [<xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>]. Furthermore, ReLU-type activation functions appear to be a quite good approximation to the I/O-functions of many neocortical cell types [<xref ref-type="bibr" rid="pcbi.1007263.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref064">64</xref>], and, besides, are almost the default now in deep networks due to their favorable properties in optimization [<xref ref-type="bibr" rid="pcbi.1007263.ref044">44</xref>], a feature our iterative state inference algorithm exploits as well. Finally, in contrast to most previous approaches, here we demonstrated that the prior PLRNN model on its own, after training, can produce the same attractor dynamics in state space as the true DS.</p>
<p>In the physics literature, several other methods based on reservoir computing [<xref ref-type="bibr" rid="pcbi.1007263.ref070">70</xref>], RNNs formed from feedforward networks trained directly on the flow field [see also <xref ref-type="bibr" rid="pcbi.1007263.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref028">28</xref>], or LASSO regression combined with polynomial basis expansions [<xref ref-type="bibr" rid="pcbi.1007263.ref071">71</xref>], have recently been discussed for identifying DS. Process noise is usually not included in these models, i.e. the latent dynamics is deterministic, which entails the risk that noise in the process is wrongly attributed to deterministic aspects of the dynamics. While some of these methods required hundreds of hidden states and millions of samples to reconstruct the van der Pol or Lorenz attractors [<xref ref-type="bibr" rid="pcbi.1007263.ref028">28</xref>], we found that as few as just eight latent states and a single time series of length 1000, within the range of typical fMRI data, can be sufficient for the PLRNN-SSM to rebuild the chaotic Lorenz attractor, another tremendous advantage in empirical settings.</p>
</sec>
<sec id="sec012">
<title>Applications in fMRI research and beyond</title>
<p>In this contribution, we have derived a new observation model for fMRI that accounts for the HRF filtering of the BOLD signal. The HRF implies that current observations do not depend only on the system’s current state (the common assumption in SSMs), but on a sequence of previous states, a situation handled relatively seamlessly by our PLRNN-SSM inference algorithm. fMRI is still the most common recording technique for monitoring brain function during cognitive and emotional processing in healthy and psychiatric subjects. Huge data bases have been compiled in large cohort studies over the past decade or so (e.g., the German National Cohort Study initiated by the Helmholtz association: <ext-link ext-link-type="uri" xlink:href="https://www.helmholtz.de/en/research_infrastructures/national_cohort_study/" xlink:type="simple">https://www.helmholtz.de/en/research_infrastructures/national_cohort_study/</ext-link>; see also Collins and Varmus [<xref ref-type="bibr" rid="pcbi.1007263.ref072">72</xref>]) as a reference for monitoring and assessing neurological and psychiatric dysfunction. Although other noninvasive recording techniques with finer temporal resolution, like MEG/ EEG, may be more suitable for addressing questions about the DS basis of cognition, clinical research cannot afford to discard this large body of medically relevant data.</p>
<p>On the other hand, important hypotheses about the neural underpinnings of psychiatric conditions like schizophrenia, attention deficit hyperactivity disorder, or depression, have been formulated in terms of altered system dynamics [see <xref ref-type="bibr" rid="pcbi.1007263.ref073">73</xref> for a recent review]. For instance, based on physiological single unit and synapse data combined with biophysical network models on dopamine modulation in prefrontal cortex, it has been suggested that a dysregulated dopamine system by overly ‘deepening’ cortical attractor landscapes may inhibit transitions among states, and thereby cause some of the (cognitive) symptoms in schizophrenia [<xref ref-type="bibr" rid="pcbi.1007263.ref074">74</xref>]. This proposal has been supported by a number of neurophysiological and neuropsychological observations [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref075">75</xref>], but a direct experimental evaluation of the specific changes in attractor basins in schizophrenia is still lacking. Tools like the one proposed here could be applied to directly test these types of hypotheses in human subjects recorded with fMRI. More generally, however, an extensive literature suggests that dynamical properties assessed from fMRI predict psychopathological conditions [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref076">76</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref077">77</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref078">78</xref>], where the methodological framework proposed here could help to better understand the underlying dynamics and define targets for intervention (e.g. in the context of neurofeedback).</p>
<p>Beyond fMRI, most neuroimaging techniques, including, e.g., calcium imaging [<xref ref-type="bibr" rid="pcbi.1007263.ref079">79</xref>] or imaging by voltage-sensitive dyes [<xref ref-type="bibr" rid="pcbi.1007263.ref080">80</xref>] in neural tissue, involve some form of filtering that has to be taken into account when the goal is to capture underlying dynamical processes (like neural interactions) that evolve at a faster time scale. Through introduction of a filtering observation model (<xref ref-type="disp-formula" rid="pcbi.1007263.e011">Eq 3</xref>), the present paper establishes a framework for inferring nonlinear dynamics in such situations where the measurement technique involves low- or band-pass-filtering of the process of interest. More generally, while we chose fMRI data here as our applicational example, we emphasize that our methodological framework is generic and could ultimately be applied to any other recording modality, like EEG, MEG, multiple single-unit data, or time series from mobile sensors, ecological momentary assessments [<xref ref-type="bibr" rid="pcbi.1007263.ref081">81</xref>], or electronic health records, for instance, by simply swapping the observation model (Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e011">3</xref>).</p>
</sec>
<sec id="sec013">
<title>Open issues and outlook</title>
<p>There is room for improvement in both our training algorithm and the measures used to evaluate its success in empirical situations. Our stepwise training algorithm was devised based on an intuitive heuristic, namely that by shifting the workload for fitting the observations onto the latent model and gradually increasing the requirements for its temporal consistency, a better representation of the unobserved system dynamics could be achieved. We could show that this was indeed the case when compared to a bootstrap (random) sample of models trained in the ‘standard’ way, and that our procedure seemed to work in general, but a more systematic theoretical derivation and testing of alternative schemes and explicitly designed optimization criteria (directly utilizing <xref ref-type="disp-formula" rid="pcbi.1007263.e075">Eq 10</xref>, or combining our geometric measure with a time series measure) would certainly be desirable in future work.</p>
<p>We also find it important that in testing the performance of different reconstruction algorithms not only ‘good examples’ that prove the basic concept (‘my algorithm works’) are shown, but a more thorough quantitative statistical evaluation of precisely how well it performed in what percentage of cases is provided, like the one attempted here (<xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref>). For applications to empirical data, for which we do not know the ground truth, an open issue is how we could best quantify how much confidence we could have in the reconstructed stochastic equations of motion. Cross-validation and out-of-sample prediction errors provide a guidance, but for DS it is less clear in terms of what these should be measured: It is known that for nonlinear systems with complex or chaotic dynamics standard squared-error or likelihood-based measures evaluated along time series are not too useful [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref051">51</xref>], since miniscule differences in initial conditions or noise perturbations may cause quick decorrelation of trajectories even if they come from the very same DS. We therefore decided to compare true and simulated data in terms of probability distributions across state space, arguing that if the observations come from the same attractor or system dynamics they should fill roughly the same volume of state space–this is more along the lines of a DS view which compares dynamical objects in terms of their geometrical or topological equivalence in state space [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref050">50</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref082">82</xref>], rather than the literal overlap among time series. Another corollary of this view is that to establish the equivalence between two DS, it is neither sufficient nor potentially even useful to predict observations just a couple of time steps ahead: In a chaotic noisy system, the prediction horizon is inherently limited to begin with (because of exponential divergence of trajectories). One also has to demonstrate that the ‘general type’ of long-term behavior in the limit is the same (e.g. a limit cycle of a certain periodicity and order), potentially in combination with other measures that quantify temporal aspects in the form of summary statistics (e.g., power spectrum). Here we therefore suggested to evaluate performance in terms of completely newly generated (‘faked’) trajectories that the trained system produces when no longer guided by the actual observations (i.e., the prior <italic>p</italic><sub><italic>gen</italic></sub>(<bold>Z</bold>) rather than the posterior <italic>p</italic><sub><italic>inf</italic></sub>(<bold>Z</bold>|<bold>X</bold>)).</p>
<p>Especially in fMRI, however, the data space is often very high-dimensional (&gt;10<sup>3</sup>) while at the same time often only a single time series sample of limited length (<italic>T</italic>≤1000) is available, i.e. the <bold>x</bold>-space is very sparse. In these cases we cannot obtain a good approximation of the distribution <italic>p</italic>(<bold>x</bold>), as we could for the benchmarks, and hence our original measure is not directly applicable. Hence we reverted to performing the comparison in latent space, between two distributions we do have in principle available, the one constrained by the observations, <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>), and the other, <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>), obtained from the completely freely running (simulated) system. We argued that if our actual observations <bold>X</bold> reflect the true dynamics well, then states obtained under <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>) should be highly likely a priori, i.e. under <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>), and hence these distributions should highly overlap. As direct sampling from <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>) is difficult and time-consuming, due to degeneracy problems, and the latent space dimensionality may also be prohibitively high, we approximated it by a mixture of Gaussians, which is a reasonable assumption for our ReLU-based RNN model and allows for an efficient analytical approximation to <italic>KL</italic><sub><bold>z</bold></sub> [<xref ref-type="bibr" rid="pcbi.1007263.ref083">83</xref>]. More generally, if we are only interested in topological equivalence [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref049">49</xref>], we may also want to accept translations, rotations, rescaling, and potentially other deformations of the true state space that do not change topological aspects. Procrustes analysis [<xref ref-type="bibr" rid="pcbi.1007263.ref084">84</xref>] could be performed to (partly) allow for such transformations (on the other hand, since <italic>p</italic><sub><italic>gen</italic></sub>(<bold>Z</bold>) and <italic>p</italic><sub><italic>inf</italic></sub>(<bold>Z</bold>|<bold>X</bold>) come from the same underlying model, in our specific case such transformations may neither be necessary, nor necessarily desired).</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Model specification and inference</title>
<p>The formulation of the state space model for BOLD time series (PLRNN-BOLD-SSM) is given in the Results section. To infer the parameters and latent variables of the model, we used Expectation-Maximization (EM) [<xref ref-type="bibr" rid="pcbi.1007263.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref085">85</xref>]. The EM algorithm maximizes a lower bound <inline-formula id="pcbi.1007263.e047"><alternatives><graphic id="pcbi.1007263.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> (also called the evidence lower bound, ELBO) of the log-likelihood log <italic>p</italic>(<bold>X</bold>|<bold>θ</bold>) given by (see <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref> sect. ‘PLRNN-BOLD-SSM model inference’ for full details):
<disp-formula id="pcbi.1007263.e048">
<alternatives>
<graphic id="pcbi.1007263.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e048" xlink:type="simple"/>
<mml:math display="block" id="M48">
<mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
with <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>) some proposal density over latent states, and <italic>KL</italic>(<italic>q</italic>(<bold>Z</bold>|<bold>X</bold>), <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>)) the Kullback-Leibler divergence between proposal density <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>) and true posterior <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>). This expression can be derived by, e.g., using Jensen's inequality [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>]. From this we see that the bound becomes exact when proposal density <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>) exactly matches the true posterior density <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) (defined through the latent state model here) which we aim to determine in the E-step (in contrast to variational inference where we assume <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>) to come from some parameterized family of density functions, in EM we usually try to compute [in the linear case] or approximate <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) directly).</p>
<sec id="sec016">
<title>State estimation (E-Step)</title>
<p>In the E-step we seek <inline-formula id="pcbi.1007263.e049"><alternatives><graphic id="pcbi.1007263.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>≔</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> given a current parameter estimate <bold>θ</bold>*. Since <bold>θ</bold>* is assumed to be given, this amounts to minimizing the Kullback-Leibler divergence <italic>KL</italic>(<italic>q</italic>(<bold>Z</bold>|<bold>X</bold>), <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>)). The common procedure for linear-Gaussian models [e.g., Kalman filter-smoother; <xref ref-type="bibr" rid="pcbi.1007263.ref086">86</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref087">87</xref>] is equating <italic>q</italic>(<bold>Z</bold>|<bold>X</bold>) = <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), and then determining the first two moments of the latter for performing the M-step. For the present model <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) is a high-dimensional mixture of piecewise Gaussians for which ‘explicit’ integration (i.e., using tabulated Gaussian integrals) becomes unfeasible for large <italic>T</italic> and <italic>M</italic>. Typically, however, the piecewise Gaussians will have centers close to the origin [<xref ref-type="supplementary-material" rid="pcbi.1007263.s004">S3 Fig</xref>; cf. <xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref>], and hence we resort to solving for the maximum a-posteriori (MAP) estimate of <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>), expected to be close to E[<bold>Z</bold>|<bold>X</bold>] (which is exactly so for a single Gaussian), and instantiate the state covariance matrix with the negative inverse Hessian around this maximizer (e.g. [<xref ref-type="bibr" rid="pcbi.1007263.ref016">16</xref>]). Essentially, this is a global Gaussian approximation, or a Laplace approximation of the log-likelihood where we approximate <inline-formula id="pcbi.1007263.e050"><alternatives><graphic id="pcbi.1007263.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> using the maximizer <bold>Z</bold><sup>max</sup> of log <italic>p</italic><sub><bold>θ</bold></sub>(<bold>X</bold>,<bold>Z</bold>) (note that the Hessian <bold>L</bold><sup>max</sup> is constant around the maximizer) [<xref ref-type="bibr" rid="pcbi.1007263.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref088">88</xref>].</p>
<p>Taking this approach, letting Ω(<italic>t</italic>)⊆{1…<italic>M</italic>} refer to the set of all indices of units for which <italic>z</italic><sub><italic>m</italic>,<italic>t</italic></sub>≤0 and <bold>W</bold><sub>Ω(<italic>t</italic>)</sub> to the matrix <bold>W</bold> that has all columns corresponding to indices in Ω(<italic>t</italic>) set to 0, the optimization objective in the E-Step may be formulated as:
<disp-formula id="pcbi.1007263.e051">
<alternatives>
<graphic id="pcbi.1007263.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e051" xlink:type="simple"/>
<mml:math display="block" id="M51">
<mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≔</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<disp-formula id="pcbi.1007263.e052">
<alternatives>
<graphic id="pcbi.1007263.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e052" xlink:type="simple"/>
<mml:math display="block" id="M52">
<mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1007263.e053">
<alternatives>
<graphic id="pcbi.1007263.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e053" xlink:type="simple"/>
<mml:math display="block" id="M53">
<mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Γ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
w.r.t. (Ω,<bold>Z</bold>) subject to <italic>z</italic><sub><italic>i</italic>,<italic>t</italic></sub>≤0 ∀<italic>i</italic>∈Ω(<italic>t</italic>)∧<italic>z</italic><sub><italic>i</italic>,<italic>t</italic></sub>&gt;0 ∀<italic>i</italic>∉Ω(<italic>t</italic>)∀<italic>t</italic>.</p>
<p>Let us concatenate all state variables across <italic>m</italic> and <italic>t</italic> into one long column vector <inline-formula id="pcbi.1007263.e054"><alternatives><graphic id="pcbi.1007263.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mi mathvariant="bold">z</mml:mi><mml:msup><mml:mrow><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, and likewise arrange all matrices <bold>A</bold>, <bold>W</bold><sub>Ω(<italic>t</italic>)</sub>, and so on, into large <italic>MT</italic>x<italic>MT</italic> block tri-diagonal matrices, and let us further collect all terms quadratic in <bold>z</bold>, linear in <bold>z</bold>, or constant (see <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref> for exact composition of these matrices). Defining <bold>H</bold> as the HRF convolution matrix, <bold>d</bold><sub>Ω</sub>≔(I(<italic>z</italic><sub>11</sub>&gt;0),I(<italic>z</italic><sub>21</sub>&gt;0),…,I(<italic>z</italic><sub><italic>MT</italic></sub>&gt;0))<sup>T</sup> as an indicator vector with a 1 for all states <italic>z</italic><sub><italic>m</italic>,<italic>t</italic></sub>&gt;0 and zeros otherwise, and <bold>D</bold><sub>Ω</sub>≔<italic>diag</italic>(<bold>d</bold><sub>Ω</sub>) as the diagonal matrix formed from this vector, one can rewrite the optimization criterion (<xref ref-type="disp-formula" rid="pcbi.1007263.e051">Eq 5</xref>) compactly as
<disp-formula id="pcbi.1007263.e055">
<alternatives>
<graphic id="pcbi.1007263.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e055" xlink:type="simple"/>
<mml:math display="block" id="M55">
<mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
which is a piecewise quadratic function in <bold>z</bold> with solution vectors
<disp-formula id="pcbi.1007263.e056">
<alternatives>
<graphic id="pcbi.1007263.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e056" xlink:type="simple"/>
<mml:math display="block" id="M56">
<mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<italic>provided</italic> this solution is consistent with the current set Ω, i.e. is a true solution of <xref ref-type="disp-formula" rid="pcbi.1007263.e055">Eq 6</xref>. For solving this set of piecewise linear equations, we use a simple Newton-type iteration scheme, similar to the one suggested in [<xref ref-type="bibr" rid="pcbi.1007263.ref089">89</xref>], where we iterate between (1) solving <xref ref-type="disp-formula" rid="pcbi.1007263.e055">Eq 6</xref> for fixed <bold>d</bold><sub>Ω</sub> and (2) flipping the bits in <bold>d</bold><sub>Ω</sub> inconsistent with the obtained solution to <xref ref-type="disp-formula" rid="pcbi.1007263.e055">Eq 6</xref>, until convergence. Care is taken to avoid getting trapped in cyclic behavior, and a quadratic programming step may be added at the end to obtain the maximum given a fixed index set Ω [which seemed rarely necessary from our experience; see <xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref> for details].</p>
<p>Once a solution <bold>z</bold>* with high posterior density has been obtained, the state covariance matrix is approximated locally around this estimate as the inverse negative Hessian
<disp-formula id="pcbi.1007263.e057">
<alternatives>
<graphic id="pcbi.1007263.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e057" xlink:type="simple"/>
<mml:math display="block" id="M57">
<mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>These state covariance estimates are then used to compute, mostly analytically, the expectations E[<italic>φ</italic>(<bold>z</bold>)], E[<bold>z</bold><italic>φ</italic>(<bold>z</bold>)<sup>T</sup>], and E[<italic>φ</italic>(<bold>z</bold>)<italic>φ</italic>(<bold>z</bold>)<sup>T</sup>] required in the M-Step [please see <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref> and <xref ref-type="bibr" rid="pcbi.1007263.ref031">31</xref> for more details]. This global iterative E-Step scheme is particularly suitable for fMRI applications in which the HRF invokes temporal dependencies between current observations and latent states that reach back in time by several lags (i.e. <bold>x</bold><sub><italic>t</italic></sub> does not only depend on <bold>z</bold><sub><italic>t</italic></sub>, but on a set of previous states <bold>z</bold><sub><italic>τ</italic>:<italic>t</italic></sub>). This implies that <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) does not factorize as required for the common (unscented or extended) Kalman filter. Although our approach is global, as pointed out by Paninski et al. [<xref ref-type="bibr" rid="pcbi.1007263.ref017">17</xref>], efficient schemes for inverting block-tridiagonal matrices still scale linearly in <italic>T</italic> (but not in <italic>M</italic>).</p>
</sec>
<sec id="sec017">
<title>Parameter estimation (M-Step)</title>
<p>In the M-step, parameters are updated by seeking <inline-formula id="pcbi.1007263.e058"><alternatives><graphic id="pcbi.1007263.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msup><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>≔</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> given <italic>q</italic>* from the E-step (since <italic>q</italic>* is assumed fixed and known in the E-step, note that the entropy over <italic>q</italic> becomes a constant in <xref ref-type="disp-formula" rid="pcbi.1007263.e048">Eq 4</xref> and drops out from the maximization). This boils down to a simple linear regression problem given that the ReLU nonlinearities have been resolved within the expectations E[<italic>φ</italic>(<bold>z</bold>)], E[<bold>z</bold><italic>φ</italic>(<bold>z</bold>)<sup>T</sup>], and E[<italic>φ</italic>(<bold>z</bold>)<italic>φ</italic>(<bold>z</bold>)<sup>T</sup>], and hence criterion <xref ref-type="disp-formula" rid="pcbi.1007263.e051">Eq 5</xref> becomes simply quadratic.</p>
<p>We can (analytically) solve for the parameters <bold>θ</bold><sub><italic>obs</italic></sub> of the observation model and <bold>θ</bold><sub><italic>lat</italic></sub> of the latent model separately. Because of the off-diagonal structure of <bold>W</bold>, it is most efficient to obtain parameter solutions row-wise for the latent model parameters (i.e., separately for each state <italic>m</italic> = 1…<italic>M</italic>), as spelled out in <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref>. For the observation model parameters, concatenating matrices <bold>B</bold> and <bold>J</bold> as <inline-formula id="pcbi.1007263.e059"><alternatives><graphic id="pcbi.1007263.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mo>]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, and concatenating convolved states and nuisance variables in <inline-formula id="pcbi.1007263.e060"><alternatives><graphic id="pcbi.1007263.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, one can rewrite the observation equation term in <italic>Q</italic>(<bold>θ</bold>,<bold>Z</bold>)≔E<sub><italic>q</italic></sub>[log <italic>p</italic>(<bold>X</bold>,<bold>Z</bold>|<bold>θ</bold>)] as
<disp-formula id="pcbi.1007263.e061">
<alternatives>
<graphic id="pcbi.1007263.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e061" xlink:type="simple"/>
<mml:math display="block" id="M61">
<mml:msub><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Γ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">Γ</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>Differentiating w.r.t. to <bold>Y</bold> and setting to 0 yields
<disp-formula id="pcbi.1007263.e062">
<alternatives>
<graphic id="pcbi.1007263.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e062" xlink:type="simple"/>
<mml:math display="block" id="M62">
<mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Defining the sums of cross-products
<disp-formula id="pcbi.1007263.e063">
<alternatives>
<graphic id="pcbi.1007263.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e063" xlink:type="simple"/>
<mml:math display="block" id="M63">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1007263.e064">
<alternatives>
<graphic id="pcbi.1007263.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e064" xlink:type="simple"/>
<mml:math display="block" id="M64">
<mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
we can equivalently express the solution as
<disp-formula id="pcbi.1007263.e065">
<alternatives>
<graphic id="pcbi.1007263.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e065" xlink:type="simple"/>
<mml:math display="block" id="M65">
<mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">J</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>With these definitions, differentiating <xref ref-type="disp-formula" rid="pcbi.1007263.e061">Eq 7</xref> w.r.t <bold>Γ</bold> yields
<disp-formula id="pcbi.1007263.e066">
<alternatives>
<graphic id="pcbi.1007263.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e066" xlink:type="simple"/>
<mml:math display="block" id="M66">
<mml:mi mathvariant="bold">Γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">J</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">Ι</mml:mi>
</mml:math>
</alternatives>
</disp-formula>
where <bold>I</bold> denotes an <italic>N</italic>x<italic>N</italic> identity matrix. Solutions for the latent state parameters <bold>θ</bold><sub><italic>lat</italic></sub> are given in <xref ref-type="supplementary-material" rid="pcbi.1007263.s001">S1 Text</xref>. E- and M-steps are then iterated until convergence of the expected joint log-likelihood.</p>
</sec>
</sec>
<sec id="sec018">
<title>Stepwise model training procedure</title>
<p>We introduce here an efficient approach for pushing the latent model to capture the underlying DS that generated the observations. Our approach rests on a step-wise procedure in which we gradually increase the importance of fitting the latent state dynamics as compared to fitting the observations. Since the latent state process and the observation process account for additive terms in the joint log-likelihood (<xref ref-type="disp-formula" rid="pcbi.1007263.e051">Eq 5</xref>), the tradeoff between fitting the dynamics and fitting the observations is regulated by the ratio of the two covariance matrices <bold>Σ</bold> and <bold>Γ</bold> (Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e003">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1007263.e011">3</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e053">5</xref>). Hence, the idea of our training scheme is to begin with fitting the observation model and putting milder constraints on the latent process, using a <italic>linear</italic> latent model for initialization in a first step [or even factor analysis which places no constraints on the temporal relationship among latent states; cf. <xref ref-type="bibr" rid="pcbi.1007263.ref030">30</xref>], and then gradually decreasing “<bold>Σ</bold>:<bold>Γ</bold>” during training to enforce the temporal consistency of the latent model. Furthermore, one may force all burden of fitting the observations completely onto the latent model by fixing <bold>θ</bold><sub><italic>obs</italic></sub> from some step onwards. The complete training protocol is outlined in Algorithm-1. For inferring a linear model (LDS-SSM, LDS-BOLD-SSM), the exact same algorithm was used with <italic>φ</italic>(<bold>z</bold>) = max(<bold>z</bold>,0) just replaced by <italic>φ</italic>(<bold>z</bold>) = <bold>z</bold> in Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e003">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e009">2</xref>.</p>
</sec>
<sec id="sec019">
<title>Algorithm-1</title>
<p specific-use="line">0) Draw initial parameter estimates <bold>θ</bold><sup>(0)</sup>~p(<bold>θ</bold>) from some suitable prior, constraint to max|eig(<bold>A</bold>+<bold>W</bold>)|&lt;1 for biasing toward stable models [see also <xref ref-type="bibr" rid="pcbi.1007263.ref018">18</xref>].</p>
<p specific-use="line">1) Fix <bold>Σ</bold> = <bold>I</bold> and run linear dynamical system (LDS) SSM for initialization → <bold>θ</bold><sup>(1)</sup></p>
<p specific-use="line">2) Fix <bold>Σ</bold> = <bold>I</bold> and run PLRNN-SSM inference → <bold>θ</bold><sup>(2)</sup></p>
<p specific-use="line">3) for <italic>i</italic> = 1:3</p>
<p specific-use="line">    - Fix <bold>Σ</bold> = diag(10<sup>−<italic>i</italic></sup>), <bold>B</bold> = <bold>B</bold><sup>(2)</sup>; fix <bold>Γ</bold> = <bold>Γ</bold><sup>(2)</sup> (for fMRI data)</p>
<p specific-use="line">    - Initialize PLRNN-SSM training with previous estimate <bold>θ</bold><sup>(<italic>i</italic>+1)</sup></p>
<p specific-use="line">    - Run PLRNN-SSM inference → <bold>θ</bold><sup>(<italic>i</italic>+2)</sup></p>
<p specific-use="line">4) Re-estimate state covariance matrix Var(<bold>z</bold><sub><italic>t</italic></sub>|<bold>x</bold><sub>1:<italic>T</italic></sub>) with <bold>Σ</bold> = <bold>I</bold> fixed.</p>
</sec>
<sec id="sec020">
<title>Reconstruction of benchmark dynamical systems</title>
<p>We evaluated the performance of our PLRNN-SSM approach (and an LDS-SSM for comparison), on two popular benchmark DS, the Lorenz equations and the van der Pol nonlinear oscillator (vdP). Within some parameter range, the 3-dimensional Lorenz system exhibits a chaotic attractor and the 2-dimensional vdP-system exhibits a limit cycle (see <xref ref-type="fig" rid="pcbi.1007263.g004">Fig 4</xref> for parameter settings used, system equations, and sample trajectories of the systems). We were interested in solutions where the true system dynamics is not just reflected in the directly inferred posterior distribution <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) over the PLRNN states {<bold>z</bold><sub>1:<italic>T</italic></sub>} given the actual observations {<bold>x</bold><sub>1:<italic>T</italic></sub>}, but also in the model’s generative or prior distribution <italic>p</italic>(<bold>Z</bold>), i.e. whether the once estimated PLRNN when run on its own would produce similar trajectories with the same dynamical properties as the ground truth system.</p>
<p>For evaluation, <italic>n</italic> = 100 samples of (standardized) trajectories of length <italic>T</italic> = 1000 were drawn from the ground truth systems using Runge-Kutta numerical integration and random initial conditions. PLRNN-SSMs were trained on these sample sets as described above for <italic>M</italic> = 5…20 latent states, using <xref ref-type="disp-formula" rid="pcbi.1007263.e009">Eq 2</xref> for the observations (see also <xref ref-type="fig" rid="pcbi.1007263.g001">Fig 1</xref>). To probe our stepwise training protocol (Algorithm-1), PLRNN-SSM training under this protocol (termed ‘PLRNN-SSM-anneal’) was compared to simple EM training of the PLRNN-SSM started from random initializations of parameters (termed ‘PLRNN-SSM-random’; essentially just step 1 of Algorithm-1 with <bold>Σ</bold> directly fixed to 10<sup>−3</sup>) for <italic>M</italic> = {8, 10, 12, 14}.</p>
<p>To quantify how well the true system dynamics was captured by the ‘free-running’ PLRNN (after training, but unconstrained by the observations), we used the Kullback-Leibler divergence defined across <italic>state space</italic>, i.e. integrating across space, not across time. Similar in spirit to the criteria defined for the classical delay embedding theorems [<xref ref-type="bibr" rid="pcbi.1007263.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1007263.ref050">50</xref>], our measure therefore assessed the agreement between the original and reconstructed <italic>attractor geometries</italic>. Integrating across time (i.e., computing divergence between time series) is problematic for nonlinear DS, since two time series from the very same chaotic DS usually cannot be expected to overlap very well with even miniscule differences in initial conditions [cf. <xref ref-type="bibr" rid="pcbi.1007263.ref051">51</xref>]. For the ground truth benchmark systems, for which we have access to the true distribution <italic>p</italic><sub><italic>true</italic></sub>(<bold>x</bold>) and the complete state space, this KL divergence can be computed directly in observation space and was defined as
<disp-formula id="pcbi.1007263.e067">
<alternatives>
<graphic id="pcbi.1007263.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e067" xlink:type="simple"/>
<mml:math display="block" id="M67">
<mml:msub><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>≔</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where the integration is performed across <bold>x</bold>-space, and <italic>p</italic><sub><italic>gen</italic></sub>(<bold>x</bold>|<bold>z</bold>) is the distribution across observations generated from PLRNN simulations (i.e., after PLRNN-SSM training, but discarding the original set of time series observations <bold>X</bold><sup><italic>obs</italic></sup> = {<bold>x</bold><sub>1:<italic>T</italic></sub>} used for training). Hence, this measure assesses whether PLRNN-SSM-simulated trajectories in the limit fill the same volume of state space as the true DS trajectories, and in this sense whether the systems’ attractor objects are topologically and geometrically ‘equivalent’. (As a terminological remark, in the machine learning literature <italic>p</italic><sub><italic>gen</italic></sub>(<bold>x</bold>|<bold>z</bold>) is often called the ‘generative’ or ‘decoding’ model, while <italic>p</italic>(<bold>z</bold>|<bold>x</bold>) or <italic>q</italic>(<bold>z</bold>|<bold>x</bold>) is sometimes referred to as the ‘encoder’ or ‘recognition’ model [e.g. <xref ref-type="bibr" rid="pcbi.1007263.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref090">90</xref>]. Here we will, more generally, refer with <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>) to the (prior) distribution of latent states generated by the PLRNN <italic>independent of the training observations</italic> <bold>X</bold><sup><italic>obs</italic></sup> = {<bold>x</bold><sub>1:<italic>T</italic></sub>}, and with <italic>p</italic><sub><italic>gen</italic></sub>(<bold>x</bold>|<bold>z</bold>) to the distribution of <italic>simulated</italic> observations produced from samples <bold>z</bold><sup><italic>gen</italic></sup>~<italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>) according to the observation model [<xref ref-type="disp-formula" rid="pcbi.1007263.e009">Eq 2</xref>]).</p>
<p>Practically, we discretized the <bold>x</bold>-space into <italic>K</italic> bins of width Δ<bold>x</bold> and evaluated the probabilities ‘empirically’ as relative frequencies <inline-formula id="pcbi.1007263.e068"><alternatives><graphic id="pcbi.1007263.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> by filling the space with trajectories (<italic>T</italic> = 100,000) sampled from the true DS and trained PLRNNs (here we used Δ<bold>x</bold> = 1 across a range <italic>x</italic><sub><italic>n</italic></sub>∈[−4 4] for standardized variables, but smaller bin sizes yielded qualitatively similar results, see <xref ref-type="supplementary-material" rid="pcbi.1007263.s005">S4 Fig</xref>). To avoid <inline-formula id="pcbi.1007263.e069"><alternatives><graphic id="pcbi.1007263.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula> for the generative model, where the KL divergence is not defined, we further adjusted this relative frequency to <inline-formula id="pcbi.1007263.e070"><alternatives><graphic id="pcbi.1007263.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, with <italic>α</italic> = 10<sup>−6</sup>, also known as Laplace or additive smoothing [<xref ref-type="bibr" rid="pcbi.1007263.ref091">91</xref>] such that <xref ref-type="disp-formula" rid="pcbi.1007263.e067">Eq 8</xref> becomes
<disp-formula id="pcbi.1007263.e071">
<alternatives>
<graphic id="pcbi.1007263.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e071" xlink:type="simple"/>
<mml:math display="block" id="M71">
<mml:msub><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Lastly, to obtain an interpretable measure between 0 and 1, we normalized the KL divergence (termed <inline-formula id="pcbi.1007263.e072"><alternatives><graphic id="pcbi.1007263.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) by dividing it by the expected maximum deviation. <inline-formula id="pcbi.1007263.e073"><alternatives><graphic id="pcbi.1007263.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and the expected joint log-likelihood were compared between PLRNN-SSM-anneal and PLRNN-SSM-random via independent <italic>t</italic>-tests. For these analyses, all unstable system estimates were removed (≈14%). Furthermore, strong outliers with joint log-likelihood values &lt; -1000 (which occurred only for PLRNN-SSM-random in ≈3.8% of cases) were removed.</p>
<p>A standard measure of chaoticity in nonlinear DS is the maximal Lyapunov exponent [<xref ref-type="bibr" rid="pcbi.1007263.ref024">24</xref>]. We thus also assessed how well our KL measure correlated with the deviation in Lyapunov exponents between true and estimated systems. The Lyapunov exponent was assessed numerically by a linear regression fit to the initial slope of the log-Euclidean distance log <italic>d</italic><sub>Δ<italic>t</italic></sub>(<bold>X</bold><sup>(1)</sup>,<bold>X</bold><sup>(2)</sup>) between initially close (<italic>d</italic><sub>0</sub>&lt;10<sup>−10</sup>) trajectories <bold>X</bold><sup>(1)</sup> and <bold>X</bold><sup>(2)</sup> as a function of time lag Δ<italic>t</italic>, up to the point in the curve where a plateau indicating the full extent of the attractor object has been reached. For the van der Pol nonlinear (non-chaotic) oscillator, the agreement in the power spectra between the true and generated systems is more informative as a measure of how well the system dynamics has been captured (the maximum Lyapunov exponent for a stable limit cycle is 0), which was simply assessed by the average Pearson correlation.</p>
</sec>
<sec id="sec021">
<title>Reconstruction of dynamical systems from experimental data</title>
<sec id="sec022">
<title>Ethics statement</title>
<p>The human data analyzed here has been collected within a study approved by the local ethics committee of the University of Giessen, School of Medicine, and written informed consent was obtained from each participant prior to enrollment (AZ 63/08).</p>
</sec>
<sec id="sec023">
<title>Experimental paradigm</title>
<p>The experimental paradigm assessed three cognitive tasks, two working memory (WM) n-back tasks—the continuous delayed response task (CDRT), and the continuous matching task (CMT)—and a choice reaction task (CRT), which served as 0-back control task. In all tasks, subjects were presented with a sequence of stimuli, and they had to respond to each stimulus (a triangle or a square) according to the task instruction. While in the CDRT participants were asked to indicate which stimulus was presented last, the CMT required participants to compare the current to the last stimulus and indicate whether they were the same or different [<xref ref-type="bibr" rid="pcbi.1007263.ref092">92</xref>]. In the CRT, participants had to simply indicate the current stimulus, and WM was not required. The paradigm is known to robustly activate the WM network. Each task was preceded by a resting period and an instruction phase. Tasks only differed w.r.t. the instruction phase, otherwise participants were faced with the same stimulus sequence, presented on a central screen at variable inter-stimulus intervals.</p>
</sec>
<sec id="sec024">
<title>Data acquisition and preprocessing</title>
<p>Exact details on fMRI data acquisition and preprocessing, as well as information on the sample and consent of study participation can be found in [<xref ref-type="bibr" rid="pcbi.1007263.ref054">54</xref>]. In brief, 26 healthy subjects participated in the study, undergoing the experimental paradigm in a 1.5 GE Scanner. From these data, we chose to preselect voxel time series known to be relevant to the n-back task, as identified by a previous meta-analysis [<xref ref-type="bibr" rid="pcbi.1007263.ref055">55</xref>]. This included the following Brodmann areas (BA): BA6 (supplementary motor), BA32 (anterior cingulate), BA46, BA9 (dorsolateral prefrontal cortices), BA45, BA47 (ventrolateral prefrontal cortices), BA10 (orbitofrontal cortex), BA7, BA40 (parietal cortices), as well as the medial cerebellum. From each of these areas we extracted the first principle component. Given 10 bilateral regions, this amounted to extracting 20 voxel time series from each participant. Time series were mean centered, and mildly temporally smoothed by convolution with a Gaussian filter (<italic>σ</italic><sup>2</sup> = 1).</p>
<p>For each individual, the 20 extracted time series were entered as experimental observations <bold>X</bold> along with 6 nuisance predictors <bold>R</bold> (related to movement vectors obtained from the SPM realignment preprocessing procedure) [<xref ref-type="bibr" rid="pcbi.1007263.ref054">54</xref>] to the PLRNN-BOLD-SSM inference procedure. The LDS-BOLD-SSM was set up the same way (see above), while for the PLRNN fit directly on the observations we set <italic>M</italic> = <italic>N</italic> and restricted <bold>B</bold> (<xref ref-type="disp-formula" rid="pcbi.1007263.e011">Eq 3</xref>) to be a diagonal matrix, thus creating a strict 1:1 mapping between ‘latent states’ and observations. This essentially converts the model into a nonlinear auto-regressive-type model formulated directly on the observations and eliminates the degrees of freedom associated with true latent states.</p>
<p>All models were estimated both including and excluding experimental inputs. For the inclusion condition, experimental inputs <bold>S</bold> were defined as binary ‘design’ vectors of length <italic>K</italic> = 5. The first two entries contained 1’s for the presentation of the two stimulus types (‘triangle’ or ‘square’), and the last 3 entries indicated by 1’s the instruction phases of the three tasks; all other entries were set to 0. Note that during the actual task phases (following the instruction phases) the inference algorithm therefore (like the real subjects) received only information about the presented stimuli but not about the task phase itself. Models were estimated with <italic>L</italic><sub>2</sub> regularization and regularization factor <italic>λ</italic> = 50.</p>
</sec>
<sec id="sec025">
<title>Assessment of dynamical objects</title>
<p>For the PLRNN as formulated in <xref ref-type="disp-formula" rid="pcbi.1007263.e003">Eq 1</xref>, fixed points <bold>z</bold>* can be determined analytically by assessing the solutions <bold>z</bold>* = (<bold>I</bold><sub><italic>M</italic></sub>−<bold>A</bold>−<bold>WD</bold><sub>Ω</sub>)<sup>−1</sup><bold>h</bold> for all 2<sup><italic>M</italic></sup> configurations of the matrix <bold>D</bold><sub>Ω</sub> as defined above. A fixed point <inline-formula id="pcbi.1007263.e074"><alternatives><graphic id="pcbi.1007263.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for which the maximum absolute eigenvalue of the corresponding matrix <bold>A</bold>+<bold>WD</bold><sub>Ω</sub> is larger than 1 is unstable, and (neutrally) stable otherwise. Limit cycles and chaotic attractors were assessed by running each system from 100 random initial conditions for <italic>T = 5000</italic> time steps. If the system converged to a stable pattern in this limit, it was considered a chaotic attractor if the log-Euclidean distance between two trajectories started from infinitesimally close initial conditions was growing over time (i.e. had a positive slope, see last section on Lyapunov exponents), and a stable limit cycle otherwise (although for the results presented here this distinction does not play a role). The number of stable objects was then determined as the total number of stable fixed points, limit cycles, and chaotic attractors counted this way.</p>
</sec>
<sec id="sec026">
<title>Reconstruction measures</title>
<p>In the case of experimental data, in which the ground truth DS is not known, we do not have access to the data generating distribution <italic>p</italic><sub><italic>true</italic></sub>(<bold>X</bold>), nor to the complete state space in general. We therefore used as a proxy for <xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref> the Kullback-Leibler divergence between the distribution over latent states obtained by sampling from the data-<italic>unconstrained</italic> prior <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>) and the data-<italic>constrained</italic> (i.e., inferred) posterior distribution <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>), arguing that the former should match closely with the latter if the actually observed <bold>x</bold> represent the underlying DS well (see <xref ref-type="sec" rid="sec002">Results</xref> section; also note that the <bold>z</bold>-space is always complete by model definition, at least in the autonomous case). We again take the KL divergence across the system’s state space (not time):
<disp-formula id="pcbi.1007263.e075">
<alternatives>
<graphic id="pcbi.1007263.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e075" xlink:type="simple"/>
<mml:math display="block" id="M75">
<mml:msub><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="bold">z</mml:mi><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula></p>
<p>To evaluate this integral, sampling from <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>), however, is difficult because of the known degeneracy problems with particle filters or other numerical samplers in high dimensions [<xref ref-type="bibr" rid="pcbi.1007263.ref093">93</xref>,<xref ref-type="bibr" rid="pcbi.1007263.ref094">94</xref>]. We therefore approximated both <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>) and <italic>p</italic><sub><italic>gen</italic></sub>(<bold>z</bold>) as Gaussian mixtures across trajectory times, i.e. with <inline-formula id="pcbi.1007263.e076"><alternatives><graphic id="pcbi.1007263.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007263.e077"><alternatives><graphic id="pcbi.1007263.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which is reasonable given that the PLRNN distribution is a mixture of piecewise Gaussians (see above). Just as in Eqs <xref ref-type="disp-formula" rid="pcbi.1007263.e067">8</xref> and <xref ref-type="disp-formula" rid="pcbi.1007263.e071">9</xref> above, probabilities are therefore evaluated <italic>in space across all time points</italic>. The mean and covariance of <italic>p</italic>(<bold>z</bold><sub><italic>t</italic></sub>|<bold>x</bold><sub>1:<italic>T</italic></sub>) and <italic>p</italic>(<bold>z</bold><sub><italic>t</italic></sub>|<bold>z</bold><sub><italic>t</italic>−1</sub>) were obtained by marginalizing over the multivariate distributions <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) and <italic>p</italic><sub><italic>gen</italic></sub>(<bold>Z</bold>), respectively, yielding E[<bold>z</bold><sub><italic>t</italic></sub>|<bold>x</bold><sub>1:<italic>T</italic></sub>],E[<bold>z</bold><sub><italic>t</italic></sub>|<bold>z</bold><sub><italic>t</italic>−1</sub>], and covariance matrices Var(<bold>z</bold><sub><italic>t</italic></sub>|<bold>x</bold><sub>1:<italic>T</italic></sub>) and Var(<bold>z</bold><sub><italic>t</italic></sub>|<bold>z</bold><sub><italic>t</italic>−1</sub>). Note that the covariance matrix of <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) was re-estimated at the end of the full training procedure with the process noise matrix <bold>Σ</bold> set to the identity (i.e., to the last value it had before <bold>Γ</bold> was fixed qua Algorithm-1). Diagonal elements of the covariance matrix of <italic>p</italic>(<bold>Z</bold>|<bold>X</bold>) were further restricted to a minimum value of 1 (some lower bound on the variance turned out to be necessary to make <italic>KL</italic><sub><bold>z</bold></sub> well defined almost everywhere).</p>
<p>Finally, the integral in <xref ref-type="disp-formula" rid="pcbi.1007263.e075">Eq 10</xref> was numerically approximated through Monte Carlo (MC) sampling [<xref ref-type="bibr" rid="pcbi.1007263.ref083">83</xref>] using <italic>n</italic> = 500,000 samples:
<disp-formula id="pcbi.1007263.e078">
<alternatives>
<graphic id="pcbi.1007263.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e078" xlink:type="simple"/>
<mml:math display="block" id="M78">
<mml:msub><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>For high-dimensional latent spaces, (asymptotically unbiased) approximation through MC sampling becomes computationally inefficient or unfeasible. For these cases, Hershey and Olson (2007) [<xref ref-type="bibr" rid="pcbi.1007263.ref083">83</xref>] suggest a variational approximation to the integral in <xref ref-type="disp-formula" rid="pcbi.1007263.e075">Eq 10</xref> which we found to be in almost exact agreement with the results obtained through MC sampling:
<disp-formula id="pcbi.1007263.e079">
<alternatives>
<graphic id="pcbi.1007263.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e079" xlink:type="simple"/>
<mml:math display="block" id="M79">
<mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where the terms in the exponentials refer to KL divergences between pairs of Gaussians, for which an analytical expression exists.</p>
<p>We normalized this measure by dividing by the KL divergence between <italic>p</italic><sub><italic>inf</italic></sub>(<bold>z</bold>|<bold>x</bold>) and a reference distribution <italic>p</italic><sub><italic>ref</italic></sub>(<bold>z</bold>) which was simply given by the temporal average across state expectations and variances along trajectories of the prior <italic>p</italic><sub><italic>gen</italic></sub>(<bold>Z</bold>) (i.e., by one big Gaussian in an, on average, similar location as the Gaussian mixture components, but eliminating information about spatial trajectory flows). (Note that we may rewrite the evidence lower bound as <inline-formula id="pcbi.1007263.e080"><alternatives><graphic id="pcbi.1007263.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo><mml:mo>−</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> with <italic>KL</italic>(<italic>q</italic>(<bold>Z</bold>|<bold>X</bold>),<italic>p</italic>(<bold>Z</bold>))≈<italic>KL</italic>(<italic>p</italic>(<bold>Z</bold>|<bold>X</bold>),<italic>p</italic>(<bold>Z</bold>)), which has a similar form as <xref ref-type="disp-formula" rid="pcbi.1007263.e075">Eq 10</xref> above, but computes the divergence across trajectories (time), not across space).</p>
</sec>
</sec>
</sec>
<sec id="sec027">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007263.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Model specification and inference.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007263.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Dependence of <inline-formula id="pcbi.1007263.e081"><alternatives><graphic id="pcbi.1007263.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007263.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> on number of latent states (M) for the vdP (red) and Lorenz (blue) systems.</title>
<p><italic>M</italic> = 14 seems to be about optimal for vdP, while <italic>M</italic>≈16 may be about optimal for the Lorenz system.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007263.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Links between properties of system dynamics captured by the PLRNN-BOLD-SSM and behavioral task performance.</title>
<p>A. Number of stable (fixed points [FPs], limit cycles [LCs]) and unstable (fixed points) dynamical objects as a function of latent space dimensionality <italic>M</italic>. B. Same as <xref ref-type="fig" rid="pcbi.1007263.g010">Fig 10B</xref> for data pooled across <italic>M</italic> = 2…10 (repeated measures ANOVA for ‘performance x stability’ interaction: <italic>F</italic>(1,24) = 2.49, <italic>p</italic> = .128).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007263.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Likelihood landscape.</title>
<p>Illustration of the model’s likelihood landscape as a function of a single latent state across two consecutive time steps, z<sub>1</sub> and z<sub>2</sub>. The joint likelihood <italic>p</italic>(<bold>X</bold>,<bold>Z</bold>) consists of piecewise Gaussians which cut off at the zeros of the states; often they will cluster near the origin and give rise to a strongly elevated plateau of high-likelihood solutions, close to one full Gaussian. Red cross indicates MAP estimate.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007263.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s005" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Agreement in Kullback Leibler divergence <italic>KL</italic><sub>x</sub> (<xref ref-type="disp-formula" rid="pcbi.1007263.e071">Eq 9</xref>) on discretized observation space for different bin sizes (assessed for the Lorenz system).</title>
<p>A. <italic>KL</italic><sub><bold>x</bold></sub> for bin size Δ<bold>x</bold> = 1 (x-axis) against bin size Δ<bold>x</bold> = .5 (y-axis). B. Same as A for bin size Δ<bold>x</bold> = .5 (x-axis) against Δ<bold>x</bold> = .2 (y-axis). C. Same as A. for bin size Δ<bold>x</bold> = .2 (x-axis) against Δ<bold>x</bold> = .1 (y-axis). Measures at different bin sizes are nearly monotonically related such that rank information on the quality of DS retrieval is conserved. However, the <italic>KL</italic><sub><bold>x</bold></sub> spread is largest for Δ<bold>x</bold> = 1 such that qualitative differences in DS retrieval are differentiated more easily for this bin size, and hence this bin size was chosen for the evaluation in the main manuscript.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007263.s006" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007263.s006" xlink:type="simple">
<label>S1 Video</label>
<caption>
<title>True and generated BOLD activity for one subject performing the n-back task.</title>
<p>Top graphs show the spatio-temporal evolution of the first eigenvariates extracted from Brodmann areas 7, 40, 46, and 9 (top left), and the model generated time-series (top right) projected back onto a brain template provided by the statistical parametric mapping software. A PLRNN-BOLD-SSM with <italic>M</italic> = 9 latent states, including external stimulus information, was used (see <xref ref-type="sec" rid="sec014">Methods</xref> for details). The bottom graphs are the corresponding time series for Brodmann area 40 (blue = true data, yellow = model).</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007263.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name> (<year>1999</year>) <source>Spikes, decisions, and actions: the dynamical foundations of neurosciences</source>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name> (<year>2017</year>) <article-title>Dynamic models of large-scale brain activity</article-title>. <source>Nature Neuroscience</source> <volume>20</volume>: <fpage>340</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4497" xlink:type="simple">10.1038/nn.4497</ext-link></comment> <object-id pub-id-type="pmid">28230845</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name> (<year>2007</year>) <source>Dynamical Systems in Neuroscience</source>: <publisher-name>MIT Press</publisher-name>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3516-07.2007" xlink:type="simple">10.1523/JNEUROSCI.3516-07.2007</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1007263.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences U S A</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name> (<year>2001</year>) <article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title>. <source>Trends in Neurosciences</source> <volume>24</volume>: <fpage>455</fpage>–<lpage>463</lpage>. <object-id pub-id-type="pmid">11476885</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name> (<year>2000</year>) <article-title>Neurocomputational models of working memory</article-title>. <source>Nature Neuroscience</source> <volume>3</volume> <fpage>1184</fpage>–<lpage>1191</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/81460" xlink:type="simple">10.1038/81460</ext-link></comment> <object-id pub-id-type="pmid">11127836</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Albantakis</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name> (<year>2009</year>) <article-title>The encoding of alternatives in multiple-choice decision-making</article-title>. <source>BMC Neuroscience</source> <volume>10</volume>: <fpage>166</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname> <given-names>VS</given-names></name> (<year>2008</year>) <article-title>Transient cognitive dynamics, metastability, and decision making</article-title>. <source>PLoS Computational Biology</source> <volume>4</volume>: <fpage>e1000072</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000072" xlink:type="simple">10.1371/journal.pcbi.1000072</ext-link></comment> <object-id pub-id-type="pmid">18452000</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name> (<year>2008</year>) <article-title>Transient dynamics for neural processing</article-title>. <source>Science</source> <volume>321</volume>: <fpage>48</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1155564" xlink:type="simple">10.1126/science.1155564</ext-link></comment> <object-id pub-id-type="pmid">18599763</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lemus</surname> <given-names>L</given-names></name> (<year>1999</year>) <article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title>. <source>Nature</source> <volume>399</volume>: <fpage>470</fpage>–<lpage>473</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/20939" xlink:type="simple">10.1038/20939</ext-link></comment> <object-id pub-id-type="pmid">10365959</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name> (<year>2005</year>) <article-title>Flexible control of mutual inhibition: a neural model of two-interval discrimination</article-title>. <source>Science</source> <volume>307</volume>: <fpage>1121</fpage>–<lpage>1124</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1104171" xlink:type="simple">10.1126/science.1104171</ext-link></comment> <object-id pub-id-type="pmid">15718474</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name> (<year>2011</year>) <article-title>Robust transient dynamics and brain functions</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>5</volume>: <fpage>24</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2011.00024" xlink:type="simple">10.3389/fncom.2011.00024</ext-link></comment> <object-id pub-id-type="pmid">21716642</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Reis</surname> <given-names>BY</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name> (<year>2000</year>) <article-title>Stability of the memory of eye position in a recurrent network of conductance-based model neurons</article-title>. <source>Neuron</source> <volume>26</volume>: <fpage>259</fpage>–<lpage>271</lpage>. <object-id pub-id-type="pmid">10798409</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2003</year>) <article-title>Self-organizing neural integrator predicts interval times through climbing activity</article-title>. <source>Journal of Neuroscience</source> <volume>23</volume>: <fpage>5342</fpage>–<lpage>5353</lpage>. <object-id pub-id-type="pmid">12832560</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2017</year>) <article-title>Metastable dynamics of neural ensembles</article-title>. <source>Frontiers in Systems Neuroscience</source> <volume>11</volume>: <fpage>99</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2017.00099" xlink:type="simple">10.3389/fnsys.2017.00099</ext-link></comment> <object-id pub-id-type="pmid">29472845</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name> (<year>2003</year>) <article-title>Estimating a state-space model from point process observations</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>965</fpage>–<lpage>991</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603765202622" xlink:type="simple">10.1162/089976603765202622</ext-link></comment> <object-id pub-id-type="pmid">12803953</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ferreira</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Koyama</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rahnama Rad</surname> <given-names>K</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>A new look at state-space models for neural data</article-title>. <source>J Comput Neurosci</source> <volume>29</volume>: <fpage>107</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-009-0179-x" xlink:type="simple">10.1007/s10827-009-0179-x</ext-link></comment> <object-id pub-id-type="pmid">19649698</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ryali</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Supekar</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Menon</surname> <given-names>V</given-names></name> (<year>2011</year>) <article-title>Multivariate dynamical systems models for estimating causal interactions in fMRI</article-title>. <source>Neuroimage</source> <volume>54</volume>: <fpage>807</fpage>–<lpage>823</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2010.09.052" xlink:type="simple">10.1016/j.neuroimage.2010.09.052</ext-link></comment> <object-id pub-id-type="pmid">20884354</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref019"><label>19</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name> (<year>2015</year>) <chapter-title>Estimating State and Parameters in State Space Models of Spike Trains</chapter-title>. In: <name name-style="western"><surname>Chen</surname> <given-names>Z</given-names></name>, editor. <source>Advanced State Space Methods for Neural and Clinical Data</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>. pp. <fpage>137</fpage>–<lpage>159</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title>. <source>Journal of Neurophysiology</source> <volume>102</volume>: <fpage>614</fpage>–<lpage>635</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.90941.2008" xlink:type="simple">10.1152/jn.90941.2008</ext-link></comment> <object-id pub-id-type="pmid">19357332</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name> (<year>2003</year>) <article-title>Dynamic causal modelling</article-title>. <source>Neuroimage</source> <volume>19</volume>: <fpage>1273</fpage>–<lpage>1302</lpage>. <object-id pub-id-type="pmid">12948688</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lapish</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2011</year>) <article-title>Attracting dynamics of frontal cortex ensembles during memory-guided decision-making</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002057</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002057" xlink:type="simple">10.1371/journal.pcbi.1002057</ext-link></comment> <object-id pub-id-type="pmid">21625577</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lapish</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>aG</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2015</year>) <article-title>Amphetamine Exerts Dose-Dependent Changes in Prefrontal Cortex Attractor Dynamics during Working Memory</article-title>. <source>Journal of Neuroscience</source> <volume>35</volume>: <fpage>10172</fpage>–<lpage>10187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2421-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2421-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26180194</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref024"><label>24</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Strogatz</surname> <given-names>SH</given-names></name> (<year>2018</year>) <source>Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering</source>: <publisher-name>CRC Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2017</year>) <source>Advanced Data Analysis in Neuroscience: Integrating statistical and computational models</source>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Funahashi</surname> <given-names>K-i</given-names></name>, <name name-style="western"><surname>Nakamura</surname> <given-names>Y</given-names></name> (<year>1993</year>) <article-title>Approximation of dynamical systems by continuous time recurrent neural networks</article-title>. <source>Neural Networks</source> <volume>6</volume>: <fpage>801</fpage>–<lpage>806</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nakano</surname> <given-names>R</given-names></name> (<year>1998</year>) <article-title>Learning dynamical systems by recurrent neural networks from orbits</article-title>. <source>Neural Networks</source> <volume>11</volume>: <fpage>1589</fpage>–<lpage>1599</lpage>. <object-id pub-id-type="pmid">12662730</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trischler</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>D’Eleuterio</surname> <given-names>GM</given-names></name> (<year>2016</year>) <article-title>Synthesis of recurrent neural networks for dynamical system simulation</article-title>. <source>Neural Networks</source> <volume>80</volume>: <fpage>67</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2016.04.001" xlink:type="simple">10.1016/j.neunet.2016.04.001</ext-link></comment> <object-id pub-id-type="pmid">27182811</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref029"><label>29</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Afshar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <chapter-title>Extracting dynamical structure embedded in neural activity</chapter-title>. In: <name name-style="western"><surname>Weiss</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Platt</surname> <given-names>JC</given-names></name>, editors; <year>2005</year>. <publisher-name>MIT Press</publisher-name>. pp. <fpage>1545</fpage>–<lpage>1552</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roweis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name> (<year>2000</year>) <source>An EM algorithm for identification of nonlinear dynamical systems</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2017</year>) <article-title>A state space approach for piecewise-linear recurrent neural networks for identifying computational dynamics from neural measurements</article-title>. <source>PLoS Computational Biology</source> <volume>13</volume>: <fpage>e1005542</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005542" xlink:type="simple">10.1371/journal.pcbi.1005542</ext-link></comment> <object-id pub-id-type="pmid">28574992</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kingma</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name> (<year>2013</year>) <source>Auto-encoding variational bayes. arXiv preprint arXiv:13126114</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chung</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dinh</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Goel</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>AC</given-names></name>, <etal>et al</etal>. <source>A recurrent latent variable model for sequential data</source>; <year>2015</year>. pp. <fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bayer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Osendorfer</surname> <given-names>C</given-names></name> (<year>2015</year>) <source>Learning stochastic recurrent networks. arXiv preprint arXiv:14117610v3</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhao</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name> (<year>2018</year>) <source>Variational Joint Filtering. arXiv:170709049v3</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pandarinath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>O'Shea</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jozefowicz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Stavisky</surname> <given-names>SD</given-names></name>, <etal>et al</etal>. (<year>2018</year>) <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>. <source>Nature Methods</source> <volume>15</volume>: <fpage>805</fpage>–<lpage>815</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41592-018-0109-9" xlink:type="simple">10.1038/s41592-018-0109-9</ext-link></comment> <object-id pub-id-type="pmid">30224673</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2016</year>) <article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: A simple and flexible framework</article-title>. <source>PLoS Computational Biology</source> <volume>12</volume>: <fpage>e1004792</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004792" xlink:type="simple">10.1371/journal.pcbi.1004792</ext-link></comment> <object-id pub-id-type="pmid">26928718</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Joglekar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2019</year>) <article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title>. <source>Nature Neuroscience</source> <volume>22</volume>: <fpage>297</fpage>–<lpage>306</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-018-0310-2" xlink:type="simple">10.1038/s41593-018-0310-2</ext-link></comment> <object-id pub-id-type="pmid">30643294</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hertäg</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name> (<year>2014</year>) <article-title>Analytical approximations of the firing rate of an adaptive exponential integrate-and-fire neuron in the presence of synaptic noise</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>8</volume>: <fpage>116</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00116" xlink:type="simple">10.3389/fncom.2014.00116</ext-link></comment> <object-id pub-id-type="pmid">25278872</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name> (<year>1995</year>) <article-title>Analysis of fMRI time-series revisited—again</article-title>. <source>Neuroimage</source> <volume>2</volume>: <fpage>173</fpage>–<lpage>181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/nimg.1995.1023" xlink:type="simple">10.1006/nimg.1995.1023</ext-link></comment> <object-id pub-id-type="pmid">9343600</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref041"><label>41</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Durbin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Koopman</surname> <given-names>SJ</given-names></name> (<year>2012</year>) <source>Time series analysis by state space methods</source>: <publisher-name>OUP Oxford</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name> (<year>2015</year>) <article-title>Deep learning</article-title>. <source>Nature</source> <volume>521</volume>: <fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Osindero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name> (<year>2006</year>) <article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural Comput</source> <volume>18</volume>: <fpage>1527</fpage>–<lpage>1554</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2006.18.7.1527" xlink:type="simple">10.1162/neco.2006.18.7.1527</ext-link></comment> <object-id pub-id-type="pmid">16764513</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name> (<year>2016</year>) <source>Deep learning</source>: <publisher-name>MIT press</publisher-name> <publisher-loc>Cambridge</publisher-loc>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Talathi</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Vartak</surname> <given-names>A</given-names></name> (<year>2015</year>) <source>Improving performance of recurrent neural network with relu nonlinearity. arXiv preprint arXiv:151103771</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abarbanel</surname> <given-names>HDI</given-names></name>, <name name-style="western"><surname>Rozdeba</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Shirman</surname> <given-names>S</given-names></name> (<year>2018</year>) <article-title>Machine Learning: Deepest Learning as Statistical Data Assimilation Problems</article-title>. <source>Neural Computation</source> <volume>30</volume>: <fpage>2025</fpage>–<lpage>2055</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco_a_01094" xlink:type="simple">10.1162/neco_a_01094</ext-link></comment> <object-id pub-id-type="pmid">29894650</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lorenz</surname> <given-names>EN</given-names></name> (<year>1963</year>) <article-title>Deterministic nonperiodic flow</article-title>. <source>Journal of the Atmospheric Sciences</source> <volume>20</volume>: <fpage>130</fpage>–<lpage>141</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref048"><label>48</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Takens</surname> <given-names>F</given-names></name> (<year>1981</year>) <chapter-title>Detecting strange attractors in turbulence</chapter-title>. In: <name name-style="western"><surname>Rand</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>L-S</given-names></name>, editors. <source>Dynamical Systems and Turbulence, Lecture notes in Mathematics</source>: <publisher-name>Springer-Verlag</publisher-name>. pp. <fpage>366</fpage>–<lpage>381</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Yorke</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Casdagli</surname> <given-names>M</given-names></name> (<year>1991</year>) <article-title>Embedology</article-title>. <source>Journal of Statistical Physics</source> <volume>65</volume>: <fpage>579</fpage>–<lpage>616</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kantz</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name> (<year>2004</year>) <source>Nonlinear time series analysis</source>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wood</surname> <given-names>SN</given-names></name> (<year>2010</year>) <article-title>Statistical inference for noisy nonlinear ecological dynamic systems</article-title>. <source>Nature</source> <volume>466</volume>: <fpage>1102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09319" xlink:type="simple">10.1038/nature09319</ext-link></comment> <object-id pub-id-type="pmid">20703226</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Pol</surname> <given-names>B</given-names></name> (<year>1926</year>) <article-title>LXXXVIII. On “relaxation-oscillations”.</article-title> <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source> <volume>2</volume>: <fpage>978</fpage>–<lpage>992</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Archer</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name> (<year>2015</year>) <source>Black box variational inference for state space models. arXiv preprint arXiv:151107367</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koppe</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gruppe</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sammer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gallhofer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kirsch</surname> <given-names>P</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Temporal unpredictability of a stimulus sequence affects brain activation differently depending on cognitive task demands</article-title>. <source>Neuroimage</source> <volume>101</volume>: <fpage>236</fpage>–<lpage>244</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2014.07.008" xlink:type="simple">10.1016/j.neuroimage.2014.07.008</ext-link></comment> <object-id pub-id-type="pmid">25019681</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Owen</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>McMillan</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Laird</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Bullmore</surname> <given-names>E</given-names></name> (<year>2005</year>) <article-title>N-back working memory paradigm: a meta-analysis of normative functional neuroimaging studies</article-title>. <source>Hum Brain Mapp</source> <volume>25</volume>: <fpage>46</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.20131" xlink:type="simple">10.1002/hbm.20131</ext-link></comment> <object-id pub-id-type="pmid">15846822</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsuda</surname> <given-names>I</given-names></name> (<year>2015</year>) <article-title>Chaotic itinerancy and its roles in cognitive neurodynamics</article-title>. <source>Current Opinion in Neurobiology</source> <volume>31</volume>: <fpage>67</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2014.08.011" xlink:type="simple">10.1016/j.conb.2014.08.011</ext-link></comment> <object-id pub-id-type="pmid">25217808</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2002</year>) <article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>955</fpage>–<lpage>968</lpage>. <object-id pub-id-type="pmid">12467598</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Stopfer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Friedrich</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Volkovskii</surname> <given-names>A</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Odor encoding as an active, dynamical process: experiments, computation, and theory</article-title>. <source>Annual Review of Neuroscience</source> <volume>24</volume>: <fpage>263</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.24.1.263" xlink:type="simple">10.1146/annurev.neuro.24.1.263</ext-link></comment> <object-id pub-id-type="pmid">11283312</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name> (<year>2013</year>) <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>Nature</source> <volume>503</volume>: <fpage>78</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12742" xlink:type="simple">10.1038/nature12742</ext-link></comment> <object-id pub-id-type="pmid">24201281</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name> (<year>2007</year>) <article-title>Techniques for extracting single-trial activity patterns from large-scale neural recordings</article-title>. <source>Current opinion in neurobiology</source> <volume>17</volume>: <fpage>609</fpage>–<lpage>618</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2007.11.001" xlink:type="simple">10.1016/j.conb.2007.11.001</ext-link></comment> <object-id pub-id-type="pmid">18093826</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nichols</surname> <given-names>ALA</given-names></name>, <name name-style="western"><surname>Eichler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Zimmer</surname> <given-names>M</given-names></name> (<year>2017</year>) <article-title>A global brain state underlies C. elegans sleep behavior</article-title>. <source>Science</source> <volume>356</volume>: <fpage>eaam6851</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aam6851" xlink:type="simple">10.1126/science.aam6851</ext-link></comment> <object-id pub-id-type="pmid">28642382</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koiran</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cosnard</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Garzon</surname> <given-names>M</given-names></name> (<year>1994</year>) <article-title>Computability with low-dimensional dynamical systems</article-title>. <source>Theoretical Computer Science</source> <volume>132</volume>: <fpage>113</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref063"><label>63</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name> (<year>1982</year>) <source>Vision: A computational investigation into the human representation and processing of visual information</source>, <publisher-name>henry holt and co. Inc</publisher-name>, <publisher-loc>New York, NY</publisher-loc> <volume>2</volume>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hertäg</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hass</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Golovko</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2012</year>) <article-title>An approximation to the adaptive exponential integrate-and-fire neuron model allows fast and predictive fitting to physiological data</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>6</volume>: <fpage>62</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2012.00062" xlink:type="simple">10.3389/fncom.2012.00062</ext-link></comment> <object-id pub-id-type="pmid">22973220</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fransén</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tahvildari</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Egorov</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Alonso</surname> <given-names>AA</given-names></name> (<year>2006</year>) <article-title>Mechanism of graded persistent cellular activity of entorhinal cortex layer v neurons</article-title>. <source>Neuron</source> <volume>49</volume>: <fpage>735</fpage>–<lpage>746</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.01.036" xlink:type="simple">10.1016/j.neuron.2006.01.036</ext-link></comment> <object-id pub-id-type="pmid">16504948</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arlot</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Celisse</surname> <given-names>A</given-names></name> (<year>2010</year>) <article-title>A survey of cross-validation procedures for model selection</article-title>. <source>Statistics surveys</source> <volume>4</volume>: <fpage>40</fpage>–<lpage>79</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref067"><label>67</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hastie Т</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name> (<year>2003</year>) <source>Elements of statistical learning: data mining, inference, and prediction</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bergmeir</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Hyndman</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Koo</surname> <given-names>B</given-names></name> (<year>2018</year>) <article-title>A note on the validity of cross-validation for evaluating autoregressive time series prediction</article-title>. <source>Computational Statistics &amp; Data Analysis</source> <volume>120</volume>: <fpage>70</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref069"><label>69</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ozaki</surname> <given-names>T</given-names></name> (<year>2012</year>) <source>Time series modeling of neuroscience data</source>: <publisher-name>CRC Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pathak</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Hunt</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Girvan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ott</surname> <given-names>E</given-names></name> (<year>2017</year>) <article-title>Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos</article-title>: <source>An Interdisciplinary Journal of Nonlinear Science</source> <volume>27</volume>: <fpage>121102</fpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunton</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Proctor</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Kutz</surname> <given-names>JN</given-names></name> (<year>2016</year>) <article-title>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</article-title>. <source>Proceedings of the National Academy of Sciences U S A</source> <volume>113</volume>: <fpage>3932</fpage>–<lpage>3937</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>FS</given-names></name>, <name name-style="western"><surname>Varmus</surname> <given-names>H</given-names></name> (<year>2015</year>) <article-title>A new initiative on precision medicine</article-title>. <source>New England Journal of Medicine</source> <volume>372</volume>: <fpage>793</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1056/NEJMp1500523" xlink:type="simple">10.1056/NEJMp1500523</ext-link></comment> <object-id pub-id-type="pmid">25635347</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Koppe</surname> <given-names>G</given-names></name> (<year>2018</year>) <source>Psychiatric Illnesses as Disorders of Network Dynamics. arXiv:180906303</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name> (<year>2008</year>) <article-title>The dual-state theory of prefrontal cortex dopamine function with relevance to catechol-o-methyltransferase genotypes and schizophrenia</article-title>. <source>Biological Psychiatry</source> <volume>64</volume>: <fpage>739</fpage>–<lpage>749</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.biopsych.2008.05.015" xlink:type="simple">10.1016/j.biopsych.2008.05.015</ext-link></comment> <object-id pub-id-type="pmid">18620336</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Armbruster</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Ueltzhöffer</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Basten</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Fiebach</surname> <given-names>CJ</given-names></name> (<year>2012</year>) <article-title>Prefrontal cortical mechanisms underlying individual differences in cognitive flexibility and stability</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>24</volume>: <fpage>2385</fpage>–<lpage>2399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_00286" xlink:type="simple">10.1162/jocn_a_00286</ext-link></comment> <object-id pub-id-type="pmid">22905818</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Jin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Dynamic functional connectomics signatures for characterization and differentiation of PTSD patients</article-title>. <source>Human Brain Mapping</source> <volume>35</volume>: <fpage>1761</fpage>–<lpage>1778</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.22290" xlink:type="simple">10.1002/hbm.22290</ext-link></comment> <object-id pub-id-type="pmid">23671011</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Damaraju</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Allen</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Belger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ford</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>McEwen</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Dynamic functional connectivity analysis reveals transient states of dysconnectivity in schizophrenia</article-title>. <source>Neuroimage Clinical</source> <volume>5</volume>: <fpage>298</fpage>–<lpage>308</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.nicl.2014.07.003" xlink:type="simple">10.1016/j.nicl.2014.07.003</ext-link></comment> <object-id pub-id-type="pmid">25161896</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rashid</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Damaraju</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Pearlson</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Calhoun</surname> <given-names>VD</given-names></name> (<year>2014</year>) <article-title>Dynamic connectivity states estimated from resting fMRI Identify differences among Schizophrenia, bipolar disorder, and healthy control subjects</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>8</volume>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smetters</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Majewska</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name> (<year>1999</year>) <article-title>Detecting action potentials in neuronal populations with calcium imaging</article-title>. <source>Methods</source> <volume>18</volume>: <fpage>215</fpage>–<lpage>221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/meth.1999.0774" xlink:type="simple">10.1006/meth.1999.0774</ext-link></comment> <object-id pub-id-type="pmid">10356353</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shoham</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Glaser</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Arieli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kenet</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Wijnbergen</surname> <given-names>C</given-names></name>, <etal>et al</etal>. (<year>1999</year>) <article-title>Imaging cortical dynamics at high spatial and temporal resolution with novel blue voltage-sensitive dyes</article-title>. <source>Neuron</source> <volume>24</volume>: <fpage>791</fpage>–<lpage>802</lpage>. <object-id pub-id-type="pmid">10624943</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koppe</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Guloksuz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Reininghaus</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name> (<year>2019</year>) <article-title>Recurrent Neural Networks in Mobile Sampling and Intervention</article-title>. <source>Schizophr Bull</source> <volume>45</volume>: <fpage>272</fpage>–<lpage>276</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/schbul/sby171" xlink:type="simple">10.1093/schbul/sby171</ext-link></comment> <object-id pub-id-type="pmid">30496527</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sugihara</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ye</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>C-h</given-names></name>, <name name-style="western"><surname>Deyle</surname> <given-names>E</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Detecting Causality in Complex Ecosystems</article-title>. <source>Science</source> <volume>338</volume>: <fpage>496</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1227079" xlink:type="simple">10.1126/science.1227079</ext-link></comment> <object-id pub-id-type="pmid">22997134</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref083"><label>83</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hershey</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>PA</given-names></name>. <source>Approximating the Kullback Leibler divergence between Gaussian mixture models</source>; <year>2007</year>. <publisher-name>IEEE</publisher-name>. pp. <fpage>IV-317</fpage>–<lpage>IV-320</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref084"><label>84</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Krzanowski</surname> <given-names>W</given-names></name> (<year>2000</year>) <source>Principles of multivariate analysis</source>: <publisher-name>OUP Oxford</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name> (<year>1977</year>) <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society Series B (methodological)</source>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalman</surname> <given-names>RE</given-names></name> (<year>1960</year>) <article-title>A New Approach to Linear Filtering and Prediction Problems</article-title>. <source>Transactions of the ASME–Journal of Basic Engineering</source>: <fpage>35</fpage>–<lpage>45</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rauch</surname> <given-names>HE</given-names></name>, <name name-style="western"><surname>Striebel</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Tung</surname> <given-names>F</given-names></name> (<year>1965</year>) <source>Maximum likelihood estimates of linear dynamic systems</source>. <volume>3</volume>: <fpage>1445</fpage>–<lpage>1450</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koyama</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pérez-Bolde</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Shalizi</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name> (<year>2010</year>) <article-title>Approximate Methods for State-Space Models</article-title>. <source>Journal of the American Statistical Association</source> <volume>105</volume>: <fpage>170</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1198/jasa.2009.tm08326" xlink:type="simple">10.1198/jasa.2009.tm08326</ext-link></comment> <object-id pub-id-type="pmid">21753862</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brugnano</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Casulli</surname> <given-names>V</given-names></name> (<year>2008</year>) <article-title>Iterative Solution of Piecewise Linear Systems</article-title>. <source>SIAM Journal on Scientific Computing</source> <volume>30</volume>: <fpage>463</fpage>–<lpage>472</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rezende</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Mohamed</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wierstra</surname> <given-names>D</given-names></name> (<year>2014</year>) <source>Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:14014082</source>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref091"><label>91</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Manning</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Raghavan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schütze</surname> <given-names>M</given-names></name> (<year>2008</year>) <source>Introduction to Information Retrieval</source>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gevins</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Bressler</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Cutillo</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Illes</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>JC</given-names></name>, <etal>et al</etal>. (<year>1990</year>) <article-title>Effects of prolonged mental work on functional brain topography</article-title>. <source>Electroencephalography and Clinical Neurophysiology</source> <volume>76</volume>: <fpage>339</fpage>–<lpage>350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0013-4694(90)90035-i" xlink:type="simple">10.1016/0013-4694(90)90035-i</ext-link></comment> <object-id pub-id-type="pmid">1699727</object-id></mixed-citation></ref>
<ref id="pcbi.1007263.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bengtsson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bickel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>B</given-names></name> (<year>2008</year>) <article-title>Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems</article-title>. <source>Probability and statistics: Essays in honor of David A Freedman: Institute of Mathematical Statistics</source>. pp. <fpage>316</fpage>–<lpage>334</lpage>.</mixed-citation></ref>
<ref id="pcbi.1007263.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sattar</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Corchado</surname> <given-names>JM</given-names></name> (<year>2014</year>) <article-title>Fight sample degeneracy and impoverishment in particle filters: A review of intelligent approaches</article-title>. <source>Expert Systems with Applications</source> <volume>41</volume>: <fpage>3944</fpage>–<lpage>3954</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>