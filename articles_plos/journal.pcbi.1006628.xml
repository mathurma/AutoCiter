<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00256</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006628</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Animal studies</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Zebrafish</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Zebrafish</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Animal studies</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Animal models</subject><subj-group><subject>Zebrafish</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Fish</subject><subj-group><subject>Osteichthyes</subject><subj-group><subject>Zebrafish</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Bacteria</subject><subj-group><subject>Vibrio</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Microscopy</subject><subj-group><subject>Light microscopy</subject><subj-group><subject>Fluorescence microscopy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Gastrointestinal tract</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Gastrointestinal tract</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Bacteria</subject><subj-group><subject>Pseudomonas</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Performance of convolutional neural networks for identification of bacteria in 3D microscopy datasets</article-title>
<alt-title alt-title-type="running-head">Neural network performance for identification of bacteria in 3D microscopy datasets</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hay</surname> <given-names>Edouard A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Department of Physics, Institute of Molecular Biology, Materials Science Institute, University of Oregon, Eugene, Oregon, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>McHardy</surname> <given-names>Alice Carolyn</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Helmholtz-Zentrum fur Infektionsforschung GmbH, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">raghu@uoregon.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>12</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>3</day>
<month>12</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>12</issue>
<elocation-id>e1006628</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Hay, Parthasarathy</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006628"/>
<abstract>
<p>Three-dimensional microscopy is increasingly prevalent in biology due to the development of techniques such as multiphoton, spinning disk confocal, and light sheet fluorescence microscopies. These methods enable unprecedented studies of life at the microscale, but bring with them larger and more complex datasets. New image processing techniques are therefore called for to analyze the resulting images in an accurate and efficient manner. Convolutional neural networks are becoming the standard for classification of objects within images due to their accuracy and generalizability compared to traditional techniques. Their application to data derived from 3D imaging, however, is relatively new and has mostly been in areas of magnetic resonance imaging and computer tomography. It remains unclear, for images of discrete cells in variable backgrounds as are commonly encountered in fluorescence microscopy, whether convolutional neural networks provide sufficient performance to warrant their adoption, especially given the challenges of human comprehension of their classification criteria and their requirements of large training datasets. We therefore applied a 3D convolutional neural network to distinguish bacteria and non-bacterial objects in 3D light sheet fluorescence microscopy images of larval zebrafish intestines. We find that the neural network is as accurate as human experts, outperforms random forest and support vector machine classifiers, and generalizes well to a different bacterial species through the use of transfer learning. We also discuss network design considerations, and describe the dependence of accuracy on dataset size and data augmentation. We provide source code, labeled data, and descriptions of our analysis pipeline to facilitate adoption of convolutional neural network analysis for three-dimensional microscopy data.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The abundance of complex, three dimensional image datasets in biology calls for new image processing techniques that are both accurate and fast. Deep learning techniques, in particular convolutional neural networks, have achieved unprecedented accuracies and speeds across a large variety of image classification tasks. However, it is unclear whether or not their use is warranted in noisy, heterogeneous 3D microscopy datasets, especially considering their requirements of large, labeled datasets and their lack of comprehensible features. To asses this, we provide a case study, applying convolutional neural networks as well as feature-based methods to light sheet fluorescence microscopy datasets of bacteria in the intestines of larval zebrafish. We find that the neural network is as accurate as human experts, outperforms the feature-based methods, and generalizes well to a different bacterial species through the use of transfer learning.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>0922951</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>1427957</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000937</institution-id>
<institution>M.J. Murdock Charitable Trust</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100001201</institution-id>
<institution>Kavli Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>Kavli Microbiome Ideas Challenge</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
<institution>National Institute of General Medical Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>P50GM098911</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6006-4749</contrib-id>
<name name-style="western">
<surname>Parthasarathy</surname> <given-names>Raghuveer</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award006">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>P01HD22486</award-id>
</award-group>
<funding-statement>Research reported in this publication was supported in part by the National Science Foundation (NSF; <ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>) under awards 0922951 (to RP) and 1427957 (to RP); the M. J. Murdock Charitable Trust (<ext-link ext-link-type="uri" xlink:href="https://murdocktrust.org" xlink:type="simple">https://murdocktrust.org</ext-link>), from the University of Oregon through an Incubating Interdisciplinary Initiatives Award, and an award from the Kavli Microbiome Ideas Challenge, a project led by the American Society for Microbiology in partnership with the American Chemical Society and the American Physical Society and supported by The Kavli Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.kavlifoundation.org" xlink:type="simple">http://www.kavlifoundation.org</ext-link>). This research was supported by the National Institutes of Health (NIH; nih.gov) as follows: by the National Institute of General Medical Sciences under award number P50GM098911 and by the National Institute of Child Health and Human Development under award P01HD22486, which provided support for the University of Oregon Zebrafish Facility. The content is solely the responsibility of the authors and does not represent the official views of the NIH, NSF, or other funding agencies. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="17"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-12-13</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Analysis code is provided through a public GitHub repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>. Also, all training datasets are publicly available online, as described in the manuscript. All of the 28x28x8 348 pixel images used for bacterial classification as well as the corresponding labels are available at the Cell Image Library, <ext-link ext-link-type="uri" xlink:href="http://cellimagelibrary.org/home" xlink:type="simple">http://cellimagelibrary.org/home</ext-link>, with accession numbers 50508, 50509, 50510.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The continued development and widespread adoption of three-dimensional microscopy methods enables insightful observations into the structure and time-evolution of living systems. Techniques such as confocal microscopy [<xref ref-type="bibr" rid="pcbi.1006628.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref002">2</xref>], two-photon excitation microscopy [<xref ref-type="bibr" rid="pcbi.1006628.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref006">6</xref>], and light sheet fluorescence microscopy [<xref ref-type="bibr" rid="pcbi.1006628.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref012">12</xref>] have provided insights into neural activity, embryonic morphogenesis, plant root growth, gut bacterial competition, and more. Extracting quantitative information from biological image data often calls for identification of objects such as cells, organs, or organelles in an array of pixels, a task that can especially challenging for three-dimensional datasets from live imaging due to their large size and potentially complex backgrounds. Aberrations and scattering in deep tissue can, for example, introduce noise and distortions, and live animals often contain autofluorescent biomaterials that complicate the discrimination of labeled features of interest. Moreover, traditional image processing techniques tend to require considerable manual curation, as well as user input regarding which features, such as cell size, homogeneity, or aspect ratio, should guide and parameterize analysis algorithms. These features may be difficult to know a priori, and need not be the features that lead to the greatest classification accuracy. As data grow in both size and complexity, and as imaging methods are applied to an ever-greater variety of systems, standard approaches become increasingly unwieldy, motivating work on better computational methods.</p>
<p>Machine learning methods, in particular convolutional neural networks (ConvNets), are increasingly used in many fields and have achieved unprecedented accuracies in image classification tasks [<xref ref-type="bibr" rid="pcbi.1006628.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref016">16</xref>]. The objective of supervised machine learning is to use a labeled dataset to train a computer to make classifications or predictions given new, unlabeled data. Traditional feature-based machine learning algorithms, such as support vector machines and random forests, make use of manually determined characteristics, which in the context of image data could be the eccentricity of objects, their size, their median pixel intensity, etc. The first stages in the implementation of these algorithms, therefore, are the identification of objects by image segmentation methods and the calculation of the desired feature values. In contrast, convolutional neural networks use the raw pixel values as inputs, eliminating the need for determination of object features by the user. Convolutional neural networks use layers consisting of multiple kernels, numerical arrays acting as filters, which are convolved across the input taking advantage of locally correlated information. These kernels are updated as the algorithm is fed labeled data, converging by numerical optimization methods on the weights that best match the training data. ConvNets can contain hundreds of kernels over tens or hundreds of layers which leads to hundreds of thousands of parameters to be learned, requiring considerable computation and, importantly, large labeled datasets to constrain the parameters. Over the past decade, the use of ConvNets has been enabled by advances in GPU technology, the availability of large labeled datasets in many fields, and user-friendly deep learning software such as TensorFlow [<xref ref-type="bibr" rid="pcbi.1006628.ref017">17</xref>], Theano [<xref ref-type="bibr" rid="pcbi.1006628.ref018">18</xref>], Keras [<xref ref-type="bibr" rid="pcbi.1006628.ref019">19</xref>], and Torch [<xref ref-type="bibr" rid="pcbi.1006628.ref020">20</xref>]. In addition to high accuracy, ConvNets tend to have fast classification speeds compared to traditional image processing methods. There are drawbacks, however, to neural network approaches. As noted, they require large amounts of manually labeled data for training the network. Furthermore, their selection criteria, in other words the meanings of the kernels’ parameters, are not easily understandable by humans [<xref ref-type="bibr" rid="pcbi.1006628.ref021">21</xref>].</p>
<p>There have been several notable examples of machine learning methods applied to biological optical microscopy data [<xref ref-type="bibr" rid="pcbi.1006628.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref023">23</xref>], including bacterial identification from 2D images using deep learning [<xref ref-type="bibr" rid="pcbi.1006628.ref024">24</xref>], pixel-level image segmentation using deep learning [<xref ref-type="bibr" rid="pcbi.1006628.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref027">27</xref>], subcellular protein classification [<xref ref-type="bibr" rid="pcbi.1006628.ref028">28</xref>], detection of structures within C. elegans from 2D projections of 3D image stacks using support vector machines [<xref ref-type="bibr" rid="pcbi.1006628.ref029">29</xref>], and more [<xref ref-type="bibr" rid="pcbi.1006628.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref034">34</xref>]. Nonetheless, it is unclear whether ConvNet approaches are successful for thick, three-dimensional microscopy datasets, whether their potentially greater accuracy outweighs the drawbacks noted above, and what design principles should guide the implementation of ConvNets for 3D microscopy data.</p>
<p>To address these issues, we applied a deep convolutional neural network to analyze three-dimensional light sheet fluorescence microscopy datasets of gut bacteria in larval zebrafish (<xref ref-type="fig" rid="pcbi.1006628.g001">Fig 1a and 1b</xref>) and compared its performance to that of other methods. These image sets, in addition to representing a major research focus of our lab related to the aim of understanding the structure and dynamics of gut microbial communities [<xref ref-type="bibr" rid="pcbi.1006628.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref037">37</xref>], serve as exemplars of the large, complex data types increasingly enabled by new imaging methods. Each 3D image occupies roughly 5 GB of storage space and consists of approximately 300 slices separated by 1 micron, each slice consisting of 6000 x 2000 pixel 2D images (975x325 microns). These images include discrete bacterial cells, strong and variable autofluorescence from the mucus-rich intestinal interior [<xref ref-type="bibr" rid="pcbi.1006628.ref038">38</xref>], autofluorescent zebrafish cells, inhomogeneous illumination due to shadowing of the light sheet by pigment cells, and noise of various sorts. The bacteria examined here exist predominantly as discrete, planktonic individuals. Other species in the zebrafish gut exhibit pronounced aggregation; identification of aggregates is outside the scope of this work, though we note that the segmentation of aggregates is much less challenging than identification of discrete bacterial cells, due to their overall brightness and size. The goal of the analysis described here is to correctly classify regions of high intensity as bacteria or as non-bacterial objects.</p>
<fig id="pcbi.1006628.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006628.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Images of bacteria in the intestine of larval zebrafish.</title>
<p>a) Schematic illustration of a larval zebrafish with the intestine highlighted in red. Scale bar: 0.5 mm. b) Single optical section from light sheet fluorescence microscopy of the anterior intestine of a larval zebrafish colonized by GFP expressing bacteria of the commensal Vibrio species ZWU0020. Scale bar: 50 microns. c) z, y and x projections from 28x28x8 pixel regions of representative individual Vibrio bacteria, d) non-bacterial noise, e) individual bacteria of the genus Pseudomonas, species ZWU0006, and f) autofluorescent zebrafish cells.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.g001" xlink:type="simple"/>
</fig>
<p>Using multiple testing image sets, we compared the performance of the convolutional neural network to that of humans as well as random forest and support vector machine classifiers. In brief, the ConvNet’s accuracy is similar to that of humans, and it outperforms the other machine classifiers in both accuracy and speed across all tested datasets. In addition, the ConvNet performs well when applied to planktonic bacteria of a different genus through the use of transfer learning. Transfer learning has been shown to be effective in biological image data in which partial transference of network weights from 2D images dramatically lowers the amount of new labeled data that is required [<xref ref-type="bibr" rid="pcbi.1006628.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref039">39</xref>]. We explored the impacts on the ConvNet’s performance of network structure, the degree of data augmentation using rotations and reflections of the input data, and the size of the training data set, providing insights that will facilitate the use of ConvNets in other biological imaging contexts.</p>
<p>Analysis code as well as all ∼ 21, 000 manually labeled 3D image regions-of-interest are provided; see <xref ref-type="sec" rid="sec009">Methods</xref> for details and urls to data locations.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Data</title>
<p>The image data we sought to classify consist of three-dimensional arrays of pixels obtained from light sheet fluorescence microscopy of bacteria in the intestines of larval zebrafish [<xref ref-type="bibr" rid="pcbi.1006628.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1006628.ref037">37</xref>]. <xref ref-type="fig" rid="pcbi.1006628.g001">Fig 1B</xref> shows a typical optical section from an initially germ-free larval zebrafish, colonized by a single labeled bacterial species made up of discrete, planktonic individuals expressing green fluorescent protein; a three-dimensional scan is provided as Supplementary Movie 1. All the data assessed here were derived from fish that were reared germ free (devoid of any microbes) [<xref ref-type="bibr" rid="pcbi.1006628.ref040">40</xref>] and then either mono-associated with a commensal bacterial species or left germ free. Nine scans are of fish mono-associated with the commensal species ZWU0020 of the genus Vibrio [<xref ref-type="bibr" rid="pcbi.1006628.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref042">42</xref>], two scans are of fish in which the zebrafish remained germ-free, and a single scan is from a fish mono-associated with Pseudomonas ZWU0006 [<xref ref-type="bibr" rid="pcbi.1006628.ref036">36</xref>]. For each 3D scan, we first determined the intestinal space of the zebrafish using simple thresholding and detected bright objects (“blobs”) using a difference of Gaussians method described further in Methods. From each blob, we extracted 28x28x8 pixel arrays (4.5x4.5x8 microns), which served as the input data to the neural network, to be classified as bacterial or non-bacterial.</p>
<p>Since there is no way to obtain ground truth values for bacterial identity in images, we manually classified blobs to serve as the training data for the neural network, using our expertise derived from considerable prior work on three dimensional bacterial imaging. Notably, in prior work we showed that the total bacterial abundance determined by manually corroborated feature-based bacterial identification from light sheet data corresponds well with the total bacterial abundance as measured through gut dissection and serial plating assays [<xref ref-type="bibr" rid="pcbi.1006628.ref035">35</xref>]. In <xref ref-type="fig" rid="pcbi.1006628.g001">Fig 1C–1F</xref> we show representative images of blobs corresponding to bacteria and noise.</p>
<p>In order to estimate an upper bound on the classification accuracy we can expect from the learning algorithms, we chose a single image scan which we judged to be typical of a noisy, complex 3D image of the intestine of a larval zebrafish colonized by bacteria. We then had six lab members with least two years’ experience with light sheet microscopy of bacteria individually label each of the detected potential objects as either a bacterium or not. We show in <xref ref-type="fig" rid="pcbi.1006628.g002">Fig 2A</xref> the agreement between lab members. Excluding human 3 the agreement between any pair of humans is always above 0.87. The outlier, human 3, is the person with the least experience with the imaging data, namely the principal investigator.</p>
<fig id="pcbi.1006628.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006628.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Creation of the 3-D convolutional neural network.</title>
<p>a) Agreement matrix between six individuals (members of the authors’ research group), evaluated on a single dataset of images of Vibrio bacteria, and between those humans and the convolutional neural network. b) Accuracy vs number of kernels per layer using cross validation across the various imaging datasets, where the x-axis denotes the number of kernels in the first convolutional layer. The second convolutional layer for each plotted point has twice as many kernels as the first.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.g002" xlink:type="simple"/>
</fig>
<p>We next created a set of labeled data by manual classification of blobs from the 9 Vibrio scans and 2 scans of germ-free fish, consisting in total of over 20,000 objects. Including scans from germ-free fish is particularly important to enable accurate counting of low numbers of bacteria, which arise naturally due to extinction events [<xref ref-type="bibr" rid="pcbi.1006628.ref010">10</xref>] and population bottlenecks [<xref ref-type="bibr" rid="pcbi.1006628.ref041">41</xref>].</p>
</sec>
<sec id="sec004">
<title>Network architecture</title>
<p>As detailed in Methods, we used Google’s open-source Tensorflow framework [<xref ref-type="bibr" rid="pcbi.1006628.ref017">17</xref>] to create, test, and implement 3D convolutional neural networks. Such networks have many design parameters and options, including the number, size, and type of layers, the kernel size, the downsizing of convolution output by pooling, and parameter regularization. In general, overly small networks can lack the complexity to characterize image data, though their limited parameter space is less likely to lead to overfitting. Conversely, larger networks can tackle more complex classification schemes, but demand more training data to constrain the large number of parameters, and also carry a greater computational load. In between these extremes, many design variations will typically give similar classification accuracy. We chose a simple architecture consisting of two convolutional layers followed by a fully connected layer. The first and second convolutional layers contain 16 and 32 5x5x2 kernels, respectively. Each layer is followed by 2x2x2 max pooling as further described in Methods. The final layer is a fully connected layer consisting of 1024 neurons with a dropout rate of 0.5 during training. After this, softmax regression is used for binary classification.</p>
<p>We explored various alterations of our network architecture, and illustrate here the effect of simply varying the number of kernels per convolutional layer. We assessed the classification accuracy as a function of the number of kernels in layer 1, with the number of kernels in layer 2 being double this. Accuracy was calculated using cross validation, training on all but one image dataset (where an image dataset is a complete three-dimensional scan of the gut of one zebrafish), testing on the remaining image dataset, and repeating with different train/test combinations. The network accuracy initially increases with kernel number and plateaus at roughly 16 kernels, beyond which the variance in accuracy increases (<xref ref-type="fig" rid="pcbi.1006628.g002">Fig 2B</xref>). Therefore, increasing the number of kernels beyond approximately 16 gives little or no improvement in accuracy at the expense of model complexity and increased variability. We note that there are many ways to alter network complexity, for example adding or removing layers, all of which may be interesting to investigate. Here, a rather small model consisting of two layers is sufficient to achieve human-level accuracy, suggesting that adding layers is unlikely to be useful.</p>
</sec>
<sec id="sec005">
<title>Network accuracy across image datasets</title>
<p>We trained the ConvNet using manually labeled data from eight of the Vibrio image datasets and the two datasets from germ-free fish (devoid of gut bacteria) and then tested it on the remaining manually labeled Vibrio image dataset that was used to assess inter-human variability, described above. The agreement between the neural network and humans (mean ± std. dev. 0.89 ± 0.01) was indistinguishable from the inter-human agreement (mean ± std. dev. 0.90 ± 0.02), again excluding human 3, indicating that the ConvNet achieves the practical maximum of bacterial classification accuracy (<xref ref-type="fig" rid="pcbi.1006628.g002">Fig 2A</xref>). Examples of images for which all humans agreed on the classification, and in which there was disagreement, are provided in <xref ref-type="supplementary-material" rid="pcbi.1006628.s001">S1 Fig</xref>.</p>
<p>To further test the network’s consistency across different imaging conditions we applied it separately to each of the 3D image datasets of larval zebrafish intestines. We also tested, with the same procedure and data, random forest and support vector machine classifiers to address the question of whether or not the ConvNet outperforms typical feature based learning algorithms. We first consider two experiment types: zebrafish intestines mono-associated with Vibrio ZWU0020 (9 image datasets, i.e. 9 complete three-dimensional scans from of different zebrafish) and germ-free zebrafish (2 image datasets). Classifier accuracy for each Vibrio-colonized or empty-gut image scan was determined by cross-validation, training the network using all of the other image datasets, and testing on the dataset of interest. To test the variance in accuracy due to the training process, we performed three repetitions of each train/test combination using the same data.</p>
<p>We found that the neural network outperforms the feature based algorithms on every image dataset (<xref ref-type="fig" rid="pcbi.1006628.g003">Fig 3</xref>), and also shows less variation in accuracy between the different datasets. The enhanced accuracy from the neural network is especially dramatic for germ-free datasets, for which it achieves over 90% accuracy, in contrast to less than 75% for feature based methods. For a given test dataset, the training variance for the convolutional neural network is small but nonzero, indicating that the network training algorithm finds similar, but not identical, minima with different (random) initializations on the same training data. It is also small for the random forest classifier. Interestingly, it is zero for the SVM classifier, indicating that given the same dataset, the algorithm is finding the same minimum.</p>
<fig id="pcbi.1006628.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006628.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison of Convnet and feature based learning algorithms across all datasets.</title>
<p>Comparison of accuracies for the various learning algorithms (convolutional neural network, support vector classifier, and random forest) across different Vibrio image datasets, as well as two image datasets from fish devoid of gut bacteria. Each accuracy was determined by training on the data from all of the other datasets, and testing on the dataset of interest.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.g003" xlink:type="simple"/>
</fig>
<p>To further verify the robustness of our accuracy measures, we performed tests using a manually labeled image dataset that was completely distinct from those previously considered, and that therefore played no role in cross-validation or other prior work. This new test set consisted of 1302 images of bacteria (482 images) or noise (840 images). We determined the classification accuracy of our convolutional neural network to be 89.3%, the support vector classifier to be 83.1%, and the random forest classifier to be 78.5%, in agreement with the prior assessments.</p>
<p>The random forest, support vector machine, and neural network classifiers process roughly 300, 400, and 950 images per second, respectively; i.e. the neural network runs 2-3 times faster than the feature based learning algorithms on the same data.</p>
</sec>
<sec id="sec006">
<title>Training size and data augmentation</title>
<p>Convolutional Neural Networks famously require large amounts of training data which must often, as is the case here, be evaluated and curated by hand. To assess the scale of manual classification required for good algorithm performance, which is a key issue for future adoption of neural networks in biological image analysis, we explored the effect on the network’s accuracy of varying the amount of training data. We set aside 25% of the images from each of the Vibrio and germ-free fish image scans and trained the network using an increasing number of images from the remaining data. We increased the amount of training data in two different ways. First, we consecutively added to the training set all images from each image dataset excluding a subset of the images previously reserved for testing (labeled “New datasets” in <xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4A</xref>). Second, we randomly shuffled the training images from all the image scans, adding 1500 images to the training set over each iteration (labeled “Train/test split” in <xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4A</xref>). For the first method, enlargement of the training set corresponds to a greater amount of data as well as data from more diverse biological sources. For the second, data size increases but the biological variation sampled is held constant. In both cases, accuracy plateaus at a number of images on the order of 10,000 (<xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4A</xref>). The rise in accuracy with increasing training data size is only slightly more shallow with the first method, surprisingly, demonstrating that within-sample variation is sufficient to train the network.</p>
<fig id="pcbi.1006628.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006628.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Data augmentation.</title>
<p>Examining the accuracy of the CNN as a function of a) varying the training data size by adding images from biologically distinct datasets (New datasets) or by adding images randomly from the full set of images (Train/test split), and b) transformation of the data by image rotations and reflections. In (a), the two empty circles represent the inclusion of the datasets from empty (germ-free) zebrafish intestines.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.g004" xlink:type="simple"/>
</fig>
<p>Data augmentation, the alteration of input images through mirror reflections, rotations, cropping, and the addition of noise, etc., is commonly used in machine learning to enhance training dataset size and enable robust training of neural networks. To characterize the utility of data augmentation for 3D bacterial images, we focused in particular on image rotations and reflections, because the bacteria have no preferred orientation and hence augmentation by these methods creates realistic training images. We note that data augmentation is not necessary for feature based learning methods in which parity and rotational invariance can be built into the features used for classification. Obviously, augmented data is not independent of the actual training data, and so does not supply wholly new information. We were curious as to how including rotated and reflected versions of previously seen data compares, in terms of network performance, to adding entirely new data, a comparison that is useful if evaluating the necessity of performing additional imaging experiments. To test this, we compared the accuracies of the network when adding new data to that when adding rotated and reflected versions of existing data. We started with a fixed number of 1500 total objects randomly sampled from the entire set and, in the case of including new data, added another random 1500 objects at each iteration. For the augmented data, we applied random rotations and reflections to the original 1500 objects to iteratively increase the training size by 1500 objects. Each trained network was tested on the same test set of objects as that of <xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4A</xref>. As shown in <xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4B</xref>, the addition of new data leads to a plateau in accuracy of roughly 90% while for augmented data the plateau value is around 88%. This result demonstrates that, in the context of our network, simply augmenting existing data can raise classification accuracy to nearly the optimal level achieved by new, independent data.</p>
</sec>
<sec id="sec007">
<title>Transfer learning</title>
<p>We assessed the accuracy of the convolutional neural network on images of discrete gut bacteria of another species, of the genus Pseudomonas. Training solely on the Vibrio images and testing on Pseudomonas gives ∼ 75% accuracy (<xref ref-type="fig" rid="pcbi.1006628.g005">Fig 5</xref>). However, this is much lower than the ∼ 85 − 95% accuracy obtained on Vibrio images (<xref ref-type="fig" rid="pcbi.1006628.g004">Fig 4</xref>); the Pseudomonas species is not an exact morphological mimic of the Vibrio species. The Pseudomonas dataset is small (1190 images); using 80% of its images for de novo neural network training gives ∼ 72% accuracy in identifying Pseudomonas in test datasets (<xref ref-type="fig" rid="pcbi.1006628.g005">Fig 5</xref>). We suspected that the general similarity of each species as rod-like, few-micron-long cells would allow transfer learning, in which a model trained for one task is used as the starting point for training for another task [<xref ref-type="bibr" rid="pcbi.1006628.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref044">44</xref>]. Using the network weights from training on Vibrio image datasets, as before, as the starting values for training on the small Pseudomonas dataset gives over 85% accuracy in classifying Pseudomonas (<xref ref-type="fig" rid="pcbi.1006628.g005">Fig 5</xref>).</p>
<fig id="pcbi.1006628.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006628.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Transfer learning on new bacterial species.</title>
<p>The accuracy of Pseudomonas classification with convolutional neural networks trained in different ways. “Vibrio” indicates training on images of Vibrio bacteria, “Pseudomonas” indicates training on the small Pseudomonas image dataset, and “Transfer” indicates using the Vibrio-derived network weights as the starting point for training on Pseudomonas images. For training only on Vibrio images, the different data points come from random weight initialization, random data ordering, and random augmentation. For training only on Pseudomonas images, and for transfer learning, the different data points are from random train/test splits of the Pseudomonas data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.g005" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>We find that a 3D convolutional neural network for binary classification of bacteria and non-bacterial objects in 3D microscopy data of the larval zebrafish gut yields high accuracy without unreasonably large demands on the amount of manually curated training data. Specifically, the convolutional neural network obtains human-expert-level accuracy, runs 2-3 times faster than other standard machine learning methods, and is consistent across different datasets and across planktonic bacteria from two different genera through the use of transfer learning. It reaches these performance metrics after training on fewer than 10,000 human-classified images, which require approximately 20 person-hours of manual curation to generate. Moreover, augmented data in the form of rotations and reflections of real data contributes effectively to network training, further reducing the required manual labor. Experiments of the sort presented here typically involve many weeks of laboratory work. Neural network training, therefore, is a relatively small fraction of the total required time.</p>
<p>In many biological imaging experiments, including our own, variety and similarity are both present. Multiple distinct species or cell types may exist, each different, but with some morphological similarities. It is therefore useful to ask whether such similarities can be exploited to constrain the demands of neural network training. The concept of transfer learning addresses this issue, and we find that applying it to our bacterial images achieves high accuracy despite small labeled datasets, an observation that we suspect will apply to many image-based studies. Transfer learning is a rapidly growing area of interest, with an increasing number of tools and methods available. There are likely many possibilities for further performance enhancements to network performance via transfer learning, beyond the scope of this study. One commonly used approach is to train initially on a large, publicly available, annotated dataset such as ImageNet. It is not likely that ImageNet’s set of two-dimensional images of commonplace objects will be better than actual 3D bacterial data for classifying 3D bacterial images. Nonetheless, it would be interesting to examine whether training using ImageNet or other standard datasets could establish primitive filters on which 3D convolutional neural networks could build. In addition, given the rapid growth of machine learning approaches in biology, it is likely that large, annotated datasets of particular relevance to tasks such as those described here will be developed, further enabling transfer learning.</p>
<p>Though the data presented here came from a particular experimental system, consisting of fluorescently labeled bacterial species within a larval zebrafish intestine imaged with light sheet fluorescence microscopy, they exemplify general features of many contemporary three-dimensional live imaging applications, including large data size, high and variable backgrounds, optical aberrations, and morphological heterogeneity. As such, we suggest that the lessons and analysis tools provided here should be widely applicable to microbial communities [<xref ref-type="bibr" rid="pcbi.1006628.ref045">45</xref>] as well as eukaryotic multicellular organisms.</p>
<p>We expect the use of convolutional neural networks in biological image analysis to become increasingly widespread due to the combination of efficacy, as illustrated here, and the existence of user-friendly tools, such as TensorFlow, that make their implementation straightforward. We can imagine several extensions of the work we have described. Considering gut bacteria in particular, extending neural network methods to handle bacterial aggregates is called for by observations of a continuum of planktonic and aggregated morphologies [<xref ref-type="bibr" rid="pcbi.1006628.ref036">36</xref>]. Considering 3D images more generally, we note that the approach illustrated has as its first step detection of candidate objects (“blobs”), which requires choices of thresholding and filtering parameters. Alternatively, pixel-by-pixel segmentation is in principle possible using recently developed network architectures [<xref ref-type="bibr" rid="pcbi.1006628.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref046">46</xref>], which could enable completely automated processing of 3D fluorescence images. In addition, pixel-based identification of overall morphology (for example, the location of the zebrafish gut) could further enhance classification accuracy, by incorporating anatomical information that constrains the possible locations of particular cell types.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec010">
<title>Light sheet microscopy image data</title>
<p>Three-dimensional scans of the intestines of larval zebrafish, derived germ-free and colonized by fluorescently labeled bacteria prior to imaging, were obtained using light sheet fluorescence microscopy as described in Refs. [<xref ref-type="bibr" rid="pcbi.1006628.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref036">36</xref>]. All experiments involving zebrafish were carried out in accordance with protocols approved by the University of Oregon Institutional Animal Care and Use Committee.</p>
<p>The microscope was based on the design from Keller et al [<xref ref-type="bibr" rid="pcbi.1006628.ref006">6</xref>], and has been described elsewhere [<xref ref-type="bibr" rid="pcbi.1006628.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006628.ref045">45</xref>]. In brief: a laser is rapidly oscillated creating a thin sheet of light used to illuminate a section of the specimen, in this case, a larval zebrafish. An objective lens is seated perpendicular to the laser sheet, focusing two-dimensional images onto a sCMOS camera. The specimen is scanned through the sheet along the detection axis, thereby constructing a 3D image. The camera exposure time was 30 ms, and the laser power of the laser was 5 mW as measured between the theta-lens and excitation objective.</p>
<p>Of the twelve image datasets used for this work, nine were of the zebrafish commensal bacterium Vibrio sp. ZWU0020, one was of a Pseudomonas commensal sp. ZWU0006, and two were from germ-free fish, devoid of any bacteria.</p>
<p>An example 3D image dataset of the anterior “bulb” of one larval zebrafish gut is available at the link noted in the README.md file at github: <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>, together with the 6 lab members’ labels for each detected object in the volume, the convolutional neural network’s classification, and each of the extracted region-of-interest voxels. Other image sets are available upon request; for each zebrafish gut, the full image dataset is roughly 1 GB in size.</p>
</sec>
<sec id="sec011">
<title>Segmentation and blob detection</title>
<p>Rough segmentation of the intestine was performed using histogram equalization of each individual z-stack followed by a moving average over 30 consecutive images in the z-stack followed by hard thresholding to create a binary mask that overestimated the size of the intestine. While extremely rough, this technique requires no manual editing or outlining. After this, blob detection was performed using the difference of Gaussians technique from the scikit-image library on each two-dimensional image, and the blobs were linked together across consecutive images in each stack. Regions 28x28x8 pixels in size centered at each detected blob were then saved to be labeled by hand as either a bacterium or noise. The code for extracting the regions of interest is publicly available on Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>.</p>
<p>From the 12 datasets, 20,929 images were hand labeled of which 38% were bacteria and 62% were noise. Hand labeling took roughly 1-2 hours per scan. All of the 28x28x8 pixel images and the corresponding labels are available from links in the README.md file at the Github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>.</p>
<p>All code for the project was written in Python.</p>
</sec>
<sec id="sec012">
<title>Random forest and support vector machine classifiers</title>
<p>Over sixty features were created initially. These were assessed using scikit-learn’s feature_importances_, from which the thirty one most helpful features were retained. The features used included geometric properties obtained by ellipse-fitting and texture-based characteristics; a detailed list is provided in the python code features.py provided on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>. The data were tested using both a random forest and support vector classifier from the scikit-learn library. The random forest used 500 estimators. The support vector classifier from sci-kit learn, sklearn.svm.SVC(), was tested over a range of parameters and kernels using scikit-learn’s GridSearchCV which yielded highest accuracy when using a radial basis function kernel with penalty C = 1.</p>
</sec>
<sec id="sec013">
<title>Convolutional neural network</title>
<p>The 3D convolutional neural network was created using Google’s TensorFlow. Each input image was 28x28x8 pixels. The network consisted of two convolutional layers followed by a fully connected layer. The first layer was composed of 16, 5x5x2 kernels of stride 2 and same padding followed by 2x2x2 max pooling, the second layer contained 32 5x5x2 kernels of the same stride and padding and was also followed by 2x2x2 max pooling. We chose to double the number of kernels after max pooling as in [<xref ref-type="bibr" rid="pcbi.1006628.ref047">47</xref>]. After the final convolutional layer we employed a fully connected layer consisting of 1024 neurons. The classes were then determined using a softmax layer. The network had a dropout of 0.5, a learning rate of 0.0001 and the data was trained over 120 epochs randomly rotating and reflecting each image over each epoch unless otherwise specified. The weights were updated using the Adam optimization method and we use leaky-ReLu activation functions. During each epoch of training, each input image has a fifty percent probability of receiving a reflection in x, y and z followed by a fifty percent probability of subsequently being transposed. This particular scheme was chosen due to its low computational load. We have made the code for this convolutional neural network available on Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/rplab/Bacterial-Identification" xlink:type="simple">https://github.com/rplab/Bacterial-Identification</ext-link>.</p>
</sec>
<sec id="sec014">
<title>Computer specs and timing</title>
<p>The code was implemented on using python 3.5 on Ubuntu 16.04, with a Intel Core i7-4790 CPU with an Nvidia GeForce GTX 1060 graphics card on a computer with 32 GB of RAM. With this hardware it took roughly one minute to train and create the features for the RF and SVC using about 17,000 images, and roughly one hour to train the 3D ConvNet on the same number of images.</p>
</sec>
</sec>
<sec id="sec015">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006628.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Examples of agreement and disagreement in bacterial classification.</title>
<p>(a) Two example volumes in which all humans classified the object as a bacterium; (b) Two example volumes in which all humans classified the object as noise; (c) Two example volumes in which 50% humans classified the object as a bacterium, and 50% as noise. As in <xref ref-type="fig" rid="pcbi.1006628.g001">Fig 1</xref>, these are xy-, xz-, and yz- projections of the 3D volumes (top to bottom), with the images spanning 4.5 μm in x and y, and 8.0 μm in z. Please note that all 21000 manually classified image volumes, along with labels, are made available for the reader.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006628.s002" mimetype="video/x-msvideo" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006628.s002" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>A light sheet fluorescence microscopy scan through the anterior “bulb” of a larval zebrafish gut, colonized by Vibrio ZWU0020 expressing green fluorescent protein.</title>
<p>Each frame is one z-section, with 1 μm spacing.</p>
<p>(AVI)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Rose Sockol and the University of Oregon Zebrafish Facility staff for fish husbandry, Sophie Sichel for preparation of germ-free zebrafish, and many members of the authors’ research group for useful comments and conversations.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006628.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Burgstaller</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vierkotten</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lindner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Königshoff</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eickelberg</surname> <given-names>O</given-names></name>. <article-title>Multidimensional immunolabeling and 4D time-lapse imaging of vital ex vivo lung tissue</article-title>. <source>American Journal of Physiology-Lung Cellular and Molecular Physiology</source>. <year>2015</year>;<volume>309</volume>(<issue>4</issue>):<fpage>L323</fpage>–<lpage>L332</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/ajplung.00061.2015" xlink:type="simple">10.1152/ajplung.00061.2015</ext-link></comment> <object-id pub-id-type="pmid">26092995</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Weigert</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Porat-Shliom</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Amornphimoltham</surname> <given-names>P</given-names></name>. <article-title>Imaging cell biology in live animals: Ready for prime time</article-title>. <source>The Journal of Cell Biology</source>. <year>2013</year>;<volume>201</volume>(<issue>7</issue>):<fpage>969</fpage>–<lpage>979</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1083/jcb.201212130" xlink:type="simple">10.1083/jcb.201212130</ext-link></comment> <object-id pub-id-type="pmid">23798727</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Carvalho</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Heisenberg</surname> <given-names>CP</given-names></name>. In: <source>Imaging Zebrafish Embryos by Two-Photon Excitation Time-Lapse Microscopy</source>. <publisher-loc>Totowa, NJ</publisher-loc>: <publisher-name>Humana Press</publisher-name>; <year>2009</year>. p. <fpage>273</fpage>–<lpage>287</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-1-60327-977-2_17" xlink:type="simple">https://doi.org/10.1007/978-1-60327-977-2_17</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ahrens</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Orger</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Robson</surname> <given-names>DN</given-names></name>, <name name-style="western"><surname>Schier</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Engert</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title>. <source>Nature</source>. <year>2012</year>;<volume>485</volume>:<fpage>471</fpage> EP–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11057" xlink:type="simple">10.1038/nature11057</ext-link></comment> <object-id pub-id-type="pmid">22622571</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Yasuda</surname> <given-names>R</given-names></name>. <article-title>Principles of Two-Photon Excitation Microscopy and Its Applications to Neuroscience</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>50</volume>(<issue>6</issue>):<fpage>823</fpage>–<lpage>839</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.05.019" xlink:type="simple">https://doi.org/10.1016/j.neuron.2006.05.019</ext-link>. <object-id pub-id-type="pmid">16772166</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keller</surname> <given-names>PJ</given-names></name>. <article-title>Imaging Morphogenesis: Technological Advances and Biological Insights</article-title>. <source>Science</source>. <year>2013</year>;<volume>340</volume>(<issue>6137</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1234168" xlink:type="simple">10.1126/science.1234168</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keller</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Wittbrodt</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stelzer</surname> <given-names>EHK</given-names></name>. <article-title>Reconstruction of Zebrafish Early Embryonic Development by Scanned Light Sheet Microscopy</article-title>. <source>Science</source>. <year>2008</year>;<volume>322</volume>(<issue>5904</issue>):<fpage>1065</fpage>–<lpage>1069</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1162493" xlink:type="simple">10.1126/science.1162493</ext-link></comment> <object-id pub-id-type="pmid">18845710</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keller</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Ahrens</surname> <given-names>MB</given-names></name>. <article-title>Visualizing Whole-Brain Activity and Development at the Single-Cell Level Using Light-Sheet Microscopy</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>85</volume>(<issue>3</issue>):<fpage>462</fpage>–<lpage>483</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.039" xlink:type="simple">10.1016/j.neuron.2014.12.039</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maizel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>von Wangenheim</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Federici</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Haseloff</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stelzer</surname> <given-names>EHK</given-names></name>. <article-title>High-resolution live imaging of plant growth in near physiological bright conditions using light sheet fluorescence microscopy</article-title>. <source>The Plant Journal</source>. <year>2011</year>;<volume>68</volume>(<issue>2</issue>):<fpage>377</fpage>–<lpage>385</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1365-313X.2011.04692.x" xlink:type="simple">10.1111/j.1365-313X.2011.04692.x</ext-link></comment> <object-id pub-id-type="pmid">21711399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wiles</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Jemielita</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Schlomann</surname> <given-names>BH</given-names></name>, <name name-style="western"><surname>Logan</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Ganz</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Host Gut Motility Promotes Competitive Exclusion within a Model Intestinal Microbiota</article-title>. <source>PLOS Biology</source>. <year>2016</year>;<volume>14</volume>(<issue>7</issue>):<fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002517" xlink:type="simple">10.1371/journal.pbio.1002517</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huisken</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stainier</surname> <given-names>DYR</given-names></name>. <article-title>Selective plane illumination microscopy techniques in developmental biology</article-title>. <source>Development</source>. <year>2009</year>;<volume>136</volume>(<issue>12</issue>):<fpage>1963</fpage>–<lpage>1975</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1242/dev.022426" xlink:type="simple">10.1242/dev.022426</ext-link></comment> <object-id pub-id-type="pmid">19465594</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pantazis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Supatto</surname> <given-names>W</given-names></name>. <article-title>Advances in whole-embryo imaging: a quantitative transition is underway</article-title>. <source>Nature Reviews Molecular Cell Biology</source>. <year>2014</year>;<volume>15</volume>:<fpage>327</fpage> EP –. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrm3786" xlink:type="simple">10.1038/nrm3786</ext-link></comment> <object-id pub-id-type="pmid">24739741</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref013">
<label>13</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ronneberger</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brox</surname> <given-names>T</given-names></name>. <chapter-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</chapter-title>. In: <source>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</source>. <volume>vol. 9351</volume> of LNCS. <publisher-name>Springer</publisher-name>; <year>2015</year>. p. <fpage>234</fpage>–<lpage>241</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a" xlink:type="simple">http://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref014">
<label>14</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title>. In: <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 25</source>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2012</year>. p. <fpage>1097</fpage>–<lpage>1105</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" xlink:type="simple">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Esteva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kuprel</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Novoa</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Swetter</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Blau</surname> <given-names>HM</given-names></name>, <etal>et al</etal>. <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>. <source>Nature</source>. <year>2017</year>;<volume>542</volume>:<fpage>115</fpage> EP –. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature21056" xlink:type="simple">10.1038/nature21056</ext-link></comment> <object-id pub-id-type="pmid">28117445</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lecun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname> <given-names>P</given-names></name>. <chapter-title>Gradient-based learning applied to document recognition</chapter-title>. In: <source>Proceedings of the IEEE</source>; <year>1998</year>. p. <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref017">
<label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. TensorFlow: A system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16); 2016. p. 265–283. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf" xlink:type="simple">https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints. 2016;abs/1605.02688.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Chollet F, et al. Keras; 2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras" xlink:type="simple">https://github.com/fchollet/keras</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Collobert R, Kavukcuoglu K, Farabet C. Torch7: A Matlab-like Environment for Machine Learning. In: BigLearn, NIPS Workshop; 2011.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref021">
<label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zeiler</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Fergus</surname> <given-names>R</given-names></name>. <chapter-title>Visualizing and Understanding Convolutional Networks</chapter-title>. In: <name name-style="western"><surname>Fleet</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pajdla</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schiele</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tuytelaars</surname> <given-names>T</given-names></name>, editors. <source>Computer Vision—ECCV 2014</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2014</year>. p. <fpage>818</fpage>–<lpage>833</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhou SK, Greenspan H, Shen D. In: Deep Learning for Medical Image Analysis. Academic Press; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/B9780128104088000262" xlink:type="simple">https://www.sciencedirect.com/science/article/pii/B9780128104088000262</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Dong B, Shao L, Costa MD, Bandmann O, Frangi AF. Deep learning for automatic cell detection in wide-field microscopy zebrafish images. In: 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI); 2015. p. 772–776.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Van Valen</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kudo</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lane</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Macklin</surname> <given-names>DN</given-names></name>, <name name-style="western"><surname>Quach</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>DeFelice</surname> <given-names>MM</given-names></name>, <etal>et al</etal>. <article-title>Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>11</issue>):<fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005177" xlink:type="simple">10.1371/journal.pcbi.1005177</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Arganda-Carreras</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kaynig</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Rueden</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Eliceiri</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Schindelin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cardona</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification</article-title>. <source>Bioinformatics</source>. <year>2017</year>;<volume>33</volume>(<issue>15</issue>):<fpage>2424</fpage>–<lpage>2426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btx180" xlink:type="simple">10.1093/bioinformatics/btx180</ext-link></comment> <object-id pub-id-type="pmid">28369169</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ning</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Delhomme</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Piano</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Barbano</surname> <given-names>PE</given-names></name>. <article-title>Toward automatic phenotyping of developing embryos from videos</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2005</year>;<volume>14</volume>(<issue>9</issue>):<fpage>1360</fpage>–<lpage>1371</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2005.852470" xlink:type="simple">10.1109/TIP.2005.852470</ext-link></comment> <object-id pub-id-type="pmid">16190471</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref027">
<label>27</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Christ</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Elshaer</surname> <given-names>MEA</given-names></name>, <name name-style="western"><surname>Ettlinger</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tatavarty</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bickel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bilic</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <chapter-title>Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields</chapter-title>. In: <name name-style="western"><surname>Ourselin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Joskowicz</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sabuncu</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Unal</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wells</surname> <given-names>W</given-names></name>, editors. <source>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2016</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2016</year>. p. <fpage>415</fpage>–<lpage>423</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kraus</surname> <given-names>OZ</given-names></name>, <name name-style="western"><surname>Grys</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Ba</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Boone</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Automated analysis of high-content microscopy data with deep learning</article-title>. <source>Molecular Systems Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>4</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.15252/msb.20177551" xlink:type="simple">10.15252/msb.20177551</ext-link></comment> <object-id pub-id-type="pmid">28420678</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Crane</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Entchev</surname> <given-names>EV</given-names></name>, <name name-style="western"><surname>Caballero</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fernandes de Abreu</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Ch’ng</surname> <given-names>Q</given-names></name>, <etal>et al</etal>. <article-title>Automated Processing of Imaging Data through Multi-tiered Classification of Biological Structures Illustrated Using Caenorhabditis elegans</article-title>. <source>PLOS Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>4</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004194" xlink:type="simple">10.1371/journal.pcbi.1004194</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ounkomol</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Seshamani</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maleckar</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>GR</given-names></name>. <chapter-title>Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</chapter-title>. <source>Nature Methods</source>. <year>2018</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shin</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Nogues</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>2016</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1285</fpage>–<lpage>1298</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TMI.2016.2528162" xlink:type="simple">10.1109/TMI.2016.2528162</ext-link></comment> <object-id pub-id-type="pmid">26886976</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017. p. 3462–3471.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Madani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Arnaout</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mofrad</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Arnaout</surname> <given-names>R</given-names></name>. <article-title>Fast and accurate view classification of echocardiograms using deep learning</article-title>. <source>npj Digital Medicine</source>. <year>2018</year>;<volume>1</volume>(<issue>1</issue>):<fpage>6</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41746-017-0013-1" xlink:type="simple">10.1038/s41746-017-0013-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Mahjoubfar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tai</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Blaby</surname> <given-names>IK</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Niazi</surname> <given-names>KR</given-names></name>, <etal>et al</etal>. <article-title>Deep Learning in Label-free Cell Classification</article-title>. <source>Scientific Reports</source>. <year>2016</year>;<volume>6</volume>:<fpage>21471</fpage> EP –. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep21471" xlink:type="simple">10.1038/srep21471</ext-link></comment> <object-id pub-id-type="pmid">26975219</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jemielita</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Taormina</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Hampton</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Rolig</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Guillemin</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Spatial and Temporal Features of the Growth of a Bacterial Species Colonizing the Zebrafish Gut</article-title>. <source>mBio</source>. <year>2014</year>;<volume>5</volume>(<issue>6</issue>):<fpage>e01751</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1128/mBio.01751-14" xlink:type="simple">10.1128/mBio.01751-14</ext-link></comment> <object-id pub-id-type="pmid">25516613</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Wiles TJ, Wall ES, Schlomann BH, Hay EA, Parthasarathy R, Guillemin K. Modernized tools for streamlined genetic manipulation of wild and diverse symbiotic bacteria. bioRxiv. 2017;.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Logan</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Shields</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Xavier</surname> <given-names>JB</given-names></name>, <etal>et al</etal>. <article-title>The Vibrio cholerae type VI secretion system can modulate host intestinal mechanics to displace gut bacterial symbionts</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2018</year>;<volume>115</volume>(<issue>16</issue>):<fpage>E3779</fpage>–<lpage>E3787</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1720133115" xlink:type="simple">10.1073/pnas.1720133115</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taormina</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Hay</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Parthasarathy</surname> <given-names>R</given-names></name>. <article-title>Passive and Active Microrheology of the Intestinal Fluid of the Larval Zebrafish</article-title>. <source>Biophysical Journal</source>. <year>2017</year>;<volume>113</volume>(<issue>4</issue>):<fpage>957</fpage>–<lpage>965</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bpj.2017.06.069" xlink:type="simple">https://doi.org/10.1016/j.bpj.2017.06.069</ext-link>. <object-id pub-id-type="pmid">28834731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mathis</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mamidanna</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cury</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Abe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Murthy</surname> <given-names>VN</given-names></name>, <name name-style="western"><surname>Mathis</surname> <given-names>MW</given-names></name>, <etal>et al</etal>. <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source>. <year>2018</year>;<volume>21</volume>(<issue>9</issue>):<fpage>1281</fpage>–<lpage>1289</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-018-0209-y" xlink:type="simple">10.1038/s41593-018-0209-y</ext-link></comment> <object-id pub-id-type="pmid">30127430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Milligan-Myhre K, Charette JR, Phennicie RT, Stephens WZ, Rawls JF, Guillemin K, et al. Chapter 4—Study of Host–Microbe Interactions in Zebrafish. In: Detrich HW, Westerfield M, Zon LI, editors. The Zebrafish: Disease Models and Chemical Screens. vol. 105 of Methods in Cell Biology. Academic Press; 2011. p. 87—116. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/B9780123813206000047" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/B9780123813206000047</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephens</surname> <given-names>WZ</given-names></name>, <name name-style="western"><surname>Wiles</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>Jemielita</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Parthasarathy</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Identification of Population Bottlenecks and Colonization Factors during Assembly of Bacterial Communities within the Zebrafish Intestine</article-title>. <source>mBio</source>. <year>2015</year>;<volume>6</volume>(<issue>6</issue>):<fpage>e01163</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1128/mBio.01163-15" xlink:type="simple">10.1128/mBio.01163-15</ext-link></comment> <object-id pub-id-type="pmid">26507229</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zac Stephens</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Stagaman</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rawls</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Guillemin</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>The composition of the zebrafish intestinal microbial community varies across development</article-title>. <source>ISME J</source>. <year>2016</year>;<volume>10</volume>(<issue>3</issue>):<fpage>644</fpage>–<lpage>654</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ismej.2015.140" xlink:type="simple">10.1038/ismej.2015.140</ext-link></comment> <object-id pub-id-type="pmid">26339860</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Yosinski J, Clune J, Bengio Y, Lipson H. How Transferable Are Features in Deep Neural Networks? In: Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2. NIPS’14. Cambridge, MA, USA: MIT Press; 2014. p. 3320–3328. Available from: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=2969033.2969197" xlink:type="simple">http://dl.acm.org/citation.cfm?id=2969033.2969197</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, Tzeng E, et al. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In: Xing EP, Jebara T, editors. Proceedings of the 31st International Conference on Machine Learning. vol. 32 of Proceedings of Machine Learning Research. Bejing, China: PMLR; 2014. p. 647–655. Available from: <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v32/donahue14.html" xlink:type="simple">http://proceedings.mlr.press/v32/donahue14.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taormina</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Jemielita</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Stephens</surname> <given-names>WZ</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Troll</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Parthasarathy</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Investigating Bacterial-Animal Symbioses with Light Sheet Microscopy</article-title>. <source>The Biological Bulletin</source>. <year>2012</year>;<volume>223</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/BBLv223n1p7" xlink:type="simple">10.1086/BBLv223n1p7</ext-link></comment> <object-id pub-id-type="pmid">22983029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006628.ref046">
<label>46</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ciresan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Giusti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gambardella</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <chapter-title>Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</chapter-title>. In: <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 25</source>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2012</year>. p. <fpage>2843</fpage>–<lpage>2851</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf" xlink:type="simple">http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006628.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR. 2014;abs/1409.1556.</mixed-citation>
</ref>
</ref-list>
</back>
</article>