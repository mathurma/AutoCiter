<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006892</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00639</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Nerve fibers</subject><subj-group><subject>Axons</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Nerve fibers</subject><subj-group><subject>Axons</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information theory</subject><subj-group><subject>Background signal noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Background signal noise</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>How Dendrites Affect Online Recognition Memory</article-title>
<alt-title alt-title-type="running-head">How Dendrites Affect Online Recognition Memory</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6643-4384</contrib-id>
<name name-style="western">
<surname>Wu</surname>
<given-names>Xundong</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0846-7994</contrib-id>
<name name-style="western">
<surname>Mel</surname>
<given-names>Gabriel C.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Strouse</surname>
<given-names>D. J.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6450-4716</contrib-id>
<name name-style="western">
<surname>Mel</surname>
<given-names>Bartlett W.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Computer Science Department, University of Southern California, Los Angeles, CA, United States</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Physics Department, Princeton University, Princeton, NJ, United States</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Biomedical Engineering Department and Neuroscience Graduate Program, University of Southern California, Los Angeles, CA, United States</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Engel</surname>
<given-names>Tatiana</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Cold Spring Harbor Laboratory, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">mel@usc.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>3</day>
<month>5</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>5</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>5</issue>
<elocation-id>e1006892</elocation-id>
<history>
<date date-type="received">
<day>24</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>2</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Wu et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006892"/>
<abstract>
<p>In order to record the stream of autobiographical information that defines our unique personal history, our brains must form durable memories from single brief exposures to the patterned stimuli that impinge on them continuously throughout life. However, little is known about the computational strategies or neural mechanisms that underlie the brain's ability to perform this type of "online" learning. Based on increasing evidence that dendrites act as both signaling and learning units in the brain, we developed an analytical model that relates online recognition memory capacity to roughly a dozen dendritic, network, pattern, and task-related parameters. We used the model to determine what dendrite size maximizes storage capacity under varying assumptions about pattern density and noise level. We show that over a several-fold range of both of these parameters, and over multiple orders-of-magnitude of memory size, capacity is maximized when dendrites contain a few hundred synapses—roughly the natural number found in memory-related areas of the brain. Thus, in comparison to entire neurons, dendrites increase storage capacity by providing a larger number of better-sized learning units. Our model provides the first normative theory that explains how dendrites increase the brain’s capacity for online learning; predicts which combinations of parameter settings we should expect to find in the brain under normal operating conditions; leads to novel interpretations of an array of existing experimental results; and provides a tool for understanding which changes associated with neurological disorders, aging, or stress are most likely to produce memory deficits—knowledge that could eventually help in the design of improved clinical treatments for memory loss.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Humans can effortlessly recognize a pattern as familiar even after a single presentation and a long delay, and our capacity to do so even with complex stimuli such as images has been called "almost limitless". How is the information needed to support familiarity judgements stored so rapidly and held so reliably for such a long time? Most theoretical work aimed at understanding the brain's one-shot learning mechanisms has been based on drastically simplified neuron models which omit any representation of the most visually prominent features of neurons—their extensive dendritic arbors. Given recent evidence that individual dendritic branches generate local spikes, and function as separately thresholded learning/responding units inside neurons, we set out to capture mathematically how the numerous parameters needed to describe a dendrite-based neural learning system interact to determine the memory's storage capacity. Using the model, we show that having dendrite-sized learning units provides a large capacity boost compared to a memory based on simplified (dendriteless) neurons, attesting to the importance of dendrites for optimal memory function. Our mathematical model may also prove useful in future efforts to understand how disruptions to dendritic structure and function lead to reduced memory capacity in aging and disease.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000076</institution-id>
<institution>Directorate for Biological Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>IIS-0613583</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Mel</surname>
<given-names>Bartlett W.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>National Institute of Mental Health (US)</institution>
</funding-source>
<award-id>5R01MH065918</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Mel</surname>
<given-names>Bartlett W.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>BWM received funding from the following sources: National Science Foundation: NSF CRCNS IIS-0613583 (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/" xlink:type="simple">https://www.nsf.gov/</ext-link>) and National Institute of Mental Health: NIMH 5R01 MH065918 (<ext-link ext-link-type="uri" xlink:href="https://www.nimh.nih.gov/index.shtml" xlink:type="simple">https://www.nimh.nih.gov/index.shtml</ext-link>). The funders had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="1"/>
<page-count count="34"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-20</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data contained in figures as well as simulation code is available in <xref ref-type="supplementary-material" rid="pcbi.1006892.s002">S1 Data</xref>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>To function well in a complex world, our brains must somehow stream our everyday experiences into memory as they occur in real time. An “online” memory of this kind, once termed a “Palimpsest” [<xref ref-type="bibr" rid="pcbi.1006892.ref001">1</xref>], must be capable of forming durable memory traces from a single brief exposure to each incoming pattern, while preserving previously stored memories as long and faithfully as possible (<xref ref-type="fig" rid="pcbi.1006892.g001">Fig 1</xref>). This combined need for rapid imprinting and large capacity requires that the memory system carefully manage both its learning and forgetting processes, but we currently know little about how these processes are implemented and coordinated in the brain.</p>
<fig id="pcbi.1006892.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Online learning in a familiarity-based recognition memory.</title>
<p>Novel patterns are streamed continuously into the memory and "one-shot" learned. Memory responses to trained patterns are shown as a distribution in light blue; distribution of responses to untrained (random) patterns is shown in light red. Recognition threshold separating the two distributions is shown as a green dashed line, set to produce a 1% false positive error rate. As stored patterns approach the end of their lifetimes, their traces decay and begin to merge with the untrained background distribution, leading to an increase in the false negative error rate (i.e. "misses"). Capacity is operationally defined as the pattern age at which the miss rate (averaged over all patterns up to that age) becomes unacceptably high (chosen to be 1% here).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g001" xlink:type="simple"/>
</fig>
<p>A number of quantitative models have been proposed for palimpsest-style online memories, and have addressed a variety of different issues, including: how memory capacity scales with network size, how metaplastic learning rules can increase memory capacity, and the tradeoff between initial trace strength and memory lifetimes [<xref ref-type="bibr" rid="pcbi.1006892.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>]. A few studies with a more empirical focus have addressed the biological mechanisms underlying recency vs. familiarity memory [<xref ref-type="bibr" rid="pcbi.1006892.ref009">9</xref>]; the coordination of online learning with long-term memory processes; and the details of memory-related neuronal response properties during online learning tasks [<xref ref-type="bibr" rid="pcbi.1006892.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref012">12</xref>].</p>
<p>Nearly all previous models of online learning have assumed that the neurons involved in memory storage are classical "point neurons”, that is, simple integrative units lacking any representation of a cell’s dendritic tree. This simplification is notable, given the now substantial evidence from both modeling and experimental studies that dendritic trees are powerful, functionally compartmentalized information processors that can augment the computing capabilities of individual neurons in numerous ways [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref059">59</xref>].</p>
<p>Beyond their contributions to the computing functions of neurons, it is also increasingly apparent that dendrites help to organize and spatially compartmentalize synaptic plasticity processes [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref040">40</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref060">60</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref086">86</xref>].</p>
<p>Thus, given that dendrites can act as both signaling <italic>and</italic> learning units within a neuron, it is important to understand how having dendrites could affect the brain’s online learning and memory processes. In this paper, we focus on the role that dendrites may play in familiarity-based recognition, a function most closely associated with the perirhinal cortex [<xref ref-type="bibr" rid="pcbi.1006892.ref087">87</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref088">88</xref>].</p>
<p>Here, we introduce a mathematical model that allows us to calculate online storage capacity from the underlying parameter values of a previously proposed dendrite-based memory circuit [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>]. The model includes biophysical parameters (dendritic learning and firing thresholds, network recognition threshold), wiring-related parameters (number of axons, number of dendrites, number of synapses per dendrite), and input pattern statistics (pattern density, noise level) (see <xref ref-type="table" rid="pcbi.1006892.t001">Table 1</xref>). As an example of the model’s use, we study the interactions between memory capacity, dendrite size, and pattern statistics, and cross-check the results using full network simulations. We found that dendrites containing a few hundred synapses (as opposed to a few tens or a few thousand) maximize storage capacity, providing the first normative theory that accounts for the actual sizes of dendrites found in online memory areas of the brain.</p>
<table-wrap id="pcbi.1006892.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.t001</object-id>
<label>Table 1</label> <caption><title>List of parameter categories, and specific parameters, used in the analysis and simulations.</title></caption>
<alternatives>
<graphic id="pcbi.1006892.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Parameter Categories (in order of increasing flexibility)</th>
<th align="left">Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Task parameters (fixed across simulations)</td>
<td align="left"><italic>θ</italic><sub><italic>±</italic></sub></td>
</tr>
<tr>
<td align="left">Network parameters (mostly fixed across simulations)</td>
<td align="left"><italic>N</italic><sub><italic>A</italic></sub>, <italic>N</italic><sub><italic>S</italic></sub>, <italic>f</italic><sub><italic>s</italic></sub>, <italic>D</italic><sub><italic>j</italic></sub></td>
</tr>
<tr>
<td align="left">Signal parameters (explored across simulations)</td>
<td align="left"><italic>f</italic><sub><italic>A</italic></sub>, <italic>N</italic><sub><italic>burst</italic></sub>, <italic>P</italic><sub><italic>burst</italic></sub></td>
</tr>
<tr>
<td align="left">Threshold parameters (optimized per simulation)</td>
<td align="left"><italic>θ</italic><sub><italic>F</italic></sub>, <italic>θ</italic><sub><italic>Lpost</italic></sub>, <italic>θ</italic><sub><italic>Lpre</italic></sub>, <italic>θ</italic><sub><italic>R</italic></sub></td>
</tr>
<tr>
<td align="left">Main parameters of interest (optimized per simulation)</td>
<td align="left"><italic>K</italic>, <italic>M</italic></td>
</tr>
<tr>
<td align="left">Learned parameters (altered during learning events)</td>
<td align="left"><italic>w</italic><sub><italic>ij</italic></sub>, <italic>α</italic><sub><italic>ij</italic></sub></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We modeled the memory network depicted in <xref ref-type="fig" rid="pcbi.1006892.g002">Fig 2a</xref>, consisting of a set of axons that form sparse random connections with the dendrites of a population of target neurons. An “infinite” sequence of random binary patterns is presented by the axons to the dendrites, each one causing one-shot changes to certain synapses within the network, where the goal of the network is to respond weakly to any pattern on its first presentation, and strongly for as long as possible to patterns that have been previously experienced. We define capacity as the number of consecutive training patterns stretching from “now” back into the past that can be classified as familiar with a low false negative (i.e. “miss”) rate, while maintaining a low false-positive (i.e. “false alarm”) rate to randomly drawn distractors (<xref ref-type="fig" rid="pcbi.1006892.g001">Fig 1</xref>).</p>
<fig id="pcbi.1006892.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Architecture of the memory circuit.</title>
<p>(<bold>a</bold>) A set of input axons makes sparse random contacts with the dendrites of a set of post-synaptic neurons. Only a subset of axons and neurons are shown. Patterns are stored by modifying synaptic weights, indicated by black circles. (<bold>b</bold>) Abstraction of the memory network shown in (a). Neurons are assumed to linearly combine dendritic outputs, so that the overall network response <italic>r</italic> is effectively a sum over all dendritic responses. The assumption of linear summation at the soma is included for simplicity, but is of little practical importance: the probability that any given dendrite fires in response to a particular pattern is very low, so that a neuron almost never contains more than a single firing dendrite (making the summation rule moot).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g002" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>The network</title>
<p>The network structure and plasticity rules have been previously described in [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>], but are repeated here for clarity. A population of neurons with a total of <inline-formula id="pcbi.1006892.e001"><alternatives><graphic id="pcbi.1006892.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula> separately thresholded dendrites receives inputs from <italic>N</italic><sub><italic>A</italic></sub> input axons (<xref ref-type="fig" rid="pcbi.1006892.g002">Fig 2b</xref>). Each dendrite receives <inline-formula id="pcbi.1006892.e002"><alternatives><graphic id="pcbi.1006892.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> synaptic contacts randomly sampled from the <italic>N</italic><sub><italic>A</italic></sub> axons, for a total number of synapses <inline-formula id="pcbi.1006892.e003"><alternatives><graphic id="pcbi.1006892.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>M</mml:mi><mml:mo>∙</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>. The connectivity matrix is assumed to be fixed.</p>
<p>Input patterns are binary-valued vectors <italic>x</italic> = {<italic>x</italic><sub><italic>1</italic></sub>,…,<italic>x</italic><sub><italic>NA</italic></sub>} for which component <inline-formula id="pcbi.1006892.e004"><alternatives><graphic id="pcbi.1006892.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is 1 if the <inline-formula id="pcbi.1006892.e005"><alternatives><graphic id="pcbi.1006892.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> axon is “firing” and 0 otherwise. We quantify density/sparsity of the patterns by the fraction of axons <inline-formula id="pcbi.1006892.e006"><alternatives><graphic id="pcbi.1006892.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> firing in each pattern; the value of <inline-formula id="pcbi.1006892.e007"><alternatives><graphic id="pcbi.1006892.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> ranged from 0.008 to 0.18 in this study, as we found empirically in previous work that sparse patterns maximize capacity in this type of memory [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>]. To model a biologically realistic form of input variability, we assumed that each active axon (<inline-formula id="pcbi.1006892.e008"><alternatives><graphic id="pcbi.1006892.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>) produces a burst of spikes, where the number of spikes in the burst is drawn from a binomial distribution with mean <italic><inline-formula id="pcbi.1006892.e009"><alternatives><graphic id="pcbi.1006892.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>4</mml:mn></mml:math></alternatives></inline-formula></italic> spikes/burst. <inline-formula id="pcbi.1006892.e010"><alternatives><graphic id="pcbi.1006892.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> ranged from 1 (no noise) to 0.4 (high noise), with <inline-formula id="pcbi.1006892.e011"><alternatives><graphic id="pcbi.1006892.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> varying inversely. Inactive axons (<inline-formula id="pcbi.1006892.e012"><alternatives><graphic id="pcbi.1006892.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>) were assumed to produce no spikes. We denote the noisy spike count version of an input component <inline-formula id="pcbi.1006892.e013"><alternatives><graphic id="pcbi.1006892.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>.</p>
<p>Synapses are characterized by both a weight <inline-formula id="pcbi.1006892.e014"><alternatives><graphic id="pcbi.1006892.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, where the subscript indicates a connection between axon <inline-formula id="pcbi.1006892.e015"><alternatives><graphic id="pcbi.1006892.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula> and dendrite <inline-formula id="pcbi.1006892.e016"><alternatives><graphic id="pcbi.1006892.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi>j</mml:mi></mml:math></alternatives></inline-formula>, and an additional scalar parameter <inline-formula id="pcbi.1006892.e017"><alternatives><graphic id="pcbi.1006892.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, representing the synapse’s “age”. The weight of each synapse is binary-valued, and can change between weak (<italic>w</italic> = 0) and strong (<italic>w</italic> = 1) states when the dendrite containing the synapse undergoes a learning event; the conditions that trigger a learning event are discussed below. The age variable at each synapse tracks the number of learning events that have occurred in the parent dendrite since the synapse last participated in learning.</p>
<p>Two different measures of a dendrite’s activation level determine how the dendrite responds to an input, and whether it undergoes a learning event. The “presynaptic” activation measure is based on the activity levels of the set of axons <italic>D</italic><sub><italic>j</italic></sub> that make contact with the <italic>j</italic>th dendrite
<disp-formula id="pcbi.1006892.e018">
<alternatives>
<graphic id="pcbi.1006892.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
In words, <inline-formula id="pcbi.1006892.e019"><alternatives><graphic id="pcbi.1006892.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the total number of presynaptic spikes arriving at all the synapses impinging on the <inline-formula id="pcbi.1006892.e020"><alternatives><graphic id="pcbi.1006892.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> dendrite, regardless of their postsynaptic weights, and is thus a measure of the maximum response the dendrite <italic>could</italic> muster to that input pattern assuming all of the activated synapses were strong (<inline-formula id="pcbi.1006892.e021"><alternatives><graphic id="pcbi.1006892.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mi>w</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>).</p>
<p>The more conventional “postsynaptic” activation level takes account of the synaptic weights in the usual way:
<disp-formula id="pcbi.1006892.e022">
<alternatives>
<graphic id="pcbi.1006892.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mi>.</mml:mi></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>When the postsynaptic activation level exceeds the “firing” threshold <inline-formula id="pcbi.1006892.e023"><alternatives><graphic id="pcbi.1006892.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the dendrite is said to fire, that is, generates a response <italic>r</italic><sub><italic>j</italic></sub> = 1. The responses of all dendrites within a neuron sum linearly to produce the neuron’s response (<xref ref-type="fig" rid="pcbi.1006892.g002">Fig 2b</xref>), and the responses of all neurons in the network sum linearly to produce the overall network response <inline-formula id="pcbi.1006892.e024"><alternatives><graphic id="pcbi.1006892.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mi>r</mml:mi></mml:math></alternatives></inline-formula>. The overall response of the network can therefore be written directly as a sum over all the <inline-formula id="pcbi.1006892.e025"><alternatives><graphic id="pcbi.1006892.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula> dendritic responses:
<disp-formula id="pcbi.1006892.e026">
<alternatives>
<graphic id="pcbi.1006892.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>so that the network can be viewed as a single “super neuron” with <inline-formula id="pcbi.1006892.e027"><alternatives><graphic id="pcbi.1006892.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula> dendrites.</p>
<p>Finally, an input pattern is classified as “familiar” if <inline-formula id="pcbi.1006892.e028"><alternatives><graphic id="pcbi.1006892.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, and “novel” if <inline-formula id="pcbi.1006892.e029"><alternatives><graphic id="pcbi.1006892.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, where <italic>θ</italic><sub><italic>R</italic></sub> is the recognition threshold (<xref ref-type="fig" rid="pcbi.1006892.g002">Fig 2b</xref>).</p>
</sec>
<sec id="sec004">
<title>The synaptic learning rule</title>
<p>The goal of learning is to ensure that learned patterns going back as far as possible in time produce suprathreshold network responses <inline-formula id="pcbi.1006892.e030"><alternatives><graphic id="pcbi.1006892.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, while randomly drawn patterns do not. Learning of any given pattern occurs in only the small fraction of dendrites that cross both the presynaptic and postsynaptic learning thresholds (<inline-formula id="pcbi.1006892.e031"><alternatives><graphic id="pcbi.1006892.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e032"><alternatives><graphic id="pcbi.1006892.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>). When this occurs, a “learning event” is triggered in the dendrite, and all active synapses belonging to that dendrite “learn”, as follows. If an active synapse is currently in the weak state, it is “potentiated” (i.e. both strengthened and “juvenated”: <inline-formula id="pcbi.1006892.e033"><alternatives><graphic id="pcbi.1006892.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>), or if it is already in the strong state, then it remains strong but is juvenated (<inline-formula id="pcbi.1006892.e034"><alternatives><graphic id="pcbi.1006892.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:math></alternatives></inline-formula> <inline-formula id="pcbi.1006892.e035"><alternatives><graphic id="pcbi.1006892.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>). All strong synapses in the dendrite that are <italic>not</italic> active during the learning event remain strong but grow older (<inline-formula id="pcbi.1006892.e036"><alternatives><graphic id="pcbi.1006892.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Thus <inline-formula id="pcbi.1006892.e037"><alternatives><graphic id="pcbi.1006892.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/></mml:math></alternatives></inline-formula>counts the number of learning events that have occurred in the dendrite since the synapse last learned, and thus represents the age of the most recent information that that synapse is involved in storing. Note that a synapse’s age variable counts learning events within its parent dendrite only, and any given dendrite learns only rarely, so the counter need have only a small number of distinct values, on the order of ~12 under the simulation conditions explored in this paper. To maintain a constant fraction of strong synapses (we used <inline-formula id="pcbi.1006892.e038"><alternatives><graphic id="pcbi.1006892.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>0.5</mml:mn></mml:math></alternatives></inline-formula>), and thereby to prevent saturation of the memory, in each dendrite undergoing learning, a number of strong synapses are depressed (<inline-formula id="pcbi.1006892.e039"><alternatives><graphic id="pcbi.1006892.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>) equal to the number of weak synapses potentiated during that learning event. A key feature of the learning rule is that the synapses targeted for depression are those that learned least recently (i.e. having the largest values of <inline-formula id="pcbi.1006892.e040"><alternatives><graphic id="pcbi.1006892.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>), so that the information erased during depression is the “oldest” stored information. This “age-ordered depression” strategy substantially increases online storage capacity [<xref ref-type="bibr" rid="pcbi.1006892.ref005">5</xref>], especially in a 2-layer dendrite-based memory where the very sparse use of synapses during pattern storage gives each strong synapse, and the information it represents, the opportunity to grow old [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>].</p>
</sec>
<sec id="sec005">
<title>Calculating memory capacity</title>
<p>One of the key quantities involved in calculating storage capacity is <inline-formula id="pcbi.1006892.e041"><alternatives><graphic id="pcbi.1006892.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>, the length of the age queue within a dendrite (see <xref ref-type="fig" rid="pcbi.1006892.g003">Fig 3</xref>). An approximate expression for <inline-formula id="pcbi.1006892.e042"><alternatives><graphic id="pcbi.1006892.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> is given here; the derivation can be found in the Methods.</p>
<fig id="pcbi.1006892.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Synapse ages and the associated markov model.</title>
<p>Conceptual bar graph at top shows steady state probabilities of synapse ages within a typical dendrite; age is counted in learning events. Markov model shows the <inline-formula id="pcbi.1006892.e043"><alternatives><graphic id="pcbi.1006892.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> age states of a strong synapse and the one weak state, with transitions of four types as indicated in the legend. Transition probabilities are shown on the arrows.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g003" xlink:type="simple"/>
</fig>
<disp-formula id="pcbi.1006892.e044">
<alternatives>
<graphic id="pcbi.1006892.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e044" xlink:type="simple"/>
<mml:math display="block" id="M44">
<mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<p><inline-formula id="pcbi.1006892.e045"><alternatives><graphic id="pcbi.1006892.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> is a measure of the time a pattern feature persists in a dendrite, and given that age queues progress at roughly equal rates in all the dendrites involved in storing a pattern, it also effectively measures a pattern’s lifetime in memory–counted in units of dendritic learning events. <inline-formula id="pcbi.1006892.e046"><alternatives><graphic id="pcbi.1006892.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> can be understood intuitively through an oversimplified example: If 10 synapses are strengthened on a dendrite during a learning event, and there are 120 strong synapses on the dendrite, then L would be ~12. That is, after ~12 learning events have elapsed since a pattern was first stored, the 10 synapses involved in storing the pattern are now the oldest on the dendrite and must be depressed, and the memory is lost. The actual expression for L is more complex as it takes into account the fact that strong synapses do not inexorably progress to the ends of their age queues–they can be rejuvenated one or more times during the course of their lifetimes, in which case the same strong synapse participates in the representation of more than one pattern.</p>
<p>To convert from <inline-formula id="pcbi.1006892.e047"><alternatives><graphic id="pcbi.1006892.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> to a number of training patterns, we must multiply <inline-formula id="pcbi.1006892.e048"><alternatives><graphic id="pcbi.1006892.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> by the approximate number of patterns per dendritic learning event, or “learning interval” <inline-formula id="pcbi.1006892.e049"><alternatives><graphic id="pcbi.1006892.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006892.e050"><alternatives><graphic id="pcbi.1006892.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the probability that an arbitrary dendrite learns a particular pattern. This gives an expression for capacity:
<disp-formula id="pcbi.1006892.e051">
<alternatives>
<graphic id="pcbi.1006892.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e051" xlink:type="simple"/>
<mml:math display="block" id="M51">
<mml:mrow><mml:mi>C</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
Although <inline-formula id="pcbi.1006892.e052"><alternatives><graphic id="pcbi.1006892.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is conceptually simple, its expression is complicated since it depends on pattern density, noise level, two learning thresholds, dendrite size, and <inline-formula id="pcbi.1006892.e053"><alternatives><graphic id="pcbi.1006892.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, and so it is omitted here for clarity (see in the <xref ref-type="sec" rid="sec023">Methods</xref> section for the full expression and some discussion).</p>
</sec>
<sec id="sec006">
<title>Calculating memory capacity</title>
<p>The expression for <inline-formula id="pcbi.1006892.e054"><alternatives><graphic id="pcbi.1006892.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula> measures how long patterns persist in memory, but a different calculation is needed in order to predict the memory’s recognition performance, that is, the false positive and false negative error rates <inline-formula id="pcbi.1006892.e055"><alternatives><graphic id="pcbi.1006892.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e056"><alternatives><graphic id="pcbi.1006892.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> that we can expect to obtain during a pattern’s lifetime. These error rates depend on the separation of the distributions of responses to trained vs. untrained patterns (<xref ref-type="fig" rid="pcbi.1006892.g001">Fig 1</xref>). These two distributions can be computed from the network parameters to determine whether the allowable error rate tolerances <inline-formula id="pcbi.1006892.e057"><alternatives><graphic id="pcbi.1006892.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e058"><alternatives><graphic id="pcbi.1006892.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> will be met during the lifetime calculated in <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref> (see <xref ref-type="sec" rid="sec023">Methods</xref>).</p>
</sec>
<sec id="sec007">
<title>Determining optimal dendrite size</title>
<p>How can the expression for online storage capacity (<xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref>) be exploited? Given that one of the unique features of our model is that dendrites are the learning units, we used the model to determine how capacity varies with dendrite size, which in turn allows us to determine the optimal dendrite size. In particular, we asked: for a fixed total number of synapses in the memory network (<inline-formula id="pcbi.1006892.e059"><alternatives><graphic id="pcbi.1006892.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>M</mml:mi><mml:mo>∙</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>), if the goal is to maximize online storage capacity, is it better to have many short dendrites (i.e. large <inline-formula id="pcbi.1006892.e060"><alternatives><graphic id="pcbi.1006892.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>, small <inline-formula id="pcbi.1006892.e061"><alternatives><graphic id="pcbi.1006892.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>), a few long dendrites (small <inline-formula id="pcbi.1006892.e062"><alternatives><graphic id="pcbi.1006892.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>, large <inline-formula id="pcbi.1006892.e063"><alternatives><graphic id="pcbi.1006892.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>), or something in between? Furthermore, how does the optimal dendrite size vary with properties of the input patterns, such as pattern density and input noise level? To address these questions, we fixed network parameters <inline-formula id="pcbi.1006892.e064"><alternatives><graphic id="pcbi.1006892.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/></mml:math></alternatives></inline-formula>and <inline-formula id="pcbi.1006892.e065"><alternatives><graphic id="pcbi.1006892.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and then for varying combinations of the pattern-related parameters (<inline-formula id="pcbi.1006892.e066"><alternatives><graphic id="pcbi.1006892.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, we computed <inline-formula id="pcbi.1006892.e067"><alternatives><graphic id="pcbi.1006892.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula> as a function of dendrite size <inline-formula id="pcbi.1006892.e068"><alternatives><graphic id="pcbi.1006892.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>, using values of the learning, firing, and recognition thresholds (<inline-formula id="pcbi.1006892.e069"><alternatives><graphic id="pcbi.1006892.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) optimized for each value of <inline-formula id="pcbi.1006892.e070"><alternatives><graphic id="pcbi.1006892.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> through a semi-automated grid search. The “optimal” dendrite size under a particular set of input conditions was the value of <inline-formula id="pcbi.1006892.e071"><alternatives><graphic id="pcbi.1006892.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> that maximized capacity, subject to the constraint that immediately after training, responses to trained patterns were strong enough, and responses to random patterns were weak enough, that both the false positive (<inline-formula id="pcbi.1006892.e072"><alternatives><graphic id="pcbi.1006892.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) and false negative (<inline-formula id="pcbi.1006892.e073"><alternatives><graphic id="pcbi.1006892.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) error rates fell below specified tolerances (we used 1% for both). Note that though <inline-formula id="pcbi.1006892.e074"><alternatives><graphic id="pcbi.1006892.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> appears explicitly only once in <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref>, as a result of the capacity optimization process, all of the thresholds, and consequently <inline-formula id="pcbi.1006892.e075"><alternatives><graphic id="pcbi.1006892.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e076"><alternatives><graphic id="pcbi.1006892.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref> depend implicitly on <inline-formula id="pcbi.1006892.e077"><alternatives><graphic id="pcbi.1006892.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>. The net effect of these dependencies is analyzed in detail in the sections below on penalties for long and short dendrites.</p>
<p>Capacity is plotted in <xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4a</xref> as a function of <inline-formula id="pcbi.1006892.e078"><alternatives><graphic id="pcbi.1006892.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> for pattern density values ranging from 0.8% to 18%. In the case with <inline-formula id="pcbi.1006892.e079"><alternatives><graphic id="pcbi.1006892.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1.5</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>, capacity peaked at ~30,000 patterns when dendrites each contained 256 synapses, and declined substantially for both short (<inline-formula id="pcbi.1006892.e080"><alternatives><graphic id="pcbi.1006892.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>&lt;100) and long (<inline-formula id="pcbi.1006892.e081"><alternatives><graphic id="pcbi.1006892.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>&gt;1000) dendrites. As the pattern density increased (to 18%) or decreased (to 0.8%), peak capacity varied nearly 5-fold, favoring sparser patterns, but over the more than 20-fold range of pattern densities tested, peak capacity always occurred for dendrites ranging from 100–500 synapses (grey shaded area). Focusing on the high-capacity (sparse) end of the range with <inline-formula id="pcbi.1006892.e082"><alternatives><graphic id="pcbi.1006892.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>3</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>, peak capacity was confined to the narrower range of 200–500 (i.e. “a few hundred”) synapses. We also observed that sparser patterns led to a preference for longer dendrites, an effect we unpack below using full network simulations. It is important to clarify that the higher recognition capacity seen for sparser patterns does not result from the fact that sparser patterns contain less information, thereby reducing storage costs per pattern (see <xref ref-type="supplementary-material" rid="pcbi.1006892.s001">S1 Text</xref>). We also note that in the more realistic conditions modeled in the full network simulations (see below and <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5</xref>), peak capacity saturates at slightly higher pattern activation densities (around 1.5%) than is predicted by the analytical model, and the optimal pattern density may be higher still under conditions of increased background noise (<xref ref-type="supplementary-material" rid="pcbi.1006892.s003">S1 Fig</xref> shows strong susceptibility to background noise even at 3% pattern density).</p>
<fig id="pcbi.1006892.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Capacity as a function of dendrite size.</title>
<p>(<bold>a</bold>) Capacity curves are plotted for pattern densities ranging from 0.8% to 18%. Dendrite size is plotted on a log scale. Peak capacities lie in the range of 100–500 synapses per dendrite. Sparser patterns lead to a preference for longer dendrites and produce higher storage capacities (but not because sparse patterns contain less information–see main text and <xref ref-type="supplementary-material" rid="pcbi.1006892.s001">S1 Text</xref>). “Jagged” capacity curves for short dendrites and/or low pattern densities are due to a combination of (1) small numbers of synapses active per dendrite, and (2) quantization of dendritic learning and firing threshold to integer values, which may be optimal for some dendrite sizes but suboptimal for others. (<bold>b</bold>) Capacity curves for increasing values of input burst noise. Distributions of spike counts per burst are shown as bar plots. Dashed magenta curve corresponds to the solid magenta curve in (a); this curve represented a medium noise condition with <inline-formula id="pcbi.1006892.e083"><alternatives><graphic id="pcbi.1006892.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e083" xlink:type="simple"/><mml:math display="inline" id="M83"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>4</mml:mn><mml:mo>/</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>N</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>7</mml:mn></mml:math></alternatives></inline-formula>. Noisier inputs reduce capacity, and lead to a preference for longer dendrites. (<bold>c</bold>) Capacity curves for increasing number of synapses. Capacity is plotted on a log scale. Magenta curves are vertically shifted (therefore scaled) versions of the 1x curve, to show that the dependence of storage capacity on dendrite size remains stable over a wide range of network scales. (<bold>d</bold>) Capacity scales nearly linearly for increasing network sizes, shown for three dendrite sizes (corresponding to vertical dashed lines in c). Dashed diagonal shows slope of 1 (representing perfect linear scaling) for comparison.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006892.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Validating the analytical model with full network simulations.</title>
<p>(<bold>a</bold>) Dots show trace strengths of individual trained (blue) and untrained (red) patterns. The time at which the false-negative "miss" rate climbs to 1% (at a fixed 1% false negative rate) is called the capacity (analogous to <xref ref-type="fig" rid="pcbi.1006892.g001">Fig 1</xref>). (<bold>b</bold>) Histogram of synapse ages within a dendrite. Red line shows exponential decay. Synapses reach the end of the age queue at 10–12 learning events in this example. (<bold>c-d</bold>). Capacity graphs comparable to those produced by the analytical model in <xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4a and 4b</xref>. (<bold>e</bold>). Synapse usage and dendrite usage during the storing of one pattern, as a function of dendrite size. Plots are linked by color to overlying capacity plots. (<bold>f</bold>). Capacity for 3 levels of pattern “correlation”, quantified by redundancy factor <inline-formula id="pcbi.1006892.e084"><alternatives><graphic id="pcbi.1006892.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e084" xlink:type="simple"/><mml:math display="inline" id="M84"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula> (solid lines). Peak capacity was still found for dendrites in the range of “a few hundred synapses”. Avoiding duplication of synapses on dendrites almost completely eliminated the deleterious effects of pattern correlations (dashed lines).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g005" xlink:type="simple"/>
</fig>
<p>To test the effect of pattern noise on capacity, we varied the input noise level by choosing combinations of <inline-formula id="pcbi.1006892.e085"><alternatives><graphic id="pcbi.1006892.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e086"><alternatives><graphic id="pcbi.1006892.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e086" xlink:type="simple"/><mml:math display="inline" id="M86"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> whose product was always <inline-formula id="pcbi.1006892.e087"><alternatives><graphic id="pcbi.1006892.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>4</mml:mn></mml:math></alternatives></inline-formula> spikes, but that yielded narrow or broad spike count distributions for each active pattern component (<xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4b</xref>, see histogram insets). In this way, we varied the degree to which a trained pattern resembled itself upon repeated presentations. The variation in event counts arising from the above scheme could be viewed as representing either variation in the number of action potentials arriving at the presynaptic terminal from trial to trial, or variation in the number of synaptic release events caused by a given number of action potentials, or a combination of both effects. As expected, higher noise levels reduced peak capacity (<xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4b</xref>), except in the long dendrite range (<inline-formula id="pcbi.1006892.e088"><alternatives><graphic id="pcbi.1006892.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>&gt;1000) where central limit effects rendered dendrites insensitive to this type of noise. In keeping with this effect, optimal dendrite size increased slightly as the noise level increased, but again, peak capacity was consistently seen for dendrites in the “few hundred” synapse range. Even higher levels of noise were not considered because a simple, biologically available saturation strategy that maps multiple release events into a relatively constant post-synaptic response can largely mitigate the effects of this type of noise. (We did not include a multi-input saturation mechanism in our model to avoid the added complexity).</p>
</sec>
<sec id="sec008">
<title>Optimal dendrite size depends little on network size</title>
<p>To verify that the preference for dendrites in the few hundred synapse range was not an artifact of “small” network size, we generated capacity curves from <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref> for networks scaled up 256-fold from a base size of <inline-formula id="pcbi.1006892.e089"><alternatives><graphic id="pcbi.1006892.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e089" xlink:type="simple"/><mml:math display="inline" id="M89"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula> = 5.12 million synapses to ~1.3 billion synapses. The results are shown on a log plot in <xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4c</xref>. As shown in <xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4d</xref>, the scaling power for dendrite sizes <inline-formula id="pcbi.1006892.e090"><alternatives><graphic id="pcbi.1006892.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> = 64, 256, and 1024 were, respectively, 0.98, 0.97, and 0.97, confirming earlier observations that storage capacity in an optimized dendrite-based memory grows essentially linearly with network size [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>]. All the while, the preference for dendrites containing a few hundred synapses remained essentially invariant.</p>
</sec>
<sec id="sec009">
<title>Validating the analytical model with full network simulations</title>
<p>To cross-check the results of the analytical model, we simulated a full memory network, and measured capacity empirically as a function of <inline-formula id="pcbi.1006892.e091"><alternatives><graphic id="pcbi.1006892.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>. Unlike the analytical case, in which capacity was assumed to be proportional to the calculated length of dendritic age queues, in the network simulations we performed explicit old-new recognition memory tests, and optimized system parameters to achieve false positive and false negative error rates of 1%. In the interests of greater biological realism, we replaced the hard dendritic firing threshold and binary input-output function with a continuous sigmoidal input-output function given by <inline-formula id="pcbi.1006892.e092"><alternatives><graphic id="pcbi.1006892.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, and optimized over the slope parameter <inline-formula id="pcbi.1006892.e093"><alternatives><graphic id="pcbi.1006892.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula> along with the 4 threshold parameters. In addition, we relaxed the strict assumption of the analytical model that every input to the network was statistically independent of every other, and instead arranged for each input axon to form <inline-formula id="pcbi.1006892.e094"><alternatives><graphic id="pcbi.1006892.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula> synaptic contacts within the memory area, rather than just one. This “redundancy” factor, <inline-formula id="pcbi.1006892.e095"><alternatives><graphic id="pcbi.1006892.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>, set by default to <inline-formula id="pcbi.1006892.e096"><alternatives><graphic id="pcbi.1006892.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:mn>200</mml:mn></mml:math></alternatives></inline-formula>, introduced some degree of correlation in the input patterns, and lowered peak capacity somewhat, but had no effect on our main conclusions.</p>
<p><xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5a</xref> depicts one such simulation with 5.12 million synapses. In the top panel, blue dots show responses to trained patterns, red dots show responses to randomly drawn (untrained) patterns that establish the baseline trace strength (green dashed line) above which stored pattern traces must rise to be recognized. Consistent with the analytical model, responses to trained patterns remain essentially constant during an extended post-training period, in this example spanning ~10,000 patterns. After the flat post-training phase, in contrast to the relatively abrupt fall in trace strength envisioned by the analytical model, a more gradual decline is seen, reflecting the variable times at which the synapses encoding each pattern reach the end of their age queues in different dendrites. Note that the false negative error rate begins to climb during this trace decay period, as the lower fringe of the trained response distribution (blue) progressively merges with the untrained background distribution (red). In this simulation, capacity was reached at ~21,000 patterns, which by our specification is the point where both false positive and false negative error rates equaled 1%. Mirroring the approach taken with the analytical model, multiple simulations were run with varying firing, learning, and recognition thresholds to find the combination of parameters that maximized capacity for each value of <inline-formula id="pcbi.1006892.e097"><alternatives><graphic id="pcbi.1006892.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>, subject to the same error rate constraints as before. As an additional check of the analytical model, we histogrammed synapse ages within a dendrite (for many dendrites) (<xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5b</xref>), and found that they conformed to a geometric distribution as predicted (red line shows a fitted exponential decay), up to the “cliff” at the end of the age queue (blue dashed line).</p>
<p>Capacity was measured for dendrite sizes between 32 and 4,096 synapses, and the results are shown in <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5c and 5d</xref>, which are the analogues of <xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4a and 4b</xref>, respectively. When compared to the curves produced by the analytical model, the capacity curves produced by full network simulations had similarly placed capacity peaks and similar qualitative dependence on pattern density and noise levels. In one minor difference, we noted that under the more realistic conditions modeled in the full network simulations, peak capacity saturated at slightly higher pattern activation densities (around 1.5%) than was predicted by the analytical model (<xref ref-type="fig" rid="pcbi.1006892.g004">Fig 4a</xref>).</p>
<p>To determine whether the predictions regarding optimal dendrite size would survive under even more challenging “real world” operating conditions, we added increasing amounts of background noise (spurious spikes added to nominally inactive pattern components), on top of the pre-existing burst noise and pattern correlations. As in the case of burst noise, the background noise level varied between 2 extremes: zero noise, which maximized capacity, and a “high noise” level that reduced storage capacity by roughly a factor of 2 compared to the no-noise case. As in the case of burst noise, we did not consider very high noise levels on the grounds that the deleterious effects of background noise can be compensated by a relatively simple mechanism, for which there is evidence: pre-synaptic terminals with low release probability for “singleton” spikes, along with paired pulse facilitation [<xref ref-type="bibr" rid="pcbi.1006892.ref089">89</xref>], would allow the effects of sporadic background spikes to be suppressed while maintaining strong responses to signal-carrying bursts. Even at background noise levels capable of causing a significant reduction in peak capacity, the effect of background noise on optimal dendrite size was negligible (<xref ref-type="supplementary-material" rid="pcbi.1006892.s003">S1 Fig</xref>). Only at very high levels of background noise, where capacity was reduced more than twofold, did optimal dendrite size change significantly, moving outside of the of the “few hundred” synapses per dendrite range (<xref ref-type="supplementary-material" rid="pcbi.1006892.s003">S1 Fig</xref>).</p>
<p>Next we examined the effect of increasing correlations in the input patterns. Given that a single axon can in fact form many thousands of synaptic contacts, corresponding to a much higher redundancy factor than we used in our base simulation, we ran simulations using redundancy factors <inline-formula id="pcbi.1006892.e098"><alternatives><graphic id="pcbi.1006892.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mi>ρ</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>5,000</mml:mn></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e099"><alternatives><graphic id="pcbi.1006892.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:mi>ρ</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>10,000</mml:mn></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5f</xref>), which meant that groups of 5,000 or 10,000 synapses scattered across the memory were activated identically. Given previous reports that input correlations can be very deleterious to capacity [<xref ref-type="bibr" rid="pcbi.1006892.ref010">10</xref>], we speculated that these drastic reductions in the effective dimensionality of the input patterns would severely challenge a memory architecture that was designed to perform optimally with random inputs, or at least significantly alter its behavior. As shown in <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5f</xref>, however, even in the high-redundancy case (with a 10,000-fold reduction in input space dimensionality), peak capacity dropped by only a factor of ~2 compared to the case with <inline-formula id="pcbi.1006892.e100"><alternatives><graphic id="pcbi.1006892.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:mi>ρ</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>200</mml:mn></mml:math></alternatives></inline-formula>, with little to no change in optimal dendrite size.</p>
<p>We next took advantage of the full network simulations to probe the mechanisms that lead to the capacity costs associated with both short and long dendrites. <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5e</xref> shows two important quantities: the average number of dendrites (<inline-formula id="pcbi.1006892.e101"><alternatives><graphic id="pcbi.1006892.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) and synapses (<inline-formula id="pcbi.1006892.e102"><alternatives><graphic id="pcbi.1006892.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) used to store a single pattern in the simulations from <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5c</xref>. The significance of these quantities is discussed below as we work through the distinct capacity penalties for long and short dendrites.</p>
</sec>
<sec id="sec010">
<title>Penalty for long dendrites</title>
<p>As shown in <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5e</xref>, as dendrites grow longer, dendrite usage per stored pattern drops from a value around 10 (at peak capacity) to a “floor” of roughly ~7 dendrites at the long-dendrite end of the range, whereas synapse usage climbs steeply from a baseline of around 150 synapses. To understand the source of the lower bound of ~7 on the average number of dendrites used to store each pattern, it is useful to consider the situation that holds when, in the interests of resource efficiency, we attempt to store each pattern with the minimum possible trace strength: one dendrite. One dendrite firing in response to a familiar pattern is in principle sufficient for recognition, if it is reliable (i.e. occurs &gt; 99% of the time), and if the network’s response to untrained patterns is reliably zero (i.e. &gt; 99% of the time). In a large network, given that each dendrite participates in learning with equal (small) probability, the distribution of the number of dendrites that undergoes a learning event is approximately Poisson with mean <inline-formula id="pcbi.1006892.e103"><alternatives><graphic id="pcbi.1006892.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>. Given that a Poisson distribution is characterized fully by its mean, setting <inline-formula id="pcbi.1006892.e104"><alternatives><graphic id="pcbi.1006892.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> by adjusting the learning thresholds, which control <inline-formula id="pcbi.1006892.e105"><alternatives><graphic id="pcbi.1006892.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, means that one dendrite will undergo a learning event for each presented pattern–on average–which is the goal. However, with a mean of 1, the probability that <italic>zero</italic> dendrites learn is surprisingly high: ~37% (<xref ref-type="fig" rid="pcbi.1006892.g006">Fig 6a</xref>, top plot). Thus, in aiming to use a single dendrite to encode a pattern on average, more than a third of all patterns presented to the network <italic>would produce no memory trace at all</italic>, leading to a false negative error rate far above the 1% acceptable threshold. To avoid this pitfall, it is critical to reduce the probability to below 1% that zero dendrites learn, which according to the Poisson distribution requires a mean <inline-formula id="pcbi.1006892.e106"><alternatives><graphic id="pcbi.1006892.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>5</mml:mn></mml:math></alternatives></inline-formula> dendrites. This requires a remarkable 5-fold increase in <inline-formula id="pcbi.1006892.e107"><alternatives><graphic id="pcbi.1006892.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> relative to the theoretical minimum, with a corresponding 5x increase in synapse resource consumption (<xref ref-type="fig" rid="pcbi.1006892.g006">Fig 6a</xref>, middle plot). Worse, given increased variability in the number of learning dendrites as well as increased readout failures due to input noise and correlations, storage capacity turns out to be maximized when an even higher value of <inline-formula id="pcbi.1006892.e108"><alternatives><graphic id="pcbi.1006892.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e108" xlink:type="simple"/><mml:math display="inline" id="M108"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>is used, achieved by further loosening the learning thresholds, which for our combination of system parameters leads to the empirically obtained optimal value of <inline-formula id="pcbi.1006892.e109"><alternatives><graphic id="pcbi.1006892.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mn>7</mml:mn></mml:math></alternatives></inline-formula> dendrites at the long-dendrite end of the range. Given this floor of ~7 dendrites, it becomes clear why synapse usage increases as dendrites grow longer: the number of synapses used in a dendrite that undergoes a learning event is roughly proportional to the dendrite length <inline-formula id="pcbi.1006892.e110"><alternatives><graphic id="pcbi.1006892.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e110" xlink:type="simple"/><mml:math display="inline" id="M110"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>, since the number of synapses that learn is roughly proportional to the number of synapses activated in the dendrite, which is proportional to dendrite size. Tied to this increase in synapse usage per pattern, as the total number of dendrites <inline-formula id="pcbi.1006892.e111"><alternatives><graphic id="pcbi.1006892.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula> in the system decreases (because each one contains a larger fraction of the synapses), the frequency with which each dendrite must participate in learning increases, which speeds the per-pattern rate at which synapses move along their age queues. Thus, from a capacity standpoint, it is ideal to choose system parameters such that the minimum encoding bound of 7 dendrites is actually used (or whatever minimum number of dendrites is needed, given the settings of the error rate thresholds and noise level), but having met this lower bound, dendrites should be kept as short as possible.</p>
<fig id="pcbi.1006892.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006892.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Why a recognition memory of this type must learn in at least 5 dendrites on average to store each pattern.</title>
<p>(<bold>a</bold>) Poisson distributions of actual numbers of dendrites that learn for a range of average rates, assuming no spiking noise. Fraction of cases where no dendrites learn establishes the immediate false-negative rate (again assuming no noise). The case with an average usage of 5 dendrites leads to an false negative (FN) rate of 1% immediately after storage. Including input burst noise pushes the optimized dendrite usage slightly higher to ~7 dendrites for a 1% FN error rate (see main text and SI). (<bold>b</bold>) False negative (FN) error rates as a function of average dendrite usage rate. Three stars represented cases from (a).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Penalty for short dendrites</title>
<p>The reasons capacity declines as dendrites grow shorter are complex, and are discussed only briefly here (see the <xref ref-type="supplementary-material" rid="pcbi.1006892.s001">S1 Text</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006892.s005">S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006892.s006">S4</xref> Figs for more details). We first consider why dendrite usage increases for short dendrites, rather than remaining at the minimal encoding bound. Short dendrites are intrinsically more susceptible to variability in crossing their learning and firing thresholds, since fewer active synapses are involved. As dendrites become very short, this requires the network to increase dendrite usage far above the nominal lower bound of <inline-formula id="pcbi.1006892.e112"><alternatives><graphic id="pcbi.1006892.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>5</mml:mn></mml:math></alternatives></inline-formula>. For example, under sparse activation (<inline-formula id="pcbi.1006892.e113"><alternatives><graphic id="pcbi.1006892.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e113" xlink:type="simple"/><mml:math display="inline" id="M113"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>), medium noise conditions (<inline-formula id="pcbi.1006892.e114"><alternatives><graphic id="pcbi.1006892.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>7</mml:mn><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> with dendrites containing ~200 synapses, when the system is optimized for capacity, <inline-formula id="pcbi.1006892.e115"><alternatives><graphic id="pcbi.1006892.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e115" xlink:type="simple"/><mml:math display="inline" id="M115"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>15</mml:mn></mml:math></alternatives></inline-formula> (blue solid curve in <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5e</xref>), substantially more than the number of dendrites used under maximum capacity conditions. While this increase in dendrite usage is more than offset by the reduced dendrite size, which tends to reduce synapse usage, the total number of synapses altered during learning in fact remains approximately constant, implying that a larger fraction of synapses is modified within each short dendrite that engages in learning. This higher synapse burn rate in short dendrites leads to shorter age queues, and in the end lowers capacity.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>The memory architecture we have studied is ordinary, in the sense that it consists of axons making contacts directly onto the neurons whose firing represents the memory trace, but is out-of-the-ordinary among online learning models in that it includes a layer of thresholded dendritic units interposed between the input axons and the final common output of the network.</p>
<p>The main contributions of this paper are (1) <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref>, which captures the interactions between key variables that influence storage capacity in a dendrite-based online recognition memory, and (2) our showing that over a wide range of input pattern statistics and network sizes, memory capacity is maximized when dendrites contain a few hundred synapses, which corresponds to the typical dendrite size found in medial temporal lobe memory areas [<xref ref-type="bibr" rid="pcbi.1006892.ref090">90</xref>]. To our knowledge, ours is the first theory that accounts for dendrite size in terms of its role in optimizing online learning capacity.</p>
<p>Beyond the uses we have shown here, our model could potentially be used (1) to help explain why different combinations of parameter settings co-occur in different recognition memory-related brain areas, for example in different animal species whose brains may be larger or smaller, whose sensory codes may be sparser or denser, or whose error tolerances may be tighter or looser; (2) to help distinguish brain areas involved in online familiarity-based recognition memory, the task we study here, from areas such as the hippocampus that (also) contribute to explicit recall [<xref ref-type="bibr" rid="pcbi.1006892.ref087">87</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref088">88</xref>]; and (3) to help identify which changes (e.g., spine loss, dendrite retraction, hyperexcitability, etc.) that occur in neurological disorders, aging and stress, are most directly responsible for producing memory deficits–knowledge that may eventually aid in the design of clinical interventions for those suffering from memory loss.</p>
<sec id="sec013">
<title>Why mid-sized dendrites are optimal for recognition memory</title>
<p>Why are dendrites of “medium” size optimal for storage capacity in the context of an online familiarity-based recognition memory? The simplest explanation is that short dendrites suffer from one set of disadvantages, and long dendrites suffer from another, leaving the optimal dendrite size somewhere in the middle. Short dendrites have relatively noisier post-synaptic response distributions because fewer synapses contribute to the response. As a result, a larger fraction of the synapses on a short dendrite must be modified during learning to ensure that the dendrite's response to previously trained patterns remains comfortably at the upper tail of the untrained pattern response distribution. Increasing the fraction of synapses used within a dendrite during each learning event shortens the dendrite's age queue, which comes at a capacity cost. This effect leads to a preference for longer dendrites.</p>
<p>But long dendrites also have their disadvantages. An online recognition memory should aim to store the weakest possible trace of each learned pattern, which in our framework corresponds to learning in a small number of dendrites near the "minimum encoding bound" (corresponding to ~7 dendrites under the conditions used in our study; see <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5e</xref>). This means that the longer the dendrites become, the more synaptic resources are consumed by each dendrite that learns, since the number of synapses used per dendrite during a learning event is roughly proportional to dendrite size. Clearly from this perspective, it's best to keep dendrites as short as possible.</p>
<p>The compromise between the need to keep dendrites long enough to avoid noise and age queue problems, and short enough to avoid excessive synapse use per learning dendrite, puts the optimal size around a few hundred synapses for biologically reasonable values of pattern activation density and noise. Of course, our assumptions regarding "biologically reasonable" pattern activation densities and noise levels are informed guesses rather than certain knowledge, and are not likely to be universal constants across brain areas, species and operating conditions. It is therefore possible that the natural dendrite sizes found in medial temporal lobe memory areas are determined in part by factors other than capacity optimization according to <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref>. For example, developmental constraints, energy constraints, space constraints, and combinations thereof, may have been responsible for pushing the actual dendrite size in one direction or another, away from the optimal length as determined by capacity considerations alone. Nonetheless, it is useful to capture basic relationships between biophysical parameters, wiring parameters, input pattern statistics, and capacity, as a starting point for a more complete online memory model.</p>
<p>That mid-sized dendrites optimize capacity can be understood from another perspective. <xref ref-type="disp-formula" rid="pcbi.1006892.e051">Eq 2</xref> shows capacity is given by the ratio of <inline-formula id="pcbi.1006892.e116"><alternatives><graphic id="pcbi.1006892.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>, the length of a dendrite's age queue, to <inline-formula id="pcbi.1006892.e117"><alternatives><graphic id="pcbi.1006892.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the probability that a dendrite learns. <inline-formula id="pcbi.1006892.e118"><alternatives><graphic id="pcbi.1006892.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, in the denominator, grows larger as dendrites grow in size because the same average number of dendrites is always used to learn, but when dendrites are long, there are fewer of them to choose from. <inline-formula id="pcbi.1006892.e119"><alternatives><graphic id="pcbi.1006892.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e119" xlink:type="simple"/><mml:math display="inline" id="M119"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>, in the numerator, grows smaller as dendrites shrink in size because of the higher value of <inline-formula id="pcbi.1006892.e120"><alternatives><graphic id="pcbi.1006892.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e120" xlink:type="simple"/><mml:math display="inline" id="M120"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> needed to compensate for noise effects. Balancing these two effects, capacity is maximized for dendrites of intermediate size, for which <inline-formula id="pcbi.1006892.e121"><alternatives><graphic id="pcbi.1006892.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e121" xlink:type="simple"/><mml:math display="inline" id="M121"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> is not too small, and <inline-formula id="pcbi.1006892.e122"><alternatives><graphic id="pcbi.1006892.e122g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e122" xlink:type="simple"/><mml:math display="inline" id="M122"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is not too large.</p>
<p>Thus, among the many roles that dendrites may play in the brain, in the context of an online familiarity-based recognition memory, separately thresholded dendrites play the critical role that they downsize the learning units from neuron-sized units (~20,000 synapses) to units containing a few hundred synapses, which are much more numerous, while still containing enough synapses to avoid the capacity costs associated with noise effects and shortened age queues. Simply put, having separately thresholded dendrites provides the memory system with more learning units of a better size. If dendrite-sized learning units were not available, so that it was necessary to construct an online recognition memory from neuron-sized units, storage capacity would be cut by an order of magnitude or more (see <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5c</xref>).</p>
</sec>
<sec id="sec014">
<title>Response variability is bad, so response normalization mechanisms are good</title>
<p>A general theme that emerges from our study is the importance of variability control for a recognition memory. The goal of a neural-style online recognition memory is to store a trace of each learned pattern that consumes as few synaptic resources as possible, but that nonetheless allows the network to produce a reliable recognition response on future encounters with a stored pattern. Variability in the magnitude of network responses to either learned or unlearned patterns, such as that produced by burst noise, or low pattern density, complicates this goal in at least two ways. First, increased variability in the responses to unlearned patterns raises the level of background noise, and thus the required minimum encoded signal strength that learned patterns must obtain. This in turn increases the number of synapses that must be devoted to storing each new memory. Second, increased variability in signal strength for learned patterns increases the rate of readout failures (for fixed firing and recognition thresholds). This increase in false negative errors must again be compensated for by increasing memory trace strength for all patterns, which wastefully strengthens patterns whose traces were already well above the recognition threshold.</p>
<p>These effects imply that a brain system devoted to recognition memory is under intense pressure to include response normalization mechanisms, presumably involving local inhibitory circuits [<xref ref-type="bibr" rid="pcbi.1006892.ref091">91</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref099">99</xref>].</p>
<p>It is intriguing to note that if network behavior could be perfectly normalized, so that every pattern is stored by learning in the exact same number of dendrites, e.g. 1 dendrite, then this would represent a 7-fold resource savings, presumably leading to a corresponding boost in capacity compared to the peak capacity conditions shown in <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5</xref> (where an optimized high capacity network chooses to learn using 7 dendrites).</p>
</sec>
<sec id="sec015">
<title>Existing experimental results that are consistent with our model</title>
<p>Several of the mechanisms and processes in our dendrite-based learning scheme are consistent with known biological mechanisms, including that:</p>
<list list-type="order">
<list-item><p>Strong stimulation of dendrites can trigger local learning processes, independent of somatic firing [<xref ref-type="bibr" rid="pcbi.1006892.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref065">65</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref066">66</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref074">74</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref078">78</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref080">80</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref086">86</xref>];</p></list-item>
<list-item><p>Under in vivo-like conditions, a local spike in a single dendrite can drive a burst of action potentials at the soma [<xref ref-type="bibr" rid="pcbi.1006892.ref100">100</xref>];</p></list-item>
<list-item><p>Dendrites have dissociable learning and firing thresholds, ordered such that strong stimulation of the dendrite can trigger LTP, while remaining below the local dendritic firing threshold [<xref ref-type="bibr" rid="pcbi.1006892.ref086">86</xref>].</p></list-item>
<list-item><p>Individual synapses transition between two (strong and weak) stable states [<xref ref-type="bibr" rid="pcbi.1006892.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref101">101</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref105">105</xref>];</p></list-item>
<list-item><p>LTP and LTD occur hand in hand within the same dendritic compartment when a learning event has been triggered (in keeping with the synaptic tagging/cross-tagging hypothesis [<xref ref-type="bibr" rid="pcbi.1006892.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref080">80</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref106">106</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref107">107</xref>];</p></list-item>
<list-item><p>Synaptic depression can be triggered heterosynaptically when a nearby synapse undergoes LTP, suggestive of a competitive, zero-sum mechanism within a dendritic locale [<xref ref-type="bibr" rid="pcbi.1006892.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref078">78</xref>];</p></list-item>
<list-item><p>LTP and LTD, rather than producing long term stable finely-graded weight changes, appear to primarily (and oppositely) affect synapse survival time [<xref ref-type="bibr" rid="pcbi.1006892.ref105">105</xref>].</p></list-item>
<list-item><p>Memories encoded by LTP have designated lifetimes, at the end of which they are erased by an active synaptic weakening process involving removal of GluA2/AMPARs [<xref ref-type="bibr" rid="pcbi.1006892.ref108">108</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref110">110</xref>]. Furthermore, blocking this depression process increases memory persistence (108).</p></list-item>
</list>
</sec>
<sec id="sec016">
<title>A weak prediction: The compound learning threshold</title>
<p>The main speculative/predictive features of our model pertain to the specific conditions for LTP and LTD. First, following [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>] we assumed here that the triggering of a learning event in a dendrite, which induces both LTP and LTD, depends on a compound threshold: in order to learn, a dendrite must both (1) receive an unusually strong presynaptic input, that is, unusually many axons impinging on the dendrite must be firing and releasing glutamate; and (2) experience an unusually strong post-synaptic response, that is, unusually many of the firing axons must be driving synapses that are already in a strong state. Note that a traditional Hebbian learning rule would tie learning to the post-synaptic response alone (<inline-formula id="pcbi.1006892.e123"><alternatives><graphic id="pcbi.1006892.e123g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e123" xlink:type="simple"/><mml:math display="inline" id="M123"><mml:mrow><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>), placing no explicit condition on the number of axons participating (<inline-formula id="pcbi.1006892.e124"><alternatives><graphic id="pcbi.1006892.e124g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e124" xlink:type="simple"/><mml:math display="inline" id="M124"><mml:mrow><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>). The pre-synaptic condition was incorporated into our model opportunistically, when we observed that doing so doubled the memory's storage capacity [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>]. We call the existence of a compound learning threshold a "prediction" of our model on the grounds that the brain would have been under evolutionary pressure to discover any small functional modifications that significantly boost storage capacity, and so the brain might have “discovered” this optimization–as we did. The prediction is weak, however, given that the memory can function in basically the same fashion with a single, conventional post-synaptic threshold, albeit with reduced capacity.</p>
</sec>
<sec id="sec017">
<title>A strong prediction: Synapses should have age counters</title>
<p>Unlike our weak prediction of a compound dendritic learning threshold, which could be falsified without dire consequences for the model, the prediction that synapses involved in an online familiarity memory should have a prescribed lifetime in the potentiated state, after which they are actively depotentiated, is a more deeply rooted feature of our model. This prediction is also a nearly inevitable consequence of the statement of the learning problem itself: any online recognition memory whose memory retention is much shorter than the animal's lifetime will be "full" at all times, except for a transient period at the beginning of the animal's life when the memory is first filling up. Once it reaches its chronically full state, each time a new pattern is written into the memory by strengthening synapses, as a matter of homeostatic necessity the equivalent of one stored pattern must be erased by weakening synapses, and in the interests of optimal performance, that one erased pattern should be the oldest stored pattern. The alternative–partially degrading many patterns of varying ages–is a poor strategy for a recognition memory, since any pattern whose signal strength is prematurely degraded to the point where it falls below the recognition threshold is functionally lost, yet its unerased detritus continues to uselessly consume space in the memory. Furthermore, since it is most efficient from a resource allocation point of view to store memory traces that are just strong enough to cross the recognition threshold, and no stronger, the system cannot abide gradual attrition of pattern traces. Thus the problem statement itself, and simple logic, dictate that a memory network in the brain devoted to online familiarity/recognition memory should attempt to target the oldest information for erasure as each new pattern is stored. It is difficult to imagine how selective erasure of old information could occur unless synapses keep track of their ages, and unless a dendrite is able to target its oldest synapses for depression as it undergoes each new learning event.</p>
<p>Age-based depression of synapses was previously explored as a strategy for increasing online learning capacity in the context of a 1-layer Willshaw network [<xref ref-type="bibr" rid="pcbi.1006892.ref005">5</xref>]. It is only in the context of a 2-layer memory, however, in which synaptic learning probabilities can be driven down to extremely low values without compromising signal strength, that synapses are given the opportunity to actually grow old [<xref ref-type="bibr" rid="pcbi.1006892.ref007">7</xref>].</p>
</sec>
<sec id="sec018">
<title>Comparison to online learning models that rely on complex synapses</title>
<p>In the 2-layer dendrite-based memory scheme we have studied, storage capacity is increased (~linearly) by increasing the number of dendrites, without altering the synapse model or the plasticity rule. As an alternative, Stefano Fusi and colleagues have developed two elegant models of online learning that boost capacity instead by increasing the complexity of individual synapses [<xref ref-type="bibr" rid="pcbi.1006892.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>]. Both models share the following basic framework: the memory consists of <inline-formula id="pcbi.1006892.e125"><alternatives><graphic id="pcbi.1006892.e125g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e125" xlink:type="simple"/><mml:math display="inline" id="M125"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula> synapses abstracted away from any particular network architecture; by default, every synapse is modified during the storage of every pattern; to store a pattern, synapses are strengthened and weakened in equal numbers; and all instructed weight changes during pattern storage overwrite previously stored information. The goal of these models is to carefully manage the plasticity-stability tradeoff that exists when each synapse is asked to encode information about many patterns that have been stored over time: synapses that are very plastic are good at rapidly storing new information but poor at preserving old information, whereas synapses that are very stable are good at preserving old information but poor at rapidly storing new information (synopsis adapted from [<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>]).</p>
<p>In the "Cascade" model [<xref ref-type="bibr" rid="pcbi.1006892.ref004">4</xref>], synaptic weights are binary valued (strong and weak), but can exist in states of varying lability/stability. The state diagram within each synapse operates according to two main principles. First, repeated potentiation instructions push a strong synapse into an increasingly stable strong state, that is, a state that shows an increasing resistance to depression. Similarly, repeated depression instructions by the learning rule have the effect of pushing a weak synapse into an ever more stable weak state, one that increasingly resists potentiation. Second, at "deeper" levels of the cascade, corresponding to more stable strong and weak states, the transitions to even deeper levels corresponding to even more stable states, and the transitions in synaptic weight value from strong to weak or weak to strong, all become increasingly improbable, so that synapses in deeper cascade states remain stable over longer and longer time scales. The variation in these transition probabilities across cascade levels can be considerable: according to [<xref ref-type="bibr" rid="pcbi.1006892.ref004">4</xref>] the optimal cascade model with 10^6 synapses has 15 cascade levels. With this many levels, the most labile synapses at the top of the cascade change weight with probability 1 (i.e. deterministically) in response to a weight change instruction, whereas the most stable synapses deep in the cascade only change weight with probability 1/16,384 in response to a weight-change instruction. Thus, a weak synapse in its most stable state would need to receive ~10,000 potentiation instructions in a row in order to reach a 50% chance of actually undergoing potentiation.</p>
<p>These two operating principles of the Cascade model are clearly distinguishable from those governing synaptic plasticity in our model. First, in the Cascade model, all synaptic state transitions are probabilistic, whereas in the dendrite-based model, all synaptic state changes are deterministic: during learning, weak synapses receiving the instruction to potentiate do so fully and immediately, and during forgetting, strong synapses that reach the end of their lifetimes are fully and immediately depressed. The logic of synapse durability is also different in the Cascade vs. dendrite-based models. In the Cascade model, when a synapse is first potentiated, it is in its most labile strong state, and therefore most vulnerable to depression. In the dendrite-based model, a synapse that has just been potentiated is in its most durable state, in the sense that it will withstand the largest number of consecutive learning events in which it does not participate before it "ages out" and finally succumbs to synaptic depression.</p>
<p>In the Benna and Fusi model [<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>], the machinery contained within each synapse consists (metaphorically) of a chain of connected fluid-filled beakers. The first beaker represents the synapse’s (graded) strength value by the level of virtual liquid relative to equilibrium, and the last beaker is tied to the equilibrium liquid level. Synaptic potentiation occurs deterministically, and consists of adding a fixed amount of liquid "weight" to the first beaker; synaptic depression consists of removing that amount of liquid from the first beaker. The equilibration of liquid levels in the beaker chain following an instructed weight change, and particularly the equilibration of the first beaker, captures the time course of the memory decay at each synapse. In the example shown in [<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>], a synapse consisted of a chain of 12 virtual beakers that doubled in capacity with each step down the chain (so that the last beaker had a capacity 2,048 times that of the first beaker), and whose fluid levels were governed by differential equations with pre-determined rate constants linking each pair of buckets. As a practical matter, the authors found the number of discrete levels per beaker could be reduced linearly from 35 in the first (smallest) beaker, corresponding to 35 levels of visible synaptic weight, down to 2 levels in the last (largest) beaker. This parameterization yielded a total of ~10^14 possible memory states within each synapse. Interestingly, unlike the cascade model whose synapses only change state in response to plasticity instructions (which can occur asynchronously), the chain-of-beakers model, if taken literally, continues to equilibrate—i.e. forget—even during periods when the rate of new learning slows or stops, such as during quiet wakefulness or sleep. Thus, an additional layer of mechanism is presumably needed to modulate the inter-beaker flow rates in a coordinated fashion depending on the external learning rate.</p>
<p>In summary, both of these models [<xref ref-type="bibr" rid="pcbi.1006892.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref008">8</xref>] achieve longer memory lifetimes by increasing the complexity of the synapse model as the size of the memory increases. In terms of cost, the machinery inside these more complex synapses requires more parameters (&gt;10), and those parameters must span large dynamic ranges (&gt;1000) to reach realistic memory sizes.</p>
<p>How does a dendrite-based model grow storage capacity without increasing the complexity of the individual synapses? Within virtually any recognition memory model, the conceptually simplest way to increase storage capacity is to reduce the fraction of synapses that are modified during the storage of each pattern (the signal), while correspondingly reducing the response of the memory to random input patterns (the noise). Practically, this can be achieved by sparsifying the input patterns inversely with pattern size as the memory grows larger. Thus, if the memory increases in size from <inline-formula id="pcbi.1006892.e126"><alternatives><graphic id="pcbi.1006892.e126g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e126" xlink:type="simple"/><mml:math display="inline" id="M126"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula> to <inline-formula id="pcbi.1006892.e127"><alternatives><graphic id="pcbi.1006892.e127g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e127" xlink:type="simple"/><mml:math display="inline" id="M127"><mml:mi>c</mml:mi><mml:mo>·</mml:mo><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula> synapses, in order to increase capacity c-fold, the pattern density 'a' must be reduced c-fold so that the same number of synapses is activated by each pattern as before. Assuming the learning rule instructs each activated synapse to become strong if it was weak, a·N/2 weak synapses would be potentiated on average (under the assumption that half of the synapses are strong), and an equal number of strong synapses would be depressed to maintain homeostasis (drawn from the N/2 strong synapses). To a rough approximation, this leads to a capacity of ~1/a patterns. Thus, if a = 1% of synapses are changed during the storage of each pattern, then after ~100 patterns are stored, the memory will have turned over completely. This simple scaling approach runs into the biological plausibility problem that very large capacities require very low pattern densities, and very low depression probabilities. To achieve a capacity of 100,000 patterns, for example, only 1 in 100,000 input neurons could be active, and synaptic depression would occur in only 1 in 100,000 strong synapses. Reliably controlling such small activity and plasticity probabilities could be difficult to achieve in neural tissue.</p>
</sec>
<sec id="sec019">
<title>Dendrites provide a means for sparsifying plasticity without sparsifying patterns</title>
<p>As an alternative both to this very simple sparsification approach, and to the "complex synapse" approach developed by Fusi and colleagues, adding a layer of dendritic learning units allows the memory to push further into the sparse plasticity regime without the need for very low pattern densities or plasticity probabilities. Relative to a flat (1-layer) memory model, dendritic learning thresholds can restrict learning to just a few dendrites from a very large pool. For example, in a simulation of a 5 million-synapse network discussed previously, with a moderate pattern sparseness level of a = 3%, the dendrite learning probability after optimization was P<sub>L</sub> = 0.0005, (corresponding to 1–2% of neurons in the network having one dendrite that crosses the learning threshold). Beyond the sparsification of learning attributable to dendritic learning thresholds, learning is sparsified even further by the fact that within each learning dendrite, only the active 3% of synapses receives (and obeys) the instruction to potentiate or refresh, and that same small fraction of synapses is depressed. Thus, in the above scenario, relative to a 1-layer network with the same coding density of 3%, the existence of a dendritic learning threshold sparsifies learning by a factor of 1/P<sub>L</sub> = 2000, significantly boosting capacity without requiring extreme, biologically unrealistic coding sparseness.</p>
</sec>
<sec id="sec020">
<title>Regarding the experimental detection of sparse dendritic learning events</title>
<p>In our model the formation of new memories is achieved through long-term potentiation (or rejuvenation) of a few activated synapses on a few strongly activated dendrites that undergo learning events. The "forgetting" of old memories involves heterosynaptic depression of the least-recently-potentiated/rejuvenated synapses in the same dendrites that are undergoing learning. Given the pressure to keep memory traces at their bare minimum strength, when our model is optimized for capacity, synaptic changes are exceedingly sparse, involving only a small fraction of the synapses on a minute fraction of dendrites. (The finding that memory capacity is optimized by sparse patterns has also been reported for 1-layer models: [<xref ref-type="bibr" rid="pcbi.1006892.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref111">111</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref114">114</xref>]). For example, in a memory network containing ~5 million synapses, under conditions that optimize storage capacity (i.e. with dendrites containing ~256 synapses, and patterns of 3% density), we found that each time a pattern is learned, only 150 of the 5 million synapses learn (0.003%), less than half of which are overtly strengthened (i.e. some are only rejuvenated), and those few altered synapses are confined to just 10 of the 20,000 dendrites contained within the network. If we consider extremely sparse synaptic plasticity to be a prediction of our model, could such sparse changes be detected experimentally? The likelihood of detecting changes in this few dendrites seems higher when it is considered that 20,000 dendrites corresponds to 500–1,000 neurons. We would thus expect that 10 (i.e. 1–2%) of the neurons in the network would contain a dendrite that participates in learning. In vivo imaging techniques with a field of view containing hundreds of neurons should make this level of detection possible.</p>
</sec>
<sec id="sec021">
<title>What is the role of structural plasticity in online learning?</title>
<p>What role might structural plasticity play in online learning? We previously explored the role that active dendrites might play in familiarity-based recognition in the very different scenario where patterns can be trained repeatedly [<xref ref-type="bibr" rid="pcbi.1006892.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref115">115</xref>]. The opportunity for repeated, interleaved training of patterns gives the system time to exploit <underline>wiring</underline> plasticity mechanisms [<xref ref-type="bibr" rid="pcbi.1006892.ref116">116</xref>], wherein existing connections between axons and dendrites can be eliminated and new ones formed in such a way that correlated inputs end up forming contacts onto the same dendrites. This type of wiring plasticity is not an option in an online learning scenario, since each pattern is experienced only once, such that all learning-related synaptic changes must be immediate–or at least immediately induced. We showed that correlation-based sorting of inputs onto different dendrites using a Hebb-type learning rule increased the storage capacity of a neuron by more than an order of magnitude compared to a neuron with the same total number of synaptic inputs that lacked dendrites. Furthermore, as here, we found that dendrites of intermediate size optimized capacity–though for different reasons.</p>
<p>It is interesting to note that in our current model, structural turnover of weak synapses has no effect on what is stored in the memory, as long as new weak synapses are added to the system at the same rate that existing weak synapses are removed. If weak synapses form a substantial fraction of the total synapse population–we have assumed 50% here (but the percentage may actually be closer to 90% in CA1 –see [<xref ref-type="bibr" rid="pcbi.1006892.ref117">117</xref>])–then high rates of spine elimination and new spine formation could be tolerated within the memory area without any loss of stored information–again, as long as the turnover is restricted to weak synapses. What would be the advantage of eliminating existing weak connections and forming new ones? Under the assumption that input axons are uncorrelated, as we have assumed in this work for simplicity, we can see no advantage to this type of structural turnover. However, if <underline>meaningful</underline> correlations between input axons do exist, then structural turnover could be a sign that wiring plasticity mechanisms are attempting to co-locate correlated synapses on the same dendrites [<xref ref-type="bibr" rid="pcbi.1006892.ref118">118</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref119">119</xref>], which <italic>could</italic> lead to a significant capacity advantage [<xref ref-type="bibr" rid="pcbi.1006892.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref115">115</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref116">116</xref>].</p>
</sec>
<sec id="sec022">
<title>Relationship to other forms of memory</title>
<p>Familiarity-based recognition is a very basic form of memory, and is most closely associated with the perirhinal cortex [<xref ref-type="bibr" rid="pcbi.1006892.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref087">87</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref088">88</xref>]. However, currently available data regarding the responses of familiarity (vs. novelty) neurons in the PRC is complex, and not easily related to our findings here (see <xref ref-type="supplementary-material" rid="pcbi.1006892.s001">S1 Text</xref> for an in depth discussion). Further work will be required to determine whether the dendrite-based architecture of <xref ref-type="fig" rid="pcbi.1006892.g002">Fig 2b</xref> will be helpful in explaining familiarity-based recognition processes in the brain.</p>
<p>What can the dendrite-based architecture we have studied here tell us about other types of memory systems? A trivial extension of our architecture in which N copies of the memory network are concatenated would allow the construction of a full N-bit binary online associative memory. This type of memory would behave exactly as ours, but would allow an arbitrary N-bit output pattern to be one-shot associated with each input pattern, as in a Willshaw network. In this scenario, only the subset of the N networks whose outputs are instructed to be 1 would learn each input pattern, while any networks instructed to produce 0 responses would simply ignore the input pattern. If the output patterns are sparse (which they needn't be), only a small fraction of the networks would need to participate in the learning of each association.</p>
<p>It might also be desirable to assign extended lifetimes to particularly important patterns; this could be accomplished in either of two ways: 1) Extended-lifetime synapses could be established during the learning of important patterns, so that the synapses representing those patterns would remain invulnerable to depotentiation for longer times, or even permanently. Doing so would of course reduce the lifetimes of other patterns in the memory. 2) The memory could be composed of multiple subnetworks having a range of pattern lifetimes, and important patterns could be stored in longer-lifetime (i.e. larger capacity or more rarely used) networks. The decision as to which or how many networks participate in the storage of each pattern could be gated by an "importance" signal provided by another brain area.</p>
<p>In other cases it might be valuable to store different trace strengths for different patterns, rather than uniform, bare-bones recognition traces for all patterns. Note this goal is inconsistent with the goal to maximize storage lifetimes for all patterns, but could also be useful in certain ecological situations. Our simple architecture allows for this directly: nothing is to prevent a larger or small number of dendrites from being used in the learning of any particular pattern, such that it's memory trace would be stronger or weaker than the norm. Regardless of trace strength, a pattern’s lifetime would remain roughly the same, since lifetimes are determined mainly by the lengths of the dendritic age queues, which do not depend on the number of dendrites used for storage. The trace strength assigned to each pattern could again be determined by a signal generated by another brain area, whose effect is to raise or lower dendritic learning thresholds.</p>
<p>In yet another scenario it might be useful to store gradually decaying memory traces so that trace strength can represent recency of learning (which is again a different goal than maximizing recognition capacity). A graded recency signal can be efficiently produced by storing each pattern simultaneously in multiple networks with a range of capacities/sizes/memory lifetimes. Early in its storage lifetime, the pattern would evoke a memory trace from all networks, so that it's total trace strength would be high, but as time progresses, and its trace progressively expires from the lower-capacity networks, its overall trace strength would gradually decay. This use of such a tiered system to achieve a graded decay time course is more resource-efficient than certain other forms of trace decay that have been considered in the online memory literature, in that the stored information in a tiered network with synapse age management expires in a controlled fashion [<xref ref-type="bibr" rid="pcbi.1006892.ref109">109</xref>].</p>
<p>Finally, it will require future work to determine which of our results can carry over to Hopfield-style recurrent networks [<xref ref-type="bibr" rid="pcbi.1006892.ref120">120</xref>–<xref ref-type="bibr" rid="pcbi.1006892.ref123">123</xref>] constructed from neurons with thresholded dendrites, where the goal in that case would be to maximize <underline>recall</underline> capacity. In one obvious difference, the ability to recall entire patterns from partial cues requires that the entire patterns be stored (in stark contrast to the need to generate only a reliable familiarity signal), so synapse resource consumption per pattern will be much higher than in the basic familiarity network. Furthermore, the need to modify recurrent synapses during the initial learning of a pattern implies that the participating neurons must fire action potentials during initial learning in order to activate those recurrent connections, which implies that their dendrites must cross both the learning and firing thresholds during learning. Interestingly, this requirement would seem to render such a memory useless for familiarity-based recognition, since the neurons that participate in the learning of a pattern must already fire on a pattern’s first presentation to the memory. This incompatibility could be one reason why the functions of familiarity and recall memory have been assigned to distinct areas within the medial temporal lobe [<xref ref-type="bibr" rid="pcbi.1006892.ref087">87</xref>,<xref ref-type="bibr" rid="pcbi.1006892.ref088">88</xref>].</p>
</sec>
</sec>
<sec id="sec023" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec024">
<title>Notation</title>
<list list-type="simple">
<list-item><p><inline-formula id="pcbi.1006892.e128"><alternatives><graphic id="pcbi.1006892.e128g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e128" xlink:type="simple"/><mml:math display="inline" id="M128"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> Age (in number of learning events) of synapses connecting axon <italic>i</italic> to dendrite <italic>j</italic></p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e129"><alternatives><graphic id="pcbi.1006892.e129g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e129" xlink:type="simple"/><mml:math display="inline" id="M129"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> Pre-synaptic activation of dendrite <italic>j</italic></p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e130"><alternatives><graphic id="pcbi.1006892.e130g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e130" xlink:type="simple"/><mml:math display="inline" id="M130"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> Post-synaptic activation of dendrite <italic>j</italic></p></list-item>
<list-item><p><italic>Bi(n</italic>,<italic>p)</italic> Binomial distribution function with <italic>n</italic> trials and success probability <italic>p</italic></p></list-item>
<list-item><p><italic>C</italic> Memory capacity of network, measured in number of patterns</p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e131"><alternatives><graphic id="pcbi.1006892.e131g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e131" xlink:type="simple"/><mml:math display="inline" id="M131"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> Set of inputs connected to dendrite <italic>j</italic></p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e132"><alternatives><graphic id="pcbi.1006892.e132g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e132" xlink:type="simple"/><mml:math display="inline" id="M132"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula><sub><italic>±</italic></sub> Error rates (plus for false positive, minus for false negative)</p></list-item>
<list-item><p><italic>f</italic><sub><italic>A</italic></sub> Pattern activation density (i.e. fraction of axons active in a given pattern)</p></list-item>
<list-item><p><italic>f</italic><sub><italic>pot</italic></sub> Average fraction of synapses that learn (i.e. are potentiated or juvenated) within a dendrite during a learning event (<inline-formula id="pcbi.1006892.e133"><alternatives><graphic id="pcbi.1006892.e133g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e133" xlink:type="simple"/><mml:math display="inline" id="M133"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>)</p></list-item>
<list-item><p><italic>f</italic><sub><italic>age</italic></sub> Average fraction of strong synapses in a dendrite that age during a learning event</p></list-item>
<list-item><p><italic>f</italic><sub><italic>S</italic></sub> Fraction of synapses in a dendrite that are strong (equal to 50% in our networks)</p></list-item>
<list-item><p><italic>K</italic> Number of synapses per dendrite (<inline-formula id="pcbi.1006892.e134"><alternatives><graphic id="pcbi.1006892.e134g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e134" xlink:type="simple"/><mml:math display="inline" id="M134"><mml:mi>K</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>)</p></list-item>
<list-item><p><italic>L</italic> Length of the age queue, measured in number of learning events</p></list-item>
<list-item><p><italic>M</italic> Number of dendrites in the network (<inline-formula id="pcbi.1006892.e135"><alternatives><graphic id="pcbi.1006892.e135g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e135" xlink:type="simple"/><mml:math display="inline" id="M135"><mml:mi>M</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>)</p></list-item>
<list-item><p><italic>N</italic><sub><italic>A</italic></sub> Number of axons providing inputs to the network, defining the dimensionality of the input</p></list-item>
<list-item><p><italic>N</italic><sub><italic>burst</italic></sub> Number of trials used in generating synaptic burst noise from a binomial distribution</p></list-item>
<list-item><p><italic>N</italic><sub><italic>S</italic></sub> Total number of synapses in network (<inline-formula id="pcbi.1006892.e136"><alternatives><graphic id="pcbi.1006892.e136g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e136" xlink:type="simple"/><mml:math display="inline" id="M136"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>M</mml:mi><mml:mi>·</mml:mi><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>)</p></list-item>
<list-item><p><italic>P</italic><sub><italic>burst</italic></sub> Probability used in generating synaptic burst noise from a binomial distribution</p></list-item>
<list-item><p><italic>P</italic><sub><italic>F</italic></sub> Probability that a random dendrite fires upon presentation of a random untrained pattern</p></list-item>
<list-item><p><italic>P</italic><sub><italic>L</italic></sub> Probability that a random dendrite is involved in the learning of a random pattern</p></list-item>
<list-item><p><italic>r</italic><sub><italic>j</italic></sub> Binary output of the <italic>j</italic><sup><italic>th</italic></sup> dendrite (signifying whether the dendrite fired or not)</p></list-item>
<list-item><p><italic>r</italic> Output of the memory network measured in the number of dendrites that fired</p></list-item>
<list-item><p><italic>s</italic> Slope parameter for dendritic activation sigmoid (only used in simulations)</p></list-item>
<list-item><p><italic>θ</italic><sub><italic>±</italic></sub> Maximum tolerated error rate (plus for false positive and minus for false negative)</p></list-item>
<list-item><p><italic>θ</italic><sub><italic>F</italic></sub> Firing threshold for a dendrite (in spikes)</p></list-item>
<list-item><p><italic>θ</italic><sub><italic>Lpost</italic></sub> Post-synaptic learning threshold (in spikes arriving at strong synapses)</p></list-item>
<list-item><p><italic>θ</italic><sub><italic>Lpre</italic></sub> Pre-synaptic learning threshold (in spikes arriving at strong or weak synapses)</p></list-item>
<list-item><p><italic>θ</italic><sub><italic>R</italic></sub> Recognition threshold for network to distinguish familiar from novel patterns (in number of dendrites)</p></list-item>
<list-item><p><italic>µ</italic><sub><italic>burst</italic></sub> Mean number of spikes produced in a burst by an active synapse (<inline-formula id="pcbi.1006892.e137"><alternatives><graphic id="pcbi.1006892.e137g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e137" xlink:type="simple"/><mml:math display="inline" id="M137"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>)</p></list-item>
<list-item><p><italic>µ</italic><sub><italic>LD</italic></sub> Average number of dendrites used for learning one pattern</p></list-item>
<list-item><p><italic>µ</italic><sub><italic>LS</italic></sub> Average total number of synapses used for learning one pattern</p></list-item>
<list-item><p><italic>µ</italic><sub><italic>pre</italic></sub> Average presynaptic activation for a random pattern</p></list-item>
<list-item><p><italic>µ</italic><sub><italic>pot</italic></sub> Average number of synapses per dendrite used for learning one pattern</p></list-item>
<list-item><p><italic>w</italic><sub><italic>ij</italic></sub> Weight of synaptic connection from axon i to dendrite j</p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e138"><alternatives><graphic id="pcbi.1006892.e138g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e138" xlink:type="simple"/><mml:math display="inline" id="M138"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula> Sparse, binary-valued vector representing an input pattern</p></list-item>
<list-item><p><inline-formula id="pcbi.1006892.e139"><alternatives><graphic id="pcbi.1006892.e139g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e139" xlink:type="simple"/><mml:math display="inline" id="M139"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></alternatives></inline-formula> Sparse, random, integer-valued vector representing the number of spikes arriving at each synapse</p></list-item>
</list>
</sec>
<sec id="sec025">
<title>Calculating memory capacity</title>
<p>As discussed in the main text, after a certain number of learning events has occurred following the storage of a pattern feature in a dendrite, the strong synapses encoding the stored feature begin to “fall off” the end of the dendrite’s age queue, and the memory trace in the dendrite is effectively lost. We refer to the number of learning events that can be endured before this loss occurs as the length of the age queue <italic>L</italic>. If we assume that the frequency of learning events is constant across dendrites in the network, given that the queue length <italic>L</italic> is also constant across dendrites, most of the strong synapses encoding a particular pattern’s features will be depressed roughly simultaneously (in different dendrites), leading to a relatively rapid decay of the network’s overall response <inline-formula id="pcbi.1006892.e140"><alternatives><graphic id="pcbi.1006892.e140g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e140" xlink:type="simple"/><mml:math display="inline" id="M140"><mml:mi>r</mml:mi></mml:math></alternatives></inline-formula> to that pattern. The value of <inline-formula id="pcbi.1006892.e141"><alternatives><graphic id="pcbi.1006892.e141g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e141" xlink:type="simple"/><mml:math display="inline" id="M141"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> is therefore a measure of the length of time that a pattern’s trace persists in the memory, and is therefore effectively a measure of capacity in units of dendritic learning events.</p>
<p><inline-formula id="pcbi.1006892.e142"><alternatives><graphic id="pcbi.1006892.e142g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e142" xlink:type="simple"/><mml:math display="inline" id="M142"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> can, in principle, be determined by framing learning as a Markov process with the state diagram shown in <xref ref-type="fig" rid="pcbi.1006892.g003">Fig 3</xref>. Consider a single synapse on a given dendrite. If <inline-formula id="pcbi.1006892.e143"><alternatives><graphic id="pcbi.1006892.e143g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e143" xlink:type="simple"/><mml:math display="inline" id="M143"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the <inline-formula id="pcbi.1006892.e144"><alternatives><graphic id="pcbi.1006892.e144g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e144" xlink:type="simple"/><mml:math display="inline" id="M144"><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> vector containing the probability that, at a given time, this synapse is in each of the <inline-formula id="pcbi.1006892.e145"><alternatives><graphic id="pcbi.1006892.e145g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e145" xlink:type="simple"/><mml:math display="inline" id="M145"><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> states shown in <xref ref-type="fig" rid="pcbi.1006892.g003">Fig 3</xref>, and <inline-formula id="pcbi.1006892.e146"><alternatives><graphic id="pcbi.1006892.e146g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e146" xlink:type="simple"/><mml:math display="inline" id="M146"><mml:mi>T</mml:mi></mml:math></alternatives></inline-formula> is the <inline-formula id="pcbi.1006892.e147"><alternatives><graphic id="pcbi.1006892.e147g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e147" xlink:type="simple"/><mml:math display="inline" id="M147"><mml:mfenced separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></alternatives></inline-formula> matrix containing the state transition probabilities, then with each learning event, <inline-formula id="pcbi.1006892.e148"><alternatives><graphic id="pcbi.1006892.e148g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e148" xlink:type="simple"/><mml:math display="inline" id="M148"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> will change as <inline-formula id="pcbi.1006892.e149"><alternatives><graphic id="pcbi.1006892.e149g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e149" xlink:type="simple"/><mml:math display="inline" id="M149"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mspace width="0.25em"/><mml:mo>→</mml:mo><mml:mi>T</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. After many learning events, <inline-formula id="pcbi.1006892.e150"><alternatives><graphic id="pcbi.1006892.e150g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e150" xlink:type="simple"/><mml:math display="inline" id="M150"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> will approach the equilibrium distribution, characterized by the condition that learning leaves it unchanged: <inline-formula id="pcbi.1006892.e151"><alternatives><graphic id="pcbi.1006892.e151g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e151" xlink:type="simple"/><mml:math display="inline" id="M151"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>T</mml:mi><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. Using the fact that for the equilibrium distribution <inline-formula id="pcbi.1006892.e152"><alternatives><graphic id="pcbi.1006892.e152g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e152" xlink:type="simple"/><mml:math display="inline" id="M152"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> of the synapses must be strong, one can solve for <inline-formula id="pcbi.1006892.e153"><alternatives><graphic id="pcbi.1006892.e153g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e153" xlink:type="simple"/><mml:math display="inline" id="M153"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> (since the <inline-formula id="pcbi.1006892.e154"><alternatives><graphic id="pcbi.1006892.e154g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e154" xlink:type="simple"/><mml:math display="inline" id="M154"><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> vector <inline-formula id="pcbi.1006892.e155"><alternatives><graphic id="pcbi.1006892.e155g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e155" xlink:type="simple"/><mml:math display="inline" id="M155"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> implicitly depends on <inline-formula id="pcbi.1006892.e156"><alternatives><graphic id="pcbi.1006892.e156g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e156" xlink:type="simple"/><mml:math display="inline" id="M156"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>). Using the eigenvectors and eigenvalues of <inline-formula id="pcbi.1006892.e157"><alternatives><graphic id="pcbi.1006892.e157g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e157" xlink:type="simple"/><mml:math display="inline" id="M157"><mml:mi>T</mml:mi></mml:math></alternatives></inline-formula>, one can also compute the distribution <inline-formula id="pcbi.1006892.e158"><alternatives><graphic id="pcbi.1006892.e158g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e158" xlink:type="simple"/><mml:math display="inline" id="M158"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></alternatives></inline-formula> after any number of learning events. However, while the Markov approach is very general, the simple dynamics of the age queue allow for a more direct and transparent derivation of <inline-formula id="pcbi.1006892.e159"><alternatives><graphic id="pcbi.1006892.e159g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e159" xlink:type="simple"/><mml:math display="inline" id="M159"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>.</p>
<p>To find <inline-formula id="pcbi.1006892.e160"><alternatives><graphic id="pcbi.1006892.e160g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e160" xlink:type="simple"/><mml:math display="inline" id="M160"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>, we might naively divide the total number of strong synapses per dendrite (<inline-formula id="pcbi.1006892.e161"><alternatives><graphic id="pcbi.1006892.e161g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e161" xlink:type="simple"/><mml:math display="inline" id="M161"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>) by the average number of synapses potentiated in each dendrite that experiences a learning event <inline-formula id="pcbi.1006892.e162"><alternatives><graphic id="pcbi.1006892.e162g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e162" xlink:type="simple"/><mml:math display="inline" id="M162"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. where <inline-formula id="pcbi.1006892.e163"><alternatives><graphic id="pcbi.1006892.e163g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e163" xlink:type="simple"/><mml:math display="inline" id="M163"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. In words, <inline-formula id="pcbi.1006892.e164"><alternatives><graphic id="pcbi.1006892.e164g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e164" xlink:type="simple"/><mml:math display="inline" id="M164"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is approximately equal to the total number of spikes impinging on all activated synapses on the dendrite, given by the threshold value <inline-formula id="pcbi.1006892.e165"><alternatives><graphic id="pcbi.1006892.e165g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e165" xlink:type="simple"/><mml:math display="inline" id="M165"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (since in most cases learning dendrites will have just crossed this threshold), divided by the average number of spikes per participating synapse <inline-formula id="pcbi.1006892.e166"><alternatives><graphic id="pcbi.1006892.e166g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e166" xlink:type="simple"/><mml:math display="inline" id="M166"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. This gives <inline-formula id="pcbi.1006892.e167"><alternatives><graphic id="pcbi.1006892.e167g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e167" xlink:type="simple"/><mml:math display="inline" id="M167"><mml:mi>L</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:mi>K</mml:mi><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. However, this would underestimate <italic>L</italic> because synapses that are only juvenated (i.e. that were already strong) do not contribute to the aging of synapses further along the age queue, so that the average rate of progression along the age queue slows as strong synapses grow older. To estimate <inline-formula id="pcbi.1006892.e168"><alternatives><graphic id="pcbi.1006892.e168g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e168" xlink:type="simple"/><mml:math display="inline" id="M168"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> more accurately, consider the equilibrium distribution of synapse ages in the queue of a single dendrite (blue histogram in <xref ref-type="fig" rid="pcbi.1006892.g003">Fig 3</xref>). The age of the right-most column of the age histogram is an indicator of the expected age (measured in learning events) at which the synapses encoding a pattern are depressed and moved to the unordered collection of weak synapses. During each learning event, a large fraction (<inline-formula id="pcbi.1006892.e169"><alternatives><graphic id="pcbi.1006892.e169g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e169" xlink:type="simple"/><mml:math display="inline" id="M169"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) of synapses in each column that were <italic>not</italic> activated move rightward to the next older column, while a small fraction (<inline-formula id="pcbi.1006892.e170"><alternatives><graphic id="pcbi.1006892.e170g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e170" xlink:type="simple"/><mml:math display="inline" id="M170"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> are juvenated (promoted to the first column). This process leads to a bias towards younger synapses in the queue, and can be well-approximated by a finite geometric sequence with length <inline-formula id="pcbi.1006892.e171"><alternatives><graphic id="pcbi.1006892.e171g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e171" xlink:type="simple"/><mml:math display="inline" id="M171"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>, decay ratio <inline-formula id="pcbi.1006892.e172"><alternatives><graphic id="pcbi.1006892.e172g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e172" xlink:type="simple"/><mml:math display="inline" id="M172"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, sum <inline-formula id="pcbi.1006892.e173"><alternatives><graphic id="pcbi.1006892.e173g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e173" xlink:type="simple"/><mml:math display="inline" id="M173"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∙</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula> (note the sum of the columns is the total number of strong synapses), and first column height <inline-formula id="pcbi.1006892.e174"><alternatives><graphic id="pcbi.1006892.e174g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e174" xlink:type="simple"/><mml:math display="inline" id="M174"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (the average number of synapses that learn per dendrite per learning event), so that:
<disp-formula id="pcbi.1006892.e175">
<alternatives>
<graphic id="pcbi.1006892.e175g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e175" xlink:type="simple"/>
<mml:math display="block" id="M175">
<mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Assuming that the synapses in a dendrite are all equally likely to be potentiated (ignoring the effects of the postsynaptic threshold–see below), with <inline-formula id="pcbi.1006892.e176"><alternatives><graphic id="pcbi.1006892.e176g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e176" xlink:type="simple"/><mml:math display="inline" id="M176"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, then we have that <inline-formula id="pcbi.1006892.e177"><alternatives><graphic id="pcbi.1006892.e177g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e177" xlink:type="simple"/><mml:math display="inline" id="M177"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> and can solve the above equation for <inline-formula id="pcbi.1006892.e178"><alternatives><graphic id="pcbi.1006892.e178g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e178" xlink:type="simple"/><mml:math display="inline" id="M178"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula>. Note that <inline-formula id="pcbi.1006892.e179"><alternatives><graphic id="pcbi.1006892.e179g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e179" xlink:type="simple"/><mml:math display="inline" id="M179"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> counts the number of dendritic learning events before a memory is eroded, whereas memory capacity <italic>C</italic> should count the number of training patterns. Thus, to approximate <inline-formula id="pcbi.1006892.e180"><alternatives><graphic id="pcbi.1006892.e180g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e180" xlink:type="simple"/><mml:math display="inline" id="M180"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula>, we must multiply <inline-formula id="pcbi.1006892.e181"><alternatives><graphic id="pcbi.1006892.e181g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e181" xlink:type="simple"/><mml:math display="inline" id="M181"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> by the approximate number of patterns per dendritic learning event, or “learning interval” <inline-formula id="pcbi.1006892.e182"><alternatives><graphic id="pcbi.1006892.e182g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e182" xlink:type="simple"/><mml:math display="inline" id="M182"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006892.e183"><alternatives><graphic id="pcbi.1006892.e183g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e183" xlink:type="simple"/><mml:math display="inline" id="M183"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the probability that an arbitrary dendrite learns a particular pattern. Although <inline-formula id="pcbi.1006892.e184"><alternatives><graphic id="pcbi.1006892.e184g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e184" xlink:type="simple"/><mml:math display="inline" id="M184"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is conceptually simple, its expression is complicated since it depends on pattern density, noise level, two learning thresholds, dendrite size, and <inline-formula id="pcbi.1006892.e185"><alternatives><graphic id="pcbi.1006892.e185g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e185" xlink:type="simple"/><mml:math display="inline" id="M185"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (see expression below). Collecting these results, we can approximate memory capacity by
<disp-formula id="pcbi.1006892.e186">
<alternatives>
<graphic id="pcbi.1006892.e186g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e186" xlink:type="simple"/>
<mml:math display="block" id="M186">
<mml:mrow><mml:mi>C</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>For simplicity, the expression for <inline-formula id="pcbi.1006892.e187"><alternatives><graphic id="pcbi.1006892.e187g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e187" xlink:type="simple"/><mml:math display="inline" id="M187"><mml:mi>L</mml:mi></mml:math></alternatives></inline-formula> in the capacity equation does not include the effect of the postsynaptic threshold <inline-formula id="pcbi.1006892.e188"><alternatives><graphic id="pcbi.1006892.e188g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e188" xlink:type="simple"/><mml:math display="inline" id="M188"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, which makes strong synapses more likely to learn, lowers <inline-formula id="pcbi.1006892.e189"><alternatives><graphic id="pcbi.1006892.e189g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e189" xlink:type="simple"/><mml:math display="inline" id="M189"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and increases absolute capacity. The synapse age distribution remains roughly geometric, however (see <xref ref-type="fig" rid="pcbi.1006892.g005">Fig 5b</xref>), and we observed that the qualitative behavior of the system depends only weakly on <inline-formula id="pcbi.1006892.e190"><alternatives><graphic id="pcbi.1006892.e190g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e190" xlink:type="simple"/><mml:math display="inline" id="M190"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, justifying its omission from the analysis.</p>
</sec>
<sec id="sec026">
<title>Derivation of <inline-formula id="pcbi.1006892.e191"><alternatives><graphic id="pcbi.1006892.e191g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e191" xlink:type="simple"/><mml:math display="inline" id="M191"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula></title>
<p>Synaptic activation on a dendrite is governed by 4 binomial random variables: <inline-formula id="pcbi.1006892.e192"><alternatives><graphic id="pcbi.1006892.e192g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e192" xlink:type="simple"/><mml:math display="inline" id="M192"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the number of active strong synapses; <inline-formula id="pcbi.1006892.e193"><alternatives><graphic id="pcbi.1006892.e193g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e193" xlink:type="simple"/><mml:math display="inline" id="M193"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the number of spikes received by strong synapses; <inline-formula id="pcbi.1006892.e194"><alternatives><graphic id="pcbi.1006892.e194g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e194" xlink:type="simple"/><mml:math display="inline" id="M194"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the number of active weak synapses; and <inline-formula id="pcbi.1006892.e195"><alternatives><graphic id="pcbi.1006892.e195g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e195" xlink:type="simple"/><mml:math display="inline" id="M195"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the number of spikes received by weak synapses. These random variables have the distributions shown below. Learning occurs when presynaptic activation crosses the presynaptic learning threshold, or <inline-formula id="pcbi.1006892.e196"><alternatives><graphic id="pcbi.1006892.e196g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e196" xlink:type="simple"/><mml:math display="inline" id="M196"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, and postsynaptic activation crosses the postsynaptic learning threshold, or <inline-formula id="pcbi.1006892.e197"><alternatives><graphic id="pcbi.1006892.e197g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e197" xlink:type="simple"/><mml:math display="inline" id="M197"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></alternatives></inline-formula> Using the distributions for <inline-formula id="pcbi.1006892.e198"><alternatives><graphic id="pcbi.1006892.e198g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e198" xlink:type="simple"/><mml:math display="inline" id="M198"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e199"><alternatives><graphic id="pcbi.1006892.e199g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e199" xlink:type="simple"/><mml:math display="inline" id="M199"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, and the fact that <inline-formula id="pcbi.1006892.e200"><alternatives><graphic id="pcbi.1006892.e200g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e200" xlink:type="simple"/><mml:math display="inline" id="M200"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.25em"/></mml:math></alternatives></inline-formula>we can write an explicit expression for <inline-formula id="pcbi.1006892.e201"><alternatives><graphic id="pcbi.1006892.e201g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e201" xlink:type="simple"/><mml:math display="inline" id="M201"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1006892.e202">
<alternatives>
<graphic id="pcbi.1006892.e202g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e202" xlink:type="simple"/>
<mml:math display="block" id="M202">
<mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mspace width="0.25em"/><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1006892.e203">
<alternatives>
<graphic id="pcbi.1006892.e203g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e203" xlink:type="simple"/>
<mml:math display="block" id="M203">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mspace width="0.25em"/><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1006892.e204">
<alternatives>
<graphic id="pcbi.1006892.e204g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e204" xlink:type="simple"/>
<mml:math display="block" id="M204">
<mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mspace width="0.25em"/><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1006892.e205">
<alternatives>
<graphic id="pcbi.1006892.e205g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e205" xlink:type="simple"/>
<mml:math display="block" id="M205">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>~</mml:mo><mml:mspace width="0.25em"/><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1006892.e206">
<alternatives>
<graphic id="pcbi.1006892.e206g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e206" xlink:type="simple"/>
<mml:math display="block" id="M206">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>j</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <inline-formula id="pcbi.1006892.e207"><alternatives><graphic id="pcbi.1006892.e207g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e207" xlink:type="simple"/><mml:math display="inline" id="M207"><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:math></alternatives></inline-formula> is the binomial pdf with parameters <inline-formula id="pcbi.1006892.e208"><alternatives><graphic id="pcbi.1006892.e208g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e208" xlink:type="simple"/><mml:math display="inline" id="M208"><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> evaluated at <inline-formula id="pcbi.1006892.e209"><alternatives><graphic id="pcbi.1006892.e209g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e209" xlink:type="simple"/><mml:math display="inline" id="M209"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>. A simpler alternative to evaluating this expression directly is to estimate it by generating a large number of samples of <inline-formula id="pcbi.1006892.e210"><alternatives><graphic id="pcbi.1006892.e210g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e210" xlink:type="simple"/><mml:math display="inline" id="M210"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mspace width="0.25em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e211"><alternatives><graphic id="pcbi.1006892.e211g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e211" xlink:type="simple"/><mml:math display="inline" id="M211"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> according to the above distributions, and directly observing the fraction of cases that cross both learning thresholds <inline-formula id="pcbi.1006892.e212"><alternatives><graphic id="pcbi.1006892.e212g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e212" xlink:type="simple"/><mml:math display="inline" id="M212"><mml:mo>.</mml:mo></mml:math></alternatives></inline-formula></p>
</sec>
<sec id="sec027">
<title>Checking error tolerances</title>
<p>Once the capacity formula is used to calculate <italic>how long</italic> a given memory trace will last, we must verify that during its lifetime, the trace is sufficiently <italic>strong</italic>. We do this by checking whether the error tolerances <inline-formula id="pcbi.1006892.e213"><alternatives><graphic id="pcbi.1006892.e213g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e213" xlink:type="simple"/><mml:math display="inline" id="M213"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e214"><alternatives><graphic id="pcbi.1006892.e214g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e214" xlink:type="simple"/><mml:math display="inline" id="M214"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> are met immediately after training.</p>
<p>First, we compute <inline-formula id="pcbi.1006892.e215"><alternatives><graphic id="pcbi.1006892.e215g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e215" xlink:type="simple"/><mml:math display="inline" id="M215"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the probability that an untrained pattern will be recognized. To be recognized, a pattern must activate at least <inline-formula id="pcbi.1006892.e216"><alternatives><graphic id="pcbi.1006892.e216g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e216" xlink:type="simple"/><mml:math display="inline" id="M216"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> dendrites in the network. For a randomly selected untrained pattern, the distribution of the number of activated dendrites will be approximately Poisson with mean <inline-formula id="pcbi.1006892.e217"><alternatives><graphic id="pcbi.1006892.e217g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e217" xlink:type="simple"/><mml:math display="inline" id="M217"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1006892.e218"><alternatives><graphic id="pcbi.1006892.e218g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e218" xlink:type="simple"/><mml:math display="inline" id="M218"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula> is the number of dendrites in the network and <inline-formula id="pcbi.1006892.e219"><alternatives><graphic id="pcbi.1006892.e219g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e219" xlink:type="simple"/><mml:math display="inline" id="M219"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the probability that a given dendrite fires in response to a randomly selected pattern. For a pattern to fire a dendrite, it must cause a postsynaptic activation <inline-formula id="pcbi.1006892.e220"><alternatives><graphic id="pcbi.1006892.e220g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e220" xlink:type="simple"/><mml:math display="inline" id="M220"><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, or <inline-formula id="pcbi.1006892.e221"><alternatives><graphic id="pcbi.1006892.e221g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e221" xlink:type="simple"/><mml:math display="inline" id="M221"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, using the notation of above. Since the distribution of <inline-formula id="pcbi.1006892.e222"><alternatives><graphic id="pcbi.1006892.e222g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e222" xlink:type="simple"/><mml:math display="inline" id="M222"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is known, it is relatively easy to write an expression for <inline-formula id="pcbi.1006892.e223"><alternatives><graphic id="pcbi.1006892.e223g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e223" xlink:type="simple"/><mml:math display="inline" id="M223"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006892.e224"><alternatives><graphic id="pcbi.1006892.e224g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e224" xlink:type="simple"/><mml:math display="inline" id="M224"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> explicitly:
<disp-formula id="pcbi.1006892.e225">
<alternatives>
<graphic id="pcbi.1006892.e225g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e225" xlink:type="simple"/>
<mml:math display="block" id="M225">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>·</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>·</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1006892.e226">
<alternatives>
<graphic id="pcbi.1006892.e226g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e226" xlink:type="simple"/>
<mml:math display="block" id="M226">
<mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mo>≥</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>θ</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>r</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
As for <inline-formula id="pcbi.1006892.e227"><alternatives><graphic id="pcbi.1006892.e227g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e227" xlink:type="simple"/><mml:math display="inline" id="M227"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></alternatives></inline-formula> the probability that a previously trained pattern is forgotten, we approximate this quantity with <inline-formula id="pcbi.1006892.e228"><alternatives><graphic id="pcbi.1006892.e228g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e228" xlink:type="simple"/><mml:math display="inline" id="M228"><mml:msubsup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, or the immediately post-training false negative rate (justified by the fact that during the “lifetime” of the memory, <inline-formula id="pcbi.1006892.e229"><alternatives><graphic id="pcbi.1006892.e229g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e229" xlink:type="simple"/><mml:math display="inline" id="M229"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula>, the trace strength is roughly constant). To calculate <inline-formula id="pcbi.1006892.e230"><alternatives><graphic id="pcbi.1006892.e230g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e230" xlink:type="simple"/><mml:math display="inline" id="M230"><mml:msubsup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, we use the following observation: when training a new pattern, it will learn in a certain set of dendrites. Immediately after training, if the pattern is re-presentated to the network, <italic>all</italic> of these dendrites should respond, since learning has significantly boosted the pattern’s features in these dendrites. In other words, dendrite readout failures immediately after learning should be very rare. Therefore, for a pattern to be too weak for recognition immediately after training, it must have learned in too few dendrites. The number of learning dendrites for a given pattern will have a Poisson distribution with mean <inline-formula id="pcbi.1006892.e231"><alternatives><graphic id="pcbi.1006892.e231g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e231" xlink:type="simple"/><mml:math display="inline" id="M231"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>. Therefore, <inline-formula id="pcbi.1006892.e232"><alternatives><graphic id="pcbi.1006892.e232g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e232" xlink:type="simple"/><mml:math display="inline" id="M232"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> can be written
<disp-formula id="pcbi.1006892.e233">
<alternatives>
<graphic id="pcbi.1006892.e233g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e233" xlink:type="simple"/>
<mml:math display="block" id="M233">
<mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mo>−</mml:mo><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>l</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
If for the given settings of the learning and firing thresholds <inline-formula id="pcbi.1006892.e234"><alternatives><graphic id="pcbi.1006892.e234g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e234" xlink:type="simple"/><mml:math display="inline" id="M234"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></alternatives></inline-formula>, the error tolerances are met–that is, <inline-formula id="pcbi.1006892.e235"><alternatives><graphic id="pcbi.1006892.e235g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006892.e235" xlink:type="simple"/><mml:math display="inline" id="M235"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mi>%</mml:mi><mml:mo>–</mml:mo></mml:math></alternatives></inline-formula>then the memory lifetime is compared to the best memory lifetime found so far. Otherwise, we continue the search through threshold space.</p>
</sec>
<sec id="sec028">
<title>Code availability</title>
<p>All data contained in figures as well as simulation code is available in <xref ref-type="supplementary-material" rid="pcbi.1006892.s002">S1 Data</xref> file titled "Plos data/code".</p>
</sec>
</sec>
<sec id="sec029">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006892.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Additional material discussing effects of various network parameters on memory capacity.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006892.s002" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s002" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Network simulation code and data.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006892.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s003" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Effect of background noise on network performance.</title>
<p>In the base case without background noise, nominally inactive axons (which were the vast majority) never fired. For the medium and high noise cases, nominally inactive axons emitted one spike with the indicated probability. The fraction of inactive axons that fired a spike was chosen so that in the medium case, aberrant spikes totaled approximately 10% of the number of “real” pattern spikes (recall that each active axon generated a burst of 4 spikes on average), and in the high noise case, aberrant spikes were 25% of the real spikes. Increasing background noise decreased memory capacity, and, at high noise levels, pushed the optimal dendrite size to shorter values. For all simulations here, the dendritic activation slope parameter was set to 3.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006892.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s004" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Network responses for perturbed patterns.</title>
<p>The memory network was trained as normal to maximize old/new recognition capacity. We then tested how a trained network responded to perturbed versions of stored patterns. As expected, as an increasing fraction of training pattern bits were changed, network response decreased (black curve). For example, when 20% of an original training pattern’s active bits were assigned to different input lines (keeping pattern density unchanged), average network response fell to roughly one third of the original response. We then tested whether the network could reliably distinguish between exact trained patterns and perturbed patterns (red curve). The network was able to distinguish exact training patterns from 20% perturbed patterns with 85% accuracy.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006892.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s005" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Explanation of dendrite "availability" problem faced by short dendrites, and the remedy.</title>
<p>(See <xref ref-type="supplementary-material" rid="pcbi.1006892.s001">S1 Text</xref> for details).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006892.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006892.s006" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Contributors to additional capacity costs for short dendrites.</title>
<p>(<bold>a</bold>) Distributions of pre-synaptic responses to random patterns for dendrites of varying size. (<bold>b</bold>) Same graph as (a) but with responses normalized to the mean response. Colored arrows indicate points where the upper 1% of the probability mass begins, to illustrate that shorter dendrites have larger response variability relative to their mean than longer dendrites. (<bold>c</bold>) Fraction of synapses used within each dendrite involved in learning increases for short dendrites. (<bold>d</bold>) Comparison of capacity for 3 cases with equivalent synapse usage (red dots); capacity drops linearly for shorter dendrites because of the higher values of <italic>f</italic><sub><italic>pot</italic></sub>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Fritz Sommer for helpful discussions in the course of this work.</p>
</ack>
<ref-list>
<title>REFERENCES</title>
<ref id="pcbi.1006892.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Toulouse</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Changeux</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Networks of formal neurons and memory palimpsests</article-title>. <source>EPL Europhys Lett</source>. <year>1986</year>;<volume>1</volume>: <fpage>535</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Learning in neural networks with material synapses</article-title>. <source>Neural Comput</source>. <year>1994</year>;<volume>6</volume>: <fpage>957</fpage>–<lpage>982</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>485</fpage>–<lpage>493</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1859" xlink:type="simple">10.1038/nn1859</ext-link></comment> <object-id pub-id-type="pmid">17351638</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>45</volume>: <fpage>599</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.02.001" xlink:type="simple">10.1016/j.neuron.2005.02.001</ext-link></comment> <object-id pub-id-type="pmid">15721245</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref005"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Henson RN, Willshaw DJ. Short-term associative memory. Proceedings of the INNS World Congress on Neural Networks. 1995. pp. 438–441. Available: <ext-link ext-link-type="uri" xlink:href="http://www.researchgate.net/publication/2358602_Short-term_Associative_Memory/file/e0b49521bd71403e73.pdf" xlink:type="simple">http://www.researchgate.net/publication/2358602_Short-term_Associative_Memory/file/e0b49521bd71403e73.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006892.ref006"><label>6</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lahiri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>. <chapter-title>A memory frontier for complex synapses</chapter-title>. In: <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 26</source>. <publisher-name>Curran Associates, Inc</publisher-name>.; <year>2013</year>. pp. <fpage>1034</fpage>–<lpage>1042</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses.pdf" xlink:type="simple">http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006892.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>XE</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Capacity-Enhancing Synaptic Learning Rules in a Medial Temporal Lobe Online Learning Model</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>: <fpage>31</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.02.021" xlink:type="simple">10.1016/j.neuron.2009.02.021</ext-link></comment> <object-id pub-id-type="pmid">19376065</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benna</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Computational principles of synaptic memory consolidation</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>: <fpage>1697</fpage>–<lpage>1706</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4401" xlink:type="simple">10.1038/nn.4401</ext-link></comment> <object-id pub-id-type="pmid">27694992</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sohal</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>. <article-title>A model for experience-dependent changes in the responses of inferotemporal neurons</article-title>. <source>Netw Comput Neural Syst</source>. <year>2000</year>;<volume>11</volume>: <fpage>169</fpage>–<lpage>190</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>MW</given-names></name>. <article-title>Comparison of computational models of familiarity discrimination in the perirhinal cortex</article-title>. <source>Hippocampus</source>. <year>2003</year>;<volume>13</volume>: <fpage>494</fpage>–<lpage>524</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.10093" xlink:type="simple">10.1002/hipo.10093</ext-link></comment> <object-id pub-id-type="pmid">12836918</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Giraud-Carrier</surname> <given-names>C</given-names></name>. <article-title>Model of familiarity discrimination in the perirhinal cortex</article-title>. <source>J Comput Neurosci</source>. <year>2001</year>;<volume>10</volume>: <fpage>5</fpage>–<lpage>23</lpage>. <object-id pub-id-type="pmid">11316340</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xiang</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>MW</given-names></name>. <article-title>Differential neuronal encoding of novelty, familiarity and recency in regions of the anterior temporal lobe</article-title>. <source>Neuropharmacology</source>. <year>1998</year>;<volume>37</volume>: <fpage>657</fpage>–<lpage>676</lpage>. <object-id pub-id-type="pmid">9705004</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amitai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connors</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Gutnick</surname> <given-names>MJ</given-names></name>. <article-title>Regenerative activity in apical dendrites of pyramidal cells in neocortex</article-title>. <source>Cereb Cortex N Y N 1991</source>. <year>1993</year>;<volume>3</volume>: <fpage>26</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Antic</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>W-L</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Short</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ikonomu</surname> <given-names>KD</given-names></name>. <article-title>The decade of the dendritic NMDA spike</article-title>. <source>J Neurosci Res</source>. <year>2010</year>;<volume>88</volume>: <fpage>2991</fpage>–<lpage>3001</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/jnr.22444" xlink:type="simple">10.1002/jnr.22444</ext-link></comment> <object-id pub-id-type="pmid">20544831</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Archie</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>A model for intradendritic computation of binocular disparity</article-title>. <source>Nat Neurosci</source>. <year>2000</year>;<volume>3</volume>: <fpage>54</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/71125" xlink:type="simple">10.1038/71125</ext-link></comment> <object-id pub-id-type="pmid">10607395</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behabadi</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jadi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Location-Dependent Excitatory Synaptic Interactions in Pyramidal Neuron Dendrites</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>: <fpage>e1002599</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002599" xlink:type="simple">10.1371/journal.pcbi.1002599</ext-link></comment> <object-id pub-id-type="pmid">22829759</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bittner</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Grienberger</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vaidya</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Milstein</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Macklin</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Suh</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Conjunctive input processing drives feature selectivity in hippocampal CA1 neurons</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>1133</fpage>–<lpage>1142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4062" xlink:type="simple">10.1038/nn.4062</ext-link></comment> <object-id pub-id-type="pmid">26167906</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Borg-Graham</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Grzywacz</surname> <given-names>NM</given-names></name>. <chapter-title>A model of the directional selectivity circuit in retina: transformations by neurons singly and in concert</chapter-title>. In: <name name-style="western"><surname>McKenna</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Zornetzer</surname> <given-names>SF</given-names></name>, editors. <source>Single neuron computation</source>. <publisher-name>Academic Press Professional, Inc</publisher-name>.; <year>1992</year>. pp. <fpage>347</fpage>–<lpage>375</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Egelhaaf</surname> <given-names>M</given-names></name>. <article-title>Dendritic processing of synaptic information by sensory interneurons</article-title>. <source>Trends Neurosci</source>. <year>1994</year>;<volume>17</volume>: <fpage>257</fpage>–<lpage>263</lpage>. <object-id pub-id-type="pmid">7521087</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>Dendritic discrimination of temporal input sequences in cortical neurons</article-title>. <source>Science</source>. <year>2010</year>;<volume>329</volume>: <fpage>1671</fpage>–<lpage>1675</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1189664" xlink:type="simple">10.1126/science.1189664</ext-link></comment> <object-id pub-id-type="pmid">20705816</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gidon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>I</given-names></name>. <article-title>Principles governing the operation of synaptic inhibition in dendrites</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>75</volume>: <fpage>330</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.05.015" xlink:type="simple">10.1016/j.neuron.2012.05.015</ext-link></comment> <object-id pub-id-type="pmid">22841317</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldman</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Levine</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Major</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Robust persistent neural activity in a model integrator with multiple hysteretic dendrites per neuron</article-title>. <source>Cereb Cortex N Y N 1991</source>. <year>2003</year>;<volume>13</volume>: <fpage>1185</fpage>–<lpage>1195</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grienberger</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Konnerth</surname> <given-names>A</given-names></name>. <article-title>Dendritic function in vivo</article-title>. <source>Trends Neurosci</source>. <year>2015</year>;<volume>38</volume>: <fpage>45</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2014.11.002" xlink:type="simple">10.1016/j.tins.2014.11.002</ext-link></comment> <object-id pub-id-type="pmid">25432423</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>. <article-title>An arithmetic rule for spatial summation of excitatory and inhibitory inputs in pyramidal neurons</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2009</year>;<volume>106</volume>: <fpage>21906</fpage>–<lpage>21911</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0912022106" xlink:type="simple">10.1073/pnas.0912022106</ext-link></comment> <object-id pub-id-type="pmid">19955407</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Helmchen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>In vivo dendritic calcium dynamics in deep-layer cortical pyramidal neurons</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>989</fpage>–<lpage>996</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14788" xlink:type="simple">10.1038/14788</ext-link></comment> <object-id pub-id-type="pmid">10526338</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jadi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Location-Dependent Effects of Inhibition on Local Spiking in Pyramidal Neuron Dendrites</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>: <fpage>e1002550</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002550" xlink:type="simple">10.1371/journal.pcbi.1002550</ext-link></comment> <object-id pub-id-type="pmid">22719240</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jadi</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Behabadi</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Poleg-Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>An Augmented Two-Layer Model Captures Nonlinear Analog Spatial Integration Effects in Pyramidal Neuron Dendrites</article-title>. <source>Proc IEEE Inst Electr Electron Eng</source>. <year>2014</year>;<volume>102</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/JPROC.2014.2312671" xlink:type="simple">10.1109/JPROC.2014.2312671</ext-link></comment> <object-id pub-id-type="pmid">25554708</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jarsky</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Roxin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kath</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>. <article-title>Conditional dendritic spike propagation following distal synaptic activation of hippocampal CA1 pyramidal neurons</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>: <fpage>1667</fpage>–<lpage>1676</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1599" xlink:type="simple">10.1038/nn1599</ext-link></comment> <object-id pub-id-type="pmid">16299501</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Katz</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Menon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nicholson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Geinisman</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kath</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>. <article-title>Synapse distribution suggests a two-stage model of dendritic integration in CA1 pyramidal neurons</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>: <fpage>171</fpage>–<lpage>177</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.06.023" xlink:type="simple">10.1016/j.neuron.2009.06.023</ext-link></comment> <object-id pub-id-type="pmid">19640476</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Torre</surname> <given-names>V</given-names></name>. <article-title>Retinal ganglion cells: a functional interpretation of dendritic morphology</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>1982</year>;<volume>298</volume>: <fpage>227</fpage>–<lpage>263</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.1982.0084" xlink:type="simple">10.1098/rstb.1982.0084</ext-link></comment> <object-id pub-id-type="pmid">6127730</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name>. <article-title>A new cellular mechanism for coupling inputs arriving at different cortical layers</article-title>. <source>Nature</source>. <year>1999</year>;<volume>398</volume>: <fpage>338</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/18686" xlink:type="simple">10.1038/18686</ext-link></comment> <object-id pub-id-type="pmid">10192334</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Lüscher</surname> <given-names>H-R</given-names></name>. <article-title>Top-down dendritic input increases the gain of layer 5 pyramidal neurons</article-title>. <source>Cereb Cortex N Y N 1991</source>. <year>2004</year>;<volume>14</volume>: <fpage>1059</fpage>–<lpage>1070</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhh065" xlink:type="simple">10.1093/cercor/bhh065</ext-link></comment> <object-id pub-id-type="pmid">15115747</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Nevian</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sandler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Synaptic Integration in Tuft Dendrites of Layer 5 Pyramidal Neurons: A New Unifying Principle</article-title>. <source>Science</source>. <year>2009</year>;<volume>325</volume>: <fpage>756</fpage>–<lpage>760</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1171958" xlink:type="simple">10.1126/science.1171958</ext-link></comment> <object-id pub-id-type="pmid">19661433</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lavzin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rapoport</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Garion</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Nonlinear dendritic processing determines angular tuning of barrel cortex neurons in vivo</article-title>. <source>Nature</source>. <year>2012</year>;<volume>490</volume>: <fpage>397</fpage>–<lpage>401</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11451" xlink:type="simple">10.1038/nature11451</ext-link></comment> <object-id pub-id-type="pmid">22940864</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Branch-Specific Plasticity Enables Self-Organization of Nonlinear Computation in Single Neurons</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>10787</fpage>–<lpage>10802</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5684-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5684-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21795531</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Losonczy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>JC</given-names></name>. <article-title>Integrative properties of radial oblique dendrites in hippocampal CA1 pyramidal neurons</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>50</volume>: <fpage>291</fpage>–<lpage>307</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.03.016" xlink:type="simple">10.1016/j.neuron.2006.03.016</ext-link></comment> <object-id pub-id-type="pmid">16630839</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Major</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Spatiotemporally Graded NMDA Spike/Plateau Potentials in Basal Dendrites of Neocortical Pyramidal Neurons</article-title>. <source>J Neurophysiol</source>. <year>2008</year>;<volume>99</volume>: <fpage>2584</fpage>–<lpage>2601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00011.2008" xlink:type="simple">10.1152/jn.00011.2008</ext-link></comment> <object-id pub-id-type="pmid">18337370</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Major</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Active properties of neocortical pyramidal neuron dendrites</article-title>. <source>Annu Rev Neurosci</source>. <year>2013</year>;<volume>36</volume>: <fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-062111-150343" xlink:type="simple">10.1146/annurev-neuro-062111-150343</ext-link></comment> <object-id pub-id-type="pmid">23841837</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>NMDA-Based Pattern Discrimination in a Modeled Cortical Neuron</article-title>. <source>Neural Comput</source>. <year>1992</year>;<volume>4</volume>: <fpage>502</fpage>–<lpage>517</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1992.4.4.502" xlink:type="simple">10.1162/neco.1992.4.4.502</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006892.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>The clusteron: toward a simple abstraction for a complex neuron</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>1992</year>;<volume>4</volume>: <fpage>35</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Ruderman</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Archie</surname> <given-names>KA</given-names></name>. <article-title>Translation-Invariant Orientation Tuning in Visual “Complex” Cells Could Derive from Intradendritic Computations</article-title>. <source>J Neurosci</source>. <year>1998</year>;<volume>18</volume>: <fpage>4325</fpage>–<lpage>4334</lpage>. <object-id pub-id-type="pmid">9592109</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Migliore</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Novara</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tegolo</surname> <given-names>D</given-names></name>. <article-title>Single neuron binding properties and the magical number 7</article-title>. <source>Hippocampus</source>. <year>2008</year>;<volume>18</volume>: <fpage>1122</fpage>–<lpage>1130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20480" xlink:type="simple">10.1002/hipo.20480</ext-link></comment> <object-id pub-id-type="pmid">18680161</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milojkovic</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Radojicic</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Antic</surname> <given-names>SD</given-names></name>. <article-title>A Strict Correlation between Dendritic and Somatic Plateau Depolarizations in the Rat Prefrontal Cortex Pyramidal Neurons</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>3940</fpage>–<lpage>3951</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5314-04.2005" xlink:type="simple">10.1523/JNEUROSCI.5314-04.2005</ext-link></comment> <object-id pub-id-type="pmid">15829646</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>. <article-title>Possible role of dendritic compartmentalization in the spatial working memory circuit</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>7699</fpage>–<lpage>7724</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0059-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0059-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18650346</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Shai</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Reeve</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Paulsen</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>. <article-title>NMDA spikes enhance action potential generation during sensory input</article-title>. <source>Nat Neurosci</source>. <year>2014</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3646" xlink:type="simple">10.1038/nn.3646</ext-link></comment> <object-id pub-id-type="pmid">24487231</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poirazi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Impact of active dendrites and structural plasticity on the memory capacity of neural tissue</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>29</volume>: <fpage>779</fpage>–<lpage>796</lpage>. <object-id pub-id-type="pmid">11301036</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poirazi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brannon</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Arithmetic of Subthreshold Synaptic Summation in a Model CA1</article-title> <source>Pyramidal Cell. Neuron</source>. <year>2003</year>;<volume>37</volume>: <fpage>977</fpage>–<lpage>987</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(03)00148-X" xlink:type="simple">10.1016/S0896-6273(03)00148-X</ext-link></comment> <object-id pub-id-type="pmid">12670426</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poirazi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brannon</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>Pyramidal Neuron as Two-Layer Neural Network</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>37</volume>: <fpage>989</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(03)00149-1" xlink:type="simple">10.1016/S0896-6273(03)00149-1</ext-link></comment> <object-id pub-id-type="pmid">12670427</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poleg-Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diamond</surname> <given-names>JS</given-names></name>. <article-title>NMDA Receptors Multiplicatively Scale Visual Signals and Enhance Directional Motion Discrimination in Retinal Ganglion Cells</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>: <fpage>1277</fpage>–<lpage>1290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.02.013" xlink:type="simple">10.1016/j.neuron.2016.02.013</ext-link></comment> <object-id pub-id-type="pmid">26948896</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Computational subunits in thin dendrites of pyramidal cells</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>: <fpage>621</fpage>–<lpage>627</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1253" xlink:type="simple">10.1038/nn1253</ext-link></comment> <object-id pub-id-type="pmid">15156147</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rall</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>I</given-names></name>. <chapter-title>Functional possibilities for synapses on dendrites and on dendritic spines</chapter-title>. In: <name name-style="western"><surname>Edelman</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Gall</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Cowan</surname> <given-names>WM</given-names></name>, editors. <source>Synaptic function</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1987</year>. pp. <fpage>605</fpage>–<lpage>636</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rhodes</surname> <given-names>P</given-names></name>. <article-title>The properties and implications of NMDA spikes in neocortical pyramidal cells</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>6704</fpage>–<lpage>6715</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3791-05.2006" xlink:type="simple">10.1523/JNEUROSCI.3791-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16793878</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segev</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>London</surname> <given-names>M</given-names></name>. <article-title>Untangling dendrites with quantitative models</article-title>. <source>Science</source>. <year>2000</year>;<volume>290</volume>: <fpage>744</fpage>–<lpage>750</lpage>. <object-id pub-id-type="pmid">11052930</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepherd</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Brayton</surname> <given-names>RK</given-names></name>. <article-title>Logic operations are properties of computer-simulated interactions between excitable dendritic spines</article-title>. <source>Neuroscience</source>. <year>1987</year>;<volume>21</volume>: <fpage>151</fpage>–<lpage>165</lpage>. <object-id pub-id-type="pmid">3601072</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sivyer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>SR</given-names></name>. <article-title>Direction selectivity is computed by active dendritic integration in retinal ganglion cells</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>: <fpage>1848</fpage>–<lpage>1856</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3565" xlink:type="simple">10.1038/nn.3565</ext-link></comment> <object-id pub-id-type="pmid">24162650</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>IT</given-names></name>, <name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>Dendritic spikes enhance stimulus selectivity in cortical neurons in vivo</article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>: <fpage>115</fpage>–<lpage>120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12600" xlink:type="simple">10.1038/nature12600</ext-link></comment> <object-id pub-id-type="pmid">24162850</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref057"><label>57</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Stuart</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hausser</surname> <given-names>M</given-names></name>, editors. <source>Dendrites</source>. <edition>3 edition</edition>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vaidya</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>D</given-names></name>. <article-title>Temporal synchrony and gamma-to-theta power conversion in the dendrites of CA1 pyramidal neurons</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>: <fpage>1812</fpage>–<lpage>1820</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3562" xlink:type="simple">10.1038/nn.3562</ext-link></comment> <object-id pub-id-type="pmid">24185428</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vu</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Krasne</surname> <given-names>FB</given-names></name>. <article-title>Evidence for a computational distinction between proximal and distal neuronal inhibition</article-title>. <source>Science</source>. <year>1992</year>;<volume>255</volume>: <fpage>1710</fpage>–<lpage>1712</lpage>. <object-id pub-id-type="pmid">1553559</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>The single dendritic branch as a fundamental functional unit in the nervous system</article-title>. <source>Curr Opin Neurobiol</source>. <year>2010</year>;<volume>20</volume>: <fpage>494</fpage>–<lpage>502</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2010.07.009" xlink:type="simple">10.1016/j.conb.2010.07.009</ext-link></comment> <object-id pub-id-type="pmid">20800473</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brandalise</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Carta</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Helmchen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lisman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gerber</surname> <given-names>U</given-names></name>. <article-title>Dendritic NMDA spikes are necessary for timing-dependent associative LTP in CA3 pyramidal cells</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>: <fpage>13480</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13480" xlink:type="simple">10.1038/ncomms13480</ext-link></comment> <object-id pub-id-type="pmid">27848967</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Roo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Klauser</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>D</given-names></name>. <article-title>LTP promotes a selective long-term stabilization and clustering of dendritic spines</article-title>. <source>PLoS Biol</source>. <year>2008</year>;<volume>6</volume>: <fpage>e219</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0060219" xlink:type="simple">10.1371/journal.pbio.0060219</ext-link></comment> <object-id pub-id-type="pmid">18788894</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Froemke</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>M-M</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>. <article-title>Spike-timing-dependent synaptic plasticity depends on dendritic location</article-title>. <source>Nature</source>. <year>2005</year>;<volume>434</volume>: <fpage>221</fpage>–<lpage>225</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature03366" xlink:type="simple">10.1038/nature03366</ext-link></comment> <object-id pub-id-type="pmid">15759002</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zuo</surname> <given-names>Y</given-names></name>. <article-title>Repetitive motor learning induces coordinated formation of clustered dendritic spines in vivo</article-title>. <source>Nature</source>. <year>2012</year>;<volume>483</volume>: <fpage>92</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature10844" xlink:type="simple">10.1038/nature10844</ext-link></comment> <object-id pub-id-type="pmid">22343892</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Golding</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Staff</surname> <given-names>NP</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>. <article-title>Dendritic spikes as a mechanism for cooperative long-term potentiation</article-title>. <source>Nature</source>. <year>2002</year>;<volume>418</volume>: <fpage>326</fpage>–<lpage>331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature00854" xlink:type="simple">10.1038/nature00854</ext-link></comment> <object-id pub-id-type="pmid">12124625</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gordon</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Plasticity compartments in basal dendrites of neocortical pyramidal neurons</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>12717</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3502-06.2006" xlink:type="simple">10.1523/JNEUROSCI.3502-06.2006</ext-link></comment> <object-id pub-id-type="pmid">17151275</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Govindarajan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kelleher</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Tonegawa</surname> <given-names>S</given-names></name>. <article-title>A clustered plasticity model of long-term memory engrams</article-title>. <source>Nat Rev Neurosci</source>. <year>2006</year>;<volume>7</volume>: <fpage>575</fpage>–<lpage>583</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1937" xlink:type="simple">10.1038/nrn1937</ext-link></comment> <object-id pub-id-type="pmid">16791146</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Govindarajan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Israely</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>S-Y</given-names></name>, <name name-style="western"><surname>Tonegawa</surname> <given-names>S</given-names></name>. <article-title>The dendritic branch is the preferred integrative unit for protein synthesis-dependent LTP</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>: <fpage>132</fpage>–<lpage>146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.12.008" xlink:type="simple">10.1016/j.neuron.2010.12.008</ext-link></comment> <object-id pub-id-type="pmid">21220104</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Locally dynamic synaptic learning rules in pyramidal neuron dendrites</article-title>. <source>Nature</source>. <year>2007</year>;<volume>450</volume>: <fpage>1195</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06416" xlink:type="simple">10.1038/nature06416</ext-link></comment> <object-id pub-id-type="pmid">18097401</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kastellakis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Mednick</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Silva</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Poirazi</surname> <given-names>P</given-names></name>. <article-title>Synaptic clustering within dendrites: An emerging theory of memory formation</article-title>. <source>Prog Neurobiol</source>. <year>2015</year>;<volume>126</volume>: <fpage>19</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2014.12.002" xlink:type="simple">10.1016/j.pneurobio.2014.12.002</ext-link></comment> <object-id pub-id-type="pmid">25576663</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>C-L</given-names></name>, <name name-style="western"><surname>Cembrowski</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Mensh</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>. <article-title>Dendritic sodium spikes are required for long-term potentiation at distal synapses on hippocampal pyramidal neurons</article-title>. <source>eLife</source>. <year>2015</year>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.06414" xlink:type="simple">10.7554/eLife.06414</ext-link></comment> <object-id pub-id-type="pmid">26247712</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleindienst</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Winnubst</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Roth-Alpermann</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bonhoeffer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lohmann</surname> <given-names>C</given-names></name>. <article-title>Activity-dependent clustering of functional synaptic inputs on developing hippocampal dendrites</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>: <fpage>1012</fpage>–<lpage>1024</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.10.015" xlink:type="simple">10.1016/j.neuron.2011.10.015</ext-link></comment> <object-id pub-id-type="pmid">22196336</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Nevian</surname> <given-names>T</given-names></name>. <article-title>Synaptic clustering by dendritic signalling mechanisms</article-title>. <source>Curr Opin Neurobiol</source>. <year>2008</year>;<volume>18</volume>: <fpage>321</fpage>–<lpage>331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2008.08.013" xlink:type="simple">10.1016/j.conb.2008.08.013</ext-link></comment> <object-id pub-id-type="pmid">18804167</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Losonczy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Makara</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>JC</given-names></name>. <article-title>Compartmentalized dendritic plasticity and input feature storage in neurons</article-title>. <source>Nature</source>. <year>2008</year>;<volume>452</volume>: <fpage>436</fpage>–<lpage>441</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06725" xlink:type="simple">10.1038/nature06725</ext-link></comment> <object-id pub-id-type="pmid">18368112</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Makara</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Losonczy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wen</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>JC</given-names></name>. <article-title>Experience-dependent compartmentalized dendritic plasticity in rat hippocampal CA1 pyramidal neurons</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>: <fpage>1485</fpage>–<lpage>1487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2428" xlink:type="simple">10.1038/nn.2428</ext-link></comment> <object-id pub-id-type="pmid">19898470</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Makino</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Malinow</surname> <given-names>R</given-names></name>. <article-title>Compartmentalized versus Global Synaptic Plasticity on Dendrites Controlled by Experience</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>: <fpage>1001</fpage>–<lpage>1011</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.09.036" xlink:type="simple">10.1016/j.neuron.2011.09.036</ext-link></comment> <object-id pub-id-type="pmid">22196335</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McBride</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Rodriguez-Contreras</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Trinh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bailey</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>DeBello</surname> <given-names>WM</given-names></name>. <article-title>Learning Drives Differential Clustering of Axodendritic Contacts in the Barn Owl Auditory System</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>6960</fpage>–<lpage>6973</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1352-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1352-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18596170</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oh</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Parajuli</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Zito</surname> <given-names>K</given-names></name>. <article-title>Heterosynaptic Structural Plasticity on Local Dendritic Segments of Hippocampal CA1 Neurons</article-title>. <source>Cell Rep</source>. <year>2015</year>;<volume>10</volume>: <fpage>162</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.celrep.2014.12.016" xlink:type="simple">10.1016/j.celrep.2014.12.016</ext-link></comment> <object-id pub-id-type="pmid">25558061</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Remy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>. <article-title>Dendritic spikes induce single-burst long-term potentiation</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2007</year>;<volume>104</volume>: <fpage>17192</fpage>–<lpage>17197</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0707919104" xlink:type="simple">10.1073/pnas.0707919104</ext-link></comment> <object-id pub-id-type="pmid">17940015</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sajikumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Navakkode</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>JU</given-names></name>. <article-title>Identification of Compartment- and Process-Specific Molecules Required for “Synaptic Tagging” during Long-Term Potentiation and Long-Term Depression in Hippocampal CA1</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>5068</fpage>–<lpage>5080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4940-06.2007" xlink:type="simple">10.1523/JNEUROSCI.4940-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17494693</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sandler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shulman</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>A Novel Form of Local Plasticity in Tuft Dendrites of Neocortical Somatosensory Layer 5 Pyramidal Neurons</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>: <fpage>1028</fpage>–<lpage>1042</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.04.032" xlink:type="simple">10.1016/j.neuron.2016.04.032</ext-link></comment> <object-id pub-id-type="pmid">27210551</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sheffield</surname> <given-names>MEJ</given-names></name>, <name name-style="western"><surname>Dombeck</surname> <given-names>DA</given-names></name>. <article-title>Calcium transient prevalence across the dendritic arbour predicts place field properties</article-title>. <source>Nature</source>. <year>2015</year>;<volume>517</volume>: <fpage>200</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature13871" xlink:type="simple">10.1038/nature13871</ext-link></comment> <object-id pub-id-type="pmid">25363782</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Rancz</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>. <article-title>Dendritic excitability and synaptic plasticity</article-title>. <source>Physiol Rev</source>. <year>2008</year>;<volume>88</volume>: <fpage>769</fpage>–<lpage>840</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/physrev.00016.2007" xlink:type="simple">10.1152/physrev.00016.2007</ext-link></comment> <object-id pub-id-type="pmid">18391179</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sobczyk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Activity-Dependent Plasticity of the NMDA-Receptor Fractional Ca2+ Current</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>53</volume>: <fpage>17</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.11.016" xlink:type="simple">10.1016/j.neuron.2006.11.016</ext-link></comment> <object-id pub-id-type="pmid">17196527</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kitamura</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Matsuo</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mayford</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kano</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Matsuki</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Locally Synchronized Synaptic Inputs</article-title>. <source>Science</source>. <year>2012</year>;<volume>335</volume>: <fpage>353</fpage>–<lpage>356</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1210362" xlink:type="simple">10.1126/science.1210362</ext-link></comment> <object-id pub-id-type="pmid">22267814</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weber</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Andrásfalvy</surname> <given-names>BK</given-names></name>, <name name-style="western"><surname>Polito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Magó</surname> <given-names>Á</given-names></name>, <name name-style="western"><surname>Ujfalussy</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Makara</surname> <given-names>JK</given-names></name>. <article-title>Location-dependent synaptic plasticity rules by dendritic spine cooperativity</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>: <fpage>11380</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms11380" xlink:type="simple">10.1038/ncomms11380</ext-link></comment> <object-id pub-id-type="pmid">27098773</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Aggleton</surname> <given-names>JP</given-names></name>. <article-title>Recognition memory: what are the roles of the perirhinal cortex and hippocampus?</article-title> <source>Nat Rev Neurosci</source>. <year>2001</year>;<volume>2</volume>: <fpage>51</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35049064" xlink:type="simple">10.1038/35049064</ext-link></comment> <object-id pub-id-type="pmid">11253359</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eichenbaum</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sauvage</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fortin</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Komorowski</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lipton</surname> <given-names>P</given-names></name>. <article-title>Towards a functional organization of episodic memory in the medial temporal lobe</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2012</year>;<volume>36</volume>: <fpage>1597</fpage>–<lpage>1608</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2011.07.006" xlink:type="simple">10.1016/j.neubiorev.2011.07.006</ext-link></comment> <object-id pub-id-type="pmid">21810443</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holderith</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lorincz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Katona</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rózsa</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kulik</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Release probability of hippocampal glutamatergic terminals scales with the size of the active zone</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>988</fpage>–<lpage>997</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3137" xlink:type="simple">10.1038/nn.3137</ext-link></comment> <object-id pub-id-type="pmid">22683683</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Megías</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Emri</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Freund</surname> <given-names>TF</given-names></name>, <name name-style="western"><surname>Gulyás</surname> <given-names>AI</given-names></name>. <article-title>Total number and distribution of inhibitory and excitatory synapses on hippocampal CA1 pyramidal cells</article-title>. <source>Neuroscience</source>. <year>2001</year>;<volume>102</volume>: <fpage>527</fpage>–<lpage>540</lpage>. <object-id pub-id-type="pmid">11226691</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2012</year>;<volume>13</volume>: <fpage>51</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3136" xlink:type="simple">10.1038/nrn3136</ext-link></comment> <object-id pub-id-type="pmid">22108672</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>RA</given-names></name>. <article-title>Shunting Inhibition Modulates Neuronal Gain during Synaptic Excitation</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>38</volume>: <fpage>433</fpage>–<lpage>445</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(03)00200-9" xlink:type="simple">10.1016/S0896-6273(03)00200-9</ext-link></comment> <object-id pub-id-type="pmid">12741990</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Müller</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Remy</surname> <given-names>S</given-names></name>. <article-title>Dendritic inhibition mediated by O-LM and bistratified interneurons in the hippocampus</article-title>. <source>Front Synaptic Neurosci</source>. <year>2014</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsyn.2014.00023" xlink:type="simple">10.3389/fnsyn.2014.00023</ext-link></comment> <object-id pub-id-type="pmid">25324774</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pi</surname> <given-names>H-J</given-names></name>, <name name-style="western"><surname>Hangya</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kvitsiani</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sanders</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>. <article-title>Cortical interneurons that specialize in disinhibitory control</article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>: <fpage>521</fpage>–<lpage>524</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12676" xlink:type="simple">10.1038/nature12676</ext-link></comment> <object-id pub-id-type="pmid">24097352</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouille</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Marin-Burgin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Adesnik</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Atallah</surname> <given-names>BV</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>Input normalization by global feedforward inhibition expands cortical dynamic range</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>: <fpage>1577</fpage>–<lpage>1585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2441" xlink:type="simple">10.1038/nn.2441</ext-link></comment> <object-id pub-id-type="pmid">19881502</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prescott</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Koninck</surname> <given-names>YD</given-names></name>. <article-title>Gain control of firing rate by shunting inhibition: Roles of synaptic noise and dendritic saturation</article-title>. <source>Proc Natl Acad Sci</source>. <year>2003</year>;<volume>100</volume>: <fpage>2076</fpage>–<lpage>2081</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0337591100" xlink:type="simple">10.1073/pnas.0337591100</ext-link></comment> <object-id pub-id-type="pmid">12569169</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Thier</surname> <given-names>P</given-names></name>. <article-title>Gain Modulation: A Major Computational Principle of the Central Nervous System</article-title>. <source>Neuron</source>. <year>2000</year>;<volume>27</volume>: <fpage>15</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(00)00004-0" xlink:type="simple">10.1016/S0896-6273(00)00004-0</ext-link></comment> <object-id pub-id-type="pmid">10939327</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tremblay</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rudy</surname> <given-names>B</given-names></name>. <article-title>GABAergic Interneurons in the Neocortex: From Cellular Properties to Circuits</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>91</volume>: <fpage>260</fpage>–<lpage>292</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.06.033" xlink:type="simple">10.1016/j.neuron.2016.06.033</ext-link></comment> <object-id pub-id-type="pmid">27477017</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Qiao</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zhong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>H</given-names></name>. <article-title>Control of response reliability by parvalbumin-expressing interneurons in visual cortex</article-title>. <source>Nat Commun</source>. <year>2015</year>;<volume>6</volume>: <fpage>6802</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms7802" xlink:type="simple">10.1038/ncomms7802</ext-link></comment> <object-id pub-id-type="pmid">25869033</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Polsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>J</given-names></name>. <article-title>Encoding and decoding bursts by NMDA spikes in basal dendrites of layer 5 pyramidal neurons</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2009</year>;<volume>29</volume>: <fpage>11891</fpage>–<lpage>11903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5250-08.2009" xlink:type="simple">10.1523/JNEUROSCI.5250-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19776275</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geinisman</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ganeshina</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Yoshida</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Disterhoft</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Gallagher</surname> <given-names>M</given-names></name>. <article-title>Aging, spatial learning, and total synapse number in the rat CA1 stratum radiatum</article-title>. <source>Neurobiol Aging</source>. <year>2004</year>;<volume>25</volume>: <fpage>407</fpage>–<lpage>416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neurobiolaging.2003.12.001" xlink:type="simple">10.1016/j.neurobiolaging.2003.12.001</ext-link></comment> <object-id pub-id-type="pmid">15123345</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nicholson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Trana</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Katz</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kath</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Geinisman</surname> <given-names>Y</given-names></name>. <article-title>Distance-Dependent Differences in Synapse Number and AMPA Receptor Expression in Hippocampal CA1 Pyramidal Neurons</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>50</volume>: <fpage>431</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.03.022" xlink:type="simple">10.1016/j.neuron.2006.03.022</ext-link></comment> <object-id pub-id-type="pmid">16675397</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Connor</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wittenberg</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>SS-H</given-names></name>. <article-title>Graded bidirectional synaptic plasticity is composed of switch-like unitary events</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>: <fpage>9679</fpage>–<lpage>9684</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0502332102" xlink:type="simple">10.1073/pnas.0502332102</ext-link></comment> <object-id pub-id-type="pmid">15983385</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petersen</surname> <given-names>CCH</given-names></name>, <name name-style="western"><surname>Malenka</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nicoll</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>All-or-none potentiation at CA3-CA1 synapses</article-title>. <source>Proc Natl Acad Sci</source>. <year>1998</year>;<volume>95</volume>: <fpage>4732</fpage>–<lpage>4737</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.95.8.4732" xlink:type="simple">10.1073/pnas.95.8.4732</ext-link></comment> <object-id pub-id-type="pmid">9539807</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiegert</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Pulin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gee</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Oertner</surname> <given-names>TG</given-names></name>. <article-title>The fate of hippocampal synapses depends on the sequence of plasticity-inducing events</article-title>. <source>eLife</source>. <year>2018</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.39151" xlink:type="simple">10.7554/eLife.39151</ext-link></comment> <object-id pub-id-type="pmid">30311904</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frey</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>RGM</given-names></name>. <article-title>Synaptic tagging and long-term potentiation</article-title>. <source>Nature</source>. <year>1997</year>;<volume>385</volume>: <fpage>533</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/385533a0" xlink:type="simple">10.1038/385533a0</ext-link></comment> <object-id pub-id-type="pmid">9020359</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sajikumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>JU</given-names></name>. <article-title>Late-associativity, synaptic tagging, and the role of dopamine during LTP and LTD</article-title>. <source>Neurobiol Learn Mem</source>. <year>2004</year>;<volume>82</volume>: <fpage>12</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.nlm.2004.03.003" xlink:type="simple">10.1016/j.nlm.2004.03.003</ext-link></comment> <object-id pub-id-type="pmid">15183167</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hardt</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nader</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y-T</given-names></name>. <article-title>GluA2-dependent AMPA receptor endocytosis and the decay of early and late long-term potentiation: possible mechanisms for forgetting of short- and long-term memories</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2014</year>;<fpage>369</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0141" xlink:type="simple">10.1098/rstb.2013.0141</ext-link></comment> <object-id pub-id-type="pmid">24298143</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Migues</surname> <given-names>PV</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Archbold</surname> <given-names>GEB</given-names></name>, <name name-style="western"><surname>Einarsson</surname> <given-names>EÖ</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bonasia</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Blocking Synaptic Removal of GluA2-Containing AMPA Receptors Prevents the Natural Forgetting of Long-Term Memories</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>3481</fpage>–<lpage>3494</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3333-15.2016" xlink:type="simple">10.1523/JNEUROSCI.3333-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27013677</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogt-Eisele</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Krüger</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Duning</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Spoelgen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pitzer</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>KIBRA (KIdney/BRAin protein) regulates learning and memory and stabilizes Protein kinase Mζ</article-title>. <source>J Neurochem</source>. <year>2014</year>;<volume>128</volume>: <fpage>686</fpage>–<lpage>700</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/jnc.12480" xlink:type="simple">10.1111/jnc.12480</ext-link></comment> <object-id pub-id-type="pmid">24117625</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref111"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Willshaw</surname> <given-names>DJ</given-names></name>. <article-title>Optimising synaptic learning rules in linear associative memories</article-title>. <source>Biol Cybern</source>. <year>1991</year>;<volume>65</volume>: <fpage>253</fpage>–<lpage>265</lpage>. <object-id pub-id-type="pmid">1932282</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref112"><label>112</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kanerva</surname> <given-names>P</given-names></name>. <source>Sparse Distributed Memory</source>. <publisher-loc>Cambridge, Mass. u.a</publisher-loc>.: <publisher-name>The MIT Press</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palm</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Memory Capacities for Synaptic and Structural Plasticity</article-title>. <source>Neural Comput</source>. <year>2009</year>;<volume>22</volume>: <fpage>289</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2009.08-07-588" xlink:type="simple">10.1162/neco.2009.08-07-588</ext-link></comment> <object-id pub-id-type="pmid">19925281</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palm</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Information capacity in recurrent McCulloch–Pitts networks with sparsely coded memory states</article-title>. <source>Netw Comput Neural Syst</source>. <year>1992</year>;<volume>3</volume>: <fpage>177</fpage>–<lpage>186</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006892.ref115"><label>115</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Structural Plasticity, Effectual Connectivity, and Memory in Cortex</article-title>. <source>Front Neuroanat</source>. <year>2016</year>;<volume>10</volume>: <fpage>63</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnana.2016.00063" xlink:type="simple">10.3389/fnana.2016.00063</ext-link></comment> <object-id pub-id-type="pmid">27378861</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref116"><label>116</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Cortical rewiring and information storage</article-title>. <source>Nature</source>. <year>2004</year>;<volume>431</volume>: <fpage>782</fpage>–<lpage>788</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature03012" xlink:type="simple">10.1038/nature03012</ext-link></comment> <object-id pub-id-type="pmid">15483599</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref117"><label>117</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Menon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Musial</surname> <given-names>TF</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Katz</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kath</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Spruston</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Balanced Synaptic Impact via Distance-Dependent Synapse Distribution and Complementary Expression of AMPARs and NMDARs in Hippocampal Dendrites</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>: <fpage>1451</fpage>–<lpage>1463</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.09.027" xlink:type="simple">10.1016/j.neuron.2013.09.027</ext-link></comment> <object-id pub-id-type="pmid">24360547</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref118"><label>118</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Druckmann</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Feng</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Yook</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhao</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>JC</given-names></name>, <etal>et al</etal>. <article-title>Structured Synaptic Connectivity between Hippocampal Regions</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>: <fpage>629</fpage>–<lpage>640</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.11.026" xlink:type="simple">10.1016/j.neuron.2013.11.026</ext-link></comment> <object-id pub-id-type="pmid">24412418</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref119"><label>119</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Winnubst</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cheyne</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Niculescu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lohmann</surname> <given-names>C</given-names></name>. <article-title>Spontaneous Activity Drives Local Synaptic Plasticity In Vivo</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>87</volume>: <fpage>399</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.06.029" xlink:type="simple">10.1016/j.neuron.2015.06.029</ext-link></comment> <object-id pub-id-type="pmid">26182421</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref120"><label>120</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>RGM</given-names></name>. <article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title>. <source>Trends Neurosci</source>. <year>1987</year>;<volume>10</volume>: <fpage>408</fpage>–<lpage>415</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0166-2236(87)90011-7" xlink:type="simple">10.1016/0166-2236(87)90011-7</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006892.ref121"><label>121</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neher</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wiskott</surname> <given-names>L</given-names></name>. <article-title>Memory Storage Fidelity in the Hippocampal Circuit: The Role of Subregions and Input Statistics</article-title>. <source>PLOS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>e1004250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004250" xlink:type="simple">10.1371/journal.pcbi.1004250</ext-link></comment> <object-id pub-id-type="pmid">25954996</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref122"><label>122</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rennó-Costa</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lisman</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Verschure</surname> <given-names>PFMJ</given-names></name>. <article-title>A Signature of Attractor Dynamics in the CA3 Region of the Hippocampus</article-title>. <source>PLOS Comput Biol</source>. <year>2014</year>;<volume>10</volume>: <fpage>e1003641</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003641" xlink:type="simple">10.1371/journal.pcbi.1003641</ext-link></comment> <object-id pub-id-type="pmid">24854425</object-id></mixed-citation></ref>
<ref id="pcbi.1006892.ref123"><label>123</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>. <article-title>An attractor network in the hippocampus: Theory and neurophysiology</article-title>. <source>Learn Mem</source>. <year>2007</year>;<volume>14</volume>: <fpage>714</fpage>–<lpage>731</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/lm.631207" xlink:type="simple">10.1101/lm.631207</ext-link></comment> <object-id pub-id-type="pmid">18007016</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>