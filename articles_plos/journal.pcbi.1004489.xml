<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00451</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004489</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural Networks</article-title>
<alt-title alt-title-type="running-head">Reinforcement Learning of Linking and Tracing Contours in RNNs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Brosch</surname> <given-names>Tobias</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Neumann</surname> <given-names>Heiko</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Roelfsema</surname> <given-names>Pieter R.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>University of Ulm, Institute of Neural Information Processing, Ulm, Germany</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Vision &amp; Cognition, Netherlands Institute for Neuroscience (KNAW), Amsterdam, The Netherlands</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Integrative Neurophysiology, Center for Neurogenomics and Cognitive Research, VU University, Amsterdam, The Netherlands</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Psychiatry Department, Academic Medical Center, Amsterdam, The Netherlands</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: TB HN PRR. Performed the experiments: TB. Analyzed the data: TB HN PRR. Contributed reagents/materials/analysis tools: TB HN PRR. Wrote the paper: TB HN PRR.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">heiko.neumann@uni-ulm.de</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>23</day>
<month>10</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>10</issue>
<elocation-id>e1004489</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>3</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>8</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Brosch et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004489" xlink:type="simple"/>
<abstract>
<p>The processing of a visual stimulus can be subdivided into a number of stages. Upon stimulus presentation there is an early phase of feedforward processing where the visual information is propagated from lower to higher visual areas for the extraction of basic and complex stimulus features. This is followed by a later phase where horizontal connections within areas and feedback connections from higher areas back to lower areas come into play. In this later phase, image elements that are behaviorally relevant are grouped by Gestalt grouping rules and are labeled in the cortex with enhanced neuronal activity (object-based attention in psychology). Recent neurophysiological studies revealed that reward-based learning influences these recurrent grouping processes, but it is not well understood how rewards train recurrent circuits for perceptual organization. This paper examines the mechanisms for reward-based learning of new grouping rules. We derive a learning rule that can explain how rewards influence the information flow through feedforward, horizontal and feedback connections. We illustrate the efficiency with two tasks that have been used to study the neuronal correlates of perceptual organization in early visual cortex. The first task is called contour-integration and demands the integration of collinear contour elements into an elongated curve. We show how reward-based learning causes an enhancement of the representation of the to-be-grouped elements at early levels of a recurrent neural network, just as is observed in the visual cortex of monkeys. The second task is curve-tracing where the aim is to determine the endpoint of an elongated curve composed of connected image elements. If trained with the new learning rule, neural networks learn to propagate enhanced activity over the curve, in accordance with neurophysiological data. We close the paper with a number of model predictions that can be tested in future neurophysiological and computational studies.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Our experience with the visual world allows us to group image elements that belong to the same perceptual object and to segregate them from other objects and the background. If subjects learn to group contour elements, this experience influences neuronal activity in early visual cortical areas, including the primary visual cortex (V1). Learning presumably depends on alterations in the pattern of connections within and between areas of the visual cortex. However, the processes that control changes in connectivity are not well understood. Here we present the first computational model that can train a neural network to integrate collinear contour elements into elongated curves and to trace a curve through the visual field. The new learning algorithm trains fully recurrent neural networks, provided the connectivity causes the networks to reach a stable state. The model reproduces the behavioral performance of monkeys trained in these tasks and explains the patterns of neuronal activity in the visual cortex that emerge during learning, which is remarkable because the only feedback for the model is a reward for successful trials. We discuss a number of the model predictions that can be tested in future neuroscientific work.</p>
</abstract>
<funding-group>
<funding-statement>PRR is supported by grants from the Netherlands Organization for Scientific Research (ALW grant no. 823.02.010, Natural Artificial Intelligence project 656.000.002, and grant no. 433-09-208 from the Excellence Program Brain &amp; Cognition) and from the EU (ERC Grant Agreement no. 339490 and Marie-Curie Action PITN-GA-2011-290011). The work has been supported by the Transregional Collaborative Research Center “A Companion Technology for Cognitive Technical Systems” (SFB/TR-62) funded by the German Research Foundation (DFG). TB is further supported by the graduate school MAEIC at Ulm University. Part of the research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (Human Brain Project). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="0"/>
<page-count count="36"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Introspectively, visual perception appears to be remarkable effortless and automatic. Our perceptual world is filled with familiar objects and we do not experience many difficulties in judging where an object ends and the next one begins. The quality of image segmentation by the human brain surpasses segmentation in computer vision, which is known to be a hard problem [<xref ref-type="bibr" rid="pcbi.1004489.ref001">1</xref>], yet the precise mechanisms responsible for image segmentation in humans are only partially understood. In the present study we aim to explore the mechanisms that allow a neural network to learn to segment task-relevant image elements from a background of irrelevant elements. Image processing in humans and non-human primates can be subdivided in at least two phases. Research suggests that for many tasks the first phase is dominated by feedforward processing [<xref ref-type="bibr" rid="pcbi.1004489.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref005">5</xref>] (see [<xref ref-type="bibr" rid="pcbi.1004489.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref007">7</xref>] for evidence for recurrent interactions in early vision). When a new image is presented to the visual system, information is rapidly propagated from early to higher visual areas. In this phase, the visual system extracts many elementary features such as colors, local orientations, contrasts, motion directions in low level areas and more complex features such as shape properties (curvature, corners) in higher areas [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. This early processing phase thereby produces a pattern of activity across the various areas of the visual cortex that has been called “base representation” [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. This early representation even includes certain object-categories such as animals or vehicles, which are detected very soon after the image has been presented [<xref ref-type="bibr" rid="pcbi.1004489.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref010">10</xref>]. However, there are also many aspects of visual processing that require a more elaborate analysis than can be achieved during the first feedforward processing phase [<xref ref-type="bibr" rid="pcbi.1004489.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref011">11</xref>]. Some tasks, such as image segmentation and contour grouping, demand the evaluation of the relations between items (spatial as well as temporal). Perceptual grouping and segmentation processes depend on a later serial processing phase where lateral connections between neurons in the same area and feedback connections that propagate information from higher areas back to lower areas come into play [<xref ref-type="bibr" rid="pcbi.1004489.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref013">13</xref>]. In this second processing phase, the activity of neurons in lower areas of the visual cortex is not only determined by the visual information in the neurons’ receptive field itself, but there is an additional influence of the context given by the activity of other neurons in the same area as well as representations in higher visual areas that provide feedback [<xref ref-type="bibr" rid="pcbi.1004489.ref011">11</xref>]. An example task where lateral and feedback connections play a role is contour grouping. For example, if monkeys are trained to detect a string of collinear contour elements [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>], the neuronal responses in the primary visual cortex elicited by these elements are stronger than the responses elicited by line elements that are not part of such a perceptual group (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1A</xref>; c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref016">16</xref>]). Interestingly, this contextual effect even occurs if the information in the V1 receptive field is held constant, which implies that it depends on the lateral influences of V1 neurons with different receptive fields and/or on feedback from higher visual areas where receptive fields are larger [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. In accordance with this view, the effect of grouping on V1 activity does not occur during the initial visual response but at an additional delay (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1A</xref>).</p>
<fig id="pcbi.1004489.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Neuronal correlates of contour integration and curve tracing in primary visual cortex (area V1).</title>
<p>A) Contour integration task. If monkeys have been trained to make a saccade to a pattern with a string of collinear contour elements (1, 3, 5, 7 or 9 collinear bars, left panel), the neuronal responses in V1 elicited by these elements are stronger than the responses elicited by a single line element that is not part of such a perceptual group (right panel). This influence of colinearity on V1 activity is not present before training. The purple circle in the upper panel illustrates the V1 receptive field. Re-drawn from [<xref ref-type="bibr" rid="pcbi.1004489.ref015">15</xref>]. <bold>B)</bold> Curve-tracing task. Monkeys were trained to mentally trace a target curve (T) that is connected to a fixation point (FP) because they had to make an eye movement to a larger red circle at the end of this curve. They had to ignore a distractor curve (D). After training in this task, V1 activity elicited by the target curve (red response in lower panel) became stronger than that elicited by the distractor (blue response). The green circle in the upper panel shows the V1 receptive field. Adapted from [<xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>].</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g001"/>
</fig>
<p>Another contour grouping task that is thought to rely on feedback and lateral connectivity is curve-tracing (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1B</xref>) [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref019">19</xref>]. In one version of this task, monkeys had to determine the endpoint of a target curve that started at the fixation point (“FP” in <xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1B</xref>) as target for an eye movement, while ignoring another, distractor curve that was not connected to the fixation point. Also in this task, the contour elements that belong to the target curve were labeled in the visual cortex with enhanced spiking activity, and again, only after a delay relative to the initial visual response. The labeling of image elements with enhanced neuronal activity for grouping them into a coherent representation has been called <italic>incremental grouping</italic> [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. The delayed response modulation reflects the monkeys’ belief about the stimulus [<xref ref-type="bibr" rid="pcbi.1004489.ref020">20</xref>]. It is much weaker if the monkey fails to perceive the perceptual group [<xref ref-type="bibr" rid="pcbi.1004489.ref015">15</xref>]. Furthermore, if the monkey groups the wrong set of image elements in his perception, this erroneous set of contour elements is labeled with enhanced activity [<xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref021">21</xref>].</p>
<p>A number of previous modeling studies have investigated how feedforward, lateral and feedback connections determine the response of visual cortical neurons in these contour grouping tasks [<xref ref-type="bibr" rid="pcbi.1004489.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref025">25</xref>]. A key question that remains to be addressed, however, is how these grouping operations are learned during perceptual experience, because visual experience improves the detection and integration of image features [<xref ref-type="bibr" rid="pcbi.1004489.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref031">31</xref>]. That visual experience aids in image segmentation and perceptual grouping also follows from the fact that these processes are more efficient when objects are presented in their familiar orientations than when they are shown upside down [<xref ref-type="bibr" rid="pcbi.1004489.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref033">33</xref>]. Perceptual learning in the contour integration task (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1A</xref>) has been well documented in previous work. In this task, two weeks of experience greatly improve the accuracy of monkeys in detecting collinear image elements. Importantly, this training also increases the strength of the neuronal response modulation in V1 [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref016">16</xref>]. During learning, the only feedback that the monkeys receive about their performance is a reward when they correctly detect the string of collinear contour elements and the omission of a reward when they fail to make the appropriate response. However, the neuronal mechanisms that underlie these reward-based perceptual grouping improvements are not well understood. How does the visual brain change connections between the appropriate neurons when feedback about performance is so limited?</p>
<p>In the present study we build upon previous models called AGREL (Attention-Gated Reinforcement Learning) and AuGMEnT (Attention-Gated MEmory Tagging) that have been proposed for the learning of feedforward connections from lower to higher areas in various cognitive tasks [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref036">36</xref>] and extend it to recurrent neural networks. These previous learning models proposed a three-stage mechanism for the adaptation of synaptic weights. In a first step, feedforward processing determines a winning unit in the output layer of the network that encodes the chosen action. In a second step, an attentional feedback (AFB) signal originating from the winning unit assigns credit to those connections that were responsible for the chosen action by creating synaptic tags. In the third step, a global learning signal determines the changes of the weights of those synapses that carry the plasticity tag. This global learning signal presumably corresponds to a neuromodulator such as dopamine, serotonin or acetylcholine that encodes the reward-prediction error, i.e. the difference between the amount of reward that was expected and the amount of reward that was actually received by the network [<xref ref-type="bibr" rid="pcbi.1004489.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref039">39</xref>]. These previous models only used attentional feedback signals originating from the output layer (step two) to highlight task relevant synapses, but did not use feedback or lateral connections for the labeling of image elements that belong to the same perceptual group with an enhancement of neuronal firing rates (as in <xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1</xref>). Thus the mechanisms that permit reward-based learning in perceptual grouping tasks that rely on activity propagation through feedback and lateral connections have remained relatively unexplored.</p>
<p>To address learning in tasks with recurrent networks that utilize feedforward, lateral and feedback connections we will here propose a new learning scheme called RELEARNN (REinforcement LEArning in Recurrent Neural Networks) where the activity of the network depends in a complex manner on the visual input pattern, as recurrent connections allow the recirculation of activity. We will illustrate the capabilities of the new learning algorithm with the two contour grouping tasks of <xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1</xref>, namely contour linking and curve tracing [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref040">40</xref>]. We tested if RELEARNN can train the same neural network layout (and with the same parameters) to perform either task if the only feedback is a reward for correct performance and the omission of a reward in case of an error. We also investigated whether these networks develop coding strategies that resemble those in the visual cortex, where image elements of the same perceptual group are labeled with enhanced neuronal activity, and if and how these groups can be read out by higher areas to support the selection of the appropriate behavioral response. Our results demonstrate that (1) the behavior of the neural networks during learning and the pattern of errors is similar to that of monkeys that are trained in these tasks [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref040">40</xref>], (2) the model reproduces the changes in neuronal firing rates and the emergence of incremental grouping during the learning process, (3) that the labeling with enhanced neuronal activity for grouping is an efficient code that can be used to guide behavior, and (4) that RELEARNN is a comprehensive and powerful learning scheme, which captures fundamental aspects of synaptic plasticity in a recurrent neural network. The model may thereby help to understand how neuromodulatory systems that code reward-prediction errors enable the learning of complex perceptual tasks that require the interactions between many units through feedforward, lateral and feedback connections. RELEARNN may represent the first <italic>recurrent</italic> neural network learning scheme that can explain the learning of both contour linking and curve tracing with a biologically plausible learning scheme.</p>
<p>In the following, we will first describe the proposed learning algorithm and the architecture of the neural network model that can be trained to perform the contour linking and curve tracing tasks. We will then present the simulations and compare them to the behavior and neuronal activity in monkeys trained on the same tasks, and will close with a comparison between RELEARNN and previous learning models.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Models</title>
<p>We devised a novel learning algorithm to train recurrent networks by trial and error. In this section, we start by a description of the network model and its units. We will then describe the novel learning algorithm called “REinforcement LEArning in Recurrent Neural Networks” (RELEARNN).</p>
<sec id="sec003">
<title>Model Units and Network Structure</title>
<p>The aim of the model is to compute the value of actions when it is presented with a visual stimulus. The model contains a number of output units (<xref ref-type="fig" rid="pcbi.1004489.g002">Fig 2</xref>), and it aims to approximate the value of each of the possible actions. These action values (known as <italic>Q</italic>-values [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>]) are coded by the activity of the output units. The model usually chooses the action with the highest <italic>Q</italic>, but it will occasionally also explore other actions to promote learning. To find an appropriate balance between biological detail and mathematical tractability we chose model units with a scalar activation value, but to not include spiking neurons in our model. The model units represent the average activity in a cortical column with mean membrane potential <italic>p</italic> and mean firing rate <italic>g</italic>(<italic>p</italic>). As inputs, the model units receive excitation, inhibition as well as modulatory influences and the units, in turn, can inhibit, excite or modulate other model units. The role of the modulatory connections is to amplify the influence of excitatory connections, but these modulatory connections are unable to drive the units themselves (c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref042">42</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref046">46</xref>]). Note that modulatory connections have an effect that is synapse specific. Their role differs from the more global neuromodulatory signals like dopamine, acetylcholine or serotonine that are globally available in the network and gate plasticity. The model units used here have proven to be versatile building blocks of neural networks in previous studies (e.g. [<xref ref-type="bibr" rid="pcbi.1004489.ref047">47</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref056">56</xref>]). The membrane potential <italic>p</italic> depends on the excitatory, inhibitory and modulatory inputs <italic>I</italic><sup><italic>ex</italic></sup>, <italic>I</italic><sup><italic>inh</italic></sup> and <italic>I</italic><sup><italic>mod</italic></sup> as follows (<xref ref-type="fig" rid="pcbi.1004489.g002">Fig 2</xref>, right):
<disp-formula id="pcbi.1004489.e001"><alternatives><graphic id="pcbi.1004489.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mi>p</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mi>I</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
The decay rate of the potential of model units is controlled by <italic>α</italic> &gt; 0, the maximal potential by <italic>β</italic> &gt; 0, the minimal potential by <italic>ζ</italic> &gt; 0, and the parameter <italic>γ</italic> &gt; 0 determines the impact of modulatory input. The mean spike rate <italic>r</italic> depends on the potential and is calculated as
<disp-formula id="pcbi.1004489.e002"><alternatives><graphic id="pcbi.1004489.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>·</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>/</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
with <italic>a</italic> = 0.001.</p>
<fig id="pcbi.1004489.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g002</object-id>
<label>Fig 2</label>
<caption>
<p><bold>Left</bold>: Illustration of the network structure in its most general form, as used in the description of the learning algorithm. Every model unit <italic>n</italic><sub><italic>i</italic></sub> can excite, inhibit or modulate the activity of any other unit <italic>n</italic><sub><italic>j</italic></sub> (indicated by non-directional black connections). Units <inline-formula id="pcbi.1004489.e003"><alternatives><graphic id="pcbi.1004489.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e003"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mi>I</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of the input layer provide input to the network but their activity does not depend on the activity of other units in the network. <bold>Right</bold>: A model unit (corresponding to a cortical column) can be excited, inhibited or modulated by other cortical columns.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g002"/>
</fig>
<p>Because the membrane potential is bounded by <italic>β</italic> (and −<italic>ζ</italic>), and because the network’s aim is to compute the action values as the activity <italic>p</italic> of the output units, we chose the reward of correctly performed trials below <italic>β</italic> and gave no reward if the model made an erroneous response.</p>
<p>We will now derive the learning algorithm, considering a network of <italic>N</italic> dynamically interacting model units with activities <italic>p</italic><sub><italic>i</italic></sub> receiving excitatory input <bold><italic>I</italic></bold><sup><italic>inp</italic></sup> (see <xref ref-type="fig" rid="pcbi.1004489.g002">Fig 2</xref> for the general structure of such a network, which may or may not be fully connected). Once the input has been given, the activity circulates through the excitatory, inhibitory and modulatory connections until the network activity stabilizes (convergence to a stable state is required for the current version of the learning algorithm). The overall dynamics are described by the following system of coupled differential equations (same as <xref ref-type="disp-formula" rid="pcbi.1004489.e001">Eq (1)</xref>, but now presented in vector notation)
<disp-formula id="pcbi.1004489.e004"><alternatives><graphic id="pcbi.1004489.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
The excitatory, inhibitory and modulatory inputs <bold><italic>I</italic></bold><sup><italic>ex</italic></sup>,<bold><italic>I</italic></bold><sup><italic>inh</italic></sup> and <bold><italic>I</italic></bold><sup><italic>mod</italic></sup> depend on the presynaptic firing rates and the input into the network <bold><italic>I</italic></bold><sup><italic>inp</italic></sup> <disp-formula id="pcbi.1004489.e005"><alternatives><graphic id="pcbi.1004489.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo> <mml:mspace width="2.em"/><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi> <mml:mo>/</mml:mo> <mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi> <mml:mo>/</mml:mo> <mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
Here, <italic>α</italic>, <italic>β</italic>, <italic>γ</italic>, <italic>ζ</italic> ≥ 0 and <italic>g</italic> (applied element-wise) are defined as in <xref ref-type="disp-formula" rid="pcbi.1004489.e001">Eq (1)</xref> and <bold><italic>p</italic></bold>, <bold><italic>I</italic></bold><sup><italic>inp</italic></sup> ∈ ℝ<sup><italic>N</italic></sup> are column vectors of the activations and inputs of each unit. The positive elements <inline-formula id="pcbi.1004489.e006"><alternatives><graphic id="pcbi.1004489.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mo>⋅</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> of the weight matrices <bold><italic>W</italic></bold><sup>(⋅)</sup> ∈ ℝ<sup><italic>N</italic> × <italic>N</italic></sup> determine the connection strength from unit <italic>k</italic> to unit <italic>l</italic> and <inline-formula id="pcbi.1004489.e007"><alternatives><graphic id="pcbi.1004489.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e007"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> determines the excitatory connection strength from feature <italic>k</italic> to input unit <italic>l</italic> (here products of two column vectors like <bold><italic>p</italic></bold> ⋅ <bold><italic>I</italic></bold> ∈ ℝ<sup><italic>N</italic></sup> are defined element-wise). When the activity in the network has converged to a stable state (usually within a few hundred iterations), the network chooses one action based on the activation of the output units that encode the action (<italic>Q</italic>) values. We used the softmax rule to determine the probability <italic>φ</italic><sub><italic>a</italic></sub> of an output unit <italic>a</italic> to win the competition between all possible actions based on their values:
<disp-formula id="pcbi.1004489.e008"><alternatives><graphic id="pcbi.1004489.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>φ</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">O</mml:mi></mml:mrow> <mml:mrow/></mml:msubsup> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <inline-formula id="pcbi.1004489.e009"><alternatives><graphic id="pcbi.1004489.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e009"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the set of all output units and <italic>τ</italic> is called a temperature parameter [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>], which was adjusted to fit the experimental data (c.f. Suppl. A in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref>; adjusted by grid search). We here did not model how the softmax action selection process is implemented in the neural network, although this has been adressed in previous work [<xref ref-type="bibr" rid="pcbi.1004489.ref057">57</xref>]. Moreover, the choice of softmax as an action selection rule is not critical. We expect that other action selection mechanisms used in the reinforcement literature (e.g. <italic>ɛ</italic>-greedy [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>] or max-Bolzman [<xref ref-type="bibr" rid="pcbi.1004489.ref058">58</xref>]) will give qualitatively similar results.</p>
</sec>
<sec id="sec004">
<title>Reinforcement Learning Algorithm for Recurrent Neural Networks (RELEARNN)</title>
<p>Although research in past years has increased our knowledge about the mechanisms for the long-term modification of synaptic strength (e.g. [<xref ref-type="bibr" rid="pcbi.1004489.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref060">60</xref>]), we will here focus on rather simple rules for synaptic modification. It is convenient to subdivide the mechanisms that lead to synaptic plasticity into a number of phases (c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref061">61</xref>]). Here we will distinguish between three phases. Phase one starts in response to the input and ends when the network converges to a stable state <bold><italic>p</italic></bold><sup>∞</sup> and stochastically selects action <italic>a</italic> according to <xref ref-type="disp-formula" rid="pcbi.1004489.e008">Eq (5)</xref>. In the second phase, the selected output unit <italic>a</italic> causes an attentional feedback signal (AFB) that propagates through the network through a separate set of units (one per column; small circles in <xref ref-type="fig" rid="pcbi.1004489.g003">Fig 3</xref>) that change their response by Δ<bold><italic>p</italic></bold> during this phase so that their total activity becomes <bold><italic>p</italic></bold><sup>∞</sup> + Δ<bold><italic>p</italic></bold>. We call the network of units sensitive to the AFB the “accessory network” (see below for details), which is important for the guidance of synaptic plasticity.</p>
<fig id="pcbi.1004489.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Illustration of the learning phases.</title>
<p>Each regular unit (large circles) is accompanied by an accessory unit (small circles), which are hypothesized to be situated in the same cortical column. In phase 1, the sensory input leads to a stable state <bold><italic>p</italic></bold><sup>∞</sup> of the regular units (note that we only illustrated the excitatory connections in this scheme) and the model represents estimates of the value of all the actions in the output layer. In phase 2, the winning output unit injects extra activity into the accessory network. The strength of the connections of the accessory network is reciprocal (equally strong) to that of the regular network. Accessory units that are paired with a regular unit that has a strong impact on the activity of the winning unit exhibit a strong increase in activity Δ<bold><italic>p</italic></bold> during this phase. In phase 3 the changes in synaptic strength depend on Δ<bold><italic>p</italic></bold> and a neuromodulatory signal that encodes the reward-prediction error <italic>δ</italic> (green cloud in phase 3; right panel).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g003"/>
</fig>
<p>Although many neurophysiological studies on the neuronal correlates of action selection targeted frontal and parietal cortex [<xref ref-type="bibr" rid="pcbi.1004489.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref063">63</xref>], more recent studies reported that some neurons in the visual cortex are indeed also influenced by action selection, which is in accordance with the propagation of selection signals through an accessory network [<xref ref-type="bibr" rid="pcbi.1004489.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref065">65</xref>]. More generally, there is evidence for a ubiquitous bi-directional “counter stream” interaction between higher and lower cortical regions [<xref ref-type="bibr" rid="pcbi.1004489.ref066">66</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref069">69</xref>] also justified by theoretical considerations (e.g. the selective tuning model for visual attention [<xref ref-type="bibr" rid="pcbi.1004489.ref070">70</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref072">72</xref>]). Furthermore, there is anatomical data to support a dichotomy of cortico-cortical connections. Studies from the lab of Sherman on the connectivity between areas of mouse cortex distinguished between class I and class II connections [<xref ref-type="bibr" rid="pcbi.1004489.ref073">73</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref074">74</xref>]. Class I connections were called “drivers” because they are thought to activate neurons. They project from lower to higher areas but also in the opposite direction, from higher areas back to lower areas. Class II connections also project both in the feedforward and feedback direction, but they differ from class I connections because they are weaker and utilize the metabotropic glutamate receptor, involved in synaptic plasticity. It is therefore conceivable that the accessory network might employ class II connections, while the regular network employs class I connections, although this mapping is presently still speculative.</p>
<p>We will assume that the strength of the connections between units in the accessory network is similar (or proportional) to the strength of connections between the regular units (larger circles in <xref ref-type="fig" rid="pcbi.1004489.g003">Fig 3</xref>). This reciprocitity of regular and accessory connections is not a strong assumption because it can also be learned [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>]. As we will demonstrate below, the boost in the membrane potential Δ<italic>p</italic><sub><italic>l</italic></sub> of the accessory unit <italic>l</italic> during the second phase is proportional to the influence of a change in <italic>p</italic><sub><italic>l</italic></sub> on the activity of the current winning unit <inline-formula id="pcbi.1004489.e010"><alternatives><graphic id="pcbi.1004489.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> during the first phase. Therefore, the AFB can be used to assign credit to those units that had an impact on the decision to take action <italic>a</italic>. Specifically, we will show that if an increase in the activity of unit <italic>n</italic><sub><italic>l</italic></sub> would increase the activity of the winning unit <inline-formula id="pcbi.1004489.e011"><alternatives><graphic id="pcbi.1004489.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> during the first phase, then Δ<italic>p</italic><sub><italic>l</italic></sub> &gt; 0 during the second phase. If, on the other hand, unit <italic>n</italic><sub><italic>i</italic></sub> decreases <inline-formula id="pcbi.1004489.e012"><alternatives><graphic id="pcbi.1004489.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e012"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, this implies that Δ<italic>p</italic><sub><italic>l</italic></sub> &lt; 0. The sign and magnitude of Δ<italic>p</italic><sub><italic>l</italic></sub> can be used to guide plasticity of synapses onto unit <italic>l</italic> if the aim is to decrease or increase <inline-formula id="pcbi.1004489.e013"><alternatives><graphic id="pcbi.1004489.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, i.e. to adjust the value of this action. Finally, in the third phase (c.f. <xref ref-type="fig" rid="pcbi.1004489.g003">Fig 3</xref>), the network receives a reward if it selects the correct action, but reward is omitted otherwise.</p>
<p>The output units of the network aim to represent the expected value if their action is chosen in the current sensory state (c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref075">75</xref>]). Neurons in the frontal cortex, basal ganglia and midbrain are known to code for action values [<xref ref-type="bibr" rid="pcbi.1004489.ref076">76</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref077">77</xref>], i.e. their activity appears to approximate the so-called <italic>Q</italic>-value, defined as the expected return <italic>ϱ</italic> when choosing action <italic>a</italic> in state <italic>s</italic> (c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>]; note that rewards are often delayed; see discussion for extensions that deal with delayed rewards)
<disp-formula id="pcbi.1004489.e014"><alternatives><graphic id="pcbi.1004489.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>Q</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>π</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>ϱ</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
When the network performs action <italic>a</italic>, it receives a reward <italic>ϱ</italic> and the aim of the learning rule is to adjust the current estimate of <italic>Q</italic><sub><italic>a</italic></sub>, represented by the activity of the winning output unit <inline-formula id="pcbi.1004489.e015"><alternatives><graphic id="pcbi.1004489.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. To this aim, the network computes a reward prediction error <italic>δ</italic> by comparing the outcome of the trial <italic>ϱ</italic> to the predicted <italic>Q</italic>-value, i.e. a SARSA style prediction error for immediately rewarded tasks [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>]. In accordance with previous studies of reinforcement learning [<xref ref-type="bibr" rid="pcbi.1004489.ref078">78</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref079">79</xref>], we assume that this reward prediction error is coded by a neuromodulatory signal that is globally released into the network so that it can influence the plasticity of all synapses (<xref ref-type="fig" rid="pcbi.1004489.g003">Fig 3</xref>, right panel).
<disp-formula id="pcbi.1004489.e016"><alternatives><graphic id="pcbi.1004489.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>δ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>ϱ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>ϱ</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Many dopamine neurons in the ventral tegmental area and substantia nigra encode the reward prediction error <italic>δ</italic> [<xref ref-type="bibr" rid="pcbi.1004489.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref080">80</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref081">81</xref>].</p>
<p>Once the network has received feedback about the chosen action <italic>a</italic>, the learning rule changes the connections of the network in order to decrease the reward prediction error for this action. Specifically, plasticity of the connection <italic>w</italic><sub><italic>kl</italic></sub> from unit <italic>k</italic> to unit <italic>l</italic> depends on four factors: (1) the presynaptic activity <inline-formula id="pcbi.1004489.e017"><alternatives><graphic id="pcbi.1004489.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi> <mml:mi>k</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, (2) the postsynaptic membrane potential <inline-formula id="pcbi.1004489.e018"><alternatives><graphic id="pcbi.1004489.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, (3) the activity of the accessory unit <italic>l</italic> <inline-formula id="pcbi.1004489.e019"><alternatives><graphic id="pcbi.1004489.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, which represents the influence of unit <italic>l</italic> on the activity of <italic>a</italic> and (4) the reward prediction error <italic>δ</italic>:
<disp-formula id="pcbi.1004489.e020"><alternatives><graphic id="pcbi.1004489.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mo>·</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi>r</mml:mi> <mml:mi>k</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>η</italic> is the learning rate. Note that the signals that determine plasticity are all available locally in the cortical column <italic>l</italic> and that <xref ref-type="disp-formula" rid="pcbi.1004489.e020">Eq (8)</xref> implements a form of Hebbian plasticity, because it depends on the product of presynaptic activity <inline-formula id="pcbi.1004489.e021"><alternatives><graphic id="pcbi.1004489.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi> <mml:mi>k</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and a function <italic>f</italic>(⋅) of the postsynaptic activity <inline-formula id="pcbi.1004489.e022"><alternatives><graphic id="pcbi.1004489.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e022"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. We will derive below that the form of <italic>f</italic>(⋅) differs between excitatory, inhibitory and modulatory connections projecting to column <italic>l</italic>:
<disp-formula id="pcbi.1004489.e023"><alternatives><graphic id="pcbi.1004489.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e023"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi> <mml:mi>l</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>∞</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1004489.e024"><alternatives><graphic id="pcbi.1004489.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi> <mml:mi>l</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>∞</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>l</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula> <disp-formula id="pcbi.1004489.e025"><alternatives><graphic id="pcbi.1004489.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi> <mml:mi>l</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
These equations follow from Eqs (<xref ref-type="disp-formula" rid="pcbi.1004489.e001">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004489.e028">3</xref>) and can be obtained by differentiating with respect to the corresponding input type (they represent the effective impact of the connection on the post-synaptic unit <italic>l</italic>).</p>
</sec>
<sec id="sec005">
<title>Credit Assignment by the Accessory Network</title>
<p>The aim of the accessory network is to determine <inline-formula id="pcbi.1004489.e026"><alternatives><graphic id="pcbi.1004489.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e026"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> for every unit <italic>l</italic>, which estimates the influence of <italic>l</italic> on the value of the selected action <italic>a</italic>. In the sequel we will capitalize on results of Almeida and Pineda [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref083">83</xref>] who proposed a method to compute the error gradient in weight space for fully recurrent networks. It is based on the error-backpropagation rule, which computes the required changes in synaptic weights but it is thought to be biologically implausible [<xref ref-type="bibr" rid="pcbi.1004489.ref084">84</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref085">85</xref>]. However, previous work on the AGREL [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>] and AuGMEnT [<xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>] learning rules demonstrated that in case of a reinforcement learning problem it is possible to replace the backpropagation of errors by two factors that are biologically plausible (reviewed by [<xref ref-type="bibr" rid="pcbi.1004489.ref086">86</xref>]): (1) an attentional feedback signal, which propagates activity rather than error signals from the output units to earlier processing levels, and (2) a globally released neuromodulatory signal that codes for the reward prediction error. We will here provide an equivalent result for the learning rule proposed by [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref083">83</xref>] for fully recurrent networks.</p>
<p>Almeida [<xref ref-type="bibr" rid="pcbi.1004489.ref083">83</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref088">88</xref>] showed that once a fully recurrent network has settled in a stable state <bold><italic>p</italic></bold><sup>∞</sup>, the error gradient of all synapses can be computed by an accessory network if it is the transpose of the linearized original network. Thus, in the accessory network of [<xref ref-type="bibr" rid="pcbi.1004489.ref083">83</xref>] connections <italic>W</italic><sub><italic>kl</italic></sub> are replaced by connections <inline-formula id="pcbi.1004489.e027"><alternatives><graphic id="pcbi.1004489.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e027"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of equal strength but running in the opposite direction. His method injects the error signal in every output unit, i.e. a positive signal for output units that are not active enough and a negative signal for output units that are too active. The accessory network of Almeida then propagates the activity to units connected to the output units, and from there, successively to all other units in the network. In RELEARNN only the winning output unit <italic>a</italic> injects activity into the accessory network and this activity circulates until the accessory units reach a stable state.</p>
<sec id="sec006">
<title>Derivative of weight impact</title>
<p>We will now describe the application of the Almeida-Pineda algorithm [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>] to our network model. Our aim is to demonstrate that the activity of an accessory unit is proportional to the influence of the respective regular unit on the action value. We will first compute the influence of any synaptic weight <italic>W</italic><sub><italic>kl</italic></sub> on the unit coding the <italic>Q</italic>-value of the selected action <italic>a</italic>. To simplify the derivation, we will examine the propagation of signals in the accessory network under the assumption that the network contains only excitatory connections (for a full derivation using matrix calculus [<xref ref-type="bibr" rid="pcbi.1004489.ref087">89</xref>] including inhibitory and modulatory connections required to reproduce experimental findings see Suppl. C in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref>). In the episode when the regular network reaches its steady state, the change in the membrane potential of the units is governed by (recalling <xref ref-type="disp-formula" rid="pcbi.1004489.e028">Eq (3)</xref>, above):
<disp-formula id="pcbi.1004489.e028"><alternatives><graphic id="pcbi.1004489.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>o</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
Under the assumption that there are no inhibitory and modulatory connections, this can be rewritten as
<disp-formula id="pcbi.1004489.e029"><alternatives><graphic id="pcbi.1004489.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mover><mml:mo>=</mml:mo> <mml:mo>!</mml:mo></mml:mover> <mml:mn>0</mml:mn> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo> <mml:mspace width="2.em"/><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:mi>N</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
which is zero at equilibrium. We can use this equation that governs the steady state to compute the influence of the synaptic weight <italic>W</italic><sub><italic>kl</italic></sub> on the activity of all units of the regular network, including the unit coding for the selected action. For simplicity, we omit the ∞ and just write <bold><italic>p</italic></bold>, <bold><italic>I</italic></bold> instead of <inline-formula id="pcbi.1004489.e030"><alternatives><graphic id="pcbi.1004489.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e030"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>∞</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∞</mml:mo><mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Computing the derivative (denoted by “′”) with respect to <italic>W</italic><sub><italic>kl</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004489.e042">Eq (12)</xref> results in
<disp-formula id="pcbi.1004489.e031"><alternatives><graphic id="pcbi.1004489.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e031"/><mml:math id="M31" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mover><mml:mover accent="true"><mml:mrow><mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow> <mml:mo>︷</mml:mo></mml:mover> <mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mover> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mover><mml:mover accent="true"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︷</mml:mo></mml:mover> <mml:mrow><mml:mo>=</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>m</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mover></mml:mrow> <mml:mo>︷</mml:mo></mml:mover> <mml:mrow><mml:mtext>product</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>rule</mml:mtext> <mml:mspace width="4.pt"/><mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>m</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula> <disp-formula id="pcbi.1004489.e032"><alternatives><graphic id="pcbi.1004489.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:msubsup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:munder> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula> <disp-formula id="pcbi.1004489.e033"><alternatives><graphic id="pcbi.1004489.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e033"/><mml:math id="M33" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">e</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>m</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where (⋅)<sup><italic>T</italic></sup> denotes the transpose of a matrix or vector, <italic>δ</italic><sub><italic>lm</italic></sub> = 1 if and only if <italic>l</italic> = = <italic>m</italic> (Kronecker delta), and the vector <bold>e</bold><sub><italic>l</italic></sub> ∈ ℝ<sup><italic>N</italic></sup> is zero except for the <italic>l</italic>–th entry which is one, i.e.
<disp-formula id="pcbi.1004489.e034"><alternatives><graphic id="pcbi.1004489.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e034"/><mml:math id="M34" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mn>11</mml:mn></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mn>21</mml:mn></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mn>12</mml:mn></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mn>22</mml:mn></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:munder> <mml:mo>·</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
By multiplying this linear system with the matrix inverse <inline-formula id="pcbi.1004489.e035"><alternatives><graphic id="pcbi.1004489.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e035"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, we obtain
<disp-formula id="pcbi.1004489.e036"><alternatives><graphic id="pcbi.1004489.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e036"/><mml:math id="M36" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mi>l</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>l</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>N</mml:mi> <mml:mi>l</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
This equation describes the influence of <italic>W</italic><sub><italic>kl</italic></sub> on the activity of all network units. We are particularly interested in the influence <inline-formula id="pcbi.1004489.e037"><alternatives><graphic id="pcbi.1004489.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e037"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo> <mml:mrow><mml:mo>∂</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> of <italic>W</italic><sub><italic>kl</italic></sub> on <italic>p</italic><sub><italic>a</italic></sub>, i.e. the membrane potential of the unit that codes the value of the selected action <italic>a</italic>, which is given by
<disp-formula id="pcbi.1004489.e038"><alternatives><graphic id="pcbi.1004489.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e038"/><mml:math id="M38" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
Note that the <italic>only non-local impact</italic> of the synaptic weight from unit <italic>k</italic> to <italic>l</italic> on <italic>p</italic><sub><italic>a</italic></sub> is determined by <inline-formula id="pcbi.1004489.e039"><alternatives><graphic id="pcbi.1004489.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e039"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, which describes the impact of the postsynaptic unit <italic>l</italic> on the winning output unit <italic>a</italic>. We will now demonstrate that <inline-formula id="pcbi.1004489.e040"><alternatives><graphic id="pcbi.1004489.e040g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e040"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> corresponds to the activation level <inline-formula id="pcbi.1004489.e041"><alternatives><graphic id="pcbi.1004489.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e041"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> in the accessory network.</p>
</sec>
<sec id="sec007">
<title>Accessory network</title>
<p>The learning rule ensures that the connections of the accessory network are reciprocal to those of the (linearized) regular network. The accessory network is linear because accessory units do not influence the gains determined by the (non-linear) regular network. These gains are taken into account by the accessory network during the activity propagation but will be treated as constants.</p>
<p>Before we describe the activity propagation within the accessory network itself, we will first describe a version of the regular network that we will linearize around its fixed point. Our aim is to determine the influence of a change of the synaptic weight <italic>W</italic><sub><italic>kl</italic></sub> on the position of this fixed point. At equilibrium, the activity of the regular network, defined in <xref ref-type="disp-formula" rid="pcbi.1004489.e042">Eq (12)</xref>, can be reformulated from
<disp-formula id="pcbi.1004489.e042"><alternatives><graphic id="pcbi.1004489.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e042"/><mml:math id="M42" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
to the following linear form (using Taylor’s formula)
<disp-formula id="pcbi.1004489.e043"><alternatives><graphic id="pcbi.1004489.e043g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e043"/><mml:math id="M43" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>≈</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>m</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:munder> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>1</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>1</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>1</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>2</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>2</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mn>2</mml:mn> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>N</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>N</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>…</mml:mo></mml:mtd> <mml:mtd><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>N</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>·</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>N</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <inline-formula id="pcbi.1004489.e044"><alternatives><graphic id="pcbi.1004489.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e044"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the deviation of unit <italic>i</italic> from its original value at equilibrium (see Suppl. B in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref> for the Taylor expansion of the full interaction). The terms in the matrix of <xref ref-type="disp-formula" rid="pcbi.1004489.e043">Eq (19)</xref> are defined as follows
<disp-formula id="pcbi.1004489.e045"><alternatives><graphic id="pcbi.1004489.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e045"/><mml:math id="M45" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula> <disp-formula id="pcbi.1004489.e046"><alternatives><graphic id="pcbi.1004489.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>α</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
Thus, <xref ref-type="disp-formula" rid="pcbi.1004489.e043">Eq (19)</xref> describes the <italic>linearized interaction</italic> of the <italic>regular network at equilibrium</italic> and can be used to determine how a small perturbation influences the equilibrium state. The perturbations influence the equilibrium state as follows
<disp-formula id="pcbi.1004489.e047"><alternatives><graphic id="pcbi.1004489.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>·</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula> <disp-formula id="pcbi.1004489.e048"><alternatives><graphic id="pcbi.1004489.e048g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e048"/><mml:math id="M48" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder> <mml:mo>·</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula> <disp-formula id="pcbi.1004489.e049"><alternatives><graphic id="pcbi.1004489.e049g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e049"/><mml:math id="M49" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:msub><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
This last equation provides a convenient way to describe the linearized interaction of the <italic>regular network</italic>. Following [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref083">83</xref>], the <italic>accessory network</italic> with activities Δ<bold><italic>p</italic></bold> is reciprocal to the (linearized) regular network at equilibrium <xref ref-type="disp-formula" rid="pcbi.1004489.e049">Eq (24)</xref>, and it also takes the gain factors <italic>g</italic>′(<italic>p</italic><sub><italic>n</italic></sub>) and (<italic>β</italic> − <italic>p</italic><sub><italic>m</italic></sub>) in every column into account plus an injection of a unit activation at the winning unit <italic>δ</italic><sub><italic>am</italic></sub> (note the indices of <inline-formula id="pcbi.1004489.e050"><alternatives><graphic id="pcbi.1004489.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e050"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004489.e049">24</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004489.e051">25</xref>) that indicate the reciprocity of activation flow). The accessory network converges to its own stable state as follows:
<disp-formula id="pcbi.1004489.e051"><alternatives><graphic id="pcbi.1004489.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e051"/><mml:math id="M51" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:msub><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula> <disp-formula id="pcbi.1004489.e052"><alternatives><graphic id="pcbi.1004489.e052g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e052"/><mml:math id="M52" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula> <disp-formula id="pcbi.1004489.e053"><alternatives><graphic id="pcbi.1004489.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>n</mml:mi></mml:munder> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>g</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula>
a time derivative that becomes zero once the accessory network has settled in a stable state. Thus, the signal that propagates in the accessory network from unit <italic>n</italic> to unit <italic>m</italic> equals <italic>W</italic><sub><italic>mn</italic></sub> ⋅ [(<italic>β</italic> − <italic>p</italic><sub><italic>n</italic></sub>) ⋅ Δ<italic>p</italic><sub><italic>n</italic></sub>] and the influence of this signal depends on <italic>g</italic>′(<italic>p</italic><sub><italic>m</italic></sub>), the gain of unit <italic>m</italic>.</p>
<p>To summarize <xref ref-type="disp-formula" rid="pcbi.1004489.e053">Eq (27)</xref>, the associate network at equilibrium interacts as following:
<disp-formula id="pcbi.1004489.e054"><alternatives><graphic id="pcbi.1004489.e054g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e054"/><mml:math id="M54" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold-italic">p</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">J</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula>
with <bold><italic>J</italic></bold> = 0 except for <italic>J</italic><sub><italic>a</italic></sub> = 1. At steady state (i.e. <inline-formula id="pcbi.1004489.e055"><alternatives><graphic id="pcbi.1004489.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e055"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>Δ</mml:mo> <mml:mtext mathvariant="bold-italic">p</mml:mtext> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), for the accessory network it holds (by multiplying <xref ref-type="disp-formula" rid="pcbi.1004489.e054">Eq (28)</xref> with <inline-formula id="pcbi.1004489.e056"><alternatives><graphic id="pcbi.1004489.e056g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e056"/><mml:math id="M56" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>)
<disp-formula id="pcbi.1004489.e057"><alternatives><graphic id="pcbi.1004489.e057g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e057"/><mml:math id="M57" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mi mathvariant="bold-italic">J</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>l</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>J</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>a</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
i.e. <inline-formula id="pcbi.1004489.e058"><alternatives><graphic id="pcbi.1004489.e058g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e058"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> can be plugged into <xref ref-type="disp-formula" rid="pcbi.1004489.e038">Eq (18)</xref>. This result demonstrates that by circulating the activity initiated by the winning action <italic>a</italic>, the activity Δ<italic>p</italic><sub><italic>k</italic></sub> of units of the accessory network becomes equal to the impact that the corresponding regular units have on the action-value of the winning unit <italic>a</italic>.</p>
</sec>
<sec id="sec008">
<title>The activity of an accessory unit is proportional to the influence of the regular unit on the action value</title>
<p>With the use of Eqs (<xref ref-type="disp-formula" rid="pcbi.1004489.e057">29</xref>), (<xref ref-type="disp-formula" rid="pcbi.1004489.e038">18</xref>) can be written as
<disp-formula id="pcbi.1004489.e059"><alternatives><graphic id="pcbi.1004489.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub></mml:mrow> <mml:mi>∞</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:mo>·</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula>
Which proves that for an error term <inline-formula id="pcbi.1004489.e060"><alternatives><graphic id="pcbi.1004489.e060g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e060"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mo>ϱ</mml:mo> <mml:mo>−</mml:mo> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>δ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (describing the quadratic distance between the estimate <inline-formula id="pcbi.1004489.e061"><alternatives><graphic id="pcbi.1004489.e061g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e061"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and the experienced reward <italic>ϱ</italic>; c.f. <xref ref-type="disp-formula" rid="pcbi.1004489.e016">Eq (7)</xref>) our learning rule <xref ref-type="disp-formula" rid="pcbi.1004489.e020">Eq (8)</xref> performs gradient descent:
<disp-formula id="pcbi.1004489.e062"><alternatives><graphic id="pcbi.1004489.e062g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e062"/><mml:math id="M62" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>η</mml:mi> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>E</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mo>·</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>∞</mml:mi></mml:msubsup></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mo>·</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub></mml:mrow> <mml:mi>∞</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula>
Thus, with the help of the accessory network the information necessary to shift the equilibrium state of the regular network in a direction that improves the estimate of the value of the chosen action becomes available locally, in the cortical column. To simplify the presentation in this section we focused on networks with only excitatory connections, but we also included inhibitory and modulatory connections in our full network. The generalization to networks that include modulatory and inhibitory connections is provided in Suppl. C in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref>.</p>
</sec>
</sec>
</sec>
<sec id="sec009" sec-type="results">
<title>Results</title>
<p>The analysis described above establishes that RELEARNN can train a fully recurrent network to adjust its fixed point and to thereby improve its estimate of the value of a selected action. To test the performance of the learning algorithm, we investigated if RELEARNN would train a neural network to group contour elements and to trace curves, two tasks thought to require feedback and horizontal connections in the brain. Will the network solve either task if the only feedback from the environment is a reward for correct responses? Although RELEARNN can train recurrent neural networks with many different structures, we focused on networks with a single achitecture that has been illustrated in <xref ref-type="fig" rid="pcbi.1004489.g004">Fig 4</xref>. Input units of the network represent the state of the environment and it is the task of the network to select a saccadic eye movement by activating one of the units in the motor layer, where units code for a range of possible eye movements. The learning algorithm tries to adapt the connection weights so that the activity of units in the motor layer code the expected reward for making a saccade to that location. The softmax rule for action selection ensures that saccades with a higher action value (higher activity of the motor unit) have a higher probability to be selected. We provide the detailed parameters for the simulations in Suppl. A in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref>. The model structure includes horizontal and feedback connections. Units in the input layer can activate excitatory units in the linking layer or provide disynaptic inhibition. Excitatory units in the linking layer can modulate the activity of their four neighbors that are selective for the same feature and they can also provide modulatory input to units at the same spatial location that are tuned to a different feature. Furthermore, units of the linking layer excite units in the association layer and they are the target of modulatory feedback connections from the association layer. Units in the association layer, in turn, excite units in the motor layer. Thus, the lateral connections and feedback connections from the association layer back to the linking layer enable the recirculation of activity through the network, i.e. recurrent processing. The structure of the input and motor layers differed between the contour-linking and curve-tracing tasks, because the inputs and the eye movement responses were different (described below). To keep the complexity of the models at a minimum, we did not include all connection types. For example, we omitted feedback connections from the motor units to the association layer as well as lateral and inhibitory connections within the motor layer. We found that the set of connections included in the model was rich enough to solve the two tasks. It is likely, however, that other tasks that we did not model might benefit from additional connection types.</p>
<fig id="pcbi.1004489.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Neural network structure.</title>
<p>The input layer contains 2D-maps of feature selective units (corresponding to representations in cortical area V1), which provide input to a “linking layer” that can establish perceptual groups. Input units activate excitatory and inhibitory units in the linking layer, and inhibitory units provide disynaptic inhibition to the excitatory units in this layer. Modulatory connections (green), which increase the excitatory impact, interconnect excitatory units with adjacent receptive fields in the same feature maps and units with overlapping receptive fields in different feature maps. Excitatory units in the linking layer can activate any unit in the association layer (e.g. in extrastriate or parietal cortex), and receive modulatory feedback connections. Units in the association layer, in turn, activate units in the motor layer (corresponding to neurons in the frontal eye field) that represent action-values and select one of a number of actions. Black connections have a fixed strength and excitatory (red), inhibitory (blue) and modulatory (green) connections undergo synaptic plasticity.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g004"/>
</fig>
<sec id="sec010">
<title>Learning to Link Visual Contours</title>
<p>Li, Piëch and Gilbert trained monkeys to group colinearly aligned contour elements using the task illustrated in <xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1A</xref> [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]. In every trial, the monkey saw two sets of short line elements within circular apertures. One of the apertures contained a contour pattern composed of a number of colinear line elements and the other pattern contained line elements with a random orientation. The overall orientation of the colinear contour was fixed during a training session, but it could vary across sessions. At the start of a trial, the monkey had to direct gaze to the center of the display. The animals’ task was to make an eye movement to the aperture with the contour pattern (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1</xref>) and the animals performed approximately 1500 trials per day. The authors started with a pretraining phase of several days where the target contour was shown at a high luminance but the distractor elements were less visible because they had a lower luminance. This was followed by the main training sessions where the distractor elements had the same luminance as the target elements so that only colinearity remained as a cue to solve the task. We investigated whether RELEARNN could train the network of <xref ref-type="fig" rid="pcbi.1004489.g004">Fig 4</xref> by rewarding saccades to the contour pattern. Will the network learn to detect the colinearly aligned contour elements and will it reproduce the monkeys’ behavior during learning? Moreover, how does the activity of network units compare to the neuronal activity in the visual cortex of the monkeys [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]?</p>
<p>Li et al. trained their monkeys to maintain fixation for several hundreds of milliseconds upon presentation of the patterns and to make the saccade after this delay [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]. We trained the neural network on a version of the task that was simpler because we did not model the different phases of every trial. The model could immediately select the eye-movement upon presentation of the pattern and convergence to a stable activity state (learning of multiple task epochs by a feedforward network has been addressed by [<xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref036">36</xref>]). The input to the network consisted of two patterns of bars with four possible orientations presented on a grid with 9 × 9 spatial positions on either side of the fixation point. There were a total of 648 input units (81 locations per pattern × 4 orientations × 2 patterns) that provided direct excitation and disynaptic inhibition via inhibitory units to the excitatory units of the linking layer, with 2 × 4 × 9 × 9 units, one for every orientation at each retinotopic location. Excitatory units in the linking layer projected to units with adjacent receptive fields tuned to the same orientation (four nearest neighbors) and also to the three units with the same receptive field tuned to other orientations with modulatory connections. Units of the linking layer propagated activity to a total of four units in the association layer, which, in turn, projected to the two units of the motor layer that had to learn the expected reward associated with a saccade to the left or right pattern. To facilitate learning, we started with a short (500 stimulus presentations) preliminary procedural training phase (just like [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]; this was not necessary, however, to successfully learn the task). During the pre-training phase, we made the background surrounding the contour elements less salient than the target pattern by reducing their contrast (activity of input units representing background elements was set to 50%). We then increased the contrast of the background contrast in increments of 10% (100 trials per contrast step) until all elements had 100% contrast.</p>
<p>The pre-training phase was followed by training in the full task (<xref ref-type="fig" rid="pcbi.1004489.g005">Fig 5</xref>). Training caused a gradual increase in the model’s accuracy and the effect of training was particularly pronounced for patterns with a larger number of colinear line elements. The effect of training on the accuracy of the model resembled the improvement in the accuracy of the monkeys (c.f. <xref ref-type="fig" rid="pcbi.1004489.g005">Fig 5</xref> left vs. right). The model was able to learn the contour linking task within ∼ 15,000 trials (∼ 3,000 trials per contour length), which is comparable to the time-course of the monkeys’ behavioral improvement (10 days with ∼ 1,500 trials per day). The results of these simulations were not critically dependent on the exact parameter values, i.e. slight parameter changes did not qualitatively alter the outcome, although less optimal values of the parameters decreased learning speed.</p>
<fig id="pcbi.1004489.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Accuracy of the model and comparison to the accuracy of monkeys.</title>
<p>Left, performance of monkeys re-drawn from [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]. The five panels show the accuracy across training days with patterns of increasing contour length; 1 line (blue), 3 (purple), 5 (green), 7 (orange) or 9 colinear lines (light blue). Note that the monkeys performed at chance level for patterns with line length 1, which are indistinguishable from distractor patterns. Solid lines are cubic spline fits. <bold>Right</bold>, Accuracy of the model for the same stimuli, smoothed with a Gaussian (<italic>σ</italic> = 100 trials). The number of iterations refers to repetitions of the same contour length, i.e. the total number of iterations is five times as large because the model was exposed to five different contour lengths.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g005"/>
</fig>
<p>We next investigated how learning of the contour integration task changed the activity of network units (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6</xref>). After the pre-training, but before the training in the full task, the activity of the excitatory units in the linking layer did not depend strongly on the number of colinearly aligned contours in the target pattern (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6A</xref>). After training, however, the activity of these units clearly depended on the number of colinearly aligned contour elements (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6B</xref>). Patterns with more colinearly aligned contour elements elicited stronger responses, thereby reproducing the effect of training in area V1 of monkeys (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6D and 6E</xref>). It is also of interest to compare the time course of activity in the model’s linking layer to the time-course of activity in monkey V1. In the trained model, tuning to the number of colinear line elements was relatively weak during the initial response, but it became stronger when the network units started to converge to their stable activity level. The feedforward activation from the input layer only conveys information about the orientation of the contour element in the receptive field and not about the context provided by image elements in the surround. Thus, tuning to colinearity must be due to alterations in horizontal and feedback influences on the units, and this explains the delay in the emergence of this tuning. The temporal profile of the response of model units is quite comparable to that of V1 neurons, for which tuning to the colinearity of image elements also emerges during the later phase of the response.</p>
<fig id="pcbi.1004489.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison between the activity of model units and neurons in area V1 of monkeys.</title>
<p><bold>A,B</bold> Simulation results (ordinate in arbitrary units). Mean activity <italic>p</italic> (membrane potential) of excitatory units in the linking layer (red in <xref ref-type="fig" rid="pcbi.1004489.g004">Fig 4</xref>) representing the center bar of the target stimulus and the 95% confidence interval (across 10,000 stimulus presentations) before <bold>(A)</bold> and after training in the contour detection task <bold>(B)</bold>. <bold>C</bold>, Activity in a trained model when the strength of the modulatory horizontal and feedback connections is set to zero. <bold>D,E</bold>, Mean activity of V1 neurons elicited by the target stimulus before <bold>(D)</bold> and after training <bold>(E)</bold>. Training caused V1 neurons to be tuned to number of colinearly aligned bars. <bold>F</bold>, Under anesthesia, the sensitivity to contour length was abolished and the visual responses were reduced in strength but not abolished. <bold>D,E,F</bold> redrawn from [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>]. <bold>(G)</bold> Activity in a model where the lateral connections in the linking layer were removed. <bold>(H)</bold> Activity in a network where feedback connections from the association layer to the linking layer were removed.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g006"/>
</fig>
<p>In their experiment, Li et al. [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>] also anesthetized the monkeys after the training procedure with the aim to block the top-down and horizontal influences. During anesthesia, the tuning to colinearity was indeed abolished in area V1, whereas visually driven activity was maintained, albeit weaker (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6F</xref>). We could replicate the effect of anesthesia in our model by removing the feedback and lateral connections (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6C</xref>), which confirmed that these contextual influences in the model are mediated by recurrent, i.e. lateral and feedback connections. In addition, we tested the respective contribution of lateral and feedback interactions by deleting one set of connections or the other (<xref ref-type="fig" rid="pcbi.1004489.g006">Fig 6G and 6H</xref>). The results indicate that lateral interactions have a more pronounced effect than feedback interactions in this task, a prediction that can be tested in future experiments.</p>
<p>We next investigated how training in the contour linking task influenced the pattern of connectivity. We first examined the horizontal connections in the linking layer of an example network that had been trained to criterion (<xref ref-type="fig" rid="pcbi.1004489.g007">Fig 7</xref>). It can be seen that the lateral connections were strengthened along the direction corresponding to the axis of the unit’s orientation tuning. We next examined the pattern of excitatory feedforward connections from the linking layer to units in the association layer. In this analysis we focused on association units with a strong connection to motor units, which have an impact on the computation of the action values. We found that the association units integrated the activity of units along the target contours, with a specific set of connections from each of the four orientation maps (<xref ref-type="fig" rid="pcbi.1004489.g008">Fig 8A</xref>). When we examined the pattern of modulatory feedback connections from the association layer back to the linking layer we found that connections of the regular network had become largely reciprocal (in addition to the accessory network connections, which are enforced to be reciprocal). Association units tended to have strong feedback connections to units that provided them with feedforward excitation (<xref ref-type="fig" rid="pcbi.1004489.g008">Fig 8B</xref>). This reciprocity of feedforward and feedback connections is in accordance with anatomical findings [<xref ref-type="bibr" rid="pcbi.1004489.ref090">90</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref091">91</xref>]. To summarize, the new RELEARNN algorithm reproduced the time-course of neuronal activity underlying contour linking as observed by Li et al. [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>] and illustrates how it can be explained by a modification of the pattern of feedforward, lateral and feedback connections.</p>
<fig id="pcbi.1004489.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Stimulus and lateral connectivity in the “linking layer”.</title>
<p><bold>A</bold> Example stimulus (the patterns presented to the two hemispheres are plotted above each other). Re-drawn from [<xref ref-type="bibr" rid="pcbi.1004489.ref015">15</xref>] <bold>B</bold> Lateral connectivity in the “linking layer” for one orientation and hemisphere for units selective for diagonal line elements before training. <bold>C</bold> Lateral connectivity after training (we observed similar patterns for the other orientations). Line thickness corresponds to connection strength (thickest line corresponds to a connection strength of 0.36). Note that due to the small amount of simulated units (one unit per spatial input location) we did not introduce jitter into the stimulus which would have resulted in multiple parallel “thick lines”.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g007"/>
</fig>
<fig id="pcbi.1004489.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Connection strengths between linking units with different orientation preferences (O1–O4 denote the different orientations of contour elements) in <italic>one</italic> hemifield and an example association layer unit A2 with a strong connection to the motor unit in this hemisphere, after training to criterion (feedforward strength A and feedback strength B).</title>
<p>Similar association units were established in the other hemisphere.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g008"/>
</fig>
<p>In our main simulations, units in the linking layer were only connected if they represented nearby contour elements. To examine if this restriction was necessary for successful learning we also carried out simulations with full connectivity in the linking layer. We found that networks with full connectivity also learned the task and, interestingly, strongest connections formed between units with adjacent receptive fields as the result of the training process.</p>
<p>We next examined whether recurrent connections were necessary for detection of the colinear contour configurations. We therefore trained networks without lateral connections in the linking layer and without feedback connections from the association layer to the linking layer (we kept the feedback connections of the accessory network that guide plasticity). Activity in these feedforward networks is immediately stable and it is therefore not necessary to wait until the network converges, so that RELEARNN becomes equivalent to the AuGMEnT learning rule for feedforward networks [<xref ref-type="bibr" rid="pcbi.1004489.ref036">36</xref>]. RELEARNN could train these purely feedforward networks to detect the colinear contours. This is an important result, which implies that the detection of these colinear contour patterns does not require recurrent connectivity. Nevertheless, networks with recurrent connections do utilize these connections if trained with RELEARNN, in accordance with the neurophysiological findings.</p>
</sec>
<sec id="sec011">
<title>Learning to Trace a Curve</title>
<p>Not all conceivable feature groupings can be coded by dedicated neurons [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. If confronted with a new configuration of image elements, grouping can proceed based on low level grouping criteria, such as connectedness or colinearity, even in the absence of neurons that are selective for the overall shape. In these situations, the visual brain appears to code perceptual groups by labeling the to-be-grouped image elements with enhanced neuronal activity, a process that has been called incremental grouping [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. One example task that requires the formation of incremental perceptual groups is curve-tracing [<xref ref-type="bibr" rid="pcbi.1004489.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref092">92</xref>] where subjects have to group contour elements that belong to a single elongated curve. <xref ref-type="fig" rid="pcbi.1004489.g009">Fig 9</xref> shows one example stimulus of the curve tracing task that we used to test RELEARNN. Every stimulus contained two curves that consisted of connected pixels, two potential green eye movement targets and a red cue. The model’s task was to select an eye movement to a green marker on one of the two curves, which will be referred to as “target curve”. The target curve was cued with the red marker and the model was only rewarded if it made an eye movement to the green marker on the cued target object. Thus, the model had to learn to apply connectedness grouping to determine which of the two green markers fell on the same curve as the red cue. Human observers solve such a curve-tracing task by mentally tracing along the target curve. This mental tracing process corresponds to the gradual spread of object-based attention over the target curve [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref093">93</xref>]. Previous neurophysiological studies revealed a neuronal correlate of the spread of object-based attention over a curve in the primary visual cortex because neuronal activity elicited by a target curve was stronger than that elicited by a distractor (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1B</xref>). The response enhancement does not occur during the transient response that is triggered by the onset of the visual stimulus and the appearance of contour elements in the neurons’ receptive fields but after a delay of ∼ 150ms. The enhanced neuronal activity first occurs at the cued location (i.e. the red cue in <xref ref-type="fig" rid="pcbi.1004489.g009">Fig 9</xref>) before it gradually spreads over the other image elements of the same object [<xref ref-type="bibr" rid="pcbi.1004489.ref094">94</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref095">95</xref>]. These results suggest that the enhanced neuronal activity spreads through horizontal connections in visual cortex, which link neurons that represent adjacent image elements, although it is likely that feedback connections from higher visual areas to the primary visual cortex also contribute. Similar effects have been observed in frontal cortex where the target curve also evoked stronger responses than the distractors during a delayed phase of the neuronal response [<xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref040">40</xref>]. A previous modeling study investigated the propagation of enhanced activity along a relevant curve in a handcrafted network [<xref ref-type="bibr" rid="pcbi.1004489.ref025">25</xref>]. However, to our knowledge it has not yet been studied whether the necessary connectivity can emerge during reward-based learning with a biologically plausible learning rule. Can the network architecture illustrated in <xref ref-type="fig" rid="pcbi.1004489.g004">Fig 4</xref>, which learns to link colinear line elements, also learn to trace a curve? If yes, will learning induce a propagation of enhanced neuronal activity along the relevant curve, just as is observed in the visual cortex of monkeys? We had to make adjustments to the input layer of the network because of the different format of the input. The input layer and the linking layer now consisted of a 5 × 5 grid with 3 cells at every grid location to encode the possible colors; we used a unit coding for red, green and for luminance at each location. The association layer contained 25 neurons and the motor layer also consisted of a 5 × 5 grid, because each position could be a target for a specific stimulus (the model could select one of 25 eye movements for every stimulus). To ensure that the model would learn the grouping rule and that it could not solve the task by memorizing the specific eye-movements associated with a limited set of stimuli, we randomly generated a new stimulus on every trial. In the final version of the task, the stimulus always consisted of two equally long lines of two to five pixels each. We considered two pixels to be connected when they shared an edge. According to this rule the two curves in <xref ref-type="fig" rid="pcbi.1004489.g010">Fig 10</xref> are not connected although they do touch each other. We randomly selected two of the end-points as potential eye movement targets and one other endpoint as the location of the cue. It was the model’s task to select an eye movement to the green target that was on the same curve as the red cue (<xref ref-type="fig" rid="pcbi.1004489.g009">Fig 9</xref>).</p>
<fig id="pcbi.1004489.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Curve tracing task.</title>
<p>We trained the model to make an eye movement to a green circular marker that was on the target curve, which was cued with a red circle. The other curve was a distractor and had to be be ignored. This task requires the grouping of the connected image elements of the target curve.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g009"/>
</fig>
<fig id="pcbi.1004489.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Activity in an example network trained in the curve-tracing task.</title>
<p>A, An example stimulus with two green saccade targets and a red cue that indicates which curve is target. T1 and T2 are two pixels of the target curve and D1 and D2 are pixels of the distractor (we evaluated activity for different stimuli at the equivalent locations). <bold>B</bold>, Activity <italic>p</italic> (membrane potential) of example units of the linking layer, averaged across multiple stimulus presentations. The curve tracing task induced an increase of activity of units representing the target curve (T1 and T2) and a decrease in the activity of units representing the distractor (D1 and D2; 95% point-wise confidence bands are within line width). <bold>C</bold>, We normalized the difference in activity elicited by corresponding positions of the target and distractor curve in the linking layer to investigate the time-course of the response enhancement. We found that the latency of the response enhancement increased for pixels of the target curve that are farther from the red cue, in accordance with previous neurophysiological results [<xref ref-type="bibr" rid="pcbi.1004489.ref095">95</xref>]. <bold>D</bold>, Activity in the motor layer was strongest for pixels with a green cue. Note that the activity elicited by the saccade cue on the target curve (<italic>S</italic><sub><italic>T</italic></sub>) was stronger than that elicited by the saccade cue on the distractor curve (<italic>S</italic><sub><italic>D</italic></sub>). <bold>E</bold>, Time course of normalized response differences in the motor layer. Also here the response enhancement occurred later for pixels that were farther from the red cue. The activities in the motor layer <bold>(E)</bold> and the linking layer <bold>(C)</bold> have similar time-courses as have been reported in the frontal and visual cortex of monkeys [<xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>]. Note that the propagation is quite fast due to the small network size but that it critically depends on the recurrent interaction as theoretically predicted by [<xref ref-type="bibr" rid="pcbi.1004489.ref096">96</xref>].</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g010"/>
</fig>
<p>Researchers usually train monkeys to perform such a complex task in a number of phases in which the difficulty of the task increases gradually, a strategy that is sometimes called shaping. We used a similar strategy to shape the model, because it otherwise got stuck in a local minimum without finding a solution. We started the training procedure by presenting displays with a single square with a green eye movement target on an otherwise black background and rewarded the model for making an eye movement to this square. In the subsequent phase, we introduced the lines and the red cue, gradually increasing line length until the lines were up to five pixels long (see Suppl. A in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref> for details). Training was considered successful when a greedy policy (i.e. selection of the most active output unit) would lead to correct performance for 400 consecutive stimulus presentations.</p>
<p>We found that all 12 networks that we tested reached criterion performance with an average of 164,000 iterations (range: 136,000–189,000). This extensive training procedure would correspond to ∼ 100 training days with a monkey, with 1500 trials per day. Note, however, that we used the same learning parameters as in the contour linking task, and that we have not optimized these parameters for learning speed. Furthermore, the model learned a relatively complex version of the curve-tracing task where one of the curves was cued by a red circle and where the shapes of the two curves were much more variable than those that have been used to train monkeys. In addition, it is safe to assume that the monkeys had substantial prior experience with perceptual grouping prior to their first exposure to the curve-tracing task.</p>
<p>We next examined how the networks solve the task by investigating the activity that the units had acquired during the learning process. We first examined the pattern of activity in the linking layer and found that units coding for pixels of the target curve exhibited stronger activity than units coding pixels of the distractor (<xref ref-type="fig" rid="pcbi.1004489.g010">Fig 10</xref>). Initially, the units signaled the appearance of a pixel in their receptive field. The response enhancement occurred after a delay, because it required the spread of activity from the unit coding for the red cue in the linking layer to the unit coding for the luminance of that pixel. The enhanced activity then gradually propagated across the curve until it reached the eye movement target at the other end. The gradual spread of enhanced activity across the relevant curve is qualitatively similar to the activity in area V1 of macaque monkeys during this task, although a detailed comparison of the timing of the model (in time units) and V1 activity (in ms) is not feasible because we did not model the conduction and synaptic integration delays that determine the propagation of neuronal activity in the brain. In monkey V1, the response enhancement also did not occur during the initial visual response that was elicited by the appearance of a contour element in the neurons’ receptive fields, but after a delay of ∼ 150ms [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. The enhanced activity then gradually spread over this curve with a speed of ∼ 50ms per receptive field until all its contour elements were labeled with enhanced neuronal activity [<xref ref-type="bibr" rid="pcbi.1004489.ref095">95</xref>]. To detect connectedness, it is important that the enhanced activity is selectively propagated along the representation of the curve and that it cannot spread to blank image locations in between curves [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>]. A closer look at the connection pattern of the linking layer revealed why the network indeed only spread activity among adjacent units with a pixel in their receptive field (<xref ref-type="fig" rid="pcbi.1004489.g011">Fig 11</xref>). Trained networks developed balanced feedforward excitation and di-synaptic inhibition from the input layer to the linking layer (similar to neurophysiological and anatomical findings [<xref ref-type="bibr" rid="pcbi.1004489.ref097">97</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref098">98</xref>]). Units in the linking layer were therefore not directly (or only weakly) activated by a pixel in their receptive field without concurrent modulatory input. The unit in the linking layer representing the red cue was active, however, and it had a modulatory effect that increased the impact of the excitatory input to the unit coding for luminance at the same location (see <xref ref-type="disp-formula" rid="pcbi.1004489.e001">Eq (1)</xref>), causing the unit to become active. This activity then spread through the modulatory connections to the other units representing connected pixels. It did not spread to blank space, however, because the units in the linking layer did not receive excitatory input and the modulatory connections had no effect. As a result, the enhanced activity spread selectively over the target curve until it reached the other end with the green marker, thereby highlighting the correct target for an eye movement.</p>
<fig id="pcbi.1004489.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Connectivity structure established during training in the curve tracing task.</title>
<p><bold>A</bold>, Feedforward input into the linking layer causes balanced excitation and inhibition (through the inhibitory units) preventing linking layer units to become active. The unit in the linking layer tuned to red (not shown here) provides modulatory input, thereby increasing the impact of feedforward excitation to the left unit and this extra activity can spread through horizontal connections in the linking layer. <bold>B</bold>, If a unit in the linking layer does not receive feedforward input, horizontal modulatory influences cannot occur, thereby preventing the spread of activity across gaps in the linking layer, which is important for the detection of connectedness.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g011"/>
</fig>
<p>Although we trained the models with contour lengths up to five pixels, additional tests revealed that they generalized their ability to trace a curve to longer line lengths (<xref ref-type="fig" rid="pcbi.1004489.g012">Fig 12</xref>). Thus, they learned a general solution to the curve-tracing task, which they could apply to new contour configurations that had not been presented previously. A trained model with a greedy policy solved about 97% of 10,000 random stimuli with contours of length 5 and about 81% with length 6. To our knowledge, RELEARNN is the first biologically realistic model that can train a neural network to solve the curve-tracing task. Interestingly, the model discovered the strategy to propagate enhanced neuronal activity from the cue over the rest of the curve, which is also used by monkeys trained on this task. The analysis of the established connectivity patterns presents a prediction for future neuroscientific studies, which can now examine the influence of learning curve-tracing and contour-linking tasks on the patterns of connections between neurons in early areas of the visual cortex.</p>
<fig id="pcbi.1004489.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004489.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Generalization to longer line lengths.</title>
<p>Although the model had been trained with lines with up to five pixels, it also generalized to longer line lengths. In this example stimulus, the model propagated enhanced activity over a target line of seven pixels (darker colors denote higher levels of activity).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.g012"/>
</fig>
<p>In our standard networks, units of the linking layer were only connected to units with nearby receptive fields. We also investigated if networks with full connectivity in the linking layer learn the curve-tracing task. We found that they did and that connections between units in the linking layer with nearby receptive fields became strongest.</p>
<p>Finally, we tested if it is possible to train networks without horizontal connections in the linking layer to trace a curve. We found that learning did not occur once line length increased beyond three pixels. Previous studies demonstrated that the minimal number of association units that are required to detect connectedness in feedforward networks with a similar architecture increases very rapidly with line length [<xref ref-type="bibr" rid="pcbi.1004489.ref096">96</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref099">99</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref100">100</xref>], implying that biologically realistic implementations of algorithms for the detection of connectedness strongly benefit from recurrent connections.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>We have presented a new biologically inspired learning rule that explains how a recurrent neural network can learn to perform contour-linking and curve-tracing tasks by adjusting the strengths of excitatory, inhibitory and modulatory connections. RELEARNN extends previous biologically realistic learning rules relying on two factors to gate Hebbian plasticity, a reward-prediction error and feedback from the response selection stage [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>], to recurrent neural networks. Our main result is that RELEARNN can change the attractor states of recurrent networks to improve the representation of action values. RELEARNN updates action values by a gradient descent with an accessory network for the propagation of credit assignment signals, in combination with a system that computes reward prediction errors. Jointly, these two factors ensure that the information for changing synaptic strength is available locally, within the cortical column.</p>
<p>We tested RELEARNN in two tasks that have been used to investigate perceptual organization in monkeys. We chose these tasks because previous work suggested that they are not solved in a feedforward manner in the brain but rely on recurrent processing, i.e. the recirculation of activity through feedforward, horizontal and feedback connections. Such processing has previously been formalized in counter stream architectures that can combine hypotheses at lower and higher hierarchical network levels through feedforward and feedback streams [<xref ref-type="bibr" rid="pcbi.1004489.ref066">66</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref069">69</xref>]. The first task was a contour linking task (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1A</xref>). We found that RELEARNN was able to train a recurrent network to detect colinear patterns. Interestingly, learning enhanced the representation of line elements of the target contour in the linking layer of the network, propagating extra activity through horizontal and feedback connections. The response enhancement therefore did not occur during the initial response, but after a delay, just as has been observed for neurons in the visual cortex of monkeys trained in this task [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref101">101</xref>]. The model predicts that learning in this task should strengthen the lateral connections between neurons coding colinear line elements. Interestingly, RELEARNN could also train feedforward networks to detect colinear patterns, although feedforward networks do not explain the delayed influence of colinearly aligned image elements on neuronal activity in early visual cortex.</p>
<p>The second task was a curve-tracing task where the model had to determine the connections between adjacent pixels in order to determine the location of a target of a saccadic eye movement. The development of a learning rule that permits the detection of connectedness by a neural network is of some theoretical interest, because connectedness detection was one of the examples that Minsky and Papert [<xref ref-type="bibr" rid="pcbi.1004489.ref096">96</xref>] gave to demonstrate that the perceptual capabilities of perceptrons (feedforward networks with two layers) are limited and that some of these limitations can be alleviated by serial processing, e.g. by Turing machines. One task that relies on connectedness detection is curve-tracing (<xref ref-type="fig" rid="pcbi.1004489.g001">Fig 1B</xref>) [<xref ref-type="bibr" rid="pcbi.1004489.ref102">102</xref>]. Observers determine which contour elements belong to a single, connected image component, and processing in this task is indeed serial as reaction times increase linearly with the length of curve that needs to be traced [<xref ref-type="bibr" rid="pcbi.1004489.ref018">18</xref>]. Without horizontal connections, our networks did not learn the task for contours longer than three pixels, as predicted by previous studies [<xref ref-type="bibr" rid="pcbi.1004489.ref096">96</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref099">99</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref100">100</xref>]. Psychological research established that human observers gradually spread object-based attention across the relevant curve when they trace it [<xref ref-type="bibr" rid="pcbi.1004489.ref093">93</xref>]. At a neuronal level, the tracing of a curve is associated with the gradual spread of enhanced neuronal activity along the curve [<xref ref-type="bibr" rid="pcbi.1004489.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref095">95</xref>]. It is remarkable that a neural network trained with RELEARNN developed the same strategy, gradually labeling the relevant curve with enhanced activity in the linking layer, until the enhanced response reached the saccade target so that it could be selected for an eye movement response.</p>
<p>The excellent correspondence between our modeling results and neurophysiological data in two different tasks suggests that RELEARNN captures properties of cortical learning. Our approach differs from previous studies addressing the connectedness problem like, for example, [<xref ref-type="bibr" rid="pcbi.1004489.ref100">100</xref>], because we proposed a biologically plausible learning rule, which permits a detailed analysis of the established connectivity structures and activity patterns that can be compared to neurophysiological findings. RELEARNN uses two factors to gate Hebbian plasticity, in accordance with previous modeling work, but it generalized the proposed learning rules to recurrent networks. The first factor is a reward-prediction error that has been central to many recent advances in biologically inspired learning [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref081">81</xref>]. It is likely that the reward-prediction error is made available to neurons in cortical and subcortical structures by the release of neuromodulators, such as dopamine. Schultz and his co-workers demonstrated that the firing rate of dopamine neurons in the substantia nigra and ventral tegmental area code reward-prediction errors, i.e. the difference between the amount of reward that was anticipated by a monkey and the amount of reward that was actually received [<xref ref-type="bibr" rid="pcbi.1004489.ref037">37</xref>]. Furthermore, dopamine is known to influence synaptic plasticity so that it could fulfill a role in gating plasticity [<xref ref-type="bibr" rid="pcbi.1004489.ref103">103</xref>]. However, it is conceivable that other neuromodulators such as acetylcholine or seretonin may play equivalent roles.</p>
<p>The second factor used by RELEARNN is a feedback signal initiated by the selected action, which propagates through the accessory network. RELEARNN ensures that connections of the accessory network are reciprocal to those of the regular network, so that the accessory units in the columns with a large influence on the estimated value of the selected action are very active (this reciprocitity can emerge during the learning process itself [<xref ref-type="bibr" rid="pcbi.1004489.ref034">34</xref>]). The top-down influence of action selection on activity at earlier processing levels is known as an effect of selective attention in psychology and it has also been studied in neurophysiological work. Specifically, if a subject selects a stimulus for an eye or hand movement, selective attention is directed to that stimulus [<xref ref-type="bibr" rid="pcbi.1004489.ref104">104</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref105">105</xref>], and selective attention gates learning [<xref ref-type="bibr" rid="pcbi.1004489.ref106">106</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref107">107</xref>]. Moreover, research with the curve-tracing task revealed that action selection indeed also influences neuronal activity in the visual cortex of monkeys. In the visual cortex, a curve that is selected for a behavioral response elicits stronger neuronal activity than a curve that is not selected [<xref ref-type="bibr" rid="pcbi.1004489.ref108">108</xref>], even if the selected response is wrong (i.e. if the monkey makes an error [<xref ref-type="bibr" rid="pcbi.1004489.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref021">21</xref>]). RELEARNN combines the accessory feedback effect caused by action selection with information about the reward-prediction error that is globally available through the release of neuromodulators to improve the value estimate of the selected action.</p>
<p>In what follows, we will first discuss the relation between RELEARNN and the Almeida-Pineda algorithm for the learning of attractor states in recurrent neural networks [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>]. We will then compare RELEARNN to other models for learning in recurrent neural networks and discuss its relation to previous models of perceptual grouping. RELEARNN may open a new path towards unifying theoretical and experimental research in a number of different fields: the neurophysiology and psychology of perceptual organization and the role of object-based attention therein, reinforcement learning theory and the role of feedforward and feedback connections in cortical computation.</p>
<sec id="sec013">
<title>Comparison With the Almeida-Pineda Algorithm</title>
<p>RELEARNN is best understood as a biologically inspired implementation of the supervised Almeida-Pineda learning algorithm for recurrent neural networks [<xref ref-type="bibr" rid="pcbi.1004489.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>]. The synaptic changes of RELEARNN are proportional to those that the Almeida-Pineda learning algorithm would take to adjust the value of the selected action (Suppl. C in <xref ref-type="supplementary-material" rid="pcbi.1004489.s001">S1 Text</xref>). There are two potential issues that are inherited from the Almeida-Pineda algorithm. The first is related to the convergence of the regular network to a stable attractor. It is difficult to derive general conditions that can guarantee that non-symmetric networks converge to a stable attractor [<xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref109">109</xref>]. We therefore had to rely on our simulations where we found that without strong mutual inhibition (that we omitted in the network design to reproduce the experimental findings) the network always was attracted to a stable state. In contrast, the associate network is linear and it will converge provided that the regular network is at a fixed point as was shown by [<xref ref-type="bibr" rid="pcbi.1004489.ref087">87</xref>]. The second stability issue is related to the solution that is found for the task. Although RELEARNN makes adjustments to the synaptic weights that improve the estimated value of the selected action by gradient descent, there are no guarantees that such a network will find an appropriate solution. Indeed, any gradient descent may get stuck in a local minimum in weight space where further improvements will not occur. In the curve-tracing task, for example, the learning only succeeded when we gradually increased the contour length. Without such a shaping strategy the network did not find a successful strategy to solve the task but got stuck in a local minimum.</p>
</sec>
<sec id="sec014">
<title>Comparison With Other Methods for Learning in Recurrent Neural Networks</title>
<p>Learning is one of the most important topics in neural networks research. Learning in recurrent neural networks (RNNs) is more complex than in feedforward networks, because a recurrent network typically has to evolve to a stationary state. This increase in complexity is offset by the advantage that RNNs can learn to compute and store intermediate computational results through their internal feedback structure. This property of RNNs is essential if the network has to remember previous inputs in a time series (time lagged recurrent networks; TLRNs, [<xref ref-type="bibr" rid="pcbi.1004489.ref110">110</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref111">111</xref>]) but it can also be helpful in the analysis of stationary patterns, as was the case in our simulations (simultaneous recurrent networks; SRNs, [<xref ref-type="bibr" rid="pcbi.1004489.ref100">100</xref>]). In the curve-tracing task, the RNN learned to inject activity at the cue and to gradually spread this activity over the curve. It found a serial solution for a task that is difficult to solve for a feedforward network [<xref ref-type="bibr" rid="pcbi.1004489.ref096">96</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref099">99</xref>], because connectedness is a transitive property. If pixel A is connected to pixel B and B to C, then the network should infer that A is also connected to C. The network learned to compute connectedness by serially evaluating the connectedness of adjacent pixels, first spreading the enhanced activity from pixel A to B and then onwards from B to C. This process appears to correspond well to the primitive form of serial reasoning that humans and monkeys employ to solve this task. Similar processing has been employed in other (non-biological) computational models as well [<xref ref-type="bibr" rid="pcbi.1004489.ref100">100</xref>]. Thus, in the curve-tracing example, the intermediate computational results correspond to the set of pixels that have so far been labeled with enhanced activity.</p>
<p>The learning of stable states of the network as implemented by RELEARNN differs from other recurrent neural network learning schemes, such as backpropagation through time (BPTT; [<xref ref-type="bibr" rid="pcbi.1004489.ref112">112</xref>]), recirculation algorithms [<xref ref-type="bibr" rid="pcbi.1004489.ref113">113</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref115">115</xref>] and reservoir computing [<xref ref-type="bibr" rid="pcbi.1004489.ref116">116</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref117">117</xref>]. BPTT [<xref ref-type="bibr" rid="pcbi.1004489.ref112">112</xref>] was the first method for learning by recurrent neural networks. The successive network states are unfolded over time, and the learning rule is equivalent to standard backpropagation algorithm for the unfolded network. A disadvantage of BPTT is that the unfolded network requires a lot of memory (the memory footprint of a single iteration times the number of iterations) and it does not seem to be biologically plausible. Another disadvantage is that the error gradients become very small after a number of time steps, a limitation that has been alleviated by the long short term memory model (LSTM) designed for processing time-series [<xref ref-type="bibr" rid="pcbi.1004489.ref110">110</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref118">118</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref119">119</xref>]. RELEARNN also differs from recirculation algorithms (such as GeneRec [<xref ref-type="bibr" rid="pcbi.1004489.ref113">113</xref>]), which require symmetric connections between network units. They compare the activity of network units in two phases. The first is called “minus phase” in which the input is provided to the network until it settles in a stable state. This is followed by a “plus phase” where the activity of some of the units is clamped to a target pattern. Learning is based on the comparison between the unit activity between phases, which provides a measure for the influence of the units on the target pattern. An important difference is that RELEARNN computes action values and uses a separate accessory network in combination with a reward prediction error to determine the change in synaptic weights. Unlike the recirculation algorithms, it can therefore also train networks with non-symmetric weights.</p>
<p>RELEARNN also differs from reservoir computing methods, such as echo state networks [<xref ref-type="bibr" rid="pcbi.1004489.ref116">116</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref117">117</xref>], liquid state networks [<xref ref-type="bibr" rid="pcbi.1004489.ref120">120</xref>] and backpropagation-decorrelation learning [<xref ref-type="bibr" rid="pcbi.1004489.ref121">121</xref>]. These learning schemes apply an input sequence to a randomly connected recurrent neural network, the “reservoir”, and learning takes place by changing connections between the reservoir neurons and the network’s output [<xref ref-type="bibr" rid="pcbi.1004489.ref122">122</xref>]. In RELEARNN learning also takes place within the RNN itself, so that the network can construct useful new representations of intermediate computational results.</p>
</sec>
<sec id="sec015">
<title>Comparsion With Other Reinforcement Learning Methods</title>
<p>Motivated by findings that midbrain dopamine neurons encode a quantitative reward prediction error [<xref ref-type="bibr" rid="pcbi.1004489.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref123">123</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref124">124</xref>], we employed temporal difference learning [<xref ref-type="bibr" rid="pcbi.1004489.ref041">41</xref>] with a SARSA style prediction error for immediate rewards. Each action is associated with an expected return value when performing this particular action in a given state. The learning algorithm tries to minimize the difference between the current estimate and the observed one. Another route of optimization has been described by policy gradient methods (such as REINFORCE) [<xref ref-type="bibr" rid="pcbi.1004489.ref125">125</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref126">126</xref>] which directly optimize the expected reward instead of the value function. The learning rule in this case reads Δ<italic>W</italic><sub><italic>kl</italic></sub> = <italic>η</italic>(<italic>ϱ</italic> − <italic>b</italic>) ⋅ ∂ log(<italic>ϕ</italic><sub><italic>l</italic></sub>)/∂<italic>W</italic><sub><italic>kl</italic></sub>, where <italic>b</italic> denotes an arbitrary reinforcement baseline and <italic>ϕ</italic><sub><italic>l</italic></sub> denotes the probability of selecting action <italic>l</italic> (e.g. <xref ref-type="disp-formula" rid="pcbi.1004489.e008">Eq (5)</xref>). Note that this equation looks similar to <xref ref-type="disp-formula" rid="pcbi.1004489.e020">Eq (8)</xref> at first sight but results in an entirely different update rule. For the softmax rule <xref ref-type="disp-formula" rid="pcbi.1004489.e008">Eq (5)</xref>, for example, the derivative is given by
<disp-formula id="pcbi.1004489.e063"><alternatives><graphic id="pcbi.1004489.e063g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e063"/><mml:math id="M63" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>φ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1004489.e064"><alternatives><graphic id="pcbi.1004489.e064g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e064"/><mml:math id="M64" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>∑</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo></mml:mrow> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula> <disp-formula id="pcbi.1004489.e065"><alternatives><graphic id="pcbi.1004489.e065g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004489.e065"/><mml:math id="M65" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>τ</mml:mi></mml:mfrac> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula>
The derivatives ∂/∂<italic>W</italic><sub><italic>kl</italic></sub>[<italic>p</italic><sub><italic>l</italic></sub> − <italic>p</italic><sub><italic>j</italic></sub>] are given by <xref ref-type="disp-formula" rid="pcbi.1004489.e038">Eq (18)</xref> but it is unclear how to derive an equation like <xref ref-type="disp-formula" rid="pcbi.1004489.e057">Eq (29)</xref> to avoid the calculation of the matrix inverse and the non-local sum over all units. Additionally, policy gradient has been shown not to work in all cases. A previous study [<xref ref-type="bibr" rid="pcbi.1004489.ref127">127</xref>], for example, had to add Hebbian terms in order to solve a particular task which are already part of RELEARNN (c.f. <xref ref-type="disp-formula" rid="pcbi.1004489.e020">Eq (8)</xref>).</p>
</sec>
<sec id="sec016">
<title>Previous Models of Perceptual Grouping</title>
<p>We used RELEARNN to train networks to perform tasks that require perceptual grouping of image elements. It is therefore of interest to compare our results to previous models addressing the grouping of image elements into surfaces and boundaries (see e.g. [<xref ref-type="bibr" rid="pcbi.1004489.ref128">128</xref>], for a review). Previous models examined the contributions of horizontal connections (e.g. within area V1) to perceptual grouping by enhancing the responses elicited by colinear contour elements [<xref ref-type="bibr" rid="pcbi.1004489.ref129">129</xref>–<xref ref-type="bibr" rid="pcbi.1004489.ref131">131</xref>]. Other models aimed to explain how multiple cortical modules interact to enable the correct interpretation of a visual scene [<xref ref-type="bibr" rid="pcbi.1004489.ref132">132</xref>] or how the modification of long range horizontal connections by top-down interaction can account for such findings [<xref ref-type="bibr" rid="pcbi.1004489.ref022">22</xref>]. These previous models proposed various network structures, whereas the present study investigated how these grouping operations can be learned. We started from networks with random connectivity, and investigated learning if the only feedback from the environment is a reward upon correct task performance, and we thereby complemented previous studies addressing the role of visual experience in the tuning of cortico-cortical connections [<xref ref-type="bibr" rid="pcbi.1004489.ref114">114</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref133">133</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref134">134</xref>].</p>
<p>The present results are also relevant for a current debate about the locus of perceptual learning. A few days of experience with the contour grouping task improves the performance of monkey and human subjects [<xref ref-type="bibr" rid="pcbi.1004489.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref135">135</xref>]. On the one hand, it has been suggested that these improvements are the result of a more efficient read out of sensory representations so that the sensory representations themselves can remain relatively stable [<xref ref-type="bibr" rid="pcbi.1004489.ref136">136</xref>]. On the other hand, other studies provided compelling evidence that the early sensory representations themselves can also be subject to plasticity in the adult [<xref ref-type="bibr" rid="pcbi.1004489.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref137">137</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref138">138</xref>]. RELEARNN permits the modification of connections at multiple stages. The connections from the association layer to the motor selection stage exhibit plasticity. However, synaptic changes can also occur at earlier processing levels when the synaptic modifications in higher areas do not suffice to solve the task, in accordance with previous theoretical proposals of a reversed hierarchy in perceptual learning [<xref ref-type="bibr" rid="pcbi.1004489.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref139">139</xref>].</p>
</sec>
<sec id="sec017">
<title>Possible Improvements and Extensions</title>
<p>One limitation of the model is that learning cannot occur in networks that fail to reach a steady state. Although we did not encounter unstable states in our simulations, strong excitatory-inhibitory (E-I) interactions can lead to limit-cycles [<xref ref-type="bibr" rid="pcbi.1004489.ref140">140</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref141">141</xref>]. Possible extensions of the model might permit learning while the system is oscillating, although this has not yet been explored by us. Another limitation is that we only considered tasks with immediate reward and did not simulate delayed reward tasks where the model has to go through several states in the environment before it can obtain the reward. It is possible to use biologically plausible learning rules with the same two factors, i.e. feedback from the response selection stage in combination with a reward-prediction error, to solve delayed reward tasks [<xref ref-type="bibr" rid="pcbi.1004489.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref036">36</xref>]. These approaches increase the learning rate by incorporating information about previous activations as synaptic eligibility traces and may thereby expand the capabilities of RELEARNN. More precisely, after selecting an action the model would need to store the correlation between the activity of the regular and the accessory units in some form of synaptic tag. Upon reward delivery the neuromodulator that provides the global learning signal could interact with this tag to shape the network (c.f. [<xref ref-type="bibr" rid="pcbi.1004489.ref036">36</xref>]). In the simulations we chose a minimal model architecture that was motivated by findings of modulatory feedback interactions [<xref ref-type="bibr" rid="pcbi.1004489.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004489.ref142">142</xref>]. It will be of interest to systematically investigate further model variants with, for example, additional layers so that the networks can also learn to optimize the sensory features. Future studies can also address networks where feedback connections drive excitatory and/or inhibitory units at lower network levels instead of only modulating them multiplicatively [<xref ref-type="bibr" rid="pcbi.1004489.ref143">143</xref>]. In addition, future studies could incorporate more realistic measures for propagation and synaptic integration delays in the networks to enable a more precise comparison between activity propagation in the networks and the spread of neuronal activity in the visual cortex as well as the pattern of reaction times of human observers who carry out the curve-tracing task.</p>
</sec>
<sec id="sec018">
<title>Conclusion</title>
<p>We have presented a new learning rule that explains how recurrent neural networks can learn to group image elements during contour linking and curve-tracing. The correspondence between the simulation results and the neurophysiological data obtained in monkeys trained on these tasks suggests that the proposed learning algorithm captures the influence of learning on interactions between neurons in the visual cortex. RELEARNN explains how a neural network can learn incremental, transitive grouping operations by the spread of enhanced neuronal activity by trial and error and how it can exploit these grouping results to guide behavior. We anticipate that future studies may build on these results in the search for even more powerful methods to impose structure on incoming sensory data.</p>
</sec>
</sec>
<sec id="sec019">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004489.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004489.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Simulation parameters and details of the derivation.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Dr. Friedhelm Schwenker for pointing us to the Almeida-Pineda algorithm and we thank Wu Li and Sander Bohte for helpful comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004489.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mishra</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Aloimonos</surname> <given-names>Y</given-names></name> (<year>2009</year>) <article-title>Active Segmentation</article-title>. <source>IJHR</source> <volume>6</volume>: <fpage>361</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0219843609001784" xlink:type="simple">10.1142/S0219843609001784</ext-link></comment> <object-id pub-id-type="pmid">20686671</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Wolfe</surname> <given-names>JM</given-names></name> (<year>2007</year>) <chapter-title>Guided Search 4.0: Current Progress With a Model of Visual Search</chapter-title>. In: <name name-style="western"><surname>Gray</surname> <given-names>W</given-names></name>, editor, <source>Integrated Models of Cognitive Systems</source>, <publisher-name>Oxford University Press</publisher-name>, chapter 8. pp. <fpage>99</fpage>–<lpage>119</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Treisman</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Gelade</surname> <given-names>G</given-names></name> (<year>1980</year>) <article-title>A Feature–Integration Theory of Attention</article-title>. <source>Cognitive Psychology</source> <volume>12</volume>: <fpage>97</fpage>–<lpage>136</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0285(80)90005-5" xlink:type="simple">10.1016/0010-0285(80)90005-5</ext-link></comment> <object-id pub-id-type="pmid">7351125</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Julesz</surname> <given-names>B</given-names></name> (<year>1983</year>) <article-title>Parallel Versus Serial Processing in Rapid Pattern Discrimination</article-title>. <source>Nature</source> <volume>303</volume>: <fpage>696</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/303696a0" xlink:type="simple">10.1038/303696a0</ext-link></comment> <object-id pub-id-type="pmid">6855915</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cavanagh</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Arguin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Treisman</surname> <given-names>A</given-names></name> (<year>1990</year>) <article-title>Effect of Surface Medium on Visual Search for Orientation and Size Features</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>16</volume>: <fpage>479</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-1523.16.3.479" xlink:type="simple">10.1037/0096-1523.16.3.479</ext-link></comment> <object-id pub-id-type="pmid">2144565</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Levick</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Cleland</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Dubin</surname> <given-names>MW</given-names></name> (<year>1972</year>) <article-title>Lateral Geniculate Neurons of Cat: Retinal Inputs and Physiology</article-title>. <source>Investigative Ophthalmology &amp; Visual Science</source> <volume>11</volume>: <fpage>302</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Przybyszewski</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Gaska</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Foote</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pollen</surname> <given-names>DA</given-names></name> (<year>2000</year>) <article-title>Striate Cortex Increases Contrast Gain of Macaque LGN Neurons</article-title>. <source>Visual Neuroscience</source> <volume>17</volume>: <fpage>485</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0952523800174012" xlink:type="simple">10.1017/S0952523800174012</ext-link></comment> <object-id pub-id-type="pmid">11016570</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2006</year>) <article-title>Cortical Algorithms for Perceptual Grouping</article-title>. <source>Annual Review of Neuroscience</source> <volume>29</volume>: <fpage>203</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.29.051605.112939" xlink:type="simple">10.1146/annurev.neuro.29.051605.112939</ext-link></comment> <object-id pub-id-type="pmid">16776584</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kirchner</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name> (<year>2006</year>) <article-title>Ultra–Rapid Object Detection with Saccadic Eye Movements: Visual Processing Speed Revisited</article-title>. <source>Vision Research</source> <volume>46</volume>: <fpage>1762</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2005.10.002" xlink:type="simple">10.1016/j.visres.2005.10.002</ext-link></comment> <object-id pub-id-type="pmid">16289663</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name> (<year>1996</year>) <article-title>Speed of Processing in the Human Visual System</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>520</fpage>–<lpage>2</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381520a0" xlink:type="simple">10.1038/381520a0</ext-link></comment> <object-id pub-id-type="pmid">8632824</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Tolboom</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name> (<year>2007</year>) <article-title>Different Processing Phases for Features, Figures, and Selective Attention in the Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>785</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.10.006" xlink:type="simple">10.1016/j.neuron.2007.10.006</ext-link></comment> <object-id pub-id-type="pmid">18054856</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name> (<year>1984</year>) <article-title>Visual Routines</article-title>. <source>Cognition</source> <volume>18</volume>: <fpage>97</fpage>–<lpage>159</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0277(84)90023-4" xlink:type="simple">10.1016/0010-0277(84)90023-4</ext-link></comment> <object-id pub-id-type="pmid">6543165</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2005</year>) <article-title>Elemental Operations in Vision</article-title>. <source>TiCS</source> <volume>9</volume>: <fpage>226</fpage>–<lpage>33</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Piëch</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name> (<year>2008</year>) <article-title>Learning to Link Visual Contours</article-title>. <source>Neuron</source> <volume>57</volume>: <fpage>442</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.12.011" xlink:type="simple">10.1016/j.neuron.2007.12.011</ext-link></comment> <object-id pub-id-type="pmid">18255036</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Piëch</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name> (<year>2006</year>) <article-title>Contour Saliency in Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>50</volume>: <fpage>951</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.04.035" xlink:type="simple">10.1016/j.neuron.2006.04.035</ext-link></comment> <object-id pub-id-type="pmid">16772175</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rasch</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Xiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>M</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Perceptual Training Continuously Refines Neuronal Population Codes in Primary Visual Cortex</article-title>. <source>Nature Neuroscience</source> <volume>17</volume>: <fpage>1380</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3805" xlink:type="simple">10.1038/nn.3805</ext-link></comment> <object-id pub-id-type="pmid">25195103</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pooresmaeili</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poort</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2014</year>) <article-title>Simultaneous Selection by Object–Based Attention in Visual and Frontal Cortex</article-title>. <source>PNAS</source> <volume>11</volume>: <fpage>6467</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1316181111" xlink:type="simple">10.1073/pnas.1316181111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jolicoeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mackay</surname> <given-names>M</given-names></name> (<year>1991</year>) <article-title>Visual Curve Tracing Properties</article-title>. <source>Journal of Experimental Psychology</source> <volume>17</volume>: <fpage>997</fpage>–<lpage>1022</lpage>. <object-id pub-id-type="pmid">1837310</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jolicoeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ingleton</surname> <given-names>M</given-names></name> (<year>1991</year>) <article-title>Size Invariance in Curve Tracing</article-title>. <source>Memory &amp; Cognition</source> <volume>19</volume>: <fpage>21</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03198493" xlink:type="simple">10.3758/BF03198493</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nienborg</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2015</year>) <article-title>Belief States as a Framework to Explain Extra–Retinal Influences in Visual Cortex</article-title>. <source>Current Opinion in Neurobiology</source> <volume>32</volume>: <fpage>45</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2014.10.013" xlink:type="simple">10.1016/j.conb.2014.10.013</ext-link></comment> <object-id pub-id-type="pmid">25463564</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name> (<year>2001</year>) <article-title>The Representation of Erroneously Perceived Stimuli in the Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>31</volume>: <fpage>853</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00408-1" xlink:type="simple">10.1016/S0896-6273(01)00408-1</ext-link></comment> <object-id pub-id-type="pmid">11567622</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Piëch</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Reeke</surname> <given-names>GN</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name> (<year>2013</year>) <article-title>Network Model of Top–Down Influences on Local Gain and Contextual Interactions in Visual Cortex</article-title>. <source>PNAS</source> <volume>110</volume>: <fpage>E4108</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1317019110" xlink:type="simple">10.1073/pnas.1317019110</ext-link></comment> <object-id pub-id-type="pmid">24101495</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name> (<year>1999</year>) <article-title>Contextual Influences in V1 as a Basis for Pop Out and Asymmetry in Visual Search</article-title>. <source>Proceedings of the National Academy of Sciences of the USA</source> <volume>96</volume>: <fpage>10530</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.96.18.10530" xlink:type="simple">10.1073/pnas.96.18.10530</ext-link></comment> <object-id pub-id-type="pmid">10468643</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gintautas</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ham</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Kunsberg</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Barr</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Brumby</surname> <given-names>SP</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Model Cortical Association Fields Account for the Time Course and Dependence on Target Complexity of Human Contour Perception</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002162</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002162" xlink:type="simple">10.1371/journal.pcbi.1002162</ext-link></comment> <object-id pub-id-type="pmid">21998562</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Raizada</surname> <given-names>RDS</given-names></name> (<year>2000</year>) <article-title>Contrast–Sensitive Perceptual Grouping and Object–Based Attention in the Laminar Circuits of Primary Visual Cortex</article-title>. <source>Vision Research</source> <volume>40</volume>: <fpage>1413</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(99)00229-1" xlink:type="simple">10.1016/S0042-6989(99)00229-1</ext-link></comment> <object-id pub-id-type="pmid">10788649</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dosher</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>ZL</given-names></name> (<year>1998</year>) <article-title>Perceptual Learning Reflects External Noise Filtering and Internal Noise Reduction through Channel Reweighting</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>95</volume>: <fpage>13988</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.95.23.13988" xlink:type="simple">10.1073/pnas.95.23.13988</ext-link></comment> <object-id pub-id-type="pmid">9811913</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fahle</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Perceptual Learning: Specificity versus Generalization</article-title>. <source>Current Opinion in Neurobiology</source> <volume>15</volume>: <fpage>154</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2005.03.010" xlink:type="simple">10.1016/j.conb.2005.03.010</ext-link></comment> <object-id pub-id-type="pmid">15831396</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hegdé</surname> <given-names>J</given-names></name> (<year>2006</year>) <article-title>Search for the Neural Correlates of Learning to Discriminate Orientations</article-title>. <source>Journal of Neuroscience</source> <volume>26</volume>: <fpage>8877</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3012-06.2006" xlink:type="simple">10.1523/JNEUROSCI.3012-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16948192</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nahum</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name> (<year>2009</year>) <article-title>Reverse Hierarchies and Sensory Learning</article-title>. <source>Phil Trans R Soc B</source> <volume>364</volume>: <fpage>285</fpage>–<lpage>99</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2008.0253" xlink:type="simple">10.1098/rstb.2008.0253</ext-link></comment> <object-id pub-id-type="pmid">18986968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Logan</surname> <given-names>GD</given-names></name> (<year>2008</year>) <article-title>Object–Based Attention in Chinese Readers of Chinese Words: Beyond Gestalt Principles</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>15</volume>: <fpage>945</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/PBR.15.5.945" xlink:type="simple">10.3758/PBR.15.5.945</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zhao</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cosman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Vatterott</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Vecera</surname> <given-names>SP</given-names></name> (<year>2014</year>) <article-title>Visual Statistical Learning Can Drive Object–Based Attentional Selection</article-title>. <source>Attention, Perception, &amp; Psychophysics</source> <volume>76</volume>: <fpage>2240</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13414-014-0708-1" xlink:type="simple">10.3758/s13414-014-0708-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vecera</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Farah</surname> <given-names>M</given-names></name> (<year>1997</year>) <article-title>Is Visual Image Segmentation a Bottom–up or an Interactive Process?</article-title> <source>Perception &amp; Psychophysics</source> <volume>59</volume>: <fpage>1280</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03214214" xlink:type="simple">10.3758/BF03214214</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Korjoukov</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Jeurissen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kloosterman</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Verhoeven</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The Time Course of Perceptual Grouping in Natural Scenes</article-title>. <source>Psychological Science</source> <volume>23</volume>: <fpage>1482</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797612443832" xlink:type="simple">10.1177/0956797612443832</ext-link></comment> <object-id pub-id-type="pmid">23137967</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name> (<year>2005</year>) <article-title>Attention–Gated Reinforcement Learning of Internal Representations for Classification</article-title>. <source>Neural Computation</source> <volume>17</volume>: <fpage>2176</fpage>–<lpage>214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766054615699" xlink:type="simple">10.1162/0899766054615699</ext-link></comment> <object-id pub-id-type="pmid">16105222</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="other">Rombouts JO, Bohte SM, Roelfsema PR (2012) Neurally Plausible Reinforcement Learning of Working Memory Tasks. In: NIPS. pp. 1880–8.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rombouts</surname> <given-names>JO</given-names></name>, <name name-style="western"><surname>Bohte</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2015</year>) <article-title>How Attention Can Create Synaptic Tags for the Learning of Working Memories in Sequential Tasks</article-title>. <source>PLoS Computational Biology</source> <volume>11</volume>: <fpage>e1004060</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004060" xlink:type="simple">10.1371/journal.pcbi.1004060</ext-link></comment> <object-id pub-id-type="pmid">25742003</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>2002</year>) <article-title>Getting Formal with Dopamine and Reward</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>241</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00967-4" xlink:type="simple">10.1016/S0896-6273(02)00967-4</ext-link></comment> <object-id pub-id-type="pmid">12383780</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kilgard</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Merzenich</surname> <given-names>MM</given-names></name> (<year>1998</year>) <article-title>Cortical Map Reorganization Enabled by Nucleus Basalis Activity</article-title>. <source>Science</source> <volume>279</volume>: <fpage>1714</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.279.5357.1714" xlink:type="simple">10.1126/science.279.5357.1714</ext-link></comment> <object-id pub-id-type="pmid">9497289</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Dorsal Raphe Neurons Signal Reward through 5–HT and Glutamate</article-title>. <source>Neuron</source> <volume>81</volume>: <fpage>1360</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.02.010" xlink:type="simple">10.1016/j.neuron.2014.02.010</ext-link></comment> <object-id pub-id-type="pmid">24656254</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name> (<year>2003</year>) <article-title>Subtask Sequencing in the Primary Visual Cortex</article-title>. <source>PNAS</source> <volume>100</volume>: <fpage>5467</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0431051100" xlink:type="simple">10.1073/pnas.0431051100</ext-link></comment> <object-id pub-id-type="pmid">12695564</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name> (<year>1998</year>) <source>Reinforcement Learning: An Introduction</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Burkhalter</surname> <given-names>A</given-names></name> (<year>1996</year>) <article-title>Different Balance of Excitation and Inhibition in Forward and Feedback Circuits of Rat Visual Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>16</volume>: <fpage>7353</fpage>–<lpage>65</lpage>. <object-id pub-id-type="pmid">8929442</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Guillery</surname> <given-names>RW</given-names></name> (<year>1998</year>) <article-title>On the Actions that One Nerve Cell can Have on Another: Distinguishing “Drivers” from “Modulators”</article-title>. <source>PNAS</source> <volume>95</volume>: <fpage>7121</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.95.12.7121" xlink:type="simple">10.1073/pnas.95.12.7121</ext-link></comment> <object-id pub-id-type="pmid">9618549</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Larkum</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Lüscher</surname> <given-names>HR</given-names></name> (<year>2004</year>) <article-title>Top–Down Dendritic Input Increases the Gain of Layer 5 Pyramidal Neurons</article-title>. <source>Cerebral Cortex</source> <volume>14</volume>: <fpage>1059</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhh065" xlink:type="simple">10.1093/cercor/bhh065</ext-link></comment> <object-id pub-id-type="pmid">15115747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bonin</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>The Suppressive Field of Neurons in Lateral Geniculate Nucleus</article-title>. <source>Journal of Neuroscience</source> <volume>25</volume>: <fpage>10844</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3562-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3562-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16306397</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Spratling</surname> <given-names>MW</given-names></name> (<year>2014</year>) <article-title>A Single Functional Model of Drivers and Modulators in Cortex</article-title>. <source>Journal of Computational Neuroscience</source> <volume>36</volume>: <fpage>97</fpage>–<lpage>118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-013-0471-7" xlink:type="simple">10.1007/s10827-013-0471-7</ext-link></comment> <object-id pub-id-type="pmid">23818068</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bayerl</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2004</year>) <article-title>Disambiguating Visual Motion Through Contextual Feedback Modulation</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>2041</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766041732404" xlink:type="simple">10.1162/0899766041732404</ext-link></comment> <object-id pub-id-type="pmid">15333206</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Weidenbacher</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Bayerl</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fleming</surname> <given-names>R</given-names></name> (<year>2006</year>) <article-title>Sketching Shiny Surfaces: 3D Shape Extraction and Depiction of Specular Surfaces</article-title>. <source>ACM Trans on Applied Perception</source> <volume>3</volume>: <fpage>262</fpage>–<lpage>285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/1166087.1166094" xlink:type="simple">10.1145/1166087.1166094</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Weidenbacher</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2009</year>) <article-title>Extraction of Surface–Related Features in a Recurrent Model of V1–V2 Interactions</article-title>. <source>PloS ONE</source> <volume>4</volume>: <fpage>e5909</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0005909" xlink:type="simple">10.1371/journal.pone.0005909</ext-link></comment> <object-id pub-id-type="pmid">19526061</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Raudies</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2010</year>) <article-title>A Neural Model of the Temporal Dynamics of Figure–Ground Segregation in Motion Perception</article-title>. <source>Neural Networks</source> <volume>23</volume>: <fpage>160</fpage>–<lpage>176</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2009.10.005" xlink:type="simple">10.1016/j.neunet.2009.10.005</ext-link></comment> <object-id pub-id-type="pmid">19931405</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bouecke</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Tlapale</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kornprobst</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2011</year>) <article-title>Neural Mechanisms of Motion Detection, Integration, and Segregation: From Biology to Artificial Image Processing Systems</article-title>. <source>EURASIP JASP</source> <volume>2011</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Raudies</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Mingolla</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2011</year>) <article-title>A Model of Motion Transparency Processing with Local Center–Surround Interactions and Feedback</article-title>. <source>Neural Computation</source> <volume>23</volume>: <fpage>2868</fpage>–<lpage>914</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00193" xlink:type="simple">10.1162/NECO_a_00193</ext-link></comment> <object-id pub-id-type="pmid">21851277</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="other">Brosch T, Neumann H (2012) The Brain’s Sequential Parallelism: Perceptual Decision–Making and Early Sensory Responses. In: ICONIP (Part II). volume 7664 of <italic>LNCS</italic>, pp. 41–50.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brosch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2014</year>) <article-title>Interaction of Feedforward and Feedback Streams in Visual Cortex in a Firing–Rate Model of Columnar Computations</article-title>. <source>Neural Networks</source> <volume>54</volume>: <fpage>11</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2014.02.005" xlink:type="simple">10.1016/j.neunet.2014.02.005</ext-link></comment> <object-id pub-id-type="pmid">24632344</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brosch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2014</year>) <article-title>Computing with a Canonical Neural Circuits Model with Pool Normalization and Modulating Feedback</article-title>. <source>Neural Computation</source> <volume>26</volume>: <fpage>2735</fpage>–<lpage>89</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00675" xlink:type="simple">10.1162/NECO_a_00675</ext-link></comment> <object-id pub-id-type="pmid">25248083</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brosch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tschechne</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name> (<year>2015</year>) <article-title>On Event–Based Optical Flow Detection</article-title>. <source>Frontiers in Neuroscience</source> <volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnins.2015.00137" xlink:type="simple">10.3389/fnins.2015.00137</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nowlan</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name> (<year>1995</year>) <article-title>A Selection Model for Motion Processing in Area MT of Primates</article-title>. <source>Journal of Neuroscience</source> <volume>15</volume>: <fpage>1195</fpage>–<lpage>1214</lpage>. <object-id pub-id-type="pmid">7869094</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wiering</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schmidthuber</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>HQ–Learning</article-title>. <source>Adaptive Behavior</source> <volume>6</volume>: <fpage>219</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/105971239700600202" xlink:type="simple">10.1177/105971239700600202</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Malenka</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Bear</surname> <given-names>MF</given-names></name> (<year>2004</year>) <article-title>LTP and LTD: An Embarrassment of Riches</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>5</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2004.09.012" xlink:type="simple">10.1016/j.neuron.2004.09.012</ext-link></comment> <object-id pub-id-type="pmid">15450156</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Derkach</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Oh</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Guire</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>Soderling</surname> <given-names>TR</given-names></name> (<year>2007</year>) <article-title>Regulatory Mechanisms of AMPA Receptors in Synaptic Plasticity</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>: <fpage>101</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2055" xlink:type="simple">10.1038/nrn2055</ext-link></comment> <object-id pub-id-type="pmid">17237803</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Friedrich</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2011</year>) <article-title>Spatio–Temporal Credit Assignment in Neuronal Population Learning</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002092</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002092" xlink:type="simple">10.1371/journal.pcbi.1002092</ext-link></comment> <object-id pub-id-type="pmid">21738460</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schall</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>KG</given-names></name> (<year>1999</year>) <article-title>Neural Selection and Control of Visually Guided Eye Movements</article-title>. <source>Annual Reviews of Neuroscience</source> <volume>22</volume>: <fpage>241</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.22.1.241" xlink:type="simple">10.1146/annurev.neuro.22.1.241</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Snyder</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Bradley</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Xing</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>Multimodal Representation of Space in the Posterior Parietal Cortex and its Use in Planning Movements</article-title>. <source>Annual Review of Neuroscience</source> <volume>20</volume>: <fpage>303</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.20.1.303" xlink:type="simple">10.1146/annurev.neuro.20.1.303</ext-link></comment> <object-id pub-id-type="pmid">9056716</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Steinmetz</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>T</given-names></name> (<year>2014</year>) <article-title>Eye Movement Preparation Modulates Neuronal Responses in Area V4 When Dissociated from Attentional Demands</article-title>. <source>Neuron</source> <volume>83</volume>: <fpage>496</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.06.014" xlink:type="simple">10.1016/j.neuron.2014.06.014</ext-link></comment> <object-id pub-id-type="pmid">25033188</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Poort</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Raudies</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wannig</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The Role of Attention in Figure–Ground Segregation in Areas V1 and V4 of the Visual Cortex</article-title>. <source>Neuron</source> <volume>75</volume>: <fpage>143</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.032" xlink:type="simple">10.1016/j.neuron.2012.04.032</ext-link></comment> <object-id pub-id-type="pmid">22794268</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name> (<year>1995</year>) <article-title>Sequence Seeking and Counter Streams: A Computational Model for Bidirectional Information Flow in the Visual Cortex</article-title>. <source>Cerebral Cortex</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/5.1.1" xlink:type="simple">10.1093/cercor/5.1.1</ext-link></comment> <object-id pub-id-type="pmid">7719126</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markov</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Ercsey-Ravasz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Toroczkai</surname> <given-names>Z</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Cortical High–Density Counterstream Architectures</article-title>. <source>Science</source> <volume>342</volume>: <fpage>578</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1238406" xlink:type="simple">10.1126/science.1238406</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref068">
<label>68</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markov</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Kennedy</surname> <given-names>H</given-names></name> (<year>2013</year>) <article-title>The Importance of Being Hierarchical</article-title>. <source>Current Opinion in Neurobiology</source> <volume>23</volume>: <fpage>187</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2012.12.008" xlink:type="simple">10.1016/j.conb.2012.12.008</ext-link></comment> <object-id pub-id-type="pmid">23339864</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref069">
<label>69</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markov</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Vezoli</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chameau</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Falchier</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Quilodran</surname> <given-names>R</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Anatomy of Hierarchy: Feedforward and Feedback Pathways in Macaque Visual Cortex</article-title>. <source>The Journal of Camparative Neurology</source> <volume>522</volume>: <fpage>225</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.23458" xlink:type="simple">10.1002/cne.23458</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref070">
<label>70</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Culhane</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wai</surname> <given-names>WYK</given-names></name>, <name name-style="western"><surname>Lai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>N</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Modeling Visual Attention via Selective Tuning</article-title>. <source>Artificial Intelligence</source> <volume>78</volume>: <fpage>507</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0004-3702(95)00025-9" xlink:type="simple">10.1016/0004-3702(95)00025-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref071">
<label>71</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name> (<year>2005</year>) <chapter-title>The Selective Tuning Model for Visual Attention</chapter-title>. In: <name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name>, editors, <source>Neurobiology of Attention</source>, <publisher-name>Elsevier</publisher-name>, chapter 92. p. <fpage>562</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref072">
<label>72</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cutzu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name> (<year>2003</year>) <article-title>The Selective Tuning Model of Attention: Psychophysical Evidence for a Suppressive Annulus Around an Attended Item</article-title>. <source>Vision Research</source> <volume>43</volume>: <fpage>205</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(02)00491-1" xlink:type="simple">10.1016/S0042-6989(02)00491-1</ext-link></comment> <object-id pub-id-type="pmid">12536142</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref073">
<label>73</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>De Pasquale</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name> (<year>2011</year>) <article-title>Synaptic Properties of Corticocortical Connections between the Primary and Secondary Visual Cortical Areas in the Mouse</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>: <fpage>16494</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3664-11.2011" xlink:type="simple">10.1523/JNEUROSCI.3664-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22090476</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref074">
<label>74</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Covic</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name> (<year>2011</year>) <article-title>Synaptic Properties of Connections between the Primary and Secondary Auditory Cortices in Mice</article-title>. <source>Cerebral Cortex</source> <volume>21</volume>: <fpage>2425</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhr029" xlink:type="simple">10.1093/cercor/bhr029</ext-link></comment> <object-id pub-id-type="pmid">21385835</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref075">
<label>75</label>
<mixed-citation xlink:type="simple" publication-type="other">Brosch T, Schwenker F, Neumann H (2013) Attention–Gated Reinforcement Learning in Neural Networks–A Unified View. In: ICANN. Springer, volume 8131 of <italic>LNCS</italic>, pp. 272–9.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref076">
<label>76</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Morris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Nevet</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Arkadir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name> (<year>2006</year>) <article-title>Midbrain Dopamine Neurons Encode Decisions for Future Action</article-title>. <source>Nature Neuroscience</source> <volume>9</volume>: <fpage>1057</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1743" xlink:type="simple">10.1038/nn1743</ext-link></comment> <object-id pub-id-type="pmid">16862149</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref077">
<label>77</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nakamura</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name> (<year>2006</year>) <article-title>Basal Ganglia Orient Eyes to Reward</article-title>. <source>Journal of Neurophysiology</source> <volume>95</volume>: <fpage>567</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00458.2005" xlink:type="simple">10.1152/jn.00458.2005</ext-link></comment> <object-id pub-id-type="pmid">16424448</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref078">
<label>78</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name> (<year>1997</year>) <article-title>A Neural Substrate of Prediction and Reward</article-title>. <source>Science</source> <volume>275</volume>: <fpage>1593</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref079">
<label>79</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name> (<year>2015</year>) <article-title>Reinforcement Learning Improves Behaviour from Evaluative Feedback</article-title>. <source>Nature</source> <volume>521</volume>: <fpage>445</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14540" xlink:type="simple">10.1038/nature14540</ext-link></comment> <object-id pub-id-type="pmid">26017443</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref080">
<label>80</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>2007</year>) <article-title>Multiple Dopamine Functions at Different Time Courses</article-title>. <source>Annual Reviews of Neuroscience</source> <volume>30</volume>: <fpage>259</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.28.061604.135722" xlink:type="simple">10.1146/annurev.neuro.28.061604.135722</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref081">
<label>81</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Hyman</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name> (<year>2004</year>) <article-title>Computational Roles for Dopamine in Behavioural Control</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>760</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03015" xlink:type="simple">10.1038/nature03015</ext-link></comment> <object-id pub-id-type="pmid">15483596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref082">
<label>82</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pineda</surname> <given-names>FJ</given-names></name> (<year>1987</year>) <article-title>Generalization of Back–Propagation to Recurrent Neural Networks</article-title>. <source>Physical Review Letters</source> <volume>59</volume>: <fpage>2229</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.59.2229" xlink:type="simple">10.1103/PhysRevLett.59.2229</ext-link></comment> <object-id pub-id-type="pmid">10035458</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref083">
<label>83</label>
<mixed-citation xlink:type="simple" publication-type="other">Almeida LB (1987) Backpropagation in Perceptrons with Feedback. In: NATO Advanced Research Workshop on Neural Computers. Springer, volume 41 of <italic>Neural Computers</italic>, pp. 199–206.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref084">
<label>84</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Crick</surname> <given-names>F</given-names></name> (<year>1989</year>) <article-title>The Recent Excitement About Neural Networks</article-title>. <source>Nature</source> <volume>337</volume>: <fpage>129</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/337129a0" xlink:type="simple">10.1038/337129a0</ext-link></comment> <object-id pub-id-type="pmid">2911347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref085">
<label>85</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name> (<year>1985</year>) <article-title>Learning by Statistical Cooperation of Self–Interested Neuron–Like Computing Elements</article-title>. <source>Human Neurobiology</source> <volume>4</volume>: <fpage>229</fpage>–<lpage>56</lpage>. <object-id pub-id-type="pmid">3915497</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref086">
<label>86</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>T</given-names></name> (<year>2010</year>) <article-title>Perceptual Learning Rules Based on Reinforcers and Attention</article-title>. <source>TiCS</source> <volume>14</volume>: <fpage>64</fpage>–<lpage>71</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref087">
<label>87</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Almeida</surname> <given-names>LB</given-names></name> (<year>1987</year>) <chapter-title>A learning Rule for Asynchronous Perceptrons with Feedback in a Combinatorial Environment</chapter-title>. In: <source>First Annual International Conference on Neural Networks</source>. <publisher-name>IEEE</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref088">
<label>88</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Almeida</surname> <given-names>LB</given-names></name> (<year>1989</year>) <chapter-title>Backpropagation in Non–Feedforward Networks</chapter-title>. In: <name name-style="western"><surname>Aleksander</surname> <given-names>I</given-names></name>, editor, <source>Neural Computing Architectures: The Design of Brain–Like Machines</source>, <publisher-name>MIT Press</publisher-name>, chapter 5. pp. <fpage>74</fpage>–<lpage>91</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref089">
<label>89</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Graham</surname> <given-names>A</given-names></name> (<year>1981</year>) <source>Kronecker Products and Matrix Calculus With Applications</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Horwood</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref090">
<label>90</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mao</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kusefoglu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hooks</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Petreanu</surname> <given-names>L</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Long–Range Neuronal Circuits Underlying the Interaction between Sensory and Motor Cortex</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>111</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.07.029" xlink:type="simple">10.1016/j.neuron.2011.07.029</ext-link></comment> <object-id pub-id-type="pmid">21982373</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref091">
<label>91</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Felleman</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name> (<year>1991</year>) <article-title>Distributed Hierarchical Processing in the Primate Cerebral Cortex</article-title>. <source>Cerebral Cortex</source> <volume>1</volume>: <fpage>1</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/1.1.1" xlink:type="simple">10.1093/cercor/1.1.1</ext-link></comment> <object-id pub-id-type="pmid">1822724</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref092">
<label>92</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name> (<year>1999</year>) <article-title>Temporal Constraints on the Grouping of Contour Segments into Spatially Extended Objects</article-title>. <source>Vision Research</source> <volume>39</volume>: <fpage>1509</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(98)00222-3" xlink:type="simple">10.1016/S0042-6989(98)00222-3</ext-link></comment> <object-id pub-id-type="pmid">10343818</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref093">
<label>93</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Houtkamp</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2003</year>) <article-title>A Gradual Spread of Attention During Mental Curve Tracing</article-title>. <source>Perception &amp; Psychophysics</source> <volume>65</volume>: <fpage>1136</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03194840" xlink:type="simple">10.3758/BF03194840</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref094">
<label>94</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Moro</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Tolboom</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2010</year>) <article-title>Neuronal Activity in the Visual Cortex Reveals the Temporal Order of Cognitive Operations</article-title>. <source>Journal of Neuroscience</source> <volume>30</volume>: <fpage>16293</fpage>–<lpage>303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1256-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1256-10.2010</ext-link></comment> <object-id pub-id-type="pmid">21123575</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref095">
<label>95</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pooresmaeili</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2014</year>) <article-title>A Growth–Cone Model for the Spread of Object–Based Attention During Contour Grouping</article-title>. <source>Current Biology</source> <volume>24</volume>: <fpage>2869</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2014.10.007" xlink:type="simple">10.1016/j.cub.2014.10.007</ext-link></comment> <object-id pub-id-type="pmid">25456446</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref096">
<label>96</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Minsky</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Papert</surname> <given-names>SA</given-names></name> (<year>1987</year>) <source>Perceptrons: An Introduction to Computational Geometry</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref097">
<label>97</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Carrasquillo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hooks</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Nerbonne</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Burkhalter</surname> <given-names>A</given-names></name> (<year>2013</year>) <article-title>Distinct Balance of Excitation and Inhibition in an Interareal Feedforward and Feedback Circuit of Mouse Visual Cortex</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>: <fpage>17373</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2515-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2515-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24174670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref098">
<label>98</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Haider</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name> (<year>2013</year>) <article-title>Inhibition Dominates Sensory Responses in the Awake Cortex</article-title>. <source>Nature</source> <volume>493</volume>: <fpage>97</fpage>–<lpage>100</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11665" xlink:type="simple">10.1038/nature11665</ext-link></comment> <object-id pub-id-type="pmid">23172139</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref099">
<label>99</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Bohte</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Spekreijse</surname> <given-names>H</given-names></name> (<year>1999</year>) <chapter-title>Algorithms for the Detection of Connectedness and Their Neural Implementation</chapter-title>. In: <name name-style="western"><surname>Burdet</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Combe</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Parodi</surname> <given-names>O</given-names></name>, editors, <source>Neuronal Information Processing</source>, <publisher-name>World Scientific</publisher-name>. pp. <fpage>81</fpage>–<lpage>103</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref100">
<label>100</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ilin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jozma</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Werbos</surname> <given-names>PJ</given-names></name> (<year>2008</year>) <article-title>Beyond Feedforward Models Trained by Backpropagation: A Practical Training Tool for a More Efficient Universal Approximator</article-title>. <source>Neural Networks, IEEE Transcactions on</source> <volume>19</volume>: <fpage>929</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNN.2008.2000396" xlink:type="simple">10.1109/TNN.2008.2000396</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref101">
<label>101</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gong</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>H</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Incremental Integration of Global Contours through Interplay between Visual Cortical Areas</article-title>. <source>Neuron</source> <volume>82</volume>: <fpage>682</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.03.023" xlink:type="simple">10.1016/j.neuron.2014.03.023</ext-link></comment> <object-id pub-id-type="pmid">24811385</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref102">
<label>102</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>W</given-names></name> (<year>1998</year>) <article-title>Detecting Connectedness</article-title>. <source>Cerebral Cortex</source> <volume>8</volume>: <fpage>385</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/8.5.385" xlink:type="simple">10.1093/cercor/8.5.385</ext-link></comment> <object-id pub-id-type="pmid">9722082</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref103">
<label>103</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yagishita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GCR</given-names></name>, <name name-style="western"><surname>Urakubo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>A Critical Time Window for Dopamine Actions on the Structural Plasticity of Dendritic Spines</article-title>. <source>Science</source> <volume>345</volume>: <fpage>1616</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1255514" xlink:type="simple">10.1126/science.1255514</ext-link></comment> <object-id pub-id-type="pmid">25258080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref104">
<label>104</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Deubel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>WX</given-names></name> (<year>1996</year>) <article-title>Saccade Target Selection and Object Recognition: Evidence for a Common Attentional Mechanism</article-title>. <source>Vision Research</source> <volume>36</volume>: <fpage>1827</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0042-6989(95)00294-4" xlink:type="simple">10.1016/0042-6989(95)00294-4</ext-link></comment> <object-id pub-id-type="pmid">8759451</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref105">
<label>105</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Baldauf</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Deubel</surname> <given-names>H</given-names></name> (<year>2008</year>) <article-title>Visual Attention During the Preparation of Bimanual Movements</article-title>. <source>Vision Research</source> <volume>48</volume>: <fpage>549</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2007.11.023" xlink:type="simple">10.1016/j.visres.2007.11.023</ext-link></comment> <object-id pub-id-type="pmid">18206205</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref106">
<label>106</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name> (<year>1993</year>) <article-title>Attentional Control of Early Perceptual Learning</article-title>. <source>PNAS</source> <volume>90</volume>: <fpage>5718</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.90.12.5718" xlink:type="simple">10.1073/pnas.90.12.5718</ext-link></comment> <object-id pub-id-type="pmid">8516322</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref107">
<label>107</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jiang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name> (<year>2001</year>) <article-title>Selective Attention Modulates Implicit Learning</article-title>. <source>Q J Exp Psychol</source> <volume>54</volume>: <fpage>1105</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/713756001" xlink:type="simple">10.1080/713756001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref108">
<label>108</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stǎnişor</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>van der Togt</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CMA</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2013</year>) <article-title>A Unified Selection Signal for Attention and Reward in Primary Visual Cortex</article-title>. <source>PNAS</source> <volume>110</volume>: <fpage>9136</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1300117110" xlink:type="simple">10.1073/pnas.1300117110</ext-link></comment> <object-id pub-id-type="pmid">23676276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref109">
<label>109</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cohen</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name> (<year>1983</year>) <article-title>Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks</article-title>. <source>Systems, Man and Cybernetics SMC–</source> <volume>13</volume>: <fpage>815</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMC.1983.6313075" xlink:type="simple">10.1109/TSMC.1983.6313075</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref110">
<label>110</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wierstra</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Förster</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peters</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name> (<year>2009</year>) <article-title>Recurrent Policy Gradients</article-title>. <source>Logic Journal of the IGPL</source> <volume>18</volume>: <fpage>620</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/jigpal/jzp049" xlink:type="simple">10.1093/jigpal/jzp049</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref111">
<label>111</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Marhon</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Cameron</surname> <given-names>CJF</given-names></name>, <name name-style="western"><surname>Kremer</surname> <given-names>SC</given-names></name> (<year>2013</year>) <chapter-title>Recurrent Neural Networks</chapter-title>. In: <name name-style="western"><surname>Bianchini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maggini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jain</surname> <given-names>LC</given-names></name>, editors, <source>Handbook on Neural Information Processing (Intelligent Systems Reference Library)</source>, <publisher-name>Springer</publisher-name>, chapter 2. pp. <fpage>29</fpage>–<lpage>66</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref112">
<label>112</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Werbos</surname> <given-names>PJ</given-names></name> (<year>1988</year>) <article-title>Generalization of Backpropagation with Application to a Recurrent Gas Market Model</article-title>. <source>Neural Networks</source> <volume>1</volume>: <fpage>339</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(88)90007-X" xlink:type="simple">10.1016/0893-6080(88)90007-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref113">
<label>113</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name> (<year>1996</year>) <article-title>Biologically Plausible Error–Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm</article-title>. <source>Neural Computation</source> <volume>8</volume>: <fpage>895</fpage>–<lpage>938</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1996.8.5.895" xlink:type="simple">10.1162/neco.1996.8.5.895</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref114">
<label>114</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name> (<year>1998</year>) <article-title>Six Principles for Biologically Based Computational Models of Cortical Cognition</article-title>. <source>TiCS</source> <volume>2</volume>: <fpage>455</fpage>–<lpage>62</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref115">
<label>115</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Munakata</surname> <given-names>Y</given-names></name> (<year>2000</year>) <source>Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain</source>. <publisher-loc>Cambridge, Mass, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref116">
<label>116</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Haas</surname> <given-names>H</given-names></name> (<year>2004</year>) <article-title>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</article-title>. <source>Science</source> <volume>304</volume>: <fpage>78</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1091277" xlink:type="simple">10.1126/science.1091277</ext-link></comment> <object-id pub-id-type="pmid">15064413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref117">
<label>117</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Manjunath</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name> (<year>2013</year>) <article-title>Echo State Property Linked to an Input: Exploring a Fundamental Characteristic of Recurrent Neural Networks</article-title>. <source>Neural Computation</source> <volume>25</volume>: <fpage>671</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00411" xlink:type="simple">10.1162/NECO_a_00411</ext-link></comment> <object-id pub-id-type="pmid">23272918</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref118">
<label>118</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>Long Short–Term Memory</article-title>. <source>Neural Computation</source> <volume>9</volume>: <fpage>1735</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1997.9.8.1735" xlink:type="simple">10.1162/neco.1997.9.8.1735</ext-link></comment> <object-id pub-id-type="pmid">9377276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref119">
<label>119</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gers</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cummins</surname> <given-names>F</given-names></name> (<year>2000</year>) <article-title>Learning to Forget: Continual Prediction with LSTM</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>2451</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300015015" xlink:type="simple">10.1162/089976600300015015</ext-link></comment> <object-id pub-id-type="pmid">11032042</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref120">
<label>120</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name> (<year>2002</year>) <article-title>Real–Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760407955" xlink:type="simple">10.1162/089976602760407955</ext-link></comment> <object-id pub-id-type="pmid">12433288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref121">
<label>121</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Steil</surname> <given-names>JJ</given-names></name> (<year>2004</year>) <chapter-title>Backpropagation–Decorrelation: Online Recurrent Learning with O(N) Complexity</chapter-title>. In: <source>IJCNN</source>. <publisher-name>IEEE</publisher-name>, volume <volume>2</volume>, pp. <fpage>843</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref122">
<label>122</label>
<mixed-citation xlink:type="simple" publication-type="other">Schrauwen B, Verstraeten D, van Campenhout J (2007) An Overview of Reservoir Computing: Theory, Applications and Implementations. In: European Symposium on Artificial Neural Networks. pp. 471–82.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref123">
<label>123</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bayer</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name> (<year>2005</year>) <article-title>Midbrain Dopamine Neurons Encode a Quantitative Reward Prediction Error Signal</article-title>. <source>Neuron</source> <volume>47</volume>: <fpage>129</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.05.020" xlink:type="simple">10.1016/j.neuron.2005.05.020</ext-link></comment> <object-id pub-id-type="pmid">15996553</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref124">
<label>124</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name> (<year>2011</year>) <article-title>Understanding Dopamine and Reinforcement Learning: The Dopamine Reward Prediction Error Hypothesis</article-title>. <source>PNAS</source> <volume>108</volume>: <fpage>15647</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1014269108" xlink:type="simple">10.1073/pnas.1014269108</ext-link></comment> <object-id pub-id-type="pmid">21389268</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref125">
<label>125</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name> (<year>1992</year>) <article-title>Simple Statistical Gradient–Following Algorithms for Connectionist Reinforcement Learning</article-title>. <source>Machine Learning</source> <volume>8</volume>: <fpage>229</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00992696" xlink:type="simple">10.1007/BF00992696</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref126">
<label>126</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>McAllester</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mansour</surname> <given-names>Y</given-names></name> (<year>2000</year>) <chapter-title>Policy Gradient Methods for Reinforcement Learning with Function Approximation</chapter-title>. In: <source>NIPS</source>. <publisher-name>MIT Press</publisher-name>, volume <volume>12</volume>, pp. <fpage>1057</fpage>–<lpage>63</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref127">
<label>127</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Frémaux</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2009</year>) <article-title>Spike–Based Reinforcement Learning in Continuous State and Action Space: When Policy Gradient Methods Fail</article-title>. <source>PLoS Computational Biology</source> <volume>5</volume>: <fpage>e1000586</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000586" xlink:type="simple">10.1371/journal.pcbi.1000586</ext-link></comment> <object-id pub-id-type="pmid">19997492</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref128">
<label>128</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yazdanbakhsh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mingolla</surname> <given-names>E</given-names></name> (<year>2007</year>) <article-title>Seeing Surfaces: The brain’s Vision of the World</article-title>. <source>Physics of Life Reviews</source> <volume>4</volume>: <fpage>189</fpage>–<lpage>222</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.plrev.2007.09.001" xlink:type="simple">10.1016/j.plrev.2007.09.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref129">
<label>129</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zhaoping</surname> <given-names>L</given-names></name> (<year>1998</year>) <article-title>A Neural Model of Contour Integration in the Primary Visual Cortex</article-title>. <source>Neural Computation</source> <volume>10</volume>: <fpage>903</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976698300017557" xlink:type="simple">10.1162/089976698300017557</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref130">
<label>130</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Sha’ashua</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name> (<year>1988</year>) <chapter-title>Structural Saliency The Detection of Globally Salient Structures Using a Locally Connected Network</chapter-title>. In: <source>ICCV</source>. <publisher-name>IEEE</publisher-name>, pp. <fpage>321</fpage>–<lpage>7</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref131">
<label>131</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ben-Shahar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Zucker</surname> <given-names>S</given-names></name> (<year>2004</year>) <article-title>Geometrical Computations Explain Projection Patterns of Long–Range Horizontal Connections in Visual Cortex</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>445</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976604772744866" xlink:type="simple">10.1162/089976604772744866</ext-link></comment> <object-id pub-id-type="pmid">15006089</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref132">
<label>132</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mingolla</surname> <given-names>E</given-names></name> (<year>1985</year>) <article-title>Neural Dynamics of Perceptual Grouping: Textures, Boundaries, and Emergent Segmentations</article-title>. <source>Perception &amp; Psychophysics</source> <volume>38</volume>: <fpage>141</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03198851" xlink:type="simple">10.3758/BF03198851</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref133">
<label>133</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Williamson</surname> <given-names>JR</given-names></name> (<year>2001</year>) <article-title>A Neural Model of how Horizontal and Interlaminar Connections of Visual Cortex Develop into Adult Circuits that Carry Out Perceptual Grouping and Learning</article-title>. <source>Cerebral Cortex</source> <volume>11</volume>: <fpage>37</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/11.1.37" xlink:type="simple">10.1093/cercor/11.1.37</ext-link></comment> <object-id pub-id-type="pmid">11113034</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref134">
<label>134</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name> (<year>2010</year>) <article-title>Statistical Models of Natural Images and Cortical Visual Representation</article-title>. <source>Topics in Cognitive Science</source> <volume>2</volume>: <fpage>251</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1756-8765.2009.01057.x" xlink:type="simple">10.1111/j.1756-8765.2009.01057.x</ext-link></comment> <object-id pub-id-type="pmid">25163788</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref135">
<label>135</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kourtzi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Betts</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Sarkheil</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name> (<year>2005</year>) <article-title>Distributed Neural Plasticity for Shape Learning in the Human Visual Cortex</article-title>. <source>PLoS Biology</source> <volume>3</volume>: <fpage>e204</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030204" xlink:type="simple">10.1371/journal.pbio.0030204</ext-link></comment> <object-id pub-id-type="pmid">15934786</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref136">
<label>136</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Petrov</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Dosher</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>ZL</given-names></name> (<year>2005</year>) <article-title>The Dynamics of Perceptual Learning: An Incremental Reweighting Model</article-title>. <source>Psychological Review</source> <volume>112</volume>: <fpage>715</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.112.4.715" xlink:type="simple">10.1037/0033-295X.112.4.715</ext-link></comment> <object-id pub-id-type="pmid">16262466</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref137">
<label>137</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schoups</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Qian</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>G</given-names></name> (<year>2001</year>) <article-title>Practising Orientation Identification Improves Orientation Coding in V1 Neurons</article-title>. <source>Nature</source> <volume>412</volume>: <fpage>549</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35087601" xlink:type="simple">10.1038/35087601</ext-link></comment> <object-id pub-id-type="pmid">11484056</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref138">
<label>138</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yang</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name> (<year>2004</year>) <article-title>The Effect of Perceptual Learning on Neuronal Responses in Monkey Visual Area V4</article-title>. <source>Journal of Neuroscience</source> <volume>24</volume>: <fpage>1617</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4442-03.2004" xlink:type="simple">10.1523/JNEUROSCI.4442-03.2004</ext-link></comment> <object-id pub-id-type="pmid">14973244</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref139">
<label>139</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name> (<year>2004</year>) <article-title>The Reverse Hierarchy Theory of Visual Perceptual Learning</article-title>. <source>TICS</source> <volume>8</volume>: <fpage>457</fpage>–<lpage>64</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004489.ref140">
<label>140</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Cowan</surname> <given-names>JD</given-names></name> (<year>1972</year>) <article-title>Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons</article-title>. <source>Biophysical Journal</source> <volume>12</volume>: <fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0006-3495(72)86068-5" xlink:type="simple">10.1016/S0006-3495(72)86068-5</ext-link></comment> <object-id pub-id-type="pmid">4332108</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref141">
<label>141</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Cowan</surname> <given-names>JD</given-names></name> (<year>1973</year>) <article-title>A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue</article-title>. <source>Kybernetik</source> <volume>13</volume>: <fpage>55</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00288786" xlink:type="simple">10.1007/BF00288786</ext-link></comment> <object-id pub-id-type="pmid">4767470</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref142">
<label>142</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>De Pasquale</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name> (<year>2012</year>) <article-title>Modulatory Effects of Metabotropic Glutamate Receptors on Local Cortical Circuits</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>: <fpage>7364</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0090-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0090-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22623682</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004489.ref143">
<label>143</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nassi</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Lomber</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Born</surname> <given-names>RT</given-names></name> (<year>2013</year>) <article-title>Corticocortical Feedback Contributes to Surround Suppression in V1 of the Alert Primate</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>: <fpage>8504</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5124-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5124-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23658187</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>