<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01680</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005517</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical mechanics</subject><subj-group><subject>Deformation</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Classical mechanics</subject><subj-group><subject>Damage mechanics</subject><subj-group><subject>Deformation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Fluorescence imaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Experimental organism systems</subject><subj-group><subject>Animal models</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Nematoda</subject><subj-group><subject>Caenorhabditis</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Molecular biology</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>DNA construction</subject><subj-group><subject>Vector construction</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>DNA construction</subject><subj-group><subject>Vector construction</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Molecular biology</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Molecular biology assays and analysis techniques</subject><subj-group><subject>Gene expression and vector techniques</subject><subj-group><subject>Vector construction</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Molecular biology assays and analysis techniques</subject><subj-group><subject>Gene expression and vector techniques</subject><subj-group><subject>Vector construction</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Microscopy</subject><subj-group><subject>Electron microscopy</subject><subj-group><subject>Transmission electron microscopy</subject><subj-group><subject>Dark field imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Calcium imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Calcium imaging</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Automatically tracking neurons in a moving and deforming brain</article-title>
<alt-title alt-title-type="running-head">Automatically tracking neurons in a moving and deforming brain</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Nguyen</surname> <given-names>Jeffrey P.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Linder</surname> <given-names>Ashley N.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Plummer</surname> <given-names>George S.</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Shaevitz</surname> <given-names>Joshua W.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5362-5093</contrib-id>
<name name-style="western">
<surname>Leifer</surname> <given-names>Andrew M.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Physics, Princeton University, Princeton, New Jersey, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, New Jersey, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Dyer</surname> <given-names>Eva</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Northwestern University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> AML JWS JPN.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> JPN.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> AML.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> JPN ANL.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> JPN ANL.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> AML.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> JPN ANL GSP.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> JPN ANL.</p>
</list-item>
<list-item>
<p><bold>Supervision:</bold> AML.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> JPN.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> JPN.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> AML JWS.</p>
</list-item>
</list>
</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: Tufts University School of Medicine, Boston, Massachusetts, United States of America</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">leifer@princeton.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>5</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>18</day>
<month>5</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>5</issue>
<elocation-id>e1005517</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>10</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Nguyen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005517"/>
<abstract>
<p>Advances in optical neuroimaging techniques now allow neural activity to be recorded with cellular resolution in awake and behaving animals. Brain motion in these recordings pose a unique challenge. The location of individual neurons must be tracked in 3D over time to accurately extract single neuron activity traces. Recordings from small invertebrates like <italic>C. elegans</italic> are especially challenging because they undergo very large brain motion and deformation during animal movement. Here we present an automated computer vision pipeline to reliably track populations of neurons with single neuron resolution in the brain of a freely moving <italic>C. elegans</italic> undergoing large motion and deformation. 3D volumetric fluorescent images of the animal’s brain are straightened, aligned and registered, and the locations of neurons in the images are found via segmentation. Each neuron is then assigned an identity using a new time-independent machine-learning approach we call Neuron Registration Vector Encoding. In this approach, non-rigid point-set registration is used to match each segmented neuron in each volume with a set of reference volumes taken from throughout the recording. The way each neuron matches with the references defines a feature vector which is clustered to assign an identity to each neuron in each volume. Finally, thin-plate spline interpolation is used to correct errors in segmentation and check consistency of assigned identities. The Neuron Registration Vector Encoding approach proposed here is uniquely well suited for tracking neurons in brains undergoing large deformations. When applied to whole-brain calcium imaging recordings in freely moving <italic>C. elegans</italic>, this analysis pipeline located 156 neurons for the duration of an 8 minute recording and consistently found more neurons more quickly than manual or semi-automated approaches.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computer algorithms for identifying and tracking neurons in images of a brain have struggled to keep pace with rapid advances in neuroimaging. In small transparent organism like the nematode <italic>C. elegans</italic>, it is now possible to record neural activity from all of the neurons in the animal’s head with single-cell resolution as it crawls. A critical challenge is to identify and track each individual neuron as the brain moves and bends. Previous methods required large amounts of manual human annotation. In this work, we present a fully automated algorithm for neuron segmentation and tracking in freely behaving <italic>C. elegans</italic>. Our approach uses non-rigid point-set registration to construct feature vectors describing the location of each neuron relative to other neurons and other volumes in the recording. Then we cluster feature vectors in a time-independent fashion to track neurons through time. This new approach works very well when compared to a human.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000893</institution-id>
<institution>Simons Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>SCGB 324285</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5362-5093</contrib-id>
<name name-style="western">
<surname>Leifer</surname> <given-names>Andrew M.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100006734</institution-id>
<institution>Princeton University</institution>
</institution-wrap>
</funding-source>
<award-id>Inagural Dean for REsearch Innovation Fund for New Ideas in the Natural Sciences</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5362-5093</contrib-id>
<name name-style="western">
<surname>Leifer</surname> <given-names>Andrew M.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100006734</institution-id>
<institution>Princeton University</institution>
</institution-wrap>
</funding-source>
<award-id>Inagural Dean for Research Innovation Fund for New Ideas in the Natural Sciences</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Shaevitz</surname> <given-names>Joshua W.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>Institutional Traning Grant to PNI</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Linder</surname> <given-names>Ashley N.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution>Swartz Foundation</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Nguyen</surname> <given-names>Jeffrey P.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award006">
<funding-source>
<institution>Glenn Foundation for Medical Research (US)</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Nguyen</surname> <given-names>Jeffrey P.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Simons Foundation Grant SCGB 324285 to AML (<ext-link ext-link-type="uri" xlink:href="https://www.simonsfoundation.org/life-sciences/simons-collaboration-global-brain/" xlink:type="simple">https://www.simonsfoundation.org/life-sciences/simons-collaboration-global-brain/</ext-link>) and Princeton University’s Inaugural Dean for Research Innovation Fund for New Ideas in the Natural Sciences to JWS and AML (<ext-link ext-link-type="uri" xlink:href="http://www.princeton.edu/main/news/archive/S39/38/59M52/index.xml?section=topstories" xlink:type="simple">http://www.princeton.edu/main/news/archive/S39/38/59M52/index.xml?section=topstories</ext-link>). JPN is supported by grants from the Swartz Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.theswartzfoundation.org/princeton.asp" xlink:type="simple">http://www.theswartzfoundation.org/princeton.asp</ext-link>) and the Glenn Foundation for Medical Research (<ext-link ext-link-type="uri" xlink:href="http://glennfoundation.org/glenn-centers/princeton/" xlink:type="simple">http://glennfoundation.org/glenn-centers/princeton/</ext-link>). ANL is supported by a National Institutes of Health institutional training grant NIH T32 MH065214 through the Princeton Neuroscience Institute. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="19"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data used in this publication have been made publicly available at the IEEE DataPort repository (DOI:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.21227/H2901H" xlink:type="simple">10.21227/H2901H</ext-link>) <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.21227/H2901H" xlink:type="simple">http://dx.doi.org/10.21227/H2901H</ext-link>. Worm strains used in this work have been made publicly available through the Caenorhabditis Genetics Center (<ext-link ext-link-type="uri" xlink:href="http://www.cgc.cbs.umn.edu/" xlink:type="simple">http://www.cgc.cbs.umn.edu/</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Optical neural imaging has ushered in a new frontier in neuroscience that seeks to understand how neural activity generates animal behavior by recording from large populations of neurons at cellular resolution in awake and behaving animals. Population recordings have now been used to elucidate mechanisms behind zebra finch song production [<xref ref-type="bibr" rid="pcbi.1005517.ref001">1</xref>], spatial encoding in mice [<xref ref-type="bibr" rid="pcbi.1005517.ref002">2</xref>], and limb movement in primates [<xref ref-type="bibr" rid="pcbi.1005517.ref003">3</xref>]. When applied to small transparent organisms, like <italic>Caenorhabditis elegans</italic> [<xref ref-type="bibr" rid="pcbi.1005517.ref004">4</xref>], <italic>Drosophila</italic> [<xref ref-type="bibr" rid="pcbi.1005517.ref005">5</xref>], and zebrafish [<xref ref-type="bibr" rid="pcbi.1005517.ref006">6</xref>], nearly every neuron in the brain can be recorded, permitting the study of whole brain neural dynamics at cellular resolution.</p>
<p>Methods for segmenting and tracking neurons have struggled to keep up as new imaging technologies now record from more neurons over longer times in environments with greater motion. Accounting for brain motion in particular has become a major challenge, especially in recordings of unrestrained animals. Brains in motion undergo translations and deformations in 3D that make robust tracking of individual neurons very difficult. The problem is compounded in invertebrates like <italic>C. elegans</italic> where the head of the animal is flexible and deforms greatly. If left unaccounted for, brain motion not only prevents tracking of neurons, but it can also introduce artifacts that mask the true neural signal. In this work we propose an automated approach to segment and track neurons in the presence of dramatic brain motion and deformation. Our approach is optimized for calcium imaging in unrestrained <italic>C. elegans</italic>.</p>
<p>Neural activity can be imaged optically with the use of genetically encoded calcium sensitive fluorescent indicators, such as GCaMP6s used in this work [<xref ref-type="bibr" rid="pcbi.1005517.ref007">7</xref>]. Historically calcium imaging was often conducted in head-fixed or anesthetized animals to avoid challenges involved with imaging moving samples [<xref ref-type="bibr" rid="pcbi.1005517.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref009">9</xref>]. Recently, however, whole-brain imaging was demonstrated in freely behaving <italic>C. elegans</italic> [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref011">11</xref>]. <italic>C. elegans</italic> are a small transparent nematode, approximately 1mm in length, with a compact nervous system of only 302 neurons. About half of the neurons are located in the animal’s head, which we refer to as its brain.</p>
<p>Analyzing fluorescent images of moving and deforming brains requires algorithms to detect neurons across time and extract fluorescent signals in 3D. Automated methods exist for segmenting and tracking fluorescently labeled cells during <italic>C. elegans</italic> embryogenesis [<xref ref-type="bibr" rid="pcbi.1005517.ref012">12</xref>], and semi-automated methods are even able to track specific cells during embryo motion [<xref ref-type="bibr" rid="pcbi.1005517.ref013">13</xref>], but to our knowledge these methods are not suitable for tracking neurons in adults. Generally, several strategies exist for tracking neurons in volumetric recordings. One approach is to find correspondences between neuron positions in consecutive time points, for example, by applying a distance minimization, and then stitching these correspondences together through time [<xref ref-type="bibr" rid="pcbi.1005517.ref014">14</xref>]. This type of time-dependent tracking requires that neuron displacements for each time step are less than the distance between neighboring neurons, and that the neurons remain identifiable at all times. If these requirements break down, even for only a few time points, errors can quickly accumulate. Other common methods, like independent component analysis (ICA) [<xref ref-type="bibr" rid="pcbi.1005517.ref015">15</xref>] are also exquisitely sensitive to motion and as a result they have not been successfully applied to recordings with large brain deformations.</p>
<p>Large inter-volume motion arises when the recorded image volume acquisition rate is too low compared to animal motion. Unfortunately, large inter-volume brain motion is likely to be a prominent feature of whole-brain recordings of moving brains for the foreseeable future. In all modern imaging approaches there is a fundamental tradeoff between the following attributes: acquisition rate (temporal resolution), spatial resolution, signal to noise, and the spatial extent of the recording. As recordings seek to capture larger brain regions at single cell resolution, they necessarily compromise on temporal resolution. For example, whole brain imaging in freely moving <italic>C. elegans</italic> has only been demonstrated at slow acquisition rates because of the requirements to scan the entire brain volume and expose each slice for sufficiently long time. At these rates, a significant amount of motion is present between image planes within a single brain volume. Similarly, large brain motions also remain between sequential volumes. Neurons can move the entire width of the worm’s head between sequential volumes when recording at 6 brain-volumes per second, as in [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. In addition to motion, the brain also bends and deforms as it moves. Such changes to the brain’s conformation greatly alter the pattern of neuron positions making constellations of neurons difficult to compare across time.</p>
<p>To track neurons in the presence of this motion, previous work that measured neural activity in freely moving <italic>C. elegans</italic> utilized semi-automated methods that required human proof reading or manual annotation to validate each and every neuron-time point [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref011">11</xref>]. This level of manual annotation becomes impractical as the length of recordings and the number of neurons increases. For example, 10 minutes of recorded neural activity from [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>], had over 360,000 neuron time points and required over 200 person-hours of proofreading and manual annotation. Here, we introduce a new time-independent algorithm that uses machine learning to automatically segment and track all neurons in the head of a freely moving animal without the need for manual annotation or proofreading. We call this technique Neuron Registration Vector Encoding, and we use it to extract neural signals in unrestrained <italic>C. elegans</italic> expressing the calcium indicator GCaMP6s and the fluorescent label RFP.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Overview of neuron tracking analysis</title>
<p>We introduce a method to track over 100 neurons in the brain of a freely moving <italic>C. elegans</italic>. The analysis pipeline is made of five modules and an overview is shown in <xref ref-type="fig" rid="pcbi.1005517.g001">Fig 1</xref>. The first three modules, “Centerline Detection,” “Straightening” and “Segmentation,” collectively assemble the individually recorded planes into a sequence of 3D volumes and identify each neuron’s location in each volume. The next two modules, “Registration Vector Construction” and “Clustering,” form the core of the method and represent a significant advance over previous approaches. Collectively, these two modules are called “Neuron Registration Vector Encoding.” The “Registration Vector Construction” module leverages information from across the entire recording in a time-independent way to generate feature vectors that characterize every neuron at every time point in relation to a repertoire of brain confirmations. The “Clustering” module then clusters these feature vectors to assign a consistent identity to each neuron across the entire recording. A final module corrects for errors that can arise from segmentation or assignment. The implementation and results of this approach are described below.</p>
<fig id="pcbi.1005517.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Schematic of analysis pipeline to segment and track neurons through time and extract their neural activity in a deforming brain.</title>
<p>Neurons are labeled with calcium insensitive red fluorescent proteins, RFP, and calcium sensitive green fluorescent proteins, GCaMP. Videos of the animal’s behavior and volumetric fluorescent images of the animal’s brain serve as input to the pipeline. The algorithm detects all neurons in the head and produces tracks of the neural activity across time as the animal moves.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Recording of whole-brain calcium activity and body posture in moving animal</title>
<p>Worms expressing the calcium indicator GCaMP6s and a calcium-insensitive fluorescent protein RFP in the nuclei of all neurons were imaged during unrestrained behavior in a custom 3D tracking microscope, as described in [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. Only signals close to the cell nuclei are measured. Two recordings are presented in this work: a new 8 minute recording of an animal of strain AML32 and a previously reported 4 minute recording of strain AML14 first described in [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>].</p>
<p>The signal of interest in both recordings is the green fluorescence intensity from GCaMP6s in each neuron. Red fluorescence from the RFP protein serves as a reference for locating and tracking the neurons. The microscope provides four raw image streams that serve as inputs for our neural tracking pipeline, seen in <xref ref-type="fig" rid="pcbi.1005517.g002">Fig 2A</xref>. They are: (1) low-magnification dark-field images of the animal’s body posture (2) low-magnification fluorescent images of the animal’s brain (3) high-magnification green fluorescent images of single optical slices of the brain showing GCaMP6s activity and (4) high-magnification red fluorescent images of single optical slices of the brain showing the location of RFP. The animal’s brain is kept centered in the field of view by realtime feedback loops that adjust a motorized stage to compensate for the animal’s crawling. To acquire volumetric information, the high magnification imaging plane scans back and forth along the axial dimension, <italic>z</italic>, at 3 Hz as shown in <xref ref-type="fig" rid="pcbi.1005517.g002">Fig 2B</xref>, acquiring roughly 33 optical slices per volume, sequentially, for 6 brain-volumes per second. The animal’s continuous motion causes each volume to be arbitrarily sheared. Although the image streams operate at different volume acquisition rates and on different clocks, they are later synchronized by flashes of light that are simultaneously visible to all cameras. Each image in each stream is given a timestamp on a common timeline for analysis. Each of the four imaging streams are spatially aligned to each other in software using affine transformations found by imaging fluorescent beads. An example of the high magnification RFP recording is shown in <xref ref-type="supplementary-material" rid="pcbi.1005517.s001">S1 Movie</xref>.</p>
<fig id="pcbi.1005517.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Input to the pipeline.</title>
<p>(A) Example images from all four video feeds from our imaging system. Both scale bars are 100<italic>μ</italic>m. Fluorescent images are shown with false coloring. (B)A schematic illustrating the timings from all the devices that run in open loop in our imaging setup. The camera that collects high magnification images captures at 200Hz. The two low magnification images capture at 60Hz, and the focal plane moves up and down in a 3 Hz triangle wave. The cameras are synchronized post-hoc using light flashes and each image is assigned a timestamp on a common timeline.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Centerline detection and gross brain alignment</title>
<p>The animal’s posture contains information about the brain’s orientation and about any deformations arising from the animal’s side-to-side head swings. The first step of the pipeline is to extract the centerline that describes the animal’s posture. Centerline detection in <italic>C. elegans</italic> is an active field of research. Most algorithms use intensity thresholds to detect the worm’s body and then use binary image operations to extract a centerline [<xref ref-type="bibr" rid="pcbi.1005517.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005517.ref018">18</xref>]. Here we use an open active contour approach [<xref ref-type="bibr" rid="pcbi.1005517.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref020">20</xref>] to extract the centerline from dark field images with modifications to account for cases when the worm’s body crosses over itself as occurs during so-called “Omega Turns.” In principle any method, automated or otherwise, that detects the centerlines should be sufficient. At rare times where the worm is coiled and the head position and orientation cannot be determined automatically, the head and the tail of the worm are manually identified.</p>
<p>The animal’s centerline allows us to correct for gross changes in the worm’s position, orientation, and conformation (<xref ref-type="fig" rid="pcbi.1005517.g003">Fig 3a</xref>). We use the centerlines determined by the low magnification behavior images to straighten the high magnification images of the worm’s brain. An affine transform must be applied to the centerline coordinates to transform them from the dark field coordinate system into the coordinate system of the high magnification images. Each image slice of the worm brain is straightened independently to account for motion within a single volume. The behavior images are taken at a lower acquisition rate than the high magnification brain images, so a linear interpolation is used to obtain a centerline for each slice of the brain volume. In each slice, we find the tangent and normal vectors at every point of the centerline (<xref ref-type="fig" rid="pcbi.1005517.g003">Fig 3b</xref>). The points are interpolated with a single pixel spacing along the centerline to preserve the resolution of the image. The image intensities along each of the normal directions are interpolated and the slices are stacked to produce a straightened image in each slice (<xref ref-type="fig" rid="pcbi.1005517.g003">Fig 3c</xref>). In the new coordinate system, the orientation of the animal is fixed and the gross deformations from the worm’s bending are suppressed. More subtle motion and deformation, however, remains.</p>
<fig id="pcbi.1005517.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Straightening and segmentation.</title>
<p>(A) Centerlines are detected from the low magnification dark field images. The centerline is shown in green and the tip of the worm’s head is indicated by a blue dot. (B) The centerline found from the low magnification image is overlaid on the high magnification RFP images. The lines normal to the centerline, shown in blue, are used to straighten the image. All scale bars are 100 <italic>μ</italic>m. (C) A maximum intensity projection of the straightened volume is shown. Individual neuronal nuclei are shown (D) before and (E) after segmentation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g003" xlink:type="simple"/>
</fig>
<p>We further reduce shearing between slices using standard video stabilization techniques [<xref ref-type="bibr" rid="pcbi.1005517.ref021">21</xref>]. Specifically, bright-intensity peaks in the images are tracked between neighboring image slices. The coordinates of these peaks are used to calculate the affine transformations between neighboring slices of the volume using least squares. All slices are registered to the middle slice by applying these transformations sequentially throughout the volume. Each slice would undergo transformations for every slice in between it and the middle slice to correct shear throughout the volume. A final rigid translation is required to align each volume to the first volume of the recording. The translations are found by finding an offset that maximizes the cross-correlation between each volume and the initial volume.</p>
<p>A video of straightening is shown in <xref ref-type="supplementary-material" rid="pcbi.1005517.s001">S1 Movie</xref>. Straightened images are used for the remaining steps of the analysis pipeline. Only the final measurement of fluorescence intensity is performed in the original unstraightened coordinated system.</p>
</sec>
<sec id="sec006">
<title>Segmentation</title>
<p>Before neuron identities can be matched across time, we must first segment the individual neurons within a volume to recovers each neuron’s size, location, and brightness (<xref ref-type="fig" rid="pcbi.1005517.g003">Fig 3d and 3e</xref>). Many algorithms have been developed to segment neurons in a dense region [<xref ref-type="bibr" rid="pcbi.1005517.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref023">23</xref>]. We segment the neurons by finding volumes of curvature in fluorescence intensity in the straigthened volumes. After an initial smoothing, we compute the 3D Hessian matrix at each point in space and threshold for points where all of the three eigenvalues of the Hessian matrix are negative. This process selects for regions around intensity peaks in three dimensions. In order to further divide regions into objects that are more likely to represent neurons, we use a watershed separation on the distance transform of the thresholded image. The distance transform is found by replacing each thresholded pixel with the Euclidean distance between it and the closest zero pixel in the thresholded image. This approach is sufficient to segment most neurons. Occasionally neurons are missed or two neurons are incorrectly merged together. These occasional errors are corrected automatically later in the pipeline.</p>
</sec>
<sec id="sec007">
<title>Neuron registration vector construction</title>
<p>Extracting neural signals requires the ability to match neurons found at different time points. Even after gross alignment and straightening, neurons in our images are still subject to local nonlinear deformations and there is significant movement of neurons between volumes. This remaining motion and deformation is clearly visible, for example, in <xref ref-type="supplementary-material" rid="pcbi.1005517.s001">S1 Movie</xref>. Rather than tracking neurons sequentially in time, the neurons in each volume are characterized based on how they match to neurons in a set of reference volumes. Our algorithm compares constellations of neurons in one volume to unannotated reference volumes and assigns correspondences or “matches” between the neurons in the sample and each reference volume. We modified a point-set registration algorithm developed by Jian and Vemuri [<xref ref-type="bibr" rid="pcbi.1005517.ref024">24</xref>] to do this (<xref ref-type="fig" rid="pcbi.1005517.g004">Fig 4a</xref>). The registration algorithm represents two point-sets, a sample point-set denoted by <bold>X</bold> = {<bold>x</bold><sub><italic>i</italic></sub>} and a reference point-set indicated by <bold>R</bold> = {<bold>r</bold><sub><italic>i</italic></sub>}, as Gaussian mixtures and then attempts to register them by deforming space to minimize the distance between the two mixtures. In their implementation, each point is modeled by a 3D Gaussian with uniform covariance. Since we are matching images of neurons rather than just points, we can use the additional information from the size and brightness of each neuron. We add this information to the representation of each neuron by adjusting the amplitude and standard deviation of the Gaussians. The Gaussian mixture representation of an image is given by,
<disp-formula id="pcbi.1005517.e001"><alternatives><graphic id="pcbi.1005517.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>A</italic><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>, and <italic>σ</italic><sub><italic>i</italic></sub> are the amplitude, mean, and standard deviation of the <italic>i</italic>-th Gaussian. These parameters are derived from the brightness, centroid, and size of the segmented neuron, while <bold><italic>ξ</italic></bold> is the 3D spatial coordinate. A scale factor <italic>λ</italic> is added to the standard deviation to scale the size of each Gaussian. This will be used later during gradient descent. The sample constellation of neurons is then represented by the Gaussian mixture <italic>f</italic>(<bold><italic>ξ</italic></bold>, <bold>X</bold>). Similarly, the reference constellation’s own neurons are represented as a <italic>f</italic>(<bold><italic>ξ</italic></bold>, <bold>R</bold>).</p>
<fig id="pcbi.1005517.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Schematic of Neuron Registration Vector Encoding.</title>
<p>(A) The registration between a sample volume and a single reference volume is done in several steps. I. The image is segmented into regions corresponding to each of the neurons. II. The image is represented as a Gaussian mixture, with a single Gaussian for each segmented region. The amplitude and the standard deviation of the Gaussians are derived from the brightness and the size of the segmented regions. III. Non-rigid point-set registration is then used to deform the sample points to best overlap the reference point-set. IV. Neurons from the sample and the reference point-sets are paired by minimizing distances between neurons. (B) Neuron registration vectors are constructed by assigning a feature vector <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> to each neuron <italic>x</italic><sub><italic>i</italic>,<italic>t</italic></sub> in a sample volume <bold>x</bold><sub><italic>t</italic></sub> by performing the registration between the sample volume and a set of 300 reference volumes, each denoted by <bold>r</bold><sup><italic>k</italic></sup>. Each registration of the neuron results in a neuron match, <inline-formula id="pcbi.1005517.e002"><alternatives><graphic id="pcbi.1005517.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msubsup><mml:mi>v</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, and the set of matches becomes the feature vector <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub>. (C) Vectors from all neuron-times are clustered into similar groups in a two step process: Hierarchical clustering (illustrated in the figure) is performed on a subset of neurons to define clusters, each of which is given a label <italic>S</italic><sub><italic>n</italic></sub>. Then each feature vector <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> is assigned to a cluster based on a distance metric (not illustrated). (D) The clustering of the feature vectors shown in (C) assigns an identity to each of the neurons in every volume. This allows us to track the neurons across different volumes of the recording.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g004" xlink:type="simple"/>
</fig>
<p>To match a sample constellation of neurons <bold>X</bold> with a reference constellation of neurons <bold>R</bold>, we use the non rigid transformation <inline-formula id="pcbi.1005517.e003"><alternatives><graphic id="pcbi.1005517.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mi>u</mml:mi> <mml:mo>:</mml:mo> <mml:mi mathvariant="normal">I</mml:mi> <mml:mspace width="-0.166667em"/><mml:msup><mml:mi mathvariant="normal">R</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mo>↦</mml:mo> <mml:mi mathvariant="normal">I</mml:mi> <mml:mspace width="-0.166667em"/><mml:msup><mml:mi mathvariant="normal">R</mml:mi> <mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. The transformation maps <bold>X</bold> to <italic>u</italic>[<bold>X</bold>] such that the <italic>L</italic><sub>2</sub> distance between <italic>f</italic>(<bold><italic>ξ</italic></bold>, <italic>u</italic>[<bold>X</bold>]) and <italic>f</italic>(<bold><italic>ξ</italic></bold>, <bold>R</bold>) is minimized with some constraint on the amount of deformation. This can be written as an energy minimization problem, with the energy of the transformation, <italic>E</italic>(<italic>u</italic>), written as
<disp-formula id="pcbi.1005517.e004"><alternatives><graphic id="pcbi.1005517.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>u</mml:mi> <mml:mo>[</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">R</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mtext>Deformation</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>Note that the point-sets <bold>X</bold> and <bold>R</bold> are allowed to have different numbers of points. We model the deformations as a thin-plate spline (TPS). The TPS transformation equations and resulting form of <italic>E</italic><sub>Deformation</sub>(<italic>u</italic>) are shown in the methods. The minimization of <italic>E</italic> is found by gradient descent. Working with Gaussian mixtures as opposed to the original images allows us to model the deformations and analytically compute the gradients of <xref ref-type="disp-formula" rid="pcbi.1005517.e004">Eq 2</xref> making gradient descent more efficient. The gradient descent approach used here is similar to that outlined by Jian and Vemuri [<xref ref-type="bibr" rid="pcbi.1005517.ref025">25</xref>]. Since the energy landscape has many local minima, we initially chose a large scale factor, <italic>λ</italic>, to increase the size of each Gaussian and smooth over smaller features. Gradient descent is iterated multiple times with <italic>λ</italic> decreasing multiple times. After the transformation, sample points are matched to reference points by minimizing distances between assigned pairs using an algorithm from [<xref ref-type="bibr" rid="pcbi.1005517.ref014">14</xref>]. The matching is not greedy, and neurons in the sample that are far from any neurons in the reference are not matched. A neuron at <bold>x</bold><sub><italic>i</italic></sub> is assigned a match <italic>v</italic><sub><italic>i</italic></sub> to indicate which neuron in the set <bold>R</bold> it was matched to. For example if <bold>x</bold><sub><italic>i</italic></sub> matched with <bold>r</bold><sub><italic>j</italic></sub> when <bold>X</bold> is registered to <bold>R</bold>, then <italic>v</italic><sub><italic>i</italic></sub> = <italic>j</italic>. If <bold>x</bold><sub><italic>i</italic></sub> has no match in <bold>R</bold>, then <italic>v</italic><sub><italic>i</italic></sub> = ∅.</p>
<p>The modified non-rigid point-set registration algorithm described above allows us to compare one constellation of neurons to another. In principle, neuron tracking could be achieved by registering the constellation of neurons at each time-volume to a single common reference. That approach is susceptible to failures in non-rigid point-set registration. Non-rigid point-set registration works well when the conformation of the animal in the sample and the reference are similar, but it is unreliable when there are large deformations between the sample and the reference, as happens with some regularity in our recordings. In addition, this approach is especially sensitive to any errors in segmentation, especially in the reference. An alternative approach would be to sequentially register neurons in each time volume to the next time-volume. This approach, however, accumulates even small errors and quickly becomes unreliable. Instead of either of those approaches, we use registration to compare the constellation of neurons at each time volume to a set of reference time-volumes that span a representative space of brain conformations (<xref ref-type="fig" rid="pcbi.1005517.g004">Fig 4b</xref>), as described below.</p>
<p>The constellation of neurons at a particular time in our recording is given by <bold>X</bold><sub><italic>t</italic></sub>, and the position of the <italic>i</italic>-th neuron at time <italic>t</italic> is denoted by <bold>x</bold><sub><italic>i</italic>,<italic>t</italic></sub>. We select a set of <italic>K</italic> reference constellations, each from a different time volume <bold>X</bold><sub><italic>t</italic></sub> in our recording, so as to achieve a representative sampling of the many different possible brain conformations the animal can attain. These <italic>K</italic> reference volumes are denoted by {<bold>R</bold><sup>1</sup>, <bold>R</bold><sup>2</sup>, <bold>R</bold><sup>3</sup>,…,<bold>R</bold><sup><italic>K</italic></sup>}. We use 300 volumes spaced evenly through time as our reference constellations. Each <bold>X</bold><sub><italic>t</italic></sub> is separately matched with each of the references, and each neuron in the sample, <bold>x</bold><sub><italic>i</italic>,<italic>t</italic></sub>, gets a set of matches <inline-formula id="pcbi.1005517.e005"><alternatives><graphic id="pcbi.1005517.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>1</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mn>3</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, one match for each of the <italic>K</italic> references. This set of matches is a feature vector which we call a Neuron Registration Vector. It describes the neuron’s location in relation to its neighbors when compared with the set of references. This vector can be used to identify neurons across different times.</p>
<p>We find that 300 reference volumes creates feature vectors that are sufficiently robust to identify neurons in our recordings. What determines the optimal number of reference volumes? As long as the reference volumes contain a representative sample of the space of brain conformation occupied during our recordings, the number of reference volumes needed to create a robust feature vector depends only on the size of this conformation space. Because the conformation space of a real brain in physiological conditions is finite, there exists some number of reference volumes beyond which adding more reference volumes provides no additional information. Crucially, the worm brain seems to explore this finite conformation space quickly relative to the time scales of our recordings. As a result, the number of required reference volumes should not depend on recording length, at least for the minutes-long timescales that we consider here.</p>
</sec>
<sec id="sec008">
<title>Clustering registration vectors</title>
<p>The neuron registration vector provides information about that neuron’s position relative to its neighbors, and how that relative position compares with many other reference volumes. A neuron with a particular identity will match similarly to the set of reference volumes and thus that neuron will have similar neuron registration vectors over time. Clustering similar registration vectors allows for the identification of that particular neuron across time (<xref ref-type="fig" rid="pcbi.1005517.g004">Fig 4c and 4d</xref>).</p>
<p>To illustrate the motivation for clustering, consider a neuron with identity <italic>s</italic> that is found at different times in two sample constellations <bold><italic>X</italic></bold><sub>1</sub> and <bold><italic>X</italic></bold><sub>2</sub>. When <bold><italic>X</italic></bold><sub>1</sub> and <bold><italic>X</italic></bold><sub>2</sub> have similar deformations, the neuron <italic>s</italic> from both constellations will be assigned the same set of matches when registered to the set of reference constellations, and as a result the corresponding neuron registration vectors <bold>v</bold><sub>1</sub> and <bold>v</bold><sub>2</sub> will be identical. This is true even if the registration algorithm itself fails to correctly match neuron <italic>s</italic> in the sample to its true neuron <italic>s</italic> in the reference. As the deformations separating <bold><italic>X</italic></bold><sub>1</sub> and <bold><italic>X</italic></bold><sub>2</sub> become larger, the distance between the feature vectors <bold>v</bold><sub>1</sub> and <bold>v</bold><sub>2</sub> also becomes larger. This is because the two samples will be matched to different neurons in some of the reference volumes as each sample is more likely to register poorly with references that are far from it in the space of deformations.</p>
<p>Crucially, the reference volumes consist of instances of the animal in many different deformation states. So while errors in registering some samples will exist for certain references, they do not persist across all references, and thus do not effect the entire feature vector. For the biologically relevant deformations that we observe, the distance between <bold>v</bold><sub>1</sub> and <bold>v</bold><sub>2</sub> will be smaller if both are derived from neuron <italic>s</italic> than compared to the distance between <bold>v</bold><sub>1</sub> and <bold>v</bold><sub>2</sub> if they were derived from <italic>s</italic> and another neuron. We can therefore cluster the feature vectors to produce groups that consist of the same neuron found at many different time points.</p>
<p>The goal of clustering is to assign each neuron at each volume to a cluster representing that neuron’s identity. Clustering is performed on the list of neuron registration vectors from all neurons at all times, {<bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub>}. Each match in the vector, <inline-formula id="pcbi.1005517.e006"><alternatives><graphic id="pcbi.1005517.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, is represented as a binary vector of 0s with a 1 at the <inline-formula id="pcbi.1005517.e007"><alternatives><graphic id="pcbi.1005517.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> position. The size of the vector is equal to the number of neurons in <bold>R</bold><sup><italic>k</italic></sup>. The feature vector {<bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub>} is the concatenation of all of the binary vectors from all matches to the <italic>K</italic> reference constellations.</p>
<p>For computational efficiency, a two-step process was used to perform the clustering: First agglomerative hierarchical clustering was used on the neurons from an initial subset of volumes to define the clusters. Next, neurons from all volumes at all times were assigned to the nearest cluster as defined by correlation distance to the clusters’ center of mass. Assignments were made in such a way so as to ensure that a given cluster is assigned to at most one neuron per volume. Details of this clustering approach are described in the methods. Each cluster is given a label {<italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub>, <italic>S</italic><sub>3</sub>,…} which uniquely identifies a single neuron over time, and each neuron at each time <bold>x</bold><sub><italic>i</italic>, <italic>t</italic></sub> is given an identifier <italic>s</italic><sub><italic>i</italic>, <italic>t</italic></sub> corresponding to the cluster to which that neuron-time belongs. Neurons that are not classified into one of these clusters are removed because they are likely artifactual or represent a neuron that is segmented too poorly for inclusion.</p>
</sec>
<sec id="sec009">
<title>Correcting errors in tracking and segmentation</title>
<p>Neuron Registration Vector Encoding successfully identifies segmented neurons consistently across time. A transient segmentation error, however, would necessarily lead to missing or misidentified neurons. To identify and correct for missing and misidentified neurons, we check each neuron’s locations and fill in missing neurons using a consensus comparison and interpolation in a TPS deformed space. For each neuron identifier <italic>s</italic> and time <italic>t</italic><sup>⋆</sup>, we use all other point-sets, {<bold>X</bold><sub><italic>t</italic></sub>} to guess what that neuron’s location might be. This is done by finding the TPS transformation, <italic>u</italic><sub><italic>t</italic>→<italic>t</italic><sup>⋆</sup></sub>: <bold>X</bold><sub><italic>t</italic></sub> ↦ <bold>X</bold><sub><italic>t</italic><sup>⋆</sup></sub>, that maps the identified points from <bold>X</bold><sub><italic>t</italic></sub> to the corresponding points in <bold>X</bold><sub><italic>t</italic><sup>⋆</sup></sub> excluding the point <italic>s</italic>. Since the correspondences between neurons has already been determined, <italic>u</italic><sub><italic>t</italic>→<italic>t</italic><sup>⋆</sup></sub> can be found by solving for the parameters from the TPS equation (see <xref ref-type="sec" rid="sec012">methods</xref>). The position estimate is then given by <italic>u</italic><sub><italic>t</italic>→<italic>t</italic><sup>⋆</sup></sub> [<bold>x</bold><sub><italic>i</italic>,<italic>t</italic></sub>] with <italic>i</italic> selected such that <italic>s<sub>i,t</sub></italic> = <italic>s</italic>. This results in a set of points representing the set of predicted locations of the neuron at time <italic>t</italic><sup>⋆</sup> as inferred from the other volumes. When a neuron identifier is missing for a given time, the position of that neuron <italic>s</italic> is inferred by consensus. Namely, correct location is deemed to be the centroid of the set of inferred locations weighted by the underlying image intensity. This weighted centroid is also used if the current identified location of the neuron <italic>s</italic> has a distance greater than 3 standard deviations away from the centroid of the set of locations inferred from the other volumes, implying that an error may have occurred in that neuron’s classification. This is shown in <xref ref-type="fig" rid="pcbi.1005517.g005">Fig 5</xref>, where neuron 111 is correctly identified in volume 735, but the the label for neuron 111 is incorrectly located in volume 736. In that case the weighted centroid from consensus voting was used.</p>
<fig id="pcbi.1005517.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Example of consensus voting to correct a misidentified neuron.</title>
<p>In volume 735, neuron #111 is found successfully and is indicated in green. In volume 736, however, the neuron is misidentified, shown in red. During the correction phase, all other time points vote for what the position of neuron #111 should based on a thin-plate spline deformation. A sample of votes are shown (blue ‘x’). Since the initial estimate of the position is far from the majority of consensus votes, a corrected position is assigned to be the centroid of the votes weighted by image intensity. This process is repeated to correct any errors for every neuron at every time.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Comparison with manually annotated data</title>
<p>To assess the accuracy of the Neuron Registration Vector Encoding pipeline, we applied our automated tracking system to a 4 minute recording of whole brain activity in a moving <italic>C. elegans</italic> that had previously been hand annotated and published [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. A custom Matlab GUI was used for manually identifying and tracking neurons. Nine researchers collectively annotated 70 neurons from each of the 1519 volumes in the 4 minute video. This is much less than the 181 neurons predicted to be found in the head [<xref ref-type="bibr" rid="pcbi.1005517.ref026">26</xref>]. The discrepancy is likely caused by a combination of imaging conditions and human nature. The short exposure time of our recordings makes it hard to resolve dim neurons, and the relatively long recordings tend to cause photobleaching which make the neurons even dimmer. Additionally, human researchers naturally tend to select only those neurons that are brightest and are most unambiguous for annotation, and tend to skip dim neurons or those neurons that are most densely clustered.</p>
<p>We compared human annotations to our automated analysis in this same dataset. We performed the entire pipeline including detecting centerlines, worm straightening, segmentation, and neuron registration vector encoding and clustering, and correction. Automated tracking detected 119 neurons from the video compared to 70 from the human. In each volume, we paired the automatically tracked neurons with those found by manual detection by finding the closest matches in the unstraightened coordinate system. A neuron was perfectly tracked if it matched with the same manual neuron at all times. Tracking errors were flagged when a neuron matched with a manual neuron that was different than the one it matched with most often. The locations of the detected neurons are shown in <xref ref-type="fig" rid="pcbi.1005517.g006">Fig 6A</xref>. Only one neuron was incorrectly identified for more than 5% of the time volumes (<xref ref-type="fig" rid="pcbi.1005517.g006">Fig 6B</xref>). The locations of neurons and the corresponding error rates are shown in <xref ref-type="fig" rid="pcbi.1005517.g006">Fig 6B</xref>. Neurons that were detected by the algorithm but not annotated manually are shown in gray. Upon further inspection, it was noted that some of the mismatches between our method and the manual annotation were due to human errors in the manual annotation, meaning the algorithm is able to correct humans on some occasions.</p>
<fig id="pcbi.1005517.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison of the automated Neuron Registration Vector Encoding algorithm with manual human annotation.</title>
<p>A previously published 4 minute recording of calcium activity (strain AML14) was annotated by hand, [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. (A) Spheres show position of neurons that were detected by the automated algorithm. Grey indicates a neuron detected by both the algorithm and the human. All neurons detected by the human were also detected by the algorithm (70 neurons). Red indicates neurons that were missed by the human and detected only by the algorithm (49 neurons). (B) Histogram showing number of neurons that were mismatched for a given fraction of time-volumes when comparing automated and manual approaches. Only those neurons that were consistently found by both algorithm and human were considered. An automatically identified neuron was deemed correctly matched for a given time-volume if it was paired with the correct corresponding manual neuron.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g006" xlink:type="simple"/>
</fig>
<p>GCaMP6s fluorescent intensity is ultimately the measurement of interest and this can be easily extracted from the tracks of the neuron locations across time. The pixels within an approximate 2 <italic>μm</italic> radius sphere around each point are used to calculate the average fluorescent intensity of a neuron in both the red RFP and green GCaMP6s channels at each time. This encompasses regions of the cell body, but excludes the neuron’s processes. The pixels within this sphere of interest are identified in the straightened RFP volume, but the intensity values are found by looking-up corresponding pixels in the unstraightened coordinate system in the original red- and green-channel images, respectively. We use the calcium-insensitive RFP signal to account for noise sources common to both the GCaMP6s and the RFP channel [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. These include, for example, apparent changes in intensity due to focus, motion blur, changes in local fluorophore density arising from brain deformation and apparent changes in intensity due to inhomogeneities in substrate material. We measure neural activity as a fold change over baseline of the ratio of GCaMP6s to RFP intensity,
<disp-formula id="pcbi.1005517.e008"><alternatives><graphic id="pcbi.1005517.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Activity</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>R</mml:mi></mml:mrow> <mml:msub><mml:mi>R</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>R</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:msub><mml:mi>R</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="4pt"/><mml:mi>R</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>GCaMP</mml:mtext> <mml:mn>6</mml:mn> <mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mtext>RFP</mml:mtext></mml:msub></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>The baseline for each neuron, <italic>R</italic><sub>0</sub>, is defined as the 20th percentile value of the ratio <italic>R</italic> for that neuron. <xref ref-type="fig" rid="pcbi.1005517.g007">Fig 7</xref> shows calcium imaging traces extracted from new whole-brain recordings using the registration vector pipeline. 156 neurons were tracked for approximately 8 minutes as the worm moves. Many neurons show clear correlation with reversal behaviors in the worm.</p>
<fig id="pcbi.1005517.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Calcium activity traces.</title>
<p>Neural activity traces from 156 neurons in the brain a <italic>C. elegans</italic> as it freely moves on an agarose plate for 8 minutes (strain AML32). The neural activity is expressed as a fold change over baseline of the ratio of GCaMP6s to RFP for each neuron. The behavior is indicated in the ethogram. On the right is the locations of all of the detected neurons (the head of the worm is towards the top of the page). The neurons that have significant correlation with reverse locomotion are indicated in red. White gaps indicate instances where neurons failed to segment. This is a newly acquired recording, different from that in <xref ref-type="fig" rid="pcbi.1005517.g006">Fig 6</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>The Neuron Registration Vector Encoding method presented here is able to process longer recordings and locate more neurons with less human input compared to previous examples of whole-brain imaging in freely moving <italic>C. elegans</italic> [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. Fully automated image processing means that we are no longer limited by the human labor required for manual annotation. In new recordings presented here, we are able to observe 156 of the expected 181 neurons, much larger than the approximately 80 observed in previous work from our lab and others [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005517.ref011">11</xref>]. By automating tracking and segmentation, this relieves one of the major bottlenecks to analyzing longer recordings.</p>
<p>The neuron registration vector encoding algorithm primarily relies on the local coherence of the motion of the neurons. It permits large deformations of the worm’s centerline so long as deformations around the centerline remain modest. Crucially, the algorithm’s time-independent approach allows it to tolerate large motion between consecutive time-volumes. These properties make it well suited for our neural recordings of <italic>C. elegans</italic> and we suspect that our approach would be applicable to tracking neurons in moving and deforming brains from other organisms as well.</p>
<p>Certain classes of recordings, however, would not be well suited for Neuron Registration Vector Encoding and Clustering. The approach will fail when the local coherence of neuron motion breaks down. For example, if one neuron were to completely swap locations with another neuron relative to its surroundings, registration would not detect the switch and our method would fail. In this case a time-dependent tracking approach may perform better.</p>
<p>In addition, proper clustering of the feature vectors requires the animal’s brain to explore a contiguous region of deformation space. For example, if a hypothetical brain were only ever to occupy two distinct conformations that are different enough that registration is not reliable between these two conformation states, the algorithm would fail to cluster feature vectors from the same neuron across the two states. To effectively identify the neurons in these two conformations, the animal’s brain must sample many conformations in between those two states. This way, discrepancies in registration arise gradually and the resulting feature vectors occupy a continuous region in the space of possible feature vectors. Note that a similar requirement would necessarily apply to any time-dependent tracking algorithm as well.</p>
<p>We suspect that brain recordings from most species of interest meet these two requirements: namely neuron motion will have local coherence and the brain will explore a contiguous region of deformation space. Where these conditions are satisfied, we expect registration vector encoding to work well. Tracking in <italic>C. elegans</italic> is especially challenging because the entire brain undergoes large deformations as the animal bends. In most other organisms like zebrafish and <italic>Drosophila</italic>, brains are contained within a skull or exoskeleton and relative motion of the neurons is small. In those organisms, fluctuations in neuron positions take the form of rigid global transformations as the animal moves, or local non-linear deformations due to motion of blood vessels. We expect that this approach will be applicable there as well.</p>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec013">
<title>Strains</title>
<p>Transgenic worms were cultivated on nematode growth medium (NGM) plates with OP50 bacteria. Strain AML32 (wtfIs5[P<italic>rab-3</italic>::NLS::GCaMP6s; P<italic>rab-3</italic>::NLS::tagRFP]) was generated by UV irradiating animals of strain AML14 (wtfEx4[P<italic>rab-3</italic>::NLS::GCaMP6s; P<italic>rab-3</italic>::NLS::tagRFP]) [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>] and outcrossing twice.</p>
</sec>
<sec id="sec014">
<title>Imaging <italic>C. elegans</italic></title>
<p>Imaging is performed as described in Nguyen et al [<xref ref-type="bibr" rid="pcbi.1005517.ref010">10</xref>]. The worm is placed between an agarose slab and a large glass coverslip. The coverslip is held up by 0.006” plastic shims in order to reduce the amount of pressure on the worm from the glass, and mineral oil is spread over the worm to better match refractive indices in the space between the coverglass and the worm. The dark field image is used to extract the animal’s centerline while the fluorescent image is used for tracking the worm’s brain. Only the head of the worm is illuminated by the fluorescent excitation light and can be observed in the low magnification fluorescent image.</p>
<p>The two low magnification videos and the RFP and GCaMP6 high magnification videos are aligned by imaging a slide of 4 <italic>μm</italic> “Tetraspeck” beads (ThermoFisher) that emit light in both red and green channels. We manually or automatically locate the beads from calibration images and use the bead positions to find affine transformations between each camera’s coordinate system. The affine parameters are found using a least squares fit on the coordinates of the beads in the image.</p>
</sec>
<sec id="sec015">
<title>Thin plate spline transformations</title>
<p>Thin plate spline (TPS) transformations play an important role in error correcting and are also critical for the point set registration algorithm [<xref ref-type="bibr" rid="pcbi.1005517.ref024">24</xref>]. Given a set of <italic>n</italic> initial control points <bold>X</bold> = {<bold>x</bold><sub><italic>i</italic></sub>}, and the set of transformed points, <italic>u</italic>[<bold>X</bold>], the TPS transformation <italic>u</italic> can be written as <italic>u</italic>[<bold>X</bold>] = <bold>W</bold><bold>U</bold>(<bold>X</bold>) + <bold>A</bold><bold>X</bold> + <bold>t</bold>. The affine portion of the transformation is <bold>A</bold><bold>X</bold> + <bold>t</bold>, while <bold>W</bold><bold>U</bold>(<bold>X</bold>) is the non-linear part of the transformation from TPS. <bold>U</bold>(<bold>X</bold>) is an <italic>n</italic> × <italic>n</italic> vector with <inline-formula id="pcbi.1005517.e009"><alternatives><graphic id="pcbi.1005517.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle><mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>i</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>j</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi mathvariant="bold">j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>i</mml:mi></mml:mstyle></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and <bold>W</bold> is a 3 × <italic>n</italic> matrix. The elements of <bold>W</bold>, <bold>A</bold> and <bold>t</bold> are the parameters of the transformation <italic>u</italic>. These parameters are found in different ways dependant on context. During the error correction processing step, these parameters are fit by knowing both the the set of control points <bold>X</bold> and the location of the transformed points <italic>u</italic>[<bold>X</bold>]. In the context of the point set registration algorithm, <italic>u</italic>[<bold>X</bold>] incurs an energy penalty for deforming space given by <italic>E</italic><sub>Deformation</sub>(<italic>u</italic>) = trace(<bold>W</bold><bold>U</bold><bold>W</bold><sup><bold>T</bold></sup>) [<xref ref-type="bibr" rid="pcbi.1005517.ref024">24</xref>]. This cost is used in <xref ref-type="disp-formula" rid="pcbi.1005517.e004">Eq 2</xref> to determine the total energy of the transformation. Gradient descent is then used to determine the optimal TPS transformation parameters by minimizing the total energy of the transformation.</p>
</sec>
<sec id="sec016">
<title>Clustering</title>
<p>Clustering is performed in two steps: hierarchical clustering and neuron classification. We chose to perform hierarchical clustering only on an initial subset of 800 volumes because hierarchical clustering can become prohibitively computationally intensive for larger datasets. The correlation distance, 1 − corr(<bold>v</bold><sub><italic>m</italic></sub>, <bold>v</bold><sub><italic>n</italic></sub>), was used as the pairwise distance metric for clustering. Agglomerative hierarchical clustering was implemented using complete linkage with a distance cutoff of 0.9. Clusters which are smaller than 40% of the number of subset volumes were removed. After the clusters were defined via hierarchical clustering, we then performed neuron classification.</p>
<p>To classify neurons, we assigned neurons from every volume to the cluster with the nearest centroid. Only the best matched neuron in each volume is assigned to a cluster and only if the neuron is closer than some threshold distance, described below. If two or more neurons from a volume would otherwise be assigned to a single cluster, the closest neuron retains that classification and other neurons are unassigned. As a result, some putative neurons are not assigned to any cluster and at most one neuron per volume is assigned to any given cluster. The implementation of the algorithm is shown in Algorithm 1.</p>
<p><bold>Algorithm 1</bold> Clustering the Neuron Registration Vectors</p>
<p specific-use="line"> 1: <bold>input:</bold> Set of registration vectors <italic>V</italic> = {<bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub>}</p>
<p specific-use="line"> 2: <bold>output:</bold> Cluster assignments for each of the vectors in <italic>V</italic></p>
<p specific-use="line"> 3: <bold>procedure</bold> C<sc>luster</sc>(<italic>V</italic>)</p>
<p specific-use="line"> 4:  <italic>S</italic> = subset of <italic>V</italic></p>
<p specific-use="line"> 5:  <italic>subset</italic>_<italic>assignments</italic> = hierarchically cluster <italic>S</italic> with distance cutoff 0.9</p>
<p specific-use="line"> 6:  <italic>cluster</italic>_<italic>list</italic> = unique(<italic>subset</italic>_<italic>assignments</italic>)</p>
<p specific-use="line"> 7:  <bold>for</bold> each <italic>cluster</italic> in <italic>cluster</italic>_<italic>list</italic> <bold>do</bold></p>
<p specific-use="line"> 8:   <bold>If</bold> size(<italic>cluster</italic>) &gt; 40% of volumes used <bold>then</bold></p>
<p specific-use="line"> 9:    <italic>cluster</italic>_<italic>center</italic> = average of <italic>S</italic> assigned to <italic>cluster</italic></p>
<p specific-use="line">10:   <bold>else</bold></p>
<p specific-use="line">11:    remove <italic>cluster</italic></p>
<p specific-use="line">12:   <bold>end if</bold></p>
<p specific-use="line">13:  <bold>end for</bold></p>
<p specific-use="line">14:  compute <italic>threshold</italic> from <italic>S</italic></p>
<p specific-use="line">15:  <bold>for</bold> each <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> in <italic>V</italic> <bold>do</bold></p>
<p specific-use="line">16:   <italic>d</italic> = distances from <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> to <italic>cluster</italic>_<italic>centers</italic></p>
<p specific-use="line">17:   <bold>if</bold> any(<italic>d</italic> &lt; <italic>threshold</italic>) <bold>then</bold></p>
<p specific-use="line">18:    assign <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> to closest cluster</p>
<p specific-use="line">19:   <bold>end if</bold></p>
<p specific-use="line">20:  <bold>end for</bold></p>
<p specific-use="line">21:  <bold>for</bold> each volume in the recording <bold>do</bold></p>
<p specific-use="line">22:   <bold>for</bold> each <italic>cluster</italic> in <italic>cluster</italic>_<italic>list</italic> <bold>do</bold></p>
<p specific-use="line">23:    <bold>if</bold> multiple <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> from volume assigned to cluster <bold>then</bold></p>
<p specific-use="line">24:     unassign all <bold>v</bold><sub><italic>i</italic>,<italic>t</italic></sub> from the cluster except the closest one</p>
<p specific-use="line">25:    <bold>end if</bold></p>
<p specific-use="line">26:   <bold>end for</bold></p>
<p specific-use="line">27:  <bold>end for</bold></p>
<p specific-use="line">28: <bold>end procedure</bold></p>
<p>The threshold distance to determine whether a neuron is assigned to a cluster is calculated using a statistical analysis of the clusters generated by the initial hierarchical clustering so as to discriminate between neurons that are likely correctly or incorrectly assigned. The threshold is calculated as follows: For each neuron assigned during the initial clustering, we collect the distance between that neuron and the center of the cluster it was assigned to. The distribution of these distances is the “correctly assigned” distribution. In contrast, the null distribution is found by collecting the distances between each neuron and all clusters to which that neuron is not assigned. The threshold distance is set to be the largest distance for which a distance is more likely to be found in the “correctly assigned” distribution than the null distribution.</p>
</sec>
<sec id="sec017">
<title>Algorithm implementation</title>
<p>The analysis was performed on Princeton University’s high-performance scientific computing cluster, “Della” primarily consisting of 240 nodes and 4288 cores, each with 2.4 GHz processors. Jobs were run on up to 200 cores simultaneously. Timing information for the steps listed in <xref ref-type="fig" rid="pcbi.1005517.g001">Fig 1</xref> are described below and summarized in <xref ref-type="table" rid="pcbi.1005517.t001">Table 1</xref>.</p>
<table-wrap id="pcbi.1005517.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005517.t001</object-id>
<label>Table 1</label>
<caption>
<title>Breakdown of computation time and scalings for Neuron Registration Vector Encoding pipeline.</title>
<p><italic>n</italic><sub>frames</sub> is the total number of low magnification images used to detect centerlines, <italic>n</italic><sub>vol</sub> is the total number of volumes in the recording, <italic>n</italic><sub>ref</sub> is the number of reference volumes used for creating feature vectors, <italic>n</italic><sub>neurons</sub> is the total number of neurons detected, and <italic>n</italic><sub>subset</sub> is the number of neurons in the subset of volumes used for initial clustering.</p>
</caption>
<alternatives>
<graphic id="pcbi.1005517.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2">Analysis Step</th>
<th align="left">Computation</th>
<th align="left">Approx. % of real time</th>
<th align="left">Computational time scales as</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="2">Centerline Detection</td>
<td align="left">Linear</td>
<td align="left">4</td>
<td align="left"><italic>O</italic>(<italic>n</italic><sub>frames</sub>)</td>
</tr>
<tr>
<td align="left" colspan="2">Worm Straightening</td>
<td align="left" rowspan="2">Parallel</td>
<td align="left" rowspan="2">10</td>
<td align="left" rowspan="2"><italic>O</italic>(<italic>n</italic><sub>vol</sub>)</td>
</tr>
<tr>
<td align="left" colspan="2">Segmentation</td>
</tr>
<tr>
<td align="left" colspan="2">Registration Vector Encoding</td>
<td align="left">Parallel</td>
<td align="left">80</td>
<td align="left"><italic>O</italic>(<italic>n</italic><sub>vol</sub> × <italic>n</italic><sub>ref</sub>)</td>
</tr>
<tr>
<td align="left" rowspan="2">Clustering</td>
<td align="left">Hierarchical</td>
<td align="left">Linear</td>
<td align="left">2</td>
<td align="left"><inline-formula id="pcbi.1005517.e010">
<alternatives>
<graphic id="pcbi.1005517.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e010" xlink:type="simple"/>
<mml:math display="inline" id="M10">
<mml:mrow>
<mml:mi>O</mml:mi>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mtext>subset</mml:mtext>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>×</mml:mo>
<mml:msubsup>
<mml:mi>n</mml:mi>
<mml:mtext>neurons</mml:mtext>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula></td>
</tr>
<tr>
<td align="left">Classification</td>
<td align="left">Linear</td>
<td align="left">0</td>
<td align="left"><italic>O</italic>(<italic>n</italic><sub>vol</sub> × <italic>n</italic><sub>neurons</sub>)</td>
</tr>
<tr>
<td align="left" colspan="2">Error Correction</td>
<td align="left">Parallel</td>
<td align="left">4</td>
<td align="left">up to <inline-formula id="pcbi.1005517.e011"><alternatives><graphic id="pcbi.1005517.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>n</mml:mi> <mml:mtext>vol</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>×</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mtext>neurons</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<sec id="sec018">
<title>Centerline detection</title>
<p>Centerlines in each image are calculated using information from the previous centerline and as a result must be computed linearly. Total computational time for centerline detection scales linearly with recording length. Specifically, centerlines must be fit for every frame of the low magnification video so the computation time scales as <italic>O</italic>(<italic>n</italic><sub>frames</sub>).</p>
</sec>
<sec id="sec019">
<title>Worm straighteningand and <italic>segmentation</italic></title>
<p>Straightening and segmentation are parallelized over each volume. Total computation time for worm straightening and segmentation scales as <italic>O</italic>(<italic>n</italic><sub>vol</sub>), with each volume taking ∼20 seconds on a single core.</p>
</sec>
<sec id="sec020">
<title>Registration vector construction</title>
<p>Registration vector construction is the most computationally intensive part of the algorithm. The vectors are created by performing non-rigid point-set registration between all sample volumes and all reference volumes. Total computation time scales as <italic>O</italic>(<italic>n</italic><sub>vol</sub> × <italic>n</italic><sub>ref</sub>) with the bulk of the computational time consumed by gradient descent during point-set registration. The gradient descent for matching a single sample volume to a single reference on one processing core takes ∼15s. This step is parallelized over each volume.</p>
</sec>
<sec id="sec021">
<title>Clustering</title>
<p>Clustering is broken into two subparts, hierarchical clustering and classification. Clustering is overall fast compared to point-set registration. Computation time for hierarchical clustering scales quadratically with the number of neurons in the worm times the size of the initial set of volumes used to define the clusters. This can become prohibitively slow if we increase the number of volumes used for hierarchical clustering as the length of the recordings increase. Thus, we fix the number of volumes used for initial hierarchical clustering to an arbitrary size (e.g. 800 volumes) regardless of the length of the recording. Thus, the clustering time scales as <inline-formula id="pcbi.1005517.e012"><alternatives><graphic id="pcbi.1005517.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>n</mml:mi> <mml:mtext>subset</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>×</mml:mo> <mml:msubsup><mml:mi>n</mml:mi> <mml:mtext>neurons</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. After the clusters are defined via hierarchical clustering in the first subpart, all neurons can then be quickly classified into clusters with negligible computational time in the second subpart.</p>
</sec>
<sec id="sec022">
<title>Error correction</title>
<p>Error correction is parallelized over each neuron, with each neuron checked in every volume. The error checking in each volume is done by comparing the point-set to all other point-sets using a thin-plate spline transformation. This operation is fast compared to the registration vector construction step because here the correspondences between the point sets are already known. Computation time for this process scales as <inline-formula id="pcbi.1005517.e013"><alternatives><graphic id="pcbi.1005517.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005517.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>n</mml:mi> <mml:mtext>vol</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>×</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mtext>neurons</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. We chose to compare each neuron in each volume to all other volumes because these comparisons are relatively fast. However, we also observe that comparing each neuron with merely a subset of volumes seems to suffice without loss of performance. In that case the computation time for error correction would be <italic>O</italic>(<italic>n</italic><sub>vol</sub> × <italic>n</italic><sub>neurons</sub> × <italic>n</italic><sub>subset</sub>), where <italic>n</italic><sub>subset</sub> is the number of volumes selected for comparison during error correcting.</p>
<p>An 8 minute recording of a moving animal has approximately 3000 volumes and 250 GB of raw imaging data and can be processed from start to finish on the university cluster in less than 40 hours. Major time reduction could be achieved by reducing the number of reference volumes used during Registration Vector Encoding.</p>
<p>All data used in this publication have been made publicly available at the IEEE DataPort repository (DOI:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.21227/H2901H" xlink:type="simple">10.21227/H2901H</ext-link>) <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.21227/H2901H" xlink:type="simple">http://dx.doi.org/10.21227/H2901H</ext-link>. MATLAB code implementing our pipeline is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/leiferlab/NeRVEclustering" xlink:type="simple">https://github.com/leiferlab/NeRVEclustering</ext-link>.</p>
</sec>
</sec>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005517.s001" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005517.s001" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>Example video for raw, straightened, and tracked data.</title>
<p>Left: Raw video feed from high magnification RFP video. The imaging plane is scanning up and down through the volume of the worm’s brain. The recording is shown at 1/2× speed and the time elapsed is indicated in the bottom left. Middle: Maximum intensity projection of each volume is shown after Worm Centerline Tracking and Straightening. Right: Locations of neurons are shown at the end of the pipeline (after Neuron Registration Vector Encoding, Clustering and Error Correction). Each color represents a different tracked neuron. All neurons from the volume are shown overlaid on a raw image of the middle plane of each volume. Note a light flash used for time synchronization is visible around <italic>t</italic> = 13s.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The analysis presented in this article was performed on computational resources supported by the Princeton Institute for Computational Science and Engineering (PICSciE), the Office of Information Technology’s High Performance Computing Center and Visualization Laboratory at Princeton University and by John Wiggins, Manager of Information Technology at the Princeton Neuroscience Institute.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005517.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Picardo</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Merel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Katlowitz</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Vallentin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Okobi</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Benezra</surname> <given-names>SE</given-names></name>, <etal>et al</etal>. <article-title>Population-Level Representation of a Temporal Sequence Underlying Song Production in the Zebra Finch</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>(<issue>4</issue>):<fpage>866</fpage>–<lpage>876</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2016.02.016" xlink:type="simple">10.1016/j.neuron.2016.02.016</ext-link></comment> <object-id pub-id-type="pmid">27196976</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rickgauer</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Simultaneous cellular-resolution optical perturbation and imaging of place cell firing fields</article-title>. <source>Nature neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>12</issue>):<fpage>1816</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3866" xlink:type="simple">10.1038/nn.3866</ext-link></comment> <object-id pub-id-type="pmid">25402854</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maynard</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ojakangas</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Acuna</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sanes</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Normann</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Neuronal interactions improve cortical population coding of movement direction</article-title>. <source>The journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>18</issue>):<fpage>8083</fpage>–<lpage>8093</lpage>. <object-id pub-id-type="pmid">10479708</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kato</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kaplan</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Schrödel</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Skora</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lindsay</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Yemini</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans</article-title>. <source>Cell</source>. <year>2015</year>;<volume>163</volume>(<issue>3</issue>):<fpage>656</fpage>–<lpage>669</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cell.2015.09.034" xlink:type="simple">10.1016/j.cell.2015.09.034</ext-link></comment> <object-id pub-id-type="pmid">26478179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref005">
<label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Li W, Voleti V, Schaffer E, Vaadia R, Grueber WB, Mann RS, et al. SCAPE Microscopy for High Speed, 3D Whole-Brain Imaging in Drosophila Melanogaster. In: Biomedical Optics 2016. Optical Society of America; 2016. p. BTu4D.3. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.osapublishing.org/abstract.cfm?URI=BRAIN-2016-BTu4D.3" xlink:type="simple">http://www.osapublishing.org/abstract.cfm?URI=BRAIN-2016-BTu4D.3</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005517.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Prevedel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Yoon</surname> <given-names>YG</given-names></name>, <name name-style="western"><surname>Hoffmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pak</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wetzstein</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kato</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Simultaneous whole-animal 3D imaging of neuronal activity using light-field microscopy</article-title>. <source>Nat Meth</source>. <year>2014</year>;<volume>11</volume>(<issue>7</issue>):<fpage>727</fpage>–<lpage>730</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2964" xlink:type="simple">10.1038/nmeth.2964</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Wardill</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pulver</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Renninger</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Baohan</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>. <year>2013</year>;<volume>499</volume>(<issue>7458</issue>):<fpage>295</fpage>–<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12354" xlink:type="simple">10.1038/nature12354</ext-link></comment> <object-id pub-id-type="pmid">23868258</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harvey</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Dombeck</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Intracellular dynamics of hippocampal place cells during virtual navigation</article-title>. <source>Nature</source>. <year>2009</year>;<volume>461</volume>(<issue>7266</issue>):<fpage>941</fpage>–<lpage>946</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature08499" xlink:type="simple">10.1038/nature08499</ext-link></comment> <object-id pub-id-type="pmid">19829374</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ahrens</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Orger</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Robson</surname> <given-names>DN</given-names></name>, <name name-style="western"><surname>Schier</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Engert</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title>. <source>Nature</source>. <year>2012</year>;<volume>485</volume>(<issue>7399</issue>):<fpage>471</fpage>–<lpage>477</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11057" xlink:type="simple">10.1038/nature11057</ext-link></comment> <object-id pub-id-type="pmid">22622571</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nguyen</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Shipley</surname> <given-names>FB</given-names></name>, <name name-style="western"><surname>Linder</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Plummer</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Setru</surname> <given-names>SU</given-names></name>, <etal>et al</etal>. <article-title>Whole-brain calcium imaging with cellular resolution in freely behaving Caenorhabditis elegans</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>(<issue>8</issue>):<fpage>E1074</fpage>–<lpage>E1081</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1507110112" xlink:type="simple">10.1073/pnas.1507110112</ext-link></comment> <object-id pub-id-type="pmid">26712014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Venkatachalam</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ji</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mitchell</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <source>Pan-neuronal imaging in roaming Caenorhabditis elegans</source>. <year>2016</year>;<volume>113</volume>(<issue>8</issue>):<fpage>E1082</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005517.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Boyle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ooi</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Sandel</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Waterston</surname> <given-names>RH</given-names></name>. <article-title>Automated cell lineage tracing in Caenorhabditis elegans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2006</year>;<volume>103</volume>(<issue>8</issue>):<fpage>2707</fpage>–<lpage>2712</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0511111103" xlink:type="simple">10.1073/pnas.0511111103</ext-link></comment> <object-id pub-id-type="pmid">16477039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Christensen</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Bokinsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Santella</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Marquina-Solis</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Untwisting the Caenorhabditis elegans embryo</article-title>. <source>eLife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e10070</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.10070" xlink:type="simple">10.7554/eLife.10070</ext-link></comment> <object-id pub-id-type="pmid">26633880</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crocker</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Grier</surname> <given-names>D</given-names></name>. <article-title>Methods of Digital Video Microscopy for Colloidal Studies</article-title>. <source>Journal of Colloid and Interface Science</source>. <year>1996</year>;<volume>179</volume>(<issue>1</issue>):<fpage>298</fpage>–<lpage>310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/jcis.1996.0217" xlink:type="simple">10.1006/jcis.1996.0217</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mukamel</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Nimmerjahn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schnitzer</surname> <given-names>MJ</given-names></name>. <article-title>Automated Analysis of Cellular Signals from Large-Scale Calcium Imaging Data</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>6</issue>):<fpage>747</fpage>–<lpage>760</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.08.009" xlink:type="simple">10.1016/j.neuron.2009.08.009</ext-link></comment> <object-id pub-id-type="pmid">19778505</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephens</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Johnson-Kerner</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>WS</given-names></name>. <article-title>Dimensionality and dynamics in the behavior of C. elegans</article-title>. <source>PLoS Computational Biology</source>. <year>2008</year>;<volume>4</volume>(<issue>4</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000028" xlink:type="simple">10.1371/journal.pcbi.1000028</ext-link></comment> <object-id pub-id-type="pmid">18389066</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peng</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Myers</surname> <given-names>EW</given-names></name>. <article-title>Straightening Caenorhabditis elegans images</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>2</issue>):<fpage>234</fpage>–<lpage>242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btm569" xlink:type="simple">10.1093/bioinformatics/btm569</ext-link></comment> <object-id pub-id-type="pmid">18025002</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Broekmans</surname> <given-names>OD</given-names></name>, <name name-style="western"><surname>Rodgers</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Stephens</surname> <given-names>GJ</given-names></name>. <article-title>Resolving coiled shapes reveals new reorientation behaviors in <italic>C. elegans</italic></article-title>. <source>eLife</source>. <year>2016</year>;<volume>5</volume>:<fpage>e17227</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.17227" xlink:type="simple">10.7554/eLife.17227</ext-link></comment> <object-id pub-id-type="pmid">27644113</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Deng</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Coen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shaevitz</surname> <given-names>JW</given-names></name>. <article-title>Efficient multiple object tracking using mutually repulsive active membranes</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>6</issue>):<fpage>e65769</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0065769" xlink:type="simple">10.1371/journal.pone.0065769</ext-link></comment> <object-id pub-id-type="pmid">23799046</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kass</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Witkin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Terzopoulos</surname> <given-names>D</given-names></name>. <article-title>Snakes: Active contour models</article-title>. <source>International Journal of Computer Vision</source>. <year>1988</year>;<volume>1</volume>(<issue>4</issue>):<fpage>321</fpage>–<lpage>331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00133570" xlink:type="simple">10.1007/BF00133570</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Tordoff B, Murray DW. Guided sampling and consensus for motion estimation. In: European conference on computer vision. Springer; 2002. p. 82–96.</mixed-citation>
</ref>
<ref id="pcbi.1005517.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Adiga</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Guzowski</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Barnes</surname> <given-names>Ca</given-names></name>, <name name-style="western"><surname>Roysam</surname> <given-names>B</given-names></name>. <article-title>A hybrid 3D watershed algorithm incorporating gradient cues and object models for automatic segmentation of nuclei in confocal image stacks</article-title>. <source>Cytometry Part A: the journal of the International Society for Analytical Cytology</source>. <year>2003</year>;<volume>56</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cyto.a.10079" xlink:type="simple">10.1002/cyto.a.10079</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Toyoshima</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tokunaga</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hirose</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Kanamori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Teramoto</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Jang</surname> <given-names>MS</given-names></name>, <etal>et al</etal>. <article-title>Accurate Automatic Detection of Densely Distributed Cell Nuclei in 3D Space</article-title>. <source>PLOS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>6</issue>):<fpage>e1004970</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004970" xlink:type="simple">10.1371/journal.pcbi.1004970</ext-link></comment> <object-id pub-id-type="pmid">27271939</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Jian B, Vemuri BC. A Robust Algorithm for Point Set Registration Using Mixture of Gaussians. Proceedings / IEEE International Conference on Computer Vision IEEE International Conference on Computer Vision. 2005;2:1246–1251.</mixed-citation>
</ref>
<ref id="pcbi.1005517.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jian</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vemuri</surname> <given-names>BC</given-names></name>. <article-title>Robust point set registration using gaussian mixture models</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2011</year>;<volume>33</volume>(<issue>8</issue>):<fpage>1633</fpage>–<lpage>1645</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2010.223" xlink:type="simple">10.1109/TPAMI.2010.223</ext-link></comment> <object-id pub-id-type="pmid">21173443</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005517.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>White</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Southgate</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Thomson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>S</given-names></name>. <article-title>The structure of the nervous system of the nematode Caenorhabditis elegans</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>1986</year>;<volume>314</volume>(<issue>1165</issue>):<fpage>1</fpage>–<lpage>340</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.1986.0056" xlink:type="simple">10.1098/rstb.1986.0056</ext-link></comment> <object-id pub-id-type="pmid">22462104</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>