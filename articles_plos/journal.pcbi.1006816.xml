<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01350</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006816</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Mathematical functions</subject><subj-group><subject>Curve fitting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject><subj-group><subject>Distribution curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Inferring neural circuit structure from datasets of heterogeneous tuning curves</article-title>
<alt-title alt-title-type="running-head">Inferring neural circuit structure from datasets of heterogeneous tuning curves</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6169-0100</contrib-id>
<name name-style="western">
<surname>Arakaki</surname> <given-names>Takafumi</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8148-5502</contrib-id>
<name name-style="western">
<surname>Barello</surname> <given-names>G.</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5942-0697</contrib-id>
<name name-style="western">
<surname>Ahmadian</surname> <given-names>Yashar</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Institute of Neuroscience, University of Oregon, Eugene, Oregon, USA</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Departments of Biology and Mathematics, University of Oregon, Eugene, Oregon, USA</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Marinazzo</surname> <given-names>Daniele</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Ghent University, BELGIUM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">yashar@uoregon.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>19</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>4</issue>
<elocation-id>e1006816</elocation-id>
<history>
<date date-type="received">
<day>6</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Arakaki et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006816"/>
<abstract>
<p>Tuning curves characterizing the response selectivities of biological neurons can exhibit large degrees of irregularity and diversity across neurons. Theoretical network models that feature heterogeneous cell populations or partially random connectivity also give rise to diverse tuning curves. Empirical tuning curve distributions can thus be utilized to make model-based inferences about the statistics of single-cell parameters and network connectivity. However, a general framework for such an inference or fitting procedure is lacking. We address this problem by proposing to view mechanistic network models as implicit generative models whose parameters can be optimized to fit the distribution of experimentally measured tuning curves. A major obstacle for fitting such models is that their likelihood function is not explicitly available or is highly intractable. Recent advances in machine learning provide ways for fitting implicit generative models without the need to evaluate the likelihood and its gradient. Generative Adversarial Networks (GANs) provide one such framework which has been successful in traditional machine learning tasks. We apply this approach in two separate experiments, showing how GANs can be used to fit commonly used mechanistic circuit models in theoretical neuroscience to datasets of tuning curves. This fitting procedure avoids the computationally expensive step of inferring latent variables, such as the biophysical parameters of, or synaptic connections between, particular recorded cells. Instead, it directly learns generalizable model parameters characterizing the network’s <italic>statistical structure</italic> such as the statistics of strength and spatial range of connections between different cell types. Another strength of this approach is that it fits the joint high-dimensional distribution of tuning curves, instead of matching a few summary statistics picked <italic>a priori</italic> by the user, resulting in a more accurate inference of circuit properties. More generally, this framework opens the door to direct model-based inference of circuit structure from data beyond single-cell tuning curves, such as simultaneous population recordings.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Neurons in the brain respond selectively to some stimuli or for some motor outputs, but not others. Even within a local brain network, neurons exhibit great diversity in their selectivity patterns. Recently, theorists have highlighted the computational importance of diverse neural selectivity. While many mechanistic circuit models are highly stylized and do not capture such diversity, models that feature biologically realistic heterogeneity in their structure do generate responses with diverse selectivities. However, traditionally only the average pattern of selectivity is matched between model and experimental data, and the distribution around the mean is ignored. Here, we provide a hitherto lacking methodology that exploits the full empirical and model distributions of response selectivites, in order to infer various structural circuit properties, such as the statistics of strength and spatial range of connections between different cell types. By applying this method to fit two circuit models from theoretical neuroscience to experimental or simulated data, we show that the proposed method can accurately and robustly infer circuit structure, and optimize a model to match the full range of observed response selectivities. Beyond neuroscience applications, the proposed framework can potentially serve to infer the structure of other biological networks from empirical functional data.</p>
</abstract>
<funding-group>
<funding-statement>TA, GB, and YA were supported by research startup funds provided by the University of Oregon (Eugene, OR) to YA. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562. We used XSEDE resources under allocation number TG-IBN180003. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="3"/>
<page-count count="38"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-01</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The source code for implementations of our method is now available from <ext-link ext-link-type="uri" xlink:href="https://github.com/ahmadianlab/tc-gan" xlink:type="simple">https://github.com/ahmadianlab/tc-gan</ext-link>, under the MIT license.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Neural responses in many brain areas are tuned to external parameters such as stimulus- or movement-related features. Tuning curves characterize the dependence of neural responses on such parameters, and are a key descriptive tool in neuroscience. Experimentally measured tuning curves often exhibit a rich and bewildering diversity across neurons in the same brain area, which complicates simple understanding (<italic>e</italic>.<italic>g</italic>., see [<xref ref-type="bibr" rid="pcbi.1006816.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>]). This complexity has given rise to a tendency towards biased selections of minorities of cells which exhibit pure selectivites, and have orderly and easily interpretable tuning curves. As a result the biological richness and diversity of tuning curves in the full neural population is often artificially reduced or ignored. On the theoretical side too, many network models feature homogeneous populations of cells with the same cellular parameters and with regular synaptic connectivity patterns. Neural tuning curves in such models will naturally be regular and have identical shapes.</p>
<p>New theoretical advances, however, have highlighted the computational importance of diverse tuning and mixed selectivity, as observed in biological systems [<xref ref-type="bibr" rid="pcbi.1006816.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref005">5</xref>]. Furthermore, diversity and heterogeneity can be produced in mechanistic network models which either include cell populations with heterogeneous single-cell parameters (see <italic>e</italic>.<italic>g</italic>., Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref002">2</xref>]), or connectivity that is partly random and irregular despite having statistical structure and regularity (see, <italic>e</italic>.<italic>g</italic>., Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref010">10</xref>]). However, a general effective methodology for fitting such models to experimental data, such as heterogeneous samples of biological tuning curves is lacking.</p>
<p>A related central problem in neural data analysis is that of inferring functional and synaptic connectivity from neural responses and correlations. A rich literature has addressed this problem [<xref ref-type="bibr" rid="pcbi.1006816.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref016">16</xref>]. However, we see two shortcomings in previous approaches. First, most methods are based on forward models originally developed in statistics that are primarily inspired by their ease of optimization and fitting to data, rather than purely by theoretical or biological principles. Second, in the vast majority of approaches, the outcome is the estimate of the particular connectivity matrix between the particular subset of neurons sampled and simultaneously recorded in a specific animal [<xref ref-type="bibr" rid="pcbi.1006816.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref016">16</xref>]. Post-hoc analyses may then be applied to such estimates to characterize various statistical properties and regularities of connectivity [<xref ref-type="bibr" rid="pcbi.1006816.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref016">16</xref>]. However, such statistical properties are, in most cases, the object of scientific interest, as they generalize beyond the specific recorded sample. Examples of such statistical properties are the dependence of connection probability between neurons on their physical distance [<xref ref-type="bibr" rid="pcbi.1006816.ref017">17</xref>] or preferred stimulus features [<xref ref-type="bibr" rid="pcbi.1006816.ref018">18</xref>]. Another example is the degree to which neuron pairs tend to be connected bidirectionally beyond chance [<xref ref-type="bibr" rid="pcbi.1006816.ref019">19</xref>]. A methodology for model-based inference of such circuit properties directly from simultaneously or non-simultaneously recorded neural responses is lacking.</p>
<p>Here we propose a methodology that is able to fit theoretically motivated circuit models to recorded neural responses, and infer model parameters that characterize the <italic>statistics</italic> of connectivity or of single-cell properties. Conceptually, we propose to view network models with heterogeneity and random connectivity as generative models for the observed neural data, <italic>e</italic>.<italic>g</italic>., a model that generates diverse tuning curves and hence implicitly models their (high-dimensional) distribution.</p>
<p>The generative model is determined by a set of network parameters which specify the distribution of structural circuit variables like individual synaptic connections or single-cell biophysical properties. In this picture, the particular realization of the connectivity matrix or of biological properties of particular neurons are viewed as latent variables. Traditional, likelihood-based approaches such as expectation-maximization or related approaches need to fit or marginalize out (<italic>e</italic>.<italic>g</italic>., using variational or Monte Carlo sampling methods) such latent variables, conditioned on the particular observed data sample. Such high-dimensional optimizations or integrations are computationally very expensive and often intractable.</p>
<p>Alternatively, one could fit theoretical circuit models by approaches similar to moment matching, or its Bayesian counterpart, Approximate Bayesian Computation [<xref ref-type="bibr" rid="pcbi.1006816.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref021">21</xref>]. In such approaches, one <italic>a priori</italic> comes up with a few summary statistics, perhaps motivated on theoretical grounds, which characterize the data objects (<italic>e</italic>.<italic>g</italic>., tuning curves). Then one tunes (or in the Bayesian case, samples) the model parameters (but not latent variables) so that the few selected summary statistics are approximately matched between generated tuning curve samples and experimental ones [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>]. This approach will, however, generally be biased by the <italic>a priori</italic> choice of the fit summary statistics, and does not exploit all the information available in the data for inferring circuit properties.</p>
<p>A suite of new methods have recently been developed in machine learning for fitting <italic>implicit</italic> generative models [<xref ref-type="bibr" rid="pcbi.1006816.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref024">24</xref>], <italic>i</italic>.<italic>e</italic>., generative models for which a closed or tractable expression for the likelihood or its gradient is not available. Here, we will demonstrate that a specific class of such methods, namely Generative Adversarial Networks (GANs) [<xref ref-type="bibr" rid="pcbi.1006816.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>], can address the above problems. In particular, compared to methods such as moment matching, our proposed approach fits the entire high-dimensional data distribution in a much more unbiased and data-driven manner and without the need to choose a few summary statistics <italic>a priori</italic>. As we will show, this results in a more accurate and robust inference of circuit properties. In addition to inferring circuit parameters, this approach also allows for a more unbiased model comparison: one can simply simulate the competing circuit models, after fitting them to training data, and compare their goodness of fit, possibly to unseen data including new stimulus conditions not covered in the tuning curves used for training.</p>
<p>The rest of this article is organized as follows. We start the Results section by introducing the conceptual view of circuit models as implicit generative models for tuning curves. We then introduce the GAN framework. Next, we present the results of applying GANs to fit and infer the parameters of two recent influential circuit models from theoretical neuroscience. In Experiment 1 we fit a feedforward model of motor cortex to an experimental dataset of hand-position tuning curves, and compare the match between the distributions of empirical tuning curves and those generated by the models fit using our proposed method and traditional moment matching. In Experiments 2 and 3 we apply the method to fit a recurrent network model of visual cortex to a simulated dataset of stimulus-size tuning curves generated by a ground-truth circuit model. In Experiment 2 we show that the fit model captures the statistics of observed tuning curves very accurately. In Experiment 3, we assess the accuracy of circuit parameter identification using our proposed method, and discuss factors (such as the kind of tuning curves used for inference) that affect it. In the Discussion we conclude by discussing areas for extension and improvement of our proposed methodology, and broader potential applications of it. The details of our experiments, algorithms, and the models are given in Materials and methods; the source code for all implemented examples is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/ahmadianlab/tc-gan" xlink:type="simple">https://github.com/ahmadianlab/tc-gan</ext-link> under the MIT license.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Mechanistic network models as implicit generative models</title>
<p>We consider mechanistic network models of the type developed in theoretical neuroscience, informed by knowledge of biological mechanisms and network anatomy, or by computational principles. We limit ourselves to networks evolving according to feedforward or recurrent firing rate equations (examples include Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref028">28</xref>]), although the methodology is extendable to spiking networks as well (possibly with slight modifications to ensure differentiability). In the general presentation of this subsection we focus on recurrent rate networks. Abstractly, the neural responses in such networks evolve according to a dynamical system that has the following general structure (for a concrete example see the model in Experiment 2 below, which is governed by <xref ref-type="disp-formula" rid="pcbi.1006816.e051">Eq (5)</xref>):
<disp-formula id="pcbi.1006816.e001"><alternatives><graphic id="pcbi.1006816.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mspace width="2pt"/><mml:mo>;</mml:mo> <mml:mspace width="2pt"/><mml:mi mathvariant="bold">I</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
Here, <bold>v</bold><sub><italic>t</italic></sub> is the vector of the state variables of the network’s neurons at time <italic>t</italic> (components of <bold>v</bold><sub><italic>t</italic></sub> may, <italic>e</italic>.<italic>g</italic>., include the firing rate, membrane voltage and other “fast” state variables of individual neurons or synapses) and <bold>F</bold> is a vector field on state space which is differentiable with respect to its arguments. The matrix <bold>W</bold> is the partially disordered synaptic connectivity matrix (which can include both the recurrent, as well as feedforward external connections), and <bold>γ</bold> is the vector of possibly heterogeneous single-cell biophysical constants (<italic>e</italic>.<italic>g</italic>., membrane time-constant, spiking threshold potential, parameters of input-output nonlinearity, etc.). Finally, the vector <bold>I</bold> is the external input to the network which can represent stimuli or state-dependent modulators; we let <bold>I</bold> depend on a discrete index variable <italic>s</italic> denoting the stimulus or experimental condition (or discretized parameter). <bold>I</bold>(<italic>s</italic>) can in general be time-dependent, but here we assume it is stationary for simplicity. We also assume that <bold>I</bold>(<italic>s</italic>) is deterministic, and that all quenched randomness in network structure is captured by <bold>W</bold> and <bold>γ</bold>.</p>
<p>In order to exploit tuning curve datasets to constrain mechanistic network models, and in the process make model-based inferences about circuit properties, we propose to view network models of the above type as generative models for tuning curves. This view, which we will expound below, is graphically summarized in <xref ref-type="fig" rid="pcbi.1006816.g001">Fig 1</xref>. (Even though here we take trial-averaged tuning curves as the ultimate functional output of the model, this is not central to our proposed view; as discussed at the end of the Discussion, other quantities characterizing neural activity such as pairwise correlations or higher-order population statistics can augment or replace tuning curves in general applications.)</p>
<fig id="pcbi.1006816.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Viewing mechanistic neuronal circuit models as generative models for tuning curves.</title>
<p>(A) Schematic representation of a generic recurrent circuit model from theoretical neuroscience. The network structure, in this case reduced to the full recurrent connectivity matrix <bold>W</bold>, is partially random with statistics described by the distribution <inline-formula id="pcbi.1006816.e002"><alternatives><graphic id="pcbi.1006816.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which depends on parameters <bold><italic>θ</italic></bold>. These parameters could, <italic>e</italic>.<italic>g</italic>., include the average strength or spatial range of connections between different cell types. The network receives an external input, <bold>I</bold>(<italic>s</italic>), that represents stimuli, which can be in one of <italic>S</italic> different conditions employed in an experiment. The output of the model is taken to be the sustained response, <inline-formula id="pcbi.1006816.e003"><alternatives><graphic id="pcbi.1006816.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, of a pre-selected probe neuron (blue arrow). Given a realization of the network structure <bold>W</bold> (sampled from <inline-formula id="pcbi.1006816.e004"><alternatives><graphic id="pcbi.1006816.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>), this response can be obtained in each stimulus condition, <italic>s</italic>, by simulating the network in the presence of external input <bold>I</bold>(<italic>s</italic>). The responses, <inline-formula id="pcbi.1006816.e005"><alternatives><graphic id="pcbi.1006816.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, for all stimulus conditions are then concatenated into a tuning curve vector <inline-formula id="pcbi.1006816.e006"><alternatives><graphic id="pcbi.1006816.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which is the ultimate output of the network when viewed as a generative model. (B) The deterministic mapping <italic>f</italic> stands for the simulation process that links the particular realization of the network structure <bold>W</bold> to its functional (tuning curve) output <bold>x</bold>. Since the ultimate goal is to use gradient-based methods to learn the model parameters, <bold><italic>θ</italic></bold>, the process of sampling a realization of <bold>W</bold> is cast (cyan area) using a parametrized mapping, <italic>g</italic><sub><bold><italic>θ</italic></bold></sub>, that transforms a set of standard noise variables <bold>z</bold>, sampled from a fixed distribution, into <bold>W</bold>. The composition of <italic>f</italic> and <italic>g</italic><sub><bold><italic>θ</italic></bold></sub> yields the full parametrized generator function, <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> ≡ <italic>f</italic> ◦ <italic>g</italic><sub><bold><italic>θ</italic></bold></sub>, as used in the GAN framework (bottom row). Given a set of parameters, the generator thus receives a set of noise variables, <bold>z</bold>, sampled from their standard distribution, and generates a tuning curve, <bold>x</bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g001" xlink:type="simple"/>
</fig>
<p>Given a choice of the non-dynamical variables <bold>W</bold>, <bold>γ</bold> and <bold>I</bold>(<italic>s</italic>), and the initial condition <bold>v</bold>(<italic>t</italic> = 0) (<italic>t</italic> = 0 can, <italic>e</italic>.<italic>g</italic>., be the stimulus onset time), the network dynamics can be simulated to compute the full temporal trajectory of all neural state variables. From this simulated trajectory, the average response, <inline-formula id="pcbi.1006816.e007"><alternatives><graphic id="pcbi.1006816.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, of a designated “probe” neuron in the network during a given “response interval” can be calculated in each condition <italic>s</italic> (this is the time-average of a certain component of <bold>v</bold><sub><italic>t</italic></sub> for <italic>t</italic> in the response interval). The vector <inline-formula id="pcbi.1006816.e008"><alternatives><graphic id="pcbi.1006816.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>≡</mml:mo> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>S</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is then the tuning curve of that neuron, containing its responses in <italic>S</italic> different stimulus conditions which we assume are present in the training data. (Note that once a network model is trained, it can be applied to stimuli other than those used for training.) Thus, for networks with deterministic dynamics considered here, there is a deterministic mapping between the tuple of network’s structural variables, (<bold>W</bold>, <bold>γ</bold>), and the tuning curve of a given network neuron (we assume fixed initial conditions, <bold>v</bold><sub><italic>t</italic>=0</sub>, or otherwise ignore the dependence of output on them). We call this mapping <italic>f</italic> (see <xref ref-type="fig" rid="pcbi.1006816.g001">Fig 1</xref>).</p>
<p>Note that <bold>γ</bold> and <bold>W</bold> typically have very large dimensions; for a network of <italic>N</italic> neurons, <bold>γ</bold> and <bold>W</bold> have on the order of <italic>N</italic> and <italic>N</italic><sup>2</sup> components, respectively. In the proposed methodology, these large sets of structural and physiological network constants should be viewed as latent variables rather than model <italic>parameters</italic> that are fit to data. We are interested in cases in which these heterogeneous high-dimensional vectors are sampled from statistical ensembles (distributions) that capture the structured regularities as well as disordered randomness in single-cell properties and network connectivity. Consider a statistical ensemble described by a parameterized distribution <inline-formula id="pcbi.1006816.e009"><alternatives><graphic id="pcbi.1006816.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Through the deterministic map, <italic>f</italic>, between (<bold>W</bold>, <bold>γ</bold>) and <bold>x</bold>, this distribution in turn induces a distribution, <inline-formula id="pcbi.1006816.e010"><alternatives><graphic id="pcbi.1006816.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, over the tuning curve <bold>x</bold>, which is also parameterized by <bold><italic>θ</italic></bold>. The model’s parameter vector, <bold><italic>θ</italic></bold>, which is typically low-dimensional (see the examples in Experiment 1–3 subsections below), determines the network’s statistical structure and constitutes the parameters that we would like to fit to data. Components of <bold><italic>θ</italic></bold> can control, <italic>e</italic>.<italic>g</italic>., the average strength or spatial range of synaptic connections, or the mean and dispersion of biophysical single-cell properties.</p>
<p>Traditional likelihood-based methods infer the circuit properties, encapsulated in <bold><italic>θ</italic></bold>, by maximizing the likelihood function <inline-formula id="pcbi.1006816.e011"><alternatives><graphic id="pcbi.1006816.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:msub><mml:mo>∏</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> given a dataset of tuning curves <inline-formula id="pcbi.1006816.e012"><alternatives><graphic id="pcbi.1006816.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. However, for cases of interest, the mapping <italic>f</italic> is typically very complex and practically cannot be inverted (even though it can be relatively cheaply simulated in the forward direction); therefore in practice <inline-formula id="pcbi.1006816.e013"><alternatives><graphic id="pcbi.1006816.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> cannot be computed explicitly. Moreover, most likelihood-based methods are based on expectation-maximization-like algorithms; in the expectation step, such algorithms have to infer the high-dimensional latent variables (<bold>W</bold>, <bold>γ</bold>), which is a highly expensive computation.</p>
<p>In the next subsection we discuss how recently developed methods in machine learning, in particular generative adversarial networks, can be used to fit generative models of the above type, for which the parametrized data distribution, <inline-formula id="pcbi.1006816.e014"><alternatives><graphic id="pcbi.1006816.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, is only implicitly defined. All that is required in those approaches is the generative process that, given a random seed, generates a tuning curve (or a set of tuning curves), via a function that is differentiable with respect to <bold><italic>θ</italic></bold>. More formally, in such frameworks the generative model, or the “generator”, is characterized by a parametrized function <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> which, given an input vector, <bold>z</bold>, of random noise variables that have a <italic>fixed</italic> or standard distribution, outputs a tuning curve <bold>x</bold> = <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>). This is almost identical to the case of mechanistic circuit models described above, with two technical differences. First, the generative network process, as described above and in <xref ref-type="fig" rid="pcbi.1006816.g001">Fig 1</xref>, is captured by the function <italic>f</italic> which does not directly depend on the model parameters, <bold><italic>θ</italic></bold>. Instead, it is the inputs to this function, namely (<bold>W</bold>, <bold>γ</bold>), which implicitly depend on <bold><italic>θ</italic></bold>, as they are sampled from <inline-formula id="pcbi.1006816.e015"><alternatives><graphic id="pcbi.1006816.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Thus the second difference is that the inputs to <italic>f</italic>, <italic>i</italic>.<italic>e</italic>., the network’s structural variables (<bold>W</bold>, <bold>γ</bold>), do not have a fixed standard distribution, but rather a distribution dependent on <bold><italic>θ</italic></bold>.</p>
<p>However, we can use the so-called “reparametrization trick” [<xref ref-type="bibr" rid="pcbi.1006816.ref022">22</xref>], to remedy this mismatch and cast the circuit-model-based generative processes of interest to us in the required form. To this end, we will formulate the sampling of (<bold>W</bold>, <bold>γ</bold>) from their statistical ensemble via a deterministic function or mapping, <italic>g</italic><sub><bold><italic>θ</italic></bold></sub>, parametrized by <bold><italic>θ</italic></bold>, that receives the fixed-distribution noise variables <bold>z</bold> as input. For example, a synpatic weight, <italic>w</italic>, with a gaussian distribution parametrized by its mean, <italic>μ</italic>, and variance, <italic>σ</italic><sup>2</sup>, can be generated by the function <italic>g</italic><sub><bold><italic>θ</italic></bold></sub> (<italic>z</italic>) ≡ <italic>μ</italic> + <italic>σz</italic> where <italic>z</italic> is sampled from the standard (zero-mean, unit-variance) normal distribution, and <bold><italic>θ</italic></bold> = (<italic>μ</italic>, <italic>σ</italic>) in this case. We provide biologically relevant examples of <italic>g</italic><sub><bold><italic>θ</italic></bold></sub> in Materials and methods (see Eqs (<xref ref-type="disp-formula" rid="pcbi.1006816.e108">17</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1006816.e110">19</xref>), (<xref ref-type="disp-formula" rid="pcbi.1006816.e119">24</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006816.e127">26</xref>)). Note that in the typical application of interest to us, while <bold><italic>θ</italic></bold> is low-dimensional, <bold>z</bold> has high dimensionality, on the order of the dimensions of (<bold>W</bold>, <bold>γ</bold>). The full generator function <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> is then simply the composition of <italic>f</italic> and <italic>g</italic><sub><bold><italic>θ</italic></bold></sub>: <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>) ≡ <italic>f</italic> (<italic>g</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>)) (see <xref ref-type="fig" rid="pcbi.1006816.g001">Fig 1B</xref>). In other words, first the standard noise variables <bold>z</bold> and network parameters <bold><italic>θ</italic></bold> together determine the full particular realization of network structure. The network is then simulated and a tuning curve (or a set of tuning curves) is generated. The function <italic>f</italic> is typically differentiable, and for many statistical ensembles of interest the function <italic>g</italic><sub><bold><italic>θ</italic></bold></sub> is (or can be closely approximated by a) function that is differentiable with respect to <bold><italic>θ</italic></bold>. Then <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> will also be differentiable in <bold><italic>θ</italic></bold>. As we describe in the rest of the article, with this formulation, we can use methods like generative adversarial networks to fit mechanistic neuronal circuit models to datasets of tuning curves.</p>
</sec>
<sec id="sec004">
<title>Generative Adversarial Networks</title>
<p>Generative Adversarial Networks (GANs) are a framework for training generative models developed by the deep learning community [<xref ref-type="bibr" rid="pcbi.1006816.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref030">30</xref>]. The GAN approach is powerful because it is applicable to implicit generative models, <italic>e</italic>.<italic>g</italic>., the mechanistic networks discussed in the previous subsection, for which evaluating the likelihood function or its gradient is intractable. Another advantage of GANs (in the context of the previous subsection) is that, unlike typical likelihood-based methods, they fit the model parameters <bold><italic>θ</italic></bold> directly, skipping the computationally costly step of inferring the high-dimensional latent variables, namely the particular realization of network connectivity matrix <bold>W</bold> or single-cell constants <bold>γ</bold>. Note that unlike the particular realization of the connectivity matrix between the experimentally sampled cells, the model parameters <bold><italic>θ</italic></bold>, which characterize the <italic>statistics</italic> of connectivity and single-cell properties, are generalizable and of direct scientific interest.</p>
<p>All that is required in the GAN approach is a generative process that, given a random seed, generates a sample data object, via a function that is differentiable with respect to <bold><italic>θ</italic></bold>. While in machine learning applications the generated data object is often an image (and the generative model is a model of natural images), in our case it will be a tuning curve, formalized in the Introduction as the vector <inline-formula id="pcbi.1006816.e016"><alternatives><graphic id="pcbi.1006816.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>S</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> containing the trial-averaged responses of a probed network neuron in <italic>S</italic> different experimental conditions (<italic>e</italic>.<italic>g</italic>., <italic>S</italic> different values of a stimulus parameter).</p>
<p>In a GAN there are two networks: a “generator” and a “discriminator”. The generator implements the generative model and generates sample data objects, while the discriminator (which can be a classifier) distinguishes true empirical data objects from “fake” ones generated by the generative model. Conceptually, the discriminator and generator compete: the discriminator is trained to better distinguish real from fake data, and the generator is trained to fool the discriminator.</p>
<p>More formally, the generator is characterized by a parametrized function <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> which, given an input vector of random noise variables <bold>z</bold> that have a <italic>fixed</italic> distribution, outputs a sample data object <bold>x</bold> = <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>). As we saw in the previous subsection, the process of generating a tuning curve by mechanistic neuronal circuit models can also be formulated in this manner (see <xref ref-type="fig" rid="pcbi.1006816.g001">Fig 1</xref>). In that case, the vector <bold>z</bold> provides the random seed for the generation of the full network structure (<italic>i</italic>.<italic>e</italic>., all synaptic weights and single-cell biophysical constants), given which the network is simulated to generated an output tuning curve <bold>x</bold>. We note that while in our applications the generator parameters <bold><italic>θ</italic></bold> usually correspond to physiological and anatomical parameters with clear mechanistic interpretations, in typical machine learning usage the structure of the GAN generator (<italic>e</italic>.<italic>g</italic>., a deconvolutional deep feedforward network, generating natural images) and its parameters may have no direct mechanistic interpretation (see <xref ref-type="sec" rid="sec015">Materials and methods</xref>, under “Differences with common machine learning applications”, for further discussion of this point).</p>
<p>The second network in a GAN is the discriminator. Mathematically, the discriminator is a function <inline-formula id="pcbi.1006816.e017"><alternatives><graphic id="pcbi.1006816.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, parametrized by <bold><italic>w</italic></bold>, that receives a data object (in our case, a tuning curve) as input, and outputs a scalar. <inline-formula id="pcbi.1006816.e018"><alternatives><graphic id="pcbi.1006816.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is trained so that its output maximally discriminates between the real data samples and those generated by <italic>G</italic><sub><bold><italic>θ</italic></bold></sub>. The generator is in turn trained to fool the discriminator. If the discriminator network is sufficiently flexible, the only way for the generator to fool it is to generate samples that effectively have the same joint distribution as real data. When <inline-formula id="pcbi.1006816.e019"><alternatives><graphic id="pcbi.1006816.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is differentiable in its input and parameters and <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> is differentiable in its parameters, training can be done using gradient-based algorithms. GANs can nevertheless be difficult to train and many techniques have been developed to improve their training. In this work we employ the Wasserstein GAN (WGAN) approach which has shown promise in overcoming some of the shortcomings of the traditional GAN approach [<xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>]. (We note, however, that traditional GANs could also be used for the types of application we have in mind.) The WGAN approach is mathematically motivated by minimizing the Wasserstein or earth mover’s distance between the empirical data distribution and the distribution of the generator output [<xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>]. The Wasserstein distance provides a measure of similarity between distributions which (unlike <italic>e</italic>.<italic>g</italic>., the Kullback-Leibler divergence, which is the distance minimized in maximum-likelihood fitting) exploits the metric or similarity structure in the data space <inline-formula id="pcbi.1006816.e020"><alternatives><graphic id="pcbi.1006816.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>S</mml:mi></mml:msup></mml:math></alternatives></inline-formula>.</p>
<p>In the context of WGANs, the discriminator can be viewed as yielding a <italic>scalar</italic> measure or “summary statistic” for a (typically high-dimensional) data object or tuning curve. A single, fixed summary statistic <inline-formula id="pcbi.1006816.e021"><alternatives><graphic id="pcbi.1006816.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be used to measure the divergence between two distributions as the difference between the expectation of <inline-formula id="pcbi.1006816.e022"><alternatives><graphic id="pcbi.1006816.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> under the two distributions. However, two distributions can lead to the same average <inline-formula id="pcbi.1006816.e023"><alternatives><graphic id="pcbi.1006816.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and yet be completely different in other respects. For example, consider the case of orientation tuning curves for visual cortical neurons. An example <inline-formula id="pcbi.1006816.e024"><alternatives><graphic id="pcbi.1006816.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> is the function that receives an orientation tuning curve as input and outputs the half-width of that tuning curve (which measures the strength of orientation tuning). A generative model may produce orientation tuning curves that on average are narrower (or broader) than the average empirical tuning curve. However, even when a model matches the data distribution of tuning curve widths, its generated tuning curves may look very different from true ones along other dimensions (<italic>e</italic>.<italic>g</italic>., along the average height or maximum response dimension, or in the variance of tuning widths). One interpretation of the WGAN methodology is that instead of looking at data objects (tuning curves) along a fixed dimension using a fixed scalar measure, it optimizes that measure or probe to maximally distinguish between model-generated <italic>vs</italic>. empirical data objects. (In other flavors of GAN, the discriminator has other useful interpretations and can provide an estimate of the density of generator output in data space relative to the true data distribution; see <xref ref-type="sec" rid="sec015">Materials and methods</xref>, under “Alternatives to WGAN, and alternative views of GANs”.)</p>
<p>Let <inline-formula id="pcbi.1006816.e025"><alternatives><graphic id="pcbi.1006816.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> be the class of all possible “smooth summary statistics” that can be used to characterize tuning curves; more technically, <inline-formula id="pcbi.1006816.e026"><alternatives><graphic id="pcbi.1006816.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> is taken to be the set of all <italic>scalar</italic> functions of tuning curves, <inline-formula id="pcbi.1006816.e027"><alternatives><graphic id="pcbi.1006816.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>S</mml:mi></mml:msup> <mml:mo>→</mml:mo> <mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, that are Lipshitz-continuous with a Lipshitz constant less than one (if <inline-formula id="pcbi.1006816.e028"><alternatives><graphic id="pcbi.1006816.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> is differentiable, the latter condition is equivalent to constraining the gradient of <inline-formula id="pcbi.1006816.e029"><alternatives><graphic id="pcbi.1006816.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> to have norm less than one everywhere). Interestingly, by the Kantorovich-Rubinstein duality [<xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref031">31</xref>], the earth mover’s distance, <italic>d</italic>(<italic>ρ</italic>, <italic>ν</italic>), between two distributions, <italic>ρ</italic>(<bold>x</bold>) and <italic>ν</italic>(<bold>x</bold>), can be expressed as the difference between the expectations of a <italic>maximally discriminating</italic> smooth summary statistic under the two distributions:
<disp-formula id="pcbi.1006816.e030"><alternatives><graphic id="pcbi.1006816.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:munder> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
In our applications, one distribution (say <italic>ν</italic>) is the data distribution, and the other (say <italic>ρ</italic>) is the output distribution of <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> which we would like to move closer to the data distribution. This can thus be done by iterating between improving <inline-formula id="pcbi.1006816.e031"><alternatives><graphic id="pcbi.1006816.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> to maximize <inline-formula id="pcbi.1006816.e032"><alternatives><graphic id="pcbi.1006816.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:math></alternatives></inline-formula>, and improving <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> to minimize it.</p>
<p>To obtain a practical algorithm that in this manner approximately minimizes the earth mover’s distance, <xref ref-type="disp-formula" rid="pcbi.1006816.e030">Eq (2)</xref>, the expectations over the two distributions are approximated by mini-batch sample averages, and the discriminator class <inline-formula id="pcbi.1006816.e033"><alternatives><graphic id="pcbi.1006816.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> is approximated by single-output feedforward neural network (parametrized by weight vector <bold><italic>w</italic></bold>) with input-output gradients of norm less than one (such networks form a proper subset of <inline-formula id="pcbi.1006816.e034"><alternatives><graphic id="pcbi.1006816.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>; in practice, the gradient norm is only forced to be close to one). This gives rise to an adversarial algorithm in which the discriminator <inline-formula id="pcbi.1006816.e035"><alternatives><graphic id="pcbi.1006816.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and the generator <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> are trained by iteratively alternating between minimizing the following two loss functions
<disp-formula id="pcbi.1006816.e036"><alternatives><graphic id="pcbi.1006816.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Gradient</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Penalty</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
<disp-formula id="pcbi.1006816.e037"><alternatives><graphic id="pcbi.1006816.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <xref ref-type="disp-formula" rid="pcbi.1006816.e036">Eq 3</xref> is minimized with respect to the discriminator’s parameters (<bold><italic>w</italic></bold>), and <xref ref-type="disp-formula" rid="pcbi.1006816.e037">Eq 4</xref> is minimized with respect to the generator’s parameters (<bold><italic>θ</italic></bold>). Here <inline-formula id="pcbi.1006816.e038"><alternatives><graphic id="pcbi.1006816.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006816.e039"><alternatives><graphic id="pcbi.1006816.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub></mml:math></alternatives></inline-formula> denote averages over a batch of empirical data samples and samples from the standard noise distribution, respectively (thus <inline-formula id="pcbi.1006816.e040"><alternatives><graphic id="pcbi.1006816.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the same as the average of <inline-formula id="pcbi.1006816.e041"><alternatives><graphic id="pcbi.1006816.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> when <bold>x</bold> is sampled from the generator output, instead of the empirical data distribution). The “Gradient Penalty” term forces the gradient of <inline-formula id="pcbi.1006816.e042"><alternatives><graphic id="pcbi.1006816.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> to be close to one (see <xref ref-type="sec" rid="sec015">Materials and methods</xref>, under “Conditional Generative Adversarial Networks”, for details). Finally, a penalty term Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) can be added to the generator loss, <xref ref-type="disp-formula" rid="pcbi.1006816.e037">Eq (4)</xref>, as a regularization term for generator parameters.</p>
<sec id="sec005">
<title>Connection with moment matching</title>
<p>Here we point out a connection between the WGAN and the moment matching approach to model fitting. In the simplest version of moment matching, a certain scalar statistic of interest, <inline-formula id="pcbi.1006816.e043"><alternatives><graphic id="pcbi.1006816.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi mathvariant="script">M</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, that evaluates on data objects, is first chosen. The model parameters are then fit by matching the expectations or averages of that statistic (the “moments”) under the empirical and generative model distributions. This can be done <italic>e</italic>.<italic>g</italic>., by minimizing <inline-formula id="pcbi.1006816.e044"><alternatives><graphic id="pcbi.1006816.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mi mathvariant="script">M</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (in practice the squared difference is usually minimized). Comparing this equation with the Wasserstein distance <xref ref-type="disp-formula" rid="pcbi.1006816.e030">Eq (2)</xref> underlying WGAN, we see that (at least in the case of matching a single moment) moment matching is equivalent to using a pre-defined and fixed discriminator <inline-formula id="pcbi.1006816.e045"><alternatives><graphic id="pcbi.1006816.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi mathvariant="script">D</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">M</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. By contrast, in the GAN approach the discriminator is not pre-determined, but is optimally chosen from a large and flexible class of discriminator functions to maximally distinguish true <italic>vs</italic>. generated data objects. An intermediate method for fitting implicit generative models is provided by Generative Moment Matching Networks [<xref ref-type="bibr" rid="pcbi.1006816.ref032">32</xref>] which can be thought of as a kernel-based generalization of moment matching.</p>
</sec>
<sec id="sec006">
<title>Conditional GANs</title>
<p>In the original GAN formulation, the discriminator only receives the generator’s output, which in our case is a tuning curve, <inline-formula id="pcbi.1006816.e046"><alternatives><graphic id="pcbi.1006816.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>S</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, defined as the joint responses of a neuron across <italic>S</italic> different stimulus conditions. In particular, this means that all tuning curves in the training dataset must contain responses in all of the same <italic>S</italic> conditions, as the discriminator input size cannot vary. Any empirical tuning curve with missing conditions must be discarded from the experimental dataset.</p>
<p>These shortcomings are ameliorated by using a so-called conditional GAN [<xref ref-type="bibr" rid="pcbi.1006816.ref033">33</xref>]. In our setting, the conditional GAN’s discriminator receives as input not only the neuronal responses, <bold>x</bold>, but also a subset of the experimental or stimulus parameters at which those responses were recorded. Since the discriminator only sees the responses at one value of the explicitly provided experimental parameters at a time, not every neuron has to be recorded for all choices of those parameters. For example, in Experiment 3 below, we consider the case of primary visual cortical neurons whose responses to grating stimuli of different sizes are recorded, but the grating center is allowed to have a nonzero offset from the neuron’s receptive field center. If we consider the offset as the explicit condition for the conditional GAN, we can include size tuning curves in the dataset that contain recorded responses at all stimulus sizes, but not at all possible offsets.</p>
<p>In what follows we use both the original WGAN formulation (in Experiments 1–3), and later the conditional WGAN (cWGAN; in Experiment 3) to demonstrate the power of the latter method to fit a simulated dataset with diverse stimulus conditions.</p>
</sec>
</sec>
<sec id="sec007">
<title>Experiment 1: Complex M1 tuning curves</title>
<p>In this subsection and the next two, we illustrate the GAN-based model fitting approach by applying it to two previously published mechanistic circuit models developed in theoretical neuroscience to explain various nonlinear features of cortical responses.</p>
<p>As our first example we take a feedforward model of primary motor cortex (M1) tuning curves proposed by Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>]. The tuning curves describe the response tuning of M1 neurons as a function of 3D hand position and posture (supination or pronation). The model was proposed as a simple plausible mechanism for generating the significant complex nonlinear deviations of observed monkey M1 tuning curves from the classical linear model [<xref ref-type="bibr" rid="pcbi.1006816.ref034">34</xref>].</p>
<p>We used the “extended” model of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] with small modifications (see <xref ref-type="sec" rid="sec015">Materials and methods</xref> subsection “Feedforward Model of M1”). In particular, we did not model response tuning with respect to hand posture (we did this in order to increase the size of the dataset by combining hand-position tuning curves from both posture conditions to allow for proper cross-validated testing of model fits). The model is a two-layer feedforward network, with the input layer putatively corresponding to the parietal reach area or to premotor cortex, and the output layer modeling M1 (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2</xref>). The input layer neurons’ activations are given by 3D Gaussian tuning curves defined on the hand position space. The receptive field centers formed a fine regular grid, but their widths varied randomly across the input layer, and were sampled independently from the uniform distribution on the range [<italic>σ</italic><sub><italic>l</italic></sub>, <italic>σ</italic><sub><italic>l</italic></sub> + <italic>δσ</italic>] (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2A</xref>). The feedforward connections from the input to output layer are random and sparse, with a connection probability of 0.01. In our implementation of this model, the strength of the nonzero connections were sampled independently from the uniform distribution on the range [0, <italic>J</italic>]. The response of an output layer neuron is given by a rectified linear response function with a threshold. The thresholds were allowed to be heterogeneous across the M1 layer, and were sampled independently from the uniform distribution on the range [<italic>ϕ</italic><sub><italic>l</italic></sub>, <italic>ϕ</italic><sub><italic>l</italic></sub> + <italic>δϕ</italic>]. Thus in total the model has five trainable parameters <bold><italic>θ</italic></bold> = (<italic>σ</italic><sub><italic>l</italic></sub>, <italic>δσ</italic>, <italic>J</italic>, <italic>ϕ</italic><sub><italic>l</italic></sub>, <italic>δϕ</italic>).</p>
<fig id="pcbi.1006816.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Structure of the feedforward and recurrent generative models used in our computational experiments.</title>
<p>(A) The feedforward network model of primary motor cortex (M1) is borrowed from Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] and produces heterogeneous hand-location tuning curves. This heterogeneity is rooted in the random network structure, including the variability in the input layer (modeling a premotor or parietal area) receptive field widths, <italic>σ</italic><sub><italic>i</italic></sub>, feedforward weights to the M1 layer, <inline-formula id="pcbi.1006816.e047"><alternatives><graphic id="pcbi.1006816.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, and the threshold, <italic>ϕ</italic>, of M1 output neurons which are rectified linear units. (B) The structure of the Stabilized Supralinear Network (SSN) with one-dimensional topography (retinotopy) as a model of the primary visual cortex (V1). The SSN is a recurrent network of excitatory (<italic>E</italic>) and inhibitory (<italic>I</italic>) neurons. The visual stimulus (bottom) models the input to V1 due to a grating of diameter <italic>b</italic><sub><italic>s</italic></sub> in condition <italic>s</italic>. Heterogeneity in model output (size tuning curves) originates in the heterogeneity of feedforward and recurrent horizontal connections. The mean and variance of the horizontal connections between SSN neurons depend on the pre- and postsynaptic cell-types and their retinotopic distance, and for different connection-types falloff over different characteristic length scales. For a full description of models and their parameters see <xref ref-type="sec" rid="sec015">Materials and methods</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g002" xlink:type="simple"/>
</fig>
<p>Using the WGAN framework, we fit the five model parameters to a dataset of experimentally measured monkey M1 tuning curves recorded by Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] and available online. With hand-posture conditions ignored, the tuning curves in this dataset describe the trial-averaged responses of a given M1 neuron in 27 experimental conditions corresponding to the monkey holding its hand in one location out of a 3 × 3 × 3 cubic grid of 3D spatial locations. (We ignored the hand-position label by blindly mixing hand-position tuning curves across pronation and supination conditions, as if they belonged to different neurons.) We randomly selected half of the hand-position tuning curves (<italic>n</italic> = 257) to be used for training the model, and used the other half (<italic>n</italic> = 258) as held-out data to evaluate the goodness of model fits.</p>
<p>The results showing the performance of the fit model are summarized in <xref ref-type="fig" rid="pcbi.1006816.g003">Fig 3</xref>. The figure shows the data and trained model histograms for four test statistics or measures characterizing the tuning curves or responses: firing rate (across the 27 conditions), coding level, <italic>i</italic>.<italic>e</italic>., the fraction of conditions with rate significantly greater than zero (which we took to mean larger than 5 Hz), the <italic>R</italic><sup>2</sup> or coefficient of determination of the optimal linear fit to the tuning curve, and the complexity score. Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] defined the complexity score of a tuning curve to be the standard deviation of the absolute response differences between nearest neighbor locations on the 3 × 3 × 3 lattice of hand positions (the tuning curve was first normalized so that its responses ranged from −1 to 1).</p>
<fig id="pcbi.1006816.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Summary of the feedforward model fit to the M1 tuning curve dataset of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>].</title>
<p>(A) Data and model tuning curves at different stages of training; the plotted tuning curves are the projections of the 3D tuning curves along the preferred position (PP) vector which is obtained by a linear fit to the 3D tuning curve (see Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>]). (B) Kolmogorov-Smirnov (KS) distance throughout training between the data and model distributions, shown in panels C–F, of four summary statistics characterizing the tuning curves evaluated on held-out data. Vertical dashed lines mark the epochs at which curves in panel A are sampled. (C–F) histograms of four tuning curve statistics (<italic>R</italic><sup>2</sup>, complexity score, firing rates, and coding level) showing the data distribution (black), the initial model distribution (blue) and the final model distribution after fitting (red). Vertical lines show the mean value of each histogram.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g003" xlink:type="simple"/>
</fig>
<p>The last two statistics, the <italic>R</italic><sup>2</sup> and the complexity score, are of particular theoretical interest, as they provide two measures for the degree of irregular nonlinearity in the tuning curve and thus deviation from the classical linear model of M1 tuning curves [<xref ref-type="bibr" rid="pcbi.1006816.ref034">34</xref>]. Therefore, Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] fit their model by matching the mean and standard deviation of these two statistics between model and M1 data. By contrast, as described in the previous subsection, the WGAN, employed here, optimizes the discriminatory statistic, <inline-formula id="pcbi.1006816.e048"><alternatives><graphic id="pcbi.1006816.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula>, and (for complex enough discriminator networks) seeks to fit the entire joint multi-dimensional (in this case 27-dimensional) tuning curve distribution.</p>
<p>We measured the mismatch between the model and data distributions for each of the four test statistics using the Kolmogorov-Smirnov (KS) distance. As shown in <xref ref-type="fig" rid="pcbi.1006816.g003">Fig 3B</xref>, the goodness of fit improves fast during training. The goodness-of-the-fit of the distributions of complexity score and <italic>R</italic><sup>2</sup> (<xref ref-type="fig" rid="pcbi.1006816.g003">Fig 3C and 3D</xref>) are comparable to those obtained in Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] by grid search. It is also notable that the trained model fits the distribution of firing rates and coding levels (<xref ref-type="fig" rid="pcbi.1006816.g003">Fig 3E and 3F</xref>) quite well. In Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>], the authors chose to individually normalize the model and data tuning curves so that their shape, but not their overall firing rate scale, was fit to data. We did not normalize the tuning curves, but as described above added a tunable parameter, <italic>J</italic>, to the model that scales the feedforward connection strengths. We found that with this addition and without normalization, the model is actually capable of accounting for the variability of firing rates across neurons and conditions as well. In the original model of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] the thresholds were chosen separately for each output layer neuron such that the coding level of its response matched that of a randomly selected recorded M1 neuron. By contrast, in order to have all structural variability in the model in a differentiable form, we did not fit individual thresholds, but allowed them to be randomly distributed and only fit the two parameters of their distribution, <italic>ϕ</italic><sub><italic>l</italic></sub> and <italic>δϕ</italic><sub><italic>l</italic></sub>. Even though we did not match coding levels neuron-by-neuron by adjusting individual neural thresholds, the model was able to match the distribution of neural coding levels in the dataset, without the need for tuning individual thresholds.</p>
</sec>
<sec id="sec008">
<title>Experiment 2: Stabilized Supralinear Network</title>
<p>The second network model we consider is a recurrent model of local cortical circuitry, the Stabilized Supralinear Network (SSN), which has found broad success in mechanistically explaining a range of nonlinear modulations of neural responses and variability by sensory context or attention, in multiple cortical areas [<xref ref-type="bibr" rid="pcbi.1006816.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref036">36</xref>]. The SSN is a recurrent rate network of excitatory (<italic>E</italic>) and inhibitory (<italic>I</italic>) neurons which have a supralinear rectified power-law input-output function, <inline-formula id="pcbi.1006816.e049"><alternatives><graphic id="pcbi.1006816.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:msubsup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>u</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (where <italic>u</italic> is the total input to a cell, <italic>k</italic> &gt; 0 and <italic>n</italic> &gt; 1 are constants, and [<italic>u</italic>]<sub>+</sub> = max(0, <italic>u</italic>) denotes rectification). The dynamical state of the network of <italic>N</italic> neurons is the vector of firing rates <inline-formula id="pcbi.1006816.e050"><alternatives><graphic id="pcbi.1006816.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> which is governed by the differential equation
<disp-formula id="pcbi.1006816.e051"><alternatives><graphic id="pcbi.1006816.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi> <mml:mo>[</mml:mo> <mml:mi>W</mml:mi> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:msubsup><mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>W</italic> and <italic>F</italic> denote the recurrent and feedforward weight matrices (with structure described below), the diagonal matrix <inline-formula id="pcbi.1006816.e052"><alternatives><graphic id="pcbi.1006816.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:mi>τ</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>Diag</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> contains the neural relaxation time constants, <italic>τ</italic><sub><italic>i</italic></sub>, and <bold>I</bold>(<italic>s</italic>) denotes the stimulus input in condition <italic>s</italic> ∈ {1, …, <italic>S</italic>}.</p>
<p>We consider a topographically organized version of SSN with a one-dimensional topographic map which could correspond, <italic>e</italic>.<italic>g</italic>., to a one-dimensional reduction of the retinotopic map in primary visual cortex (V1). We adopt this interpretation here, and simulate the fitting of an SSN model of V1 to a dataset of V1 grating-size tuning curves, which are commonly used to characterize surround suppression [<xref ref-type="bibr" rid="pcbi.1006816.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref038">38</xref>]. The model has a neuron of each type, <italic>E</italic> and <italic>I</italic>, at each topographic spatial location. For the <italic>i</italic>-th model neuron, we denote its type by <italic>α</italic>(<italic>i</italic>) ∈ {<italic>E</italic>, <italic>I</italic>} and its topographic location by <italic>x</italic><sub><italic>i</italic></sub> (which ranged from −0.5 to 0.5 on a regular grid).</p>
<p>In many cortical areas the statistics of local recurrent connectivity, such as connection probability and average strength, systematically depend on several factors, including the types of the pre- and post-synaptic neurons, the physical distance between them, or the difference between their preferred stimulus features [<xref ref-type="bibr" rid="pcbi.1006816.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref018">18</xref>]. We made a choice for the distribution of <italic>W</italic><sub><italic>ij</italic></sub> (the connection strength from neuron <italic>j</italic> to neuron <italic>i</italic>) that accounts for such dependencies in our topographic network, and also respects Dale’s principle [<xref ref-type="bibr" rid="pcbi.1006816.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref040">40</xref>]. In order to make the GAN method applicable, another criteria was that <italic>W</italic><sub><italic>ij</italic></sub> are differentiable with respect to the parameters of their distribution. The most relevant aspects of the distribution of <italic>W</italic><sub><italic>ij</italic></sub> for the network dynamics are its first two moments (equivalently, mean and variance). We assumed that <italic>W</italic><sub><italic>ij</italic></sub>’s are independent with uniform distributions on the range [〈<italic>W</italic><sub><italic>ij</italic></sub>〉 − <italic>δW</italic><sub><italic>ij</italic></sub>/2, 〈<italic>W</italic><sub><italic>ij</italic></sub>〉 + <italic>δW</italic><sub><italic>ij</italic></sub>/2], with a mean, 〈<italic>W</italic><sub><italic>ij</italic></sub>〉, and width, <italic>δW</italic><sub><italic>ij</italic></sub>, that depend on the pre- and post-synaptic types and fall off with the distance between the pre- and post-synaptic neurons over characteristic length scales. More precisely we chose the fall off to be Gaussian, and set
<disp-formula id="pcbi.1006816.e053"><alternatives><graphic id="pcbi.1006816.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>±</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
<disp-formula id="pcbi.1006816.e054"><alternatives><graphic id="pcbi.1006816.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mrow><mml:mi>δ</mml:mi> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
where <italic>a</italic> = <italic>α</italic>(<italic>i</italic>) and <italic>b</italic> = <italic>α</italic>(<italic>j</italic>) are the <italic>E</italic>/<italic>I</italic> types of the neurons <italic>i</italic> and <italic>j</italic>, respectively. The 2 × 2 matrices <inline-formula id="pcbi.1006816.e055"><alternatives><graphic id="pcbi.1006816.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, <italic>δJ</italic><sub><italic>ab</italic></sub>, and <italic>σ</italic><sub><italic>ab</italic></sub> constitute 12 trainable model parameters; they control the average strength, the degree of random heterogeneity, and the spatial range of the recurrent horizontal connections between different <italic>E</italic>/<italic>I</italic> cell types, respectively. All parameters are constrained to be non-negative, and we also enforced the constraint <inline-formula id="pcbi.1006816.e056"><alternatives><graphic id="pcbi.1006816.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>; the sign on the right side of <xref ref-type="disp-formula" rid="pcbi.1006816.e053">Eq (6)</xref>, which is positive or negative, when the presynaptic cell type <italic>b</italic> is <italic>E</italic> or <italic>I</italic>, respectively, enforces the correct sign for <italic>W</italic><sub><italic>ij</italic></sub> according to Dale’s principle.</p>
<p>The feedforward input, <italic>F</italic><bold>I</bold>(<italic>s</italic>), to the SSN is composed of the stimulus-independent feedforward connectivity <italic>F</italic> and the topographically structured visual stimulus <bold>I</bold>(<italic>s</italic>). We chose the matrix <italic>F</italic> to be square and diagonal and hence <bold>I</bold>(<italic>s</italic>) to be <italic>N</italic>-dimensional. To model size tuning curves, we let the visual input <bold>I</bold>(<italic>s</italic>) in condition <italic>s</italic> only target neurons in a central band of the topographic grid roughly extending from location −<italic>b</italic><sub><italic>s</italic></sub>/2 to <italic>b</italic><sub><italic>s</italic></sub>/2 (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2B</xref>), as a simple model of visual input from a grating with diameter <italic>b</italic><sub><italic>s</italic></sub>. We included quenched random heterogeneity in the diagonal feedforward weights by sampling <italic>F</italic><sub><italic>ii</italic></sub> independently from the uniform two-point distribution on [1 − <italic>V</italic>, 1 + <italic>V</italic>]; thus <italic>V</italic> controls the degree of disorder in the feedforward inputs to network cells.</p>
<p>We modeled size tuning curves based on the sustained response (defined as the time averaged firing rate during the “sustained response” period; see <xref ref-type="sec" rid="sec015">Materials and methods</xref>) of a subset of SSN neurons driven by stimuli of different diameters or sizes. In experiments, typically the grating stimulus used to measure a neuron’s size tuning curve is centered on the neuron’s receptive field. To model this stimulus centering, we let the model output, <inline-formula id="pcbi.1006816.e057"><alternatives><graphic id="pcbi.1006816.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, in condition <italic>s</italic> be the sustained response of the excitatory neuron at the center of the topographic grid (which is also the center of the stimulus with size <italic>b</italic><sub><italic>s</italic></sub>). Note that in general, the tuning curves in random SSN’s show variability across neurons as well as across different realizations of the network for a fixed output neuron. Furthermore, when <italic>N</italic> is large, the variability across the tuning curves of neurons with topographic locations sufficiently near the center often approximates variability of the center neuron across different network realizations with different <bold>z</bold>. Although we do not do so here, this self-averaging property may in principle be exploited for more efficient training of the model.</p>
<p>We used the GAN framework to fit the 12 recurrent connectivity parameters and one feedforward parameter of the SSN model, <inline-formula id="pcbi.1006816.e058"><alternatives><graphic id="pcbi.1006816.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, to simulated data. The latter consisted of 2048 size tuning curves generated from a “ground truth” SSN with the connectivity parameters listed in <xref ref-type="table" rid="pcbi.1006816.t001">Table 1</xref>. (We used a slight reparametrization of <bold><italic>θ</italic></bold> in the actual WGAN implementation; see <xref ref-type="disp-formula" rid="pcbi.1006816.e131">Eq (28)</xref>.) All other model parameters were the same between the ground truth and trained SSN models (see <xref ref-type="sec" rid="sec015">Materials and methods</xref> for details).</p>
<table-wrap id="pcbi.1006816.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.t001</object-id>
<label>Table 1</label>
<caption>
<title>The parameter values for the ground-truth SSN used for generating the training set tuning curves.</title>
<p>The columns correspond to different (<italic>a</italic>, <italic>b</italic>) possibilities. The feedforward weight heterogeneity parameter is <italic>V</italic> = 0.1.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006816.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Parameter</th>
<th align="left"><italic>EE</italic></th>
<th align="left"><italic>EI</italic></th>
<th align="left"><italic>IE</italic></th>
<th align="left"><italic>II</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><italic>J</italic><sub><italic>ab</italic></sub></td>
<td align="char" char=".">0.3</td>
<td align="char" char=".">0.5</td>
<td align="char" char=".">0.4</td>
<td align="char" char=".">0.2</td>
</tr>
<tr>
<td align="left"><italic>δJ</italic><sub><italic>ab</italic></sub></td>
<td align="char" char=".">0.3</td>
<td align="char" char=".">0.4</td>
<td align="char" char=".">0.6</td>
<td align="char" char=".">0.19</td>
</tr>
<tr>
<td align="left"><italic>σ</italic><sub><italic>ab</italic></sub></td>
<td align="char" char=".">0.15</td>
<td align="char" char=".">0.025</td>
<td align="char" char=".">0.1</td>
<td align="char" char=".">0.025</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To quantify the goodness of fit between the data and trained model distributions of tuning curves, as in the previous section, we compare the distributions of four test statistics or measures characterizing the size tuning curves: preferred stimulus size, maximum firing rate, the suppression index, and normalized participation ratio. The preferred stimulus size is the stimulus size eliciting the largest response or maximum firing rate. The suppression index for a neuron (or tuning curve) measures how much the response of that neuron is suppressed at large stimulus sizes relative to response to its preferred size. Finally, the normalized participation ratio measures the fraction of tested stimulus sizes that elicit a response close to the maximum response. (See <xref ref-type="sec" rid="sec015">Materials and methods</xref> for the precise definition). Note that while to fit the model we used size tuning curves containing responses to stimuli with <italic>S</italic> = 8 different sizes, for testing purposes, we generated tuning curves from the trained and ground-truth SSN’s using a larger set of stimulus sizes. In particular, the tuning curves constituting the “true data” used for the tests in <xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4</xref> were not part of the training dataset, but were newly generated from the ground-truth model, and should thus be considered held-out test data. Moreover, they included responses to stimuli of sizes that were not covered in the tuning curves used in training.</p>
<fig id="pcbi.1006816.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Summary of the recurrent SSN model fit to simulated V1 tuning curve data.</title>
<p>(A) Data and model tuning curves at three different stages of training. (B) KS distance between the data and model distributions of four summary statistics characterizing the tuning curves, throughout training. Vertical dashed lines mark the epochs at which curves in panel A are sampled. (C) Scatter plots of peak rate <italic>vs</italic>. suppression index (S.I.) for the training data, the WGAN fit, and moment matching fit. (D) Cumulative distribution functions (CDF) for preferred stimulus size and normalized participation ratio of the data, the WGAN fit, and moment matching.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g004" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4C and 4D</xref> provide comparisons of the distributions of these tuning curve attributes under the trained and ground truth SSN models. As in Experiment 1, we measured the mismatch of these distributions using the Kolmogorov-Smirnov (KS) distance. The KS distance for all distributions becomes very small as a result of learning (<xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4B</xref>), reflecting the close fit of all test statistics from the trained model with the data (<xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4C and 4D</xref>). (Note that since the generator was not directly trained to minimize any of the above KS distances, there is no reason why they should decrease monotonically during learning.)</p>
<p>Although moment matching can capture the overall shape of the one-dimensional (marginal) distributions of some of the test statistics (<italic>e</italic>.<italic>g</italic>., see <xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4D</xref> for preferred size), it fails to capture some details in higher-dimensional joint distributions. For example, moment matching underestimates the density of the joint distribution of the suppression index and peak rate between the two peaks. By contrast, the WGAN faithfully captures this joint distribution (<xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4C</xref>). We emphasize again that such summary statistics (and their distributions) did not directly play a role in the WGAN fit; instead, as discussed in subsection “Generative Adversarial Networks” above, the WGAN’s discriminator network automatically discovers the relevant optimal statistic, and in this way fits the full high-dimensional tuning curve distribution, and in particular the low-dimensional distributions plotted in <xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4C and 4D</xref>.</p>
</sec>
<sec id="sec009">
<title>Experiment 3: Parameter identification</title>
<p>In the point of view adopted in this paper, the distribution of neural tuning curves serves as a probe into the network structure; the richer the tuning curve dataset, the stronger the probe. Richness of tuning curve data can correspond to at least two different factors. One factor is the richness of stimuli, or the breadth and dimensionality of the region of stimulus parameter space covered in the tuning curves. A second factor is the degree to which each neuron’s tuning curve is associated with other functional or anatomical information such as cell type, preferred stimulus, topographic location, etc.</p>
<p>When the probe is not sufficiently strong, it may not serve to fully uncover the network structure as encoded in model parameters. When a model class is at all capable of capturing the observed tuning curve distribution, it may be the case that models within that class but with widely different parameters are equally capable of fitting the tuning curve distribution. In such a scenario the model parameters will not be uniquely identifiable using the available tuning curve data. For example, in Experiment 2 we were able to train an SSN to accurately capture the size tuning curve distribution. However, in many runs the parameters of the trained SSN failed to match the parameters of the ground-truth SSN that had generated the training dataset. This failure is, however, not a failure of the WGAN training algorithm, as the training was consistently successful in capturing the size tuning curve distribution very well. The failure is partly due to the relative poverty of the tuning curve data used. In that example, the dataset contained only the size tuning curves of excitatory and centered neurons (neurons with receptive field or topographic location at the center of the stimulus). Moreover, stimulus parameters other than size, such as contrast, were not varied in the tuning curves. We thus set out to investigate whether enlarging the tuning curve data along some of the mentioned lines can allow for accurate identification of the network parameters using the GAN methodology.</p>
<p>We experimented with including identified inhibitory cell tuning curves in the training data, as well as size tuning curves for offset cells with topographic location not at stimulus center. However, we found that only including offset tuning curves was sufficient to enable parameter identification. We will report the results of this experiment here; in this training dataset we included the size tuning curves of neurons with the following possible offsets from stimulus center: <inline-formula id="pcbi.1006816.e059"><alternatives><graphic id="pcbi.1006816.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>O</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>16</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>4</mml:mn> <mml:mo>,</mml:mo> <mml:mn>3</mml:mn> <mml:mo>/</mml:mo> <mml:mn>8</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>To fit the SSN to the enriched dataset, here we employed the conditional WGAN (cWGAN) algorithm. As explained in the subsection Conditional GANs above, in cWGAN the discriminator and generator depend on a set of “condition” variables in addition to their inputs in the original WGAN. Here, we provided the topographic offset, <inline-formula id="pcbi.1006816.e060"><alternatives><graphic id="pcbi.1006816.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula>, of the cells as the conditional input. The cWGAN formalism allows for using tuning curves of neurons for which only some of the offsets are measured. In particular, it allows for exploiting off-center size tuning curves (which are commonly discarded) towards system identification.</p>
<p>We compare the quality of fits using the cWGAN and moment matching respectively (<xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5</xref>). Although moment matching produces a reasonable fit to the distribution of tuning curves for the individual features we explored (<xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4B</xref>), the GAN approach significantly outperforms moment matching at parameter identification (<xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5</xref>). This is summarized in the relative error plots in <xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5A and 5D</xref>; relative error was measured using the symmetric mean absolute percentage error (sMAPE), defined in <xref ref-type="disp-formula" rid="pcbi.1006816.e171">Eq (44)</xref> of Materials and methods. In particular, moment matching severely misestimates the <italic>δJ</italic><sub><italic>ab</italic></sub>’s (see <xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5E</xref>), which control the heterogeneity in recurrent horizontal connections. On the other hand, cWGAN was successful at identifying parameters with less than 10% error, and the fit of the summary statistic distributions was excellent.</p>
<fig id="pcbi.1006816.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g005</object-id>
<label>Fig 5</label>
<caption>
<title>System identification of the SSN model from simulated tuning curve data using the cWGAN method and moment matching methods.</title>
<p>Top and bottom rows show the results of cWGAN and moment matching fits, respectively. <bold>(A) &amp; (D)</bold>: symmetric mean average percent error (sMAPE), <xref ref-type="disp-formula" rid="pcbi.1006816.e171">Eq (44)</xref>, of all parameters throughout training. <bold>(B) &amp; (E)</bold>: the <inline-formula id="pcbi.1006816.e061"><alternatives><graphic id="pcbi.1006816.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (solid lines) and <italic>δJ</italic><sub><italic>ab</italic></sub> (shaded areas) parameters throughout training. The box to the right shows the true parameters as points (<inline-formula id="pcbi.1006816.e062"><alternatives><graphic id="pcbi.1006816.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) with error bars (<italic>δJ</italic><sub><italic>ab</italic></sub>). <bold>(C) &amp; (F)</bold>: <italic>σ</italic><sub><italic>ab</italic></sub> throughout training. Arrows to the right of the plot denote the true values of <italic>σ</italic><sub><italic>ab</italic></sub>; note that the <italic>E</italic> → <italic>I</italic> and <italic>I</italic> → <italic>I</italic> arrows occlude each other in panels C and F. In B-C and E-F colors represent connection-types as shown in the legend of B. The input heterogeneity parameter <italic>V</italic> (not plotted) converged to 0.100 for cWGAN, and to 0.116 for moment matching, compared to the value <italic>V</italic> = 0.1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g005" xlink:type="simple"/>
</fig>
<p>The results demonstrated in <xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5</xref> are robust. <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref> shows a histogram of percent error (quantified by sMAPE, <xref ref-type="disp-formula" rid="pcbi.1006816.e171">Eq (44)</xref>) for multiple moment matching and cWGAN fits, performed using a wide range of hyperparameters (see <xref ref-type="sec" rid="sec015">Materials and methods</xref>, under “Hyperparameters for parameter identification experiment”), including different learning rates for the generator, and for the discriminator in the cWGAN case. To provide a fair comparison of performance between the two methods, in <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref> we used an impartial termination criterion which is agnostic to the hyperparameters (see <xref ref-type="sec" rid="sec015">Materials and methods</xref>, under “Stopping criterion and performance metric”), for both cWGAN and moment matching. In all cases the cWGAN outperforms moment matching.</p>
<fig id="pcbi.1006816.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison of conditional Wasserstein GAN and moment matching.</title>
<p>Distribution of the relative error (symmetric mean average percent error; sMAPE) for our proposed method (cWGAN, in purple) and moment matching (MM, in Green) across different hyperparameters (see <xref ref-type="sec" rid="sec015">Materials and methods</xref>). The bars denote the number of trainings, possibly with different hyperparameters, resulting in an sMAPE within the corresponding bin. cWGAN consistently produced lower estimation errors compared to moment matching.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.g006" xlink:type="simple"/>
</fig>
<p>We observed that successful cWGAN fits required particularly small generator learning rates. By contrast, in other examples (not reported) in which the tuning curve dataset was richer, and inhibitory neuron tuning curves were also observed by the discriminator, trainings with ten times larger generator learning rates successfully identified the parameters. We also observed that moment matching was able to identify the parameters in those easier cases. More generally, we speculate that harder problems (<italic>i</italic>.<italic>e</italic>., those with poorer training data), such as in <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref>, may require a smaller WGAN generator learning rate for accurate parameter identification.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>Developing biologically grounded mechanistic models that can capture the diversity and heterogeneity of neural response properties and selectivities is an important aim of theoretical neuroscience. Methods that give us the ability to quantitatively constrain such models directly using neural data, such as datasets of tuning curves from a given brain area, can greatly facilitate this pursuit. Such methods allow inferring the structure of biological circuits of interest from observations of neural responses, as encoded, <italic>e</italic>.<italic>g</italic>., by tuning curve distributions.</p>
<p>The statistical fitting of most interesting mechanistic models of brain circuitry using classical likelihood-based approaches is often intractable. A practical solution used in the past has been to instead use models that are designed primarily based on the tractability of their likelihood functions and ease of fitting, as opposed to biological realism or purely theoretical criteria. Therefore the elements and parameters of such models often may not have a direct mechanistic, biological interpretation. Another approach to the challenge of fitting theoretically grounded mechanistic models has been to forego full probabilistic inference using likelihood-based approaches, and use moment matching to only match a few summary statistics (characterizing tuning curves and thus neural responses) between model and data. The drawback of this approach is that it is not information theoretically efficient and does not exploit all the available information in the dataset for the purpose of inferring network properties.</p>
<p>Here, we demonstrated that Generative Adversarial Networks (GANs) enable the fitting of mechanistic models developed in theoretical neuroscience to the <italic>full</italic> joint distribution of neural response data. Conceptually, we proposed to view mechanistic network models with randomness in their connectivity structure or other biophysical parameters as generative models for response tuning curves. Given this formulation, one can exploit a whole suite of recently developed methods in machine learning (of which GANs are an example) for fitting the full output distribution of <italic>implicit</italic> generative models (<italic>i</italic>.<italic>e</italic>., generative models for which the likelihood function is not available or is highly intractable).</p>
<p>In this paper we specifically focused on using the Wasserstein GAN (WGAN) [<xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>] approach for this purpose. In subsection “Generative Adversarial Networks” of Results we reviewed the basic GAN setup and the WGAN algorithm, and conceptually contrasted it with moment matching. In Experiment 1 and 2 we used this method to successfully fit two representative example models from theoretical neuroscience to real or simulated tuning curve data, demonstrating that this technique is applicable to inference based on a wide range of network models, including feedforward and recurrent models of cortical networks. Furthermore, in Experiment 3 we demonstrated that our method is able to consistently and accurately identify the true parameters of a cortical network model using tuning curve data. This experiment moreover provides an example application in which the WGAN approach, which exploits the full tuning curve distribution, is superior to moment matching which only considers a few moments of that distribution. However, we note that even though moment matching is information-theoretically weaker than GANs, in practice it may be advantageous for fitting simple circuit models as it can be trained faster.</p>
<p>In the rest of the Discussion, we review some of the potentials and pitfalls of our approach and some differences with other usages of GANs, in an attempt to make the path clearer for other future applications of and improvement to this approach.</p>
<sec id="sec011">
<title>Conditional <italic>vs</italic>. non-conditional GANs</title>
<p>In our experiments we used both conditional and non-conditional GANs. When using a non-conditional GAN as in Experiments 1 and 2, the discriminator only receives as input the discretized tuning curve in the form <inline-formula id="pcbi.1006816.e063"><alternatives><graphic id="pcbi.1006816.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>S</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Here, the index <italic>s</italic> denotes the stimulus condition, corresponding to combinations of different stimulus parameter values. In this formulation of the tuning curve, the relationship between stimulus parameters and the index <italic>s</italic> is thus lost to the discriminator. In particular, the discriminator is blind to the metric or similarity structure in the stimulus parameter space (<italic>e</italic>.<italic>g</italic>., it does not explicitly know whether two different components of <bold>x</bold> encode responses to very similar or widely different stimuli), and therefore cannot directly exploit that structure in discriminating between true and generated tuning curves. Another drawback of this formulation is that all neurons in the dataset must have been recorded in all stimulus conditions; when there are several stimulus parameters, however, recording each neuron in all conditions for many trials becomes experimentally prohibitive. On the other hand, non-conditional GANs are advantageous in allowing the generator to learn the joint distribution of single-neuron responses across the entire stimulus parameter space; in particular, it enables to fit the marginal distribution of “global” tuning curve features that depend jointly on responses at different values of multiple stimulus parameters.</p>
<p>The conditional GAN approach provides a complementary scheme for describing the tuning curve, <italic>i</italic>.<italic>e</italic>., the relationship between neuronal responses and stimulus parameters. In this case, different values of a subset of stimulus parameters are implicitly represented by the different components of <bold>x</bold>. However, <bold>x</bold> (and its component responses) depend also on another subset of stimulus parameters that are provided explicitly as inputs to the discriminator (as well as the generator), in the form of conditional GAN’s condition variables <italic>c</italic> (more generally, <italic>c</italic> need not be limited to subsets of stimulus parameters, and can, <italic>e</italic>.<italic>g</italic>., also denote a neuron’s cell type or preferred stimulus parameters). Since the value of these parameters is directly provided to the discriminator, the latter is not entirely blind to stimulus similarity structure. On the other hand, the conditional GAN framework only fits the conditional distributions of <bold>x</bold> at different values of <italic>c</italic>. This is beneficial in that we now do not need to record each neuron across all values of <italic>c</italic>. However, by the same virtue, the framework is blind to correlations of single-cell responses at different values of <italic>c</italic> across neurons, and may not fit the distribution of single-neuron tuning curve features that depend jointly on responses to stimuli with different <italic>c</italic>-values. By allowing a trade-off between capturing the joint distribution of single-neuron responses across the entire parameter spaces <italic>vs</italic>. handling a heterogeneous dataset with missing data, the conditional GAN provides additional flexibility in fitting theoretical models to diverse neuronal data.</p>
</sec>
<sec id="sec012">
<title>Optimization difficulties, underfitting and overfitting</title>
<p>As with any gradient-based training method, it is possible for a GAN to become stuck in suboptimal local minima for the generator (or the discriminator). It is further an open question whether GAN training will always converge [<xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref042">42</xref>]. As research in GANs and non-convex optimization advances this issue will be improved. For now avoiding this pitfall will be a matter of the user judging the quality of fit after the fit has reasonably converged. Starting the gradient descent algorithm with several different initial conditions for generator parameters can also help, with some initializations leading to better final fits.</p>
<p>Apart from the above general problems, when the generator is a recurrent neural network (RNN), other problems may arise within each step of gradient descent. When the generator output is based on a steady-state (fixed point) of the RNN, as was the case in our SSN experiment, a potential issue is lack of convergence to a stable fixed point for some choices of recurrent network parameters. In our experiment with SSN, we initialized the generator network in such a way that it initially had a stable fixed point for almost all realizations of <bold>z</bold>. For the SSN this would generically be the case when recurrent excitation (which has destabilizing effects) is sufficiently weak. Hence initializations with small <italic>J</italic><sub><italic>EE</italic></sub> and <italic>δJ</italic><sub><italic>EE</italic></sub> are good choices. In addition, a relatively large SSN size <italic>N</italic> improves the stability issue because random quenched fluctuations in total incoming synaptic weights are relatively small when the number of presynaptic neurons is large. Thus, for large networks, for a given choice of network parameters, <italic>θ</italic>, either the network converges to a stable fixed point for almost all <bold>z</bold>, or almost never does. To avoid entering parameter regions leading to instability during training, we added the additional regularizing term <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref> to the generator loss. We found that the addition of this term is crucial for the success of the algorithm.</p>
<p>An additional problem particular to optimizing GANs is mode collapse (also known as mode dropping), in which some modes of a multi-modal dataset are not represented (or in the worst case only one mode is represented) in the generative model output [<xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref044">44</xref>]. Mode collapse is an example of underfitting. The work presented here did not suffer from mode collapse, likely because of the highly structured models employed. Nevertheless, other applications may suffer from the problem of mode collapse. Many approaches have been explored to prevent mode collapse, and we do not give a comprehensive review, but instead cite a selection of interesting approaches. The WGAN itself, which is employed here, is believed to alleviate mode collapse to some degree [<xref ref-type="bibr" rid="pcbi.1006816.ref025">25</xref>]. Other more sophisticated approaches exist, including the addition of a mutual information maximizing regularizer between model output and the latent variables [<xref ref-type="bibr" rid="pcbi.1006816.ref045">45</xref>]. One particularly elegant and effective approach is the PacGAN which provides two or more independent, concatenated generator or data samples to the discriminator so that a model suffering from mode collapse will be recognizable from its lack of diversity [<xref ref-type="bibr" rid="pcbi.1006816.ref046">46</xref>].</p>
<p>Another possible problem is that of overfitting. In the context of generative model training, extreme overfitting corresponds to the generative model approximately memorizing the individual samples in the training data. In our experiments, this would have corresponded to the trained circuit model only generating a finite set of possible tuning curves, namely the samples seen during training. GANs are prone to sample memorization when their generator and discriminator have high complexity or expressivity. For typical circuit models in neuroscience, however, such as the examples we considered here, the output (<italic>e</italic>.<italic>g</italic>., tuning curve) distribution is expected to have a smooth density (in contrast to a discrete distribution with support on or near the training-set tuning curves) almost everywhere in parameter space; such a generator can never memorize a finite sample of tuning curves.</p>
</sec>
<sec id="sec013">
<title>Identifiability of circuit parameters</title>
<p>In fitting generative models it is possible for models with widely different parameters to result in nearly identical output distributions. In our case, this corresponds to cases in which networks with widely divergent connectivity or single-cell parameters nevertheless generate very similar tuning curve distributions. In such cases it would be impossible to make precise inferences about network parameters (<italic>e</italic>.<italic>g</italic>., connectivity statistics) using tuning curve data. This problem is exacerbated for the moment matching method, which discards information in the data by reducing the tuning curve distribution to a few moments. In comparison, the problem should generally be less severe for the GAN method which tries to fit the entire distribution. Irrespective of the fitting method, however, there is no general reason why the distribution of a relatively low-dimensional output of the model, such as the tuning curve with respect to one stimulus parameter, would provide sufficient information for constraining all circuit parameters. Fortunately there is nothing in our approach that prevents one from applying it to datasets of tuning curves with respect to several stimulus parameters, or tuning curves of multiple cell types. The general expectation is that the higher the dimension of the stimulus parameter space underlying the tuning curves in the training data, the more identifiable the network parameters become.</p>
<p>For example, in the case of our SSN experiments, we first trained the generative model using only tuning curves with respect to stimulus size. In the parameter identification experiment (Experiment 3), we enriched the size tuning curve dataset by adding center-offset neurons, and additional sampling conditions by adding inhibitory cell tuning curves. The result was an improvement in the robustness and accuracy of model parameter identification. With datasets of sufficiently rich tuning curves, the GAN-based method provides a promising way to infer biophysical networks parameters, such as those governing connectivity statistics. This also has deep implications for experimental design: the standard approach of using optimal stimuli (in the case of our SSN example, gratings with no center offset), or only focusing on excitatory neurons may produce datasets that are insufficiently rich to allow for model-based inference of all circuit parameters of interest. In particular, a framework like GANs can in principle be used to <italic>design</italic> experiments, <italic>i</italic>.<italic>e</italic>., optimally choose the stimulus conditions and quantities to be recorded, to maximize the identifiability of the parameters of a given model.</p>
</sec>
<sec id="sec014">
<title>Future directions</title>
<p>The current work can be extended along several directions. In addition to the GAN framework, a suite of other methods have also been developed recently in machine learning for fitting generative models [<xref ref-type="bibr" rid="pcbi.1006816.ref023">23</xref>]. Examples include variational autoencoders [<xref ref-type="bibr" rid="pcbi.1006816.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref047">47</xref>], and hierarchical and deep implicit models [<xref ref-type="bibr" rid="pcbi.1006816.ref024">24</xref>]. These methods can also be fruitful for fitting circuit models from neuroscience and inferring circuit parameters. Recent progress in unifying these approaches [<xref ref-type="bibr" rid="pcbi.1006816.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref050">50</xref>] can further inform their future applications. Of special note is the Bayesian methodology of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref024">24</xref>] which in addition to point estimates for parameters, also yields estimates of their posterior uncertainty (or more generally, their approximate posterior distribution).</p>
<p>In the current study, we took the output of the mechanistic circuit model to be tuning curves composed of single-cell trial-averaged sustained responses. But the conceptual framework explained in the beginning of Results and the GAN methodology can also be used to constrain mechanistic network models using data featuring the temporal dynamics of neural activity and higher-order statistics of trial-to-trial variability. For example, the network model can be fit not only to match the distribution of tuning curves encoding trial-averaged single-cell sustained responses, but rather the entire peristimulus time histogram or also the distribution of noise correlations between cell pairs in some cortical area, extracted from simultaneously recorded neural activity.</p>
<p>Lastly, although we focus here on applications to neuroscience and neuronal networks, the proposed framework can potentially serve to fit mechanistic models from other corners of biology, in order to infer the structure of other kinds of biological networks from functional data. For example, the framework can potentially be used to infer the structure of gene regulatory networks from data on the expressions of one or a few genes in different environments.</p>
</sec>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec016">
<title>Conditional Generative Adversarial Networks</title>
<p>Here we provide the complete expressions for the loss functions used in the conditional WGAN (cWGAN) method and a pseudocode for this algorithm. The non-conditional WGAN setup was described in subsection Generative Adversarial Networks of Results. cWGAN’s are similarly composed of a generator and a discriminator, but now both the descriminator <inline-formula id="pcbi.1006816.e064"><alternatives><graphic id="pcbi.1006816.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and generator <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> depend on a “condition” argument or input, <italic>c</italic>, in addition to their primary inputs [<xref ref-type="bibr" rid="pcbi.1006816.ref033">33</xref>]. The condition variable <italic>c</italic> can be discrete or continuous and can range over a set of possibilities <italic>C</italic>. When there is only one possibility for <italic>c</italic> (in which case this argument can be dropped) we recover the original (non-conditional) WGAN.</p>
<p>The discriminator and generator loss functions for cWGAN are given by
<disp-formula id="pcbi.1006816.e065"><alternatives><graphic id="pcbi.1006816.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>Gradient</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Penalty</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
<disp-formula id="pcbi.1006816.e066"><alternatives><graphic id="pcbi.1006816.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>Penalty</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
respectively. Here <inline-formula id="pcbi.1006816.e067"><alternatives><graphic id="pcbi.1006816.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> denote the average over a batch of data samples, <bold>x</bold>, together with the conditions, <italic>c</italic>, at which they were recorded. Similarly, <inline-formula id="pcbi.1006816.e068"><alternatives><graphic id="pcbi.1006816.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> denotes averaging over a batch of noise variables <bold>z</bold>, sampled from their fixed distribution, and conditions <italic>c</italic>, sampled from their empirical distribution. Note that <bold>z</bold> and <italic>c</italic> are treated as independent random variables. The “Gradient Penalty” term forces the gradient of <inline-formula id="pcbi.1006816.e069"><alternatives><graphic id="pcbi.1006816.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> (with respect to its first argument) to be close to one; following the recipe of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>] we set it to
<disp-formula id="pcbi.1006816.e070"><alternatives><graphic id="pcbi.1006816.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e070" xlink:type="simple"/><mml:math display="block" id="M70"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Gradient</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Penalty</mml:mtext> <mml:mo>=</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mo>(</mml:mo> <mml:mrow><mml:mo>‖</mml:mo> <mml:mo>∇</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>ϵ</italic> is a random variable with uniform distribution on [0, 1], and the gradient ∇ is taken with respect to the first argument of <inline-formula id="pcbi.1006816.e071"><alternatives><graphic id="pcbi.1006816.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, and not the condition <italic>c</italic> or parameters <bold>w</bold>. Finally, the term Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) in the generator loss denotes possible generator model dependent regularization terms. We did not include any such term for the feedforward network example presented in Experiment 1. The Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) for the recurrent SSN example is described in <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref> of Materials and methods.</p>
<p>A stochastic gradient descent algorithm for cWGAN based on these loss functions is shown in Algorithm 1.</p>
<p><bold>Algorithm 1</bold>: Improved cWGAN algorithm based on Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>]. For all models, we use <inline-formula id="pcbi.1006816.e072"><alternatives><graphic id="pcbi.1006816.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. For update method, we either use Adam with <italic>β</italic><sub>1</sub> = 0.5, <italic>β</italic><sub>2</sub> = 0.9, <italic>ϵ</italic> = 10<sup>−8</sup> [<xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>] or RMSProp with <italic>ρ</italic> = 0.9, <italic>ϵ</italic> = 10<sup>−6</sup>. For the feedforward model, we use <inline-formula id="pcbi.1006816.e073"><alternatives><graphic id="pcbi.1006816.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>001</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic>m</italic> = 30, <italic>γ</italic> = 0, Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) = 0, and <inline-formula id="pcbi.1006816.e074"><alternatives><graphic id="pcbi.1006816.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:msub><mml:mtext>Update</mml:mtext> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>Update</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>Adam</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>. The generating processes are considered normal always and the test<sup>†</sup> always passes. For the SSN <inline-formula id="pcbi.1006816.e075"><alternatives><graphic id="pcbi.1006816.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>02</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic>α</italic><sub><italic>G</italic></sub> = 10<sup>−4</sup>, <italic>m</italic> = 128, <italic>γ</italic> = 0.001, Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) = <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref>. The discriminator is updated only if generating processes (<italic>G</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>z</bold>)) pass a test<sup>†</sup> for “normality” (see <xref ref-type="disp-formula" rid="pcbi.1006816.e146">Eq (33)</xref>). We use always RMSProp for the generator in the SSN experiments. We use RMSProp for the discriminator in <xref ref-type="fig" rid="pcbi.1006816.g004">Fig 4</xref>, Adam in <xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5</xref>, and aggregate the results with both RMSProp and Adam in <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref>.</p>
<p specific-use="line"><bold>Input</bold>: data distribution <inline-formula id="pcbi.1006816.e076"><alternatives><graphic id="pcbi.1006816.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, the gradient penalty coefficient λ, the number of discriminator iterations per generator iteration <inline-formula id="pcbi.1006816.e077"><alternatives><graphic id="pcbi.1006816.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:msub><mml:mi>n</mml:mi> <mml:mi mathvariant="script">D</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, the batch size <italic>m</italic>, update methods <inline-formula id="pcbi.1006816.e078"><alternatives><graphic id="pcbi.1006816.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:msub><mml:mtext>Update</mml:mtext> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, Update<sub><italic>G</italic></sub>(·), learning rates <inline-formula id="pcbi.1006816.e079"><alternatives><graphic id="pcbi.1006816.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi mathvariant="script">D</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <italic>α</italic><sub><italic>G</italic></sub>, weight decay hyperparameter <italic>γ</italic>, and the initial discriminator <bold><italic>w</italic></bold><sub>0</sub> and generator <bold><italic>θ</italic></bold><sub>0</sub> parameters.</p>
<p specific-use="line"><bold><italic>θ</italic></bold> ← <bold><italic>θ</italic></bold><sub>0</sub>;</p>
<p specific-use="line"><bold><italic>w</italic></bold> ← <bold><italic>w</italic></bold><sub>0</sub>;</p>
<p specific-use="line"><bold>while <italic>θ</italic></bold> <italic>has not converged</italic> <bold>do</bold></p>
<p specific-use="line"> <bold>repeat</bold></p>
<p specific-use="line">  <bold>for</bold> <italic>i</italic> = 1, …, <italic>m</italic> <bold>do</bold></p>
<p specific-use="line">   Sample real data <inline-formula id="pcbi.1006816.e080"><alternatives><graphic id="pcbi.1006816.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, latent variable <bold>z</bold> ∼ <italic>p</italic>(<bold>z</bold>), a random number <italic>ϵ</italic> ∼ <italic>U</italic>[0, 1].;</p>
<p specific-use="line">   <inline-formula id="pcbi.1006816.e081">
<alternatives>
<graphic id="pcbi.1006816.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e081" xlink:type="simple"/>
<mml:math display="inline" id="M81">
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
<mml:mo>←</mml:mo>
<mml:msub>
<mml:mi>G</mml:mi>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">z</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>;</p>
<p specific-use="line">   <inline-formula id="pcbi.1006816.e082">
<alternatives>
<graphic id="pcbi.1006816.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e082" xlink:type="simple"/>
<mml:math display="inline" id="M82">
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>←</mml:mo>
<mml:mi>ϵ</mml:mi>
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>+</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>-</mml:mo>
<mml:mi>ϵ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>;</p>
<p specific-use="line">   <inline-formula id="pcbi.1006816.e083">
<alternatives>
<graphic id="pcbi.1006816.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e083" xlink:type="simple"/>
<mml:math display="inline" id="M83">
<mml:mrow>
<mml:msubsup>
<mml:mtext>Loss</mml:mtext>
<mml:mrow>
<mml:mi mathvariant="script">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>←</mml:mo>
<mml:msub>
<mml:mi mathvariant="script">D</mml:mi>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
<mml:mo>;</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>-</mml:mo>
<mml:msub>
<mml:mi mathvariant="script">D</mml:mi>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mrow>
<mml:mo>λ</mml:mo>
<mml:mo>(</mml:mo>
<mml:mo>‖</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mo>∇</mml:mo>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="script">D</mml:mi>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mover accent="true">
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>;</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msup>
<mml:mrow>
<mml:msub>
<mml:mo>‖</mml:mo>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>;</p>
<p specific-use="line">  <bold>end</bold></p>
<p specific-use="line">  <bold>if</bold> <italic>generating processes are normal</italic><sup>†</sup> <bold>then</bold></p>
<p specific-use="line">   <inline-formula id="pcbi.1006816.e084">
<alternatives>
<graphic id="pcbi.1006816.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e084" xlink:type="simple"/>
<mml:math display="inline" id="M84">
<mml:mrow>
<mml:mi mathvariant="bold-italic">w</mml:mi>
<mml:mo>←</mml:mo>
<mml:msub>
<mml:mtext>Update</mml:mtext>
<mml:mi mathvariant="script">D</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mo>∇</mml:mo>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:msub>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mi>m</mml:mi>
</mml:mfrac>
<mml:msubsup>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>m</mml:mi>
</mml:msubsup>
<mml:msubsup>
<mml:mtext>Loss</mml:mtext>
<mml:mrow>
<mml:mi mathvariant="script">D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold-italic">w</mml:mi>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi mathvariant="script">D</mml:mi>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>;</p>
<p specific-use="line">  <bold>end</bold></p>
<p specific-use="line"> <bold>until</bold> <inline-formula id="pcbi.1006816.e085">
<alternatives>
<graphic id="pcbi.1006816.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e085" xlink:type="simple"/>
<mml:math display="inline" id="M85">
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mi mathvariant="script">D</mml:mi>
</mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula> <italic>updates are tried</italic>;</p>
<p specific-use="line"> Sample latent variables <inline-formula id="pcbi.1006816.e086"><alternatives><graphic id="pcbi.1006816.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e086" xlink:type="simple"/><mml:math display="inline" id="M86"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for a mini-batch.;</p>
<p specific-use="line"> Sample conditions from real data <inline-formula id="pcbi.1006816.e087"><alternatives><graphic id="pcbi.1006816.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi>c</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:msubsup> <mml:mo>∼</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">P</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> for a mini-batch.;</p>
<p specific-use="line"> <inline-formula id="pcbi.1006816.e088">
<alternatives>
<graphic id="pcbi.1006816.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e088" xlink:type="simple"/>
<mml:math display="inline" id="M88">
<mml:mrow>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
<mml:mo>←</mml:mo>
<mml:msub>
<mml:mtext>Update</mml:mtext>
<mml:mi>G</mml:mi>
</mml:msub>
<mml:mo>(</mml:mo>
<mml:mo>-</mml:mo>
<mml:msub>
<mml:mo>∇</mml:mo>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
</mml:msub>
<mml:mo>(</mml:mo>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mi>m</mml:mi>
</mml:mfrac>
<mml:msubsup>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>m</mml:mi>
</mml:msubsup>
<mml:msub>
<mml:mi mathvariant="script">D</mml:mi>
<mml:mi mathvariant="bold-italic">w</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mi>G</mml:mi>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mi mathvariant="bold">z</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>;</mml:mo>
<mml:msup>
<mml:mi>c</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mtext>Penalty</mml:mtext>
<mml:mi>G</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold-italic">θ</mml:mi>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>G</mml:mi>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>;</p>
<p specific-use="line"><bold>end</bold></p>
</sec>
<sec id="sec017">
<title>Alternatives to WGAN, and alternative views of GANs</title>
<p>In this subsection, we provide a quick review of some of the alternatives to the WGAN loss functions, Eqs (<xref ref-type="disp-formula" rid="pcbi.1006816.e036">3</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006816.e037">4</xref>), developed in the GAN literature, which can be useful in computational biology applications (for a more comprehensive review of GANs see [<xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>]). We also point out an alternative view of GANs inspired by the energy-based framework for unsupervised learning [<xref ref-type="bibr" rid="pcbi.1006816.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref052">52</xref>].</p>
<p>The original GAN developed in [<xref ref-type="bibr" rid="pcbi.1006816.ref023">23</xref>] was framed as a minimax or zero-sum game in which the generator and discriminator competed by respectively minimizing and maximizing the same loss:
<disp-formula id="pcbi.1006816.e089"><alternatives><graphic id="pcbi.1006816.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e089" xlink:type="simple"/><mml:math display="block" id="M89"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi mathvariant="script">D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mtext>log</mml:mtext> <mml:mspace width="2pt"/><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
Given this form for the loss, the discriminator, <inline-formula id="pcbi.1006816.e090"><alternatives><graphic id="pcbi.1006816.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, can be thought of as a binary classifier that estimates the conditional probability of the true or empirical category (<italic>vs</italic>. “fake” or <italic>G</italic>-generated category) given the observation <bold>x</bold>. In this case, for a fixed generator whose output has distribution <italic>P</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>x</bold>), the theoretically optimal discriminator is given by <inline-formula id="pcbi.1006816.e091"><alternatives><graphic id="pcbi.1006816.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mtext>true</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mtext>true</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>P</italic><sub>true</sub>(<bold>x</bold>) is the true data distribution or density. In that sense, a sufficiently expressive and well-trained discriminator (using <xref ref-type="disp-formula" rid="pcbi.1006816.e089">Eq (11)</xref>) learns the ratio of the model likelihood, <italic>P</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>x</bold>), to the true data density.</p>
<p>Moreover, for this optimal discriminator solution the loss <xref ref-type="disp-formula" rid="pcbi.1006816.e089">Eq (11)</xref> reduces to the Jensen-Shannon (JS) divergence (up to additive and multiplicative constants) between <italic>P</italic><sub>true</sub>(<bold>x</bold>) and <italic>P</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>x</bold>) [<xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>]. Thus the generator is theoretically trained to minimize the JS distance. By comparison, as noted in subsection “Generative Adversarial Networks” of Results, WGANs theoretically minimize the Wasserstein or earth-mover’s distance between <italic>P</italic><sub>true</sub>(<bold>x</bold>) and <italic>P</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>x</bold>).</p>
<p>Both of these divergence or distance measures are in contrast to the Kullback-Leibler (KL) divergence <italic>D</italic><sub><italic>KL</italic></sub>(<italic>P</italic><sub>true</sub>‖<italic>P<sub>θ</sub></italic>) which is effectively minimized in classical maximum-likelihood estimation of the generator (or equivalently, of its parameters <bold><italic>θ</italic></bold>). Correspondingly, others have modified the generator-loss so that, given the theoretically optimal discriminator, it reduces to the KL divergence, or other divergences [<xref ref-type="bibr" rid="pcbi.1006816.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1006816.ref056">56</xref>]. For example, to minimize the KL divergence, [<xref ref-type="bibr" rid="pcbi.1006816.ref056">56</xref>] used the same discriminator loss as in <xref ref-type="disp-formula" rid="pcbi.1006816.e089">Eq (11)</xref>, in conjunction with the modified generator loss
<disp-formula id="pcbi.1006816.e092"><alternatives><graphic id="pcbi.1006816.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e092" xlink:type="simple"/><mml:math display="block" id="M92"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1006816.e093"><alternatives><graphic id="pcbi.1006816.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>u</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>u</mml:mi></mml:mrow></mml:mfrac> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>u</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>u</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>As we mentioned in the Discussion, Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref024">24</xref>] developed a GAN-like framework for variational Bayesian estimation of implicit generative models, in which a discriminator-like network was trained to estimate the generator’s likelihood function. However, this Bayesian framework goes beyond maximum-likelihood estimation: Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref024">24</xref>] developed a three-network framework in which, in addition to the (implicit) generative model and discriminator (which estimates the generator’s likelihood), a third network is trained to provide an approximation to the Bayesian posterior over <bold><italic>θ</italic></bold> (as in more general variational Bayesian approaches [<xref ref-type="bibr" rid="pcbi.1006816.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref057">57</xref>]).</p>
<p>Finally, we mention energy-based GANs (EBGANs), which are inspired by the energy-based framework for learning [<xref ref-type="bibr" rid="pcbi.1006816.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref052">52</xref>]. EBGANs use the following loss functions for the generator and the discriminator:
<disp-formula id="pcbi.1006816.e094"><alternatives><graphic id="pcbi.1006816.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e094" xlink:type="simple"/><mml:math display="block" id="M94"><mml:mrow><mml:msubsup><mml:mtext>Loss</mml:mtext> <mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow> <mml:mtext>EBGAN</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
<disp-formula id="pcbi.1006816.e095"><alternatives><graphic id="pcbi.1006816.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e095" xlink:type="simple"/><mml:math display="block" id="M95"><mml:mrow><mml:msubsup><mml:mtext>Loss</mml:mtext> <mml:mrow><mml:mi>G</mml:mi></mml:mrow> <mml:mtext>EBGAN</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi mathvariant="bold">z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where (<italic>x</italic>)<sub>+</sub> = max(0, <italic>x</italic>) denotes rectification, <italic>m</italic> &gt; 0 is a positive margin parameter, and <inline-formula id="pcbi.1006816.e096"><alternatives><graphic id="pcbi.1006816.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is constrained to be non-negative (see [<xref ref-type="bibr" rid="pcbi.1006816.ref058">58</xref>]).</p>
<p>EBGANs are in part motivated by an alternative view of the role of the discriminator in GANs. In the viewpoint expressed in subsection “Generative Adversarial Networks” of Results, the discriminator is thought of as a flexible and trainable objective function that is used for training the generator. In this interpretation, the generator is key and the discriminator is auxiliary. However, an alternative viewpoint is also possible in which the discriminator is key and the generator is auxiliary (see appendix B of [<xref ref-type="bibr" rid="pcbi.1006816.ref058">58</xref>]). In this view, which is suggested by the energy-based framework for learning [<xref ref-type="bibr" rid="pcbi.1006816.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref052">52</xref>], the discriminator learns the relative density of the data distribution. More precisely, it is thought of as an energy function that is shaped during training to be low in regions of high data density, and high elsewhere. This is achieved by minimizing a loss functional during learning; <xref ref-type="disp-formula" rid="pcbi.1006816.e094">Eq (13)</xref> is an example of such a loss functional. Imagine for simplicity that true data lie on a subspace or manifold in the data space. The loss functional is designed such that true data samples serve to “push down” the energy function (<italic>i</italic>.<italic>e</italic>., increase the probability density) on the data manifold, while another mechanism is used to “pull-up” the energy function (<italic>i</italic>.<italic>e</italic>., reduce the probability density) outside that manifold. In simple unsupervised learning methods such as principle component analysis the pull-up of energy is implicitly achieved due to the rigidity of the energy function itself (see [<xref ref-type="bibr" rid="pcbi.1006816.ref052">52</xref>]). But when the energy function is sufficiently flexible, an explicit term in the loss functional is needed to pull it up outside the true data manifold. Fake or simulated data points, referred to as contrastive samples, can be used for energy pull-up. In <xref ref-type="disp-formula" rid="pcbi.1006816.e094">Eq (13)</xref>, the first and second term serve to push up and pull down the energy function, <inline-formula id="pcbi.1006816.e097"><alternatives><graphic id="pcbi.1006816.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:msub><mml:mi mathvariant="script">D</mml:mi> <mml:mi mathvariant="bold-italic">w</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, respectively. Accordingly, in the alternative viewpoint of GANs, the generator is viewed as merely providing such contrastive samples to the discriminator.</p>
<p>As explained below, in contrast to many applications of unsupervised learning, the generator is indeed central in our intended applications, and we therefore focused on the first interpretation of GANs in subsection “Generative Adversarial Networks” of Results. Nevertheless, unsupervised learning of generative models is often carried out with the end-goal of learning a useful representation of observed data, which can, <italic>e</italic>.<italic>g</italic>., serve to compress or reduce the dimensionality of data. In our case this corresponds to dimensionality reduction of tuning curves. Estimating the density of observed data is a related end-goal of unsupervised learning, which can, <italic>e</italic>.<italic>g</italic>., serve data restoration (<italic>e</italic>.<italic>g</italic>., image denoising) applications. Even though these were not the applications motivating the current study (see the next subsection), they do constitute potential applications of GANs in computational biology. The alternative view of the role of discriminator can be more advantageous in such settings.</p>
</sec>
<sec id="sec018">
<title>Differences with common machine learning applications</title>
<p>In most applications of GANs in machine learning and artificial intelligence, the generative model is an artificial neural network (<italic>e</italic>.<italic>g</italic>., a deconvolutional deep feedforward network), and that network’s individual connection weights constitute the generator’s trainable parameters, <bold><italic>θ</italic></bold>. In most such applications, these parameters are not objects of interest on their own and may not be mechanistically meaningful or interpretable. Similarly, the development of generative models in such domains is not necessarily concerned with capturing the true, physical mechanisms underlying the modeled data. There and in other unsupervised learning settings, the end goal is to achieve a generator that produces realistic data objects (<italic>e</italic>.<italic>g</italic>., images), or to accurately estimate the data density or its support (which can serve for dimensionality reduction or denoising of data). In such domains, the discriminator itself may be of central importance as it can potentially estimate the relative density of data objects (see the previous subsection for a discussion of this viewpoint).</p>
<p>By contrast, in applications of primary interest to us, it is the generator that is of primary interest. The core of the generator is a circuit model, developed independently of the method used to fit it (in our case GANs), with the scientific goal of uncovering the circuit mechanisms in a neural or biological system. In particular, in our applications, the generator parameters <bold><italic>θ</italic></bold> typically correspond to physiological and anatomical parameters with clear mechanistic interpretations. Correspondingly, the circuit model is highly structured, with that structure strongly informed <italic>a priori</italic> by biological domain knowledge and the scientific desire for parsimony. This allows for post-training tests of the model that go beyond testing the fit to held-out data of the same type as training data, and may not be conceivable in many machine learning applications. For example, one can imagine training an SSN circuit model on size-tuning curves (as we did in Experiment 1–3) but test it using stimuli with varying strengths or contrasts and compare the generated distribution of contrast-response function against data. Alternatively, one can feed the trained SSN dynamical noise and then compare the statistics of temporal neural variability (such as pairwise noise correlations) against empirical data. In such tests of generalization, the data-space itself changes (and not just the data points in it) between training and testing, and therefore a discriminator trained on one space simply cannot generalize to the other; the link between the two data-spaces is solely provided by the generative mechanistic circuit model. The promise of a faithful scientific model of a brain network is, in principle, to capture all such neural data at least approximately; the above scenarios can thus be used as strong tests for such models. To perform such tests faithfully and quantitatively, a strong fitting procedure is necessary.</p>
</sec>
<sec id="sec019">
<title>Feedforward model of M1</title>
<p>The model of primary motor cortex (M1) tuning curves proposed by Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] is a two-layer feedforward network, with an input layer, putatively corresponding to the parietal reach area or to premotor cortex, and an output layer corresponding to M1 (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2</xref>). Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] introduced their model in two versions, a basic one, and an “extended” version. We have used their enhanced version with small modifications noted in Experiment 1 and below which allow our approach to be used. In particular, we did not model response selectivity to hand posture (supination or pronation), and ignored that label in the dataset; <italic>i</italic>.<italic>e</italic>., we blindly mixed hand-position tuning curves across pronation and supination conditions, as if they belonged to different neurons. We further simplified the dataset by removing spatial scale information in the positions of target hand locations, which varied slightly between experimental sessions, by rescaling the distance between adjacent hand position to be 1. We randomly selected half of the hand-position tuning curves to be our training dataset, and used the other half as held-out data to evaluate the goodness of model fits (presented in <xref ref-type="fig" rid="pcbi.1006816.g003">Fig 3</xref>).</p>
<p>The input to the feedforward network is the 3D hand position <bold>x</bold><sub><italic>s</italic></sub>, with <italic>s</italic> ∈ {1, ⋯, 27} indexing the 3 × 3 × 3 grid of possible target locations. The input layer neurons have Gaussian receptive fields defined on the 3D hand position space. The activation, <italic>h</italic><sub><italic>i</italic></sub>(<italic>s</italic>) of neuron <italic>i</italic> in the input layer with receptive field centered at <bold>x</bold><sub><italic>i</italic></sub> is thus <inline-formula id="pcbi.1006816.e098"><alternatives><graphic id="pcbi.1006816.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:msup><mml:mrow><mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in condition <italic>s</italic> (when the hand is at <bold>x</bold><sub><italic>s</italic></sub>). Across the input layer, the Gaussian centers <bold>x</bold><sub><italic>i</italic></sub> form a fine cubic grid that interpolates and extends (by 3 times) the 3 × 3 × 3 stimulus grid along each dimension. Whereas Lalazar et al. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>] used a grid with 100 points along each axis, we reduced this to 40 to allow faster computations (we checked that changing this resolution beyond 40 only weakly affects the results). Across the input layer, the receptive field widths, <italic>σ</italic><sub><italic>i</italic></sub>, were randomly and independently sampled from the uniform distribution on the range [<italic>σ</italic><sub><italic>l</italic></sub>, <italic>σ</italic><sub><italic>l</italic></sub> + <italic>δσ</italic>]. This can be expressed by writing <inline-formula id="pcbi.1006816.e099"><alternatives><graphic id="pcbi.1006816.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>σ</mml:mi></mml:msubsup> <mml:mi>δ</mml:mi> <mml:mi>σ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> where <inline-formula id="pcbi.1006816.e100"><alternatives><graphic id="pcbi.1006816.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>σ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are uniformly distributed on [0, 1] and independent for different <italic>i</italic>’s across the input layer.</p>
<p>The feedforward connections from the input to output layer are sparse and random, with a connection probability of 0.01. In our implementation of this model, the strength of the nonzero connections were sampled independently from the uniform distribution on the range [0, <italic>J</italic>]. Since output layer neurons are independent, it suffices to describe the model with a single output neuron. If we denote the connections received by this neuron from the <italic>i</italic>-th input layer neuron by <inline-formula id="pcbi.1006816.e101"><alternatives><graphic id="pcbi.1006816.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, we can thus write: <inline-formula id="pcbi.1006816.e102"><alternatives><graphic id="pcbi.1006816.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>J</mml:mi> <mml:msub><mml:mi>M</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>J</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> where <inline-formula id="pcbi.1006816.e103"><alternatives><graphic id="pcbi.1006816.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>J</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>’s are sampled independently from the standard uniform distribution on [0, 1], and <italic>M</italic><sub><italic>i</italic></sub> is a binary 0/1 mask that is nonzero with probability 0.01.</p>
<p>The response of the output layer neuron is given by a rectified linear response function with threshold <italic>ϕ</italic>. The threshold <italic>ϕ</italic> was sampled uniformly from the range [<italic>ϕ</italic><sub><italic>l</italic></sub>, <italic>ϕ</italic><sub><italic>l</italic></sub> + <italic>δϕ</italic>]. Equivalently, <italic>ϕ</italic> = <italic>ϕ</italic><sub><italic>l</italic></sub> + <italic>z</italic><sup><italic>ϕ</italic></sup> <italic>δϕ</italic> with <italic>z</italic><sup><italic>ϕ</italic></sup> a standard uniform random variable.</p>
<p>The model thus has five trainable parameters <bold><italic>θ</italic></bold> = (<italic>σ</italic><sub><italic>l</italic></sub>, <italic>δσ</italic>, <italic>J</italic>, <italic>ϕ</italic><sub><italic>l</italic></sub>, <italic>δϕ</italic>) (listed in <xref ref-type="table" rid="pcbi.1006816.t002">Table 2</xref>). For a choice of <bold><italic>θ</italic></bold>, the collection of quenched noise variables, <inline-formula id="pcbi.1006816.e104"><alternatives><graphic id="pcbi.1006816.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mi>ϕ</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>σ</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mn>40</mml:mn> <mml:mn>3</mml:mn></mml:msup></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>J</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mn>40</mml:mn> <mml:mn>3</mml:mn></mml:msup></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>M</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mn>40</mml:mn> <mml:mn>3</mml:mn></mml:msup></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, fully determine the network structure: all input layer receptive field sizes, individual feedforward connection strengths, and output layer neural thresholds for a particular network realization. The response <inline-formula id="pcbi.1006816.e105"><alternatives><graphic id="pcbi.1006816.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> of an output neuron in condition <italic>s</italic> (for <italic>s</italic> ∈ {1, ⋯, 27}) is thus given by
<disp-formula id="pcbi.1006816.e106"><alternatives><graphic id="pcbi.1006816.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e106" xlink:type="simple"/><mml:math display="block" id="M106"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mn>40</mml:mn> <mml:mn>3</mml:mn></mml:msup></mml:munderover> <mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="2pt"/><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mo>]</mml:mo> <mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where
<disp-formula id="pcbi.1006816.e107"><alternatives><graphic id="pcbi.1006816.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e107" xlink:type="simple"/><mml:math display="block" id="M107"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>Z</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mtext>exp</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:msup><mml:mrow><mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
<disp-formula id="pcbi.1006816.e108"><alternatives><graphic id="pcbi.1006816.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e108" xlink:type="simple"/><mml:math display="block" id="M108"><mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>J</mml:mi> <mml:mspace width="2pt"/><mml:msub><mml:mi>M</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="2pt"/><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>J</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
<disp-formula id="pcbi.1006816.e109"><alternatives><graphic id="pcbi.1006816.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e109" xlink:type="simple"/><mml:math display="block" id="M109"><mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mi>ϕ</mml:mi></mml:msup> <mml:mi>δ</mml:mi> <mml:mi>ϕ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
<disp-formula id="pcbi.1006816.e110"><alternatives><graphic id="pcbi.1006816.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e110" xlink:type="simple"/><mml:math display="block" id="M110"><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>σ</mml:mi></mml:msubsup> <mml:mi>δ</mml:mi> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
<disp-formula id="pcbi.1006816.e111"><alternatives><graphic id="pcbi.1006816.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e111" xlink:type="simple"/><mml:math display="block" id="M111"><mml:mrow><mml:msup><mml:mi>z</mml:mi> <mml:mi>ϕ</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>J</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>σ</mml:mi></mml:msubsup></mml:mrow> <mml:mrow><mml:mover><mml:mo>∼</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>i</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:mover> <mml:mspace width="2pt"/><mml:mspace width="2pt"/><mml:mi>U</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
<disp-formula id="pcbi.1006816.e112"><alternatives><graphic id="pcbi.1006816.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e112" xlink:type="simple"/><mml:math display="block" id="M112"><mml:msub><mml:mi>M</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mover><mml:mo>∼</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>i</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:mover> <mml:mspace width="2pt"/><mml:mspace width="2pt"/><mml:mtext>Bern</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>01</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
where [<italic>u</italic>]<sub>+</sub> = max(0, <italic>u</italic>) denotes rectification, and <italic>Z</italic>(<italic>s</italic>) is a normalizing factor such that ∑<sub><italic>i</italic></sub> <italic>h</italic><sub><italic>i</italic></sub>(<italic>s</italic>) = 1. Crucially, the network’s output, <inline-formula id="pcbi.1006816.e113"><alternatives><graphic id="pcbi.1006816.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e113" xlink:type="simple"/><mml:math display="inline" id="M113"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>27</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, is differentiable with respect to each component of <bold><italic>θ</italic></bold>, we can thus use the output gradient with respect to model parameters to optimize the latter using any variant of the stochastic gradient descent algorithm. Note that Eqs (<xref ref-type="disp-formula" rid="pcbi.1006816.e108">17</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1006816.e110">19</xref>) constitute an example of the “sampler function” <italic>g</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>z</bold>) introduced in the subsection “Mechanistic network models as implicit generative models” of Results (here the vector of synaptic weights <inline-formula id="pcbi.1006816.e114"><alternatives><graphic id="pcbi.1006816.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:mrow><mml:msub><mml:mi mathvariant="script">J</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> corresponds to <bold>W</bold>, while the vector (<italic>ϕ</italic>, <bold><italic>σ</italic></bold>) capturing single-cell properties corresponds to <bold>γ</bold>).</p>
<table-wrap id="pcbi.1006816.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.t002</object-id>
<label>Table 2</label>
<caption>
<title>The description of the parameters of the feedforward model fit to tuning curve data.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006816.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Parameter</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>σ</italic><sub><italic>l</italic></sub></td>
<td align="center">Lower bound of receptive field size range</td>
</tr>
<tr>
<td align="center"><italic>δσ</italic></td>
<td align="center">Width of receptive field size range</td>
</tr>
<tr>
<td align="center"><italic>J</italic></td>
<td align="center">Scale of connection strengths</td>
</tr>
<tr>
<td align="center"><italic>ϕ</italic><sub><italic>l</italic></sub></td>
<td align="center">Lower bound of threshold range</td>
</tr>
<tr>
<td align="center"><italic>δϕ</italic></td>
<td align="center">Width of threshold range</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec020">
<title>Recurrent SSN model</title>
<p>Here we provide the technical details of the simulations, fit and analysis of the Stabilized Supralinear Network (SSN) model of the experiments in Experiment 1–3. The SSN is a recurrent network of excitatory (<italic>E</italic>) and inhibitory (<italic>I</italic>) neurons. The dynamical state of the network is the vector of firing rates <bold>r</bold>(<italic>t</italic>) of the <italic>N</italic> network neurons. The rate vector is governed by the differential equation
<disp-formula id="pcbi.1006816.e115"><alternatives><graphic id="pcbi.1006816.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e115" xlink:type="simple"/><mml:math display="block" id="M115"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>+</mml:mo> <mml:mi>f</mml:mi> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mi>W</mml:mi> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
where <italic>W</italic> and <italic>f</italic> denote the recurrent and feedforward weight matrices (with structure described below), the diagonal matrix <inline-formula id="pcbi.1006816.e116"><alternatives><graphic id="pcbi.1006816.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mrow><mml:mi>τ</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>Diag</mml:mtext> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> contains the neural relaxation time constants, <italic>τ</italic><sub><italic>i</italic></sub>, and <bold>I</bold>(<italic>s</italic>) denotes the stimulus input in condition <italic>s</italic>, with <italic>s</italic> ∈ {1, …, <italic>S</italic>}. The key feature of the SSN is the input-output nonlinearity of its neurons, which in the original model is a supralinear rectified power-law function: <inline-formula id="pcbi.1006816.e117"><alternatives><graphic id="pcbi.1006816.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:msubsup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>u</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> ([<italic>u</italic>]<sub>+</sub> = max(0, <italic>u</italic>) and <italic>n</italic> &gt; 1 and <italic>k</italic> &gt; 0 are constants).</p>
<p>During the training of the model inside the fitting algorithm, however, the model may explore non-biological regions in parameter space that may lead to divergence of model firing rates. To tame such divergences and enforce numerical stability during training, we modified the neural input-output nonlinearity in the model as follows. We let <italic>f</italic>(<italic>u</italic>) be a rectified power-law in the biologically relevant range, but smoothly connected it to a saturating branch at very high rates. More precisely we took
<disp-formula id="pcbi.1006816.e118"><alternatives><graphic id="pcbi.1006816.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e118" xlink:type="simple"/><mml:math display="block" id="M118"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>k</mml:mi> <mml:msubsup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>u</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>u</mml:mi> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mfrac><mml:msub><mml:mi>r</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>u</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
where <italic>k</italic> = 0.01, <italic>n</italic> = 2.2, <italic>r</italic><sub>0</sub> = 200 Hz, <italic>r</italic><sub>1</sub> = 1000 Hz, and <italic>u</italic><sub>0</sub> = (<italic>r</italic><sub>0</sub>/<italic>k</italic>)<sup>1/<italic>n</italic></sup>.</p>
<p>In our SSN examples, we chose to have all random structural variability (which is the source of heterogeneity manifesting in tuning curve shapes) occur in the connectivity matrices <italic>W</italic> and <italic>f</italic>. As described in Experiment 2 (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2</xref>), we experimented with SSN models with one-dimensional topographic structure, on which the structure of <italic>W</italic> and <italic>f</italic> depend. The model has a neuron of each type, <italic>E</italic> and <italic>I</italic>, at each topographic spatial location; for <italic>M</italic> topographic locations, the network thus contains <italic>N</italic> = 2<italic>M</italic> neurons. Below, for the <italic>i</italic>-th neuron, we denote its type by <italic>α</italic>(<italic>i</italic>) ∈ {<italic>E</italic>, <italic>I</italic>} and its topographic location by <italic>x</italic><sub><italic>i</italic></sub>. We let <italic>x</italic><sub><italic>i</italic></sub>’s range from −0.5 to 0.5 on a regular grid.</p>
<p>The statistical ensemble for <italic>W</italic> was described in Experiment 2: the random variability of the matrix elements <italic>W</italic><sub><italic>ij</italic></sub>’s was taken to be independent with uniform distribution, with mean and range that depend on the pre- and post-synaptic cell types and topographic distances. More precisely, for each instance of the model we generated <italic>W</italic> via
<disp-formula id="pcbi.1006816.e119"><alternatives><graphic id="pcbi.1006816.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e119" xlink:type="simple"/><mml:math display="block" id="M119"><mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ς</mml:mi> <mml:mi>b</mml:mi></mml:msub> <mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="2.em"/><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
<disp-formula id="pcbi.1006816.e120"><alternatives><graphic id="pcbi.1006816.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e120" xlink:type="simple"/><mml:math display="block" id="M120"><mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mover><mml:mo>∼</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>i</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:mover> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
where <italic>ς</italic><sub><italic>b</italic></sub> = 1 or −1 if <italic>b</italic> = <italic>E</italic> or <italic>I</italic>, respectively, and <italic>U</italic>[0, 1] denotes the uniform distribution on the interval [0, 1]. Thus the average weight is <inline-formula id="pcbi.1006816.e121"><alternatives><graphic id="pcbi.1006816.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e121" xlink:type="simple"/><mml:math display="inline" id="M121"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>〉</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where we defined <inline-formula id="pcbi.1006816.e122"><alternatives><graphic id="pcbi.1006816.e122g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e122" xlink:type="simple"/><mml:math display="inline" id="M122"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mspace width="2pt"/><mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, while the standard deviation <inline-formula id="pcbi.1006816.e123"><alternatives><graphic id="pcbi.1006816.e123g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e123" xlink:type="simple"/><mml:math display="inline" id="M123"><mml:mrow><mml:mtext>SD</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:msqrt><mml:mn>3</mml:mn></mml:msqrt></mml:mrow></mml:mfrac> <mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. All parameters, <inline-formula id="pcbi.1006816.e124"><alternatives><graphic id="pcbi.1006816.e124g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e124" xlink:type="simple"/><mml:math display="inline" id="M124"><mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, <italic>δJ</italic><sub><italic>ab</italic></sub>, <italic>σ</italic><sub><italic>ab</italic></sub> were constrained to be non-negative (which is equivalent to the constraints <inline-formula id="pcbi.1006816.e125"><alternatives><graphic id="pcbi.1006816.e125g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e125" xlink:type="simple"/><mml:math display="inline" id="M125"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <italic>σ</italic><sub><italic>ab</italic></sub> ≥ 0, for the alternative parameterization using <inline-formula id="pcbi.1006816.e126"><alternatives><graphic id="pcbi.1006816.e126g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e126" xlink:type="simple"/><mml:math display="inline" id="M126"><mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, <italic>δJ</italic><sub><italic>ab</italic></sub>, <italic>σ</italic><sub><italic>ab</italic></sub>). The first two constraints (together with the sign variable <italic>ς</italic><sub><italic>b</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006816.e119">Eq (24)</xref>) ensure that any realization of <italic>W</italic><sub><italic>ij</italic></sub> satisfies Dale’s principle [<xref ref-type="bibr" rid="pcbi.1006816.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref040">40</xref>].</p>
<p>We chose the feedforward weight matrix, <italic>f</italic>, to be diagonal with weights having independent random heterogeneity across network neurons. More precisely, for a network of <italic>N</italic> neurons, <italic>f</italic> was an <italic>N</italic> × <italic>N</italic> diagonal matrix generated via
<disp-formula id="pcbi.1006816.e127"><alternatives><graphic id="pcbi.1006816.e127g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e127" xlink:type="simple"/><mml:math display="block" id="M127"><mml:mi>F</mml:mi> <mml:mrow><mml:mo>=</mml:mo> <mml:mtext>Diag</mml:mtext> <mml:mo>(</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>F</mml:mi></mml:msubsup> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
<disp-formula id="pcbi.1006816.e128"><alternatives><graphic id="pcbi.1006816.e128g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e128" xlink:type="simple"/><mml:math display="block" id="M128"><mml:msubsup><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>F</mml:mi></mml:msubsup> <mml:mrow><mml:mover><mml:mo>∼</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>i</mml:mi> <mml:mi>d</mml:mi></mml:mrow></mml:mover> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
where the binary random variables <inline-formula id="pcbi.1006816.e129"><alternatives><graphic id="pcbi.1006816.e129g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e129" xlink:type="simple"/><mml:math display="inline" id="M129"><mml:msubsup><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>F</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are sampled independently and uniformly from [−1, 1].</p>
<p>Our recurrent and feedforward connectivity ensemble is thus characterized by 13 non-negative parameters (enumerated in <xref ref-type="table" rid="pcbi.1006816.t003">Table 3</xref>): the parameter <italic>V</italic> that controls the degree of disordered heterogeneity in feedforward weights, as well as the elements of the three 2 × 2 matrices <inline-formula id="pcbi.1006816.e130"><alternatives><graphic id="pcbi.1006816.e130g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e130" xlink:type="simple"/><mml:math display="inline" id="M130"><mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>, <italic>δJ</italic><sub><italic>ab</italic></sub>, <italic>σ</italic><sub><italic>ab</italic></sub> (where <italic>a</italic>, <italic>b</italic> ∈ {<italic>E</italic>, <italic>I</italic>}), which control the average strength, disordered heterogenetity, and spatial range of recurrent horizontal connections. These constituted the parameters
<disp-formula id="pcbi.1006816.e131"><alternatives><graphic id="pcbi.1006816.e131g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e131" xlink:type="simple"/><mml:math display="block" id="M131"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula>
that were fit using the WGAN (or moment matching). (Because enforcing the non-negativity constraints on the set <inline-formula id="pcbi.1006816.e132"><alternatives><graphic id="pcbi.1006816.e132g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e132" xlink:type="simple"/><mml:math display="inline" id="M132"><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is easier than enforcing the constraints on the set <inline-formula id="pcbi.1006816.e133"><alternatives><graphic id="pcbi.1006816.e133g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e133" xlink:type="simple"/><mml:math display="inline" id="M133"><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>J</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> introduced in Experiment 2, we used the parametrization of <xref ref-type="disp-formula" rid="pcbi.1006816.e131">Eq (28)</xref> in our WGAN implementation.) While these parameters described the statistics of connectivity, a specific realization of the network is determined by the high-dimensional fixed-distribution random variables of the GAN formalism, <bold>z</bold>, in addition to <bold><italic>θ</italic></bold>. The former is composed of the <italic>N</italic><sup>2</sup> independent, standard uniform random variables <inline-formula id="pcbi.1006816.e134"><alternatives><graphic id="pcbi.1006816.e134g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e134" xlink:type="simple"/><mml:math display="inline" id="M134"><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, and the <italic>N</italic> independent random variables <inline-formula id="pcbi.1006816.e135"><alternatives><graphic id="pcbi.1006816.e135g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e135" xlink:type="simple"/><mml:math display="inline" id="M135"><mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>F</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> which are sampled uniformly from [−1, 1]. Also note that Eqs (<xref ref-type="disp-formula" rid="pcbi.1006816.e119">24</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1006816.e127">26</xref>) constitute an example of the sampler function, <italic>g</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>), introduced in the subsection “Mechanistic network models as implicit generative models” of Results.</p>
<table-wrap id="pcbi.1006816.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006816.t003</object-id>
<label>Table 3</label>
<caption>
<title>The description of the parameters of the SSN inferred from tuning curve data (<italic>a</italic>, <italic>b</italic> ∈ {<italic>E</italic>, <italic>I</italic>}).</title>
</caption>
<alternatives>
<graphic id="pcbi.1006816.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006816.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Parameter</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>σ</italic><sub><italic>ab</italic></sub></td>
<td align="center">connection length scales</td>
</tr>
<tr>
<td align="center"><italic>J</italic><sub><italic>ab</italic></sub></td>
<td align="center">lower bounds of connection strengths</td>
</tr>
<tr>
<td align="center"><italic>δJ</italic><sub><italic>ab</italic></sub></td>
<td align="center">widths of connection strength distribution</td>
</tr>
<tr>
<td align="center"><italic>V</italic></td>
<td align="center">randomness of the stimulus amplitude</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In our example experiments, we simulated the fitting of an SSN model of V1 to datasets of stimulus size tuning curves of V1 neurons [<xref ref-type="bibr" rid="pcbi.1006816.ref008">8</xref>]. As a simple model of the visual input to V1 evoked by a grating of diameter <italic>b</italic>, the stimulus input to neuron <italic>i</italic> was modeled as
<disp-formula id="pcbi.1006816.e136"><alternatives><graphic id="pcbi.1006816.e136g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e136" xlink:type="simple"/><mml:math display="block" id="M136"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:mspace width="2pt"/><mml:mi>σ</mml:mi> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:msup><mml:mi>l</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>b</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mi>σ</mml:mi> <mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:msup><mml:mi>l</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>b</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
where <italic>σ</italic>(<italic>u</italic>) = (1 + exp(−<italic>u</italic>))<sup>−1</sup> is the logistic function, <italic>A</italic> denotes the stimulus strength or contrast. Thus, the stimulus targets a central band of width <italic>b</italic> centered on the middle of the topographic grid (see <xref ref-type="fig" rid="pcbi.1006816.g002">Fig 2B</xref>). The parameter <italic>l</italic> determines the smoothing of the edges of the stimulated region. For training the model, we chose the sizes from a set of <italic>S</italic> = 8 different sizes (0, 1/16, 1/8, 3/16, 1/4, 1/2, 3/4, 1) (measured in units of the total length of the network). Letting <italic>b</italic><sub><italic>s</italic></sub> denote the size in stimulus condition <italic>s</italic> (<italic>s</italic> ∈ {1, ⋯, <italic>S</italic>}), the <bold>I</bold>(<italic>s</italic>) of <xref ref-type="disp-formula" rid="pcbi.1006816.e115">Eq (22)</xref> and Experiment 2 is given by <bold>I</bold>(<italic>s</italic>) = <bold>I</bold>(<italic>b</italic><sub><italic>s</italic></sub>), with a slight abuse of notation.</p>
<p>The output of the SSN, considered as a generative model for tuning curves, are the size tuning curves of a subset of network neurons which we call “probe” neurons. We define the tuning curve of these neurons in terms of their sustained responses evoked by different stimuli. Thus given a specific realization of the SSN, for each stimulus <italic>s</italic> ∈ {1, …, <italic>S</italic>}, we first calculate the sustained network response vector by the temporal average between <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub> <disp-formula id="pcbi.1006816.e137"><alternatives><graphic id="pcbi.1006816.e137g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e137" xlink:type="simple"/><mml:math display="block" id="M137"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="2pt"/><mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula>
where Δ<italic>t</italic> is the Euler integration step and <italic>T</italic> = (<italic>t</italic><sub>2</sub> − <italic>t</italic><sub>1</sub>)/Δ<italic>t</italic>. We choose <italic>t</italic><sub>1</sub>/Δ<italic>t</italic> = 200 and <italic>t</italic><sub>2</sub>/Δ<italic>t</italic> = 240 to balance the computational cost and the accuracy for approximating the true steady-state. Given the sustained network response <inline-formula id="pcbi.1006816.e138"><alternatives><graphic id="pcbi.1006816.e138g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e138" xlink:type="simple"/><mml:math display="inline" id="M138"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and an <italic>a priori</italic> selected set of <italic>O</italic> probe neurons with indices <bold>i</bold> = (<italic>i</italic><sub>1</sub>, <italic>i</italic><sub>2</sub>, …, <italic>i</italic><sub><italic>O</italic></sub>) (the probe neurons can equivalently be defined by their types and topographic locations), we define the output of the SSN generative model (the GAN generator) to be the vector
<disp-formula id="pcbi.1006816.e139"><alternatives><graphic id="pcbi.1006816.e139g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e139" xlink:type="simple"/><mml:math display="block" id="M139"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>S</mml:mi></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula>
Here, <inline-formula id="pcbi.1006816.e140"><alternatives><graphic id="pcbi.1006816.e140g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e140" xlink:type="simple"/><mml:math display="inline" id="M140"><mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula> denotes the topographic location of the <italic>p</italic>-th probe neuron, and we have now made the dependency of the output on the quenched noise variables, <bold>z</bold>, and model parameters explicit. We treated the probe neuron’s topographic location, <inline-formula id="pcbi.1006816.e141"><alternatives><graphic id="pcbi.1006816.e141g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e141" xlink:type="simple"/><mml:math display="inline" id="M141"><mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula>, as the condition <italic>c</italic> in conditional WGAN (cWGAN). In this paper, we only probed excitatory neurons, <italic>i</italic>.<italic>e</italic>., <italic>α</italic>(<italic>i</italic><sub><italic>p</italic></sub>) = <italic>E</italic>.</p>
<p>In experimental recordings, typically the grating stimulus used to measure a neuron’s size tuning curve is centered on the neuron’s receptive field. To model this stimulus centering, we always set the first probe neuron <italic>i</italic><sub>1</sub> to be at the center of the topographic grid (<italic>i</italic>.<italic>e</italic>., <inline-formula id="pcbi.1006816.e142"><alternatives><graphic id="pcbi.1006816.e142g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e142" xlink:type="simple"/><mml:math display="inline" id="M142"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:msub><mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), which was the center of the stimulus. In Experiment 2 we only fit the model to the size tuning curves of “centered” excitatory neurons. Since in that case there was only one probe neuron (or cWGAN condition), we denoted the model output more succinctly by <italic>G</italic><sub><bold><italic>θ</italic></bold></sub>(<bold>z</bold>), dropping its second argument. By contrast, in Experiment 3 we included tuning curves of neurons with offset receptive fields (topographic locations) in the training dataset and employed a conditional WGAN.</p>
<p>Note that tuning curves for networks such as the SSN described here which have partially random connectivity, show variability across neurons as well as across different realizations of the network for a fixed probe neuron. When the network size <italic>N</italic> is large, typically a local self-averaging or “ergodicity” property is expected to emerge: the empirical distribution, in a single network realization, across the tuning curves of neurons of the same type and with nearby topographic locations should approximate the distribution across different <bold>z</bold> for a neuron of pre-assigned index (<italic>i</italic>.<italic>e</italic>., type and location). Although we did not do so in our experiments, one may exploit this ergodicity for more efficient training and testing of the model by sampling multiple nearby sites from each connectivity matrix realization.</p>
<p>Given a dataset of size tuning curves, we would like to find model parameters, <inline-formula id="pcbi.1006816.e143"><alternatives><graphic id="pcbi.1006816.e143g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e143" xlink:type="simple"/><mml:math display="inline" id="M143"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi>V</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, that produce a matching model distribution of tuning curves. In this paper we constructed a simulated training dataset of size tuning curves using a “ground truth” SSN model, with parameters <bold><italic>θ</italic></bold><sup>truth</sup> given in <xref ref-type="table" rid="pcbi.1006816.t001">Table 1</xref>. All other model parameters were the same between the ground truth and trained SSN models, and had the values: <italic>N</italic> = 402, <italic>k</italic> = 0.01, <italic>n</italic> = 2.2, <italic>τ</italic><sub><italic>E</italic></sub>/Δ<italic>t</italic> = 20, <italic>τ</italic><sub><italic>I</italic></sub>/<italic>τ</italic><sub><italic>E</italic></sub> = 1/2, <italic>A</italic> = 20, <italic>l</italic> = 2<sup>−5</sup>.</p>
<p>During training, according to Algorithm 1, every time <italic>G</italic><sub><bold><italic>θ</italic></bold></sub> (<bold>z</bold>; <italic>x</italic>) was evaluated, we simulated the trained SSN using the forward Euler method for <italic>t</italic><sub>2</sub>/Δ<italic>t</italic> = 240 steps (see also <xref ref-type="disp-formula" rid="pcbi.1006816.e137">Eq (30)</xref>). The gradients of the generator output or the generator loss with respect to parameters <bold><italic>θ</italic></bold> were calculated by standard back-propagation through time (BPTT). To avoid numerical instability, the parameters <inline-formula id="pcbi.1006816.e144"><alternatives><graphic id="pcbi.1006816.e144g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e144" xlink:type="simple"/><mml:math display="inline" id="M144"><mml:mrow><mml:msubsup><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow> <mml:mo>&lt;</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mi>δ</mml:mi> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> were clipped at 10<sup>−3</sup> during training. To exclude extremely large (non-biological) values, we also clipped them below 10. We also clipped <italic>V</italic> to bound it within the interval [0, 1]. (These upper bounds can be thought of as imposed Bayesian priors on these parameters.)</p>
<p>Moreover, during training, the SSN may be pushed to parameter regions in which, for some realizations of the quenched noise variables <bold>z</bold>, the network does not converge to a stable fixed point. Since an implicit model assumption of the SSN is to model sustained responses by stable fixed points of the model with rates in the biological range, dynamical non-convergennce and very high rates can be (strongly) penalized. We encouraged the firing rate of the all SSN neurons to uniformly remain below a permissive threshold of 200 Hz, by adding the following penalty term to the generator loss
<disp-formula id="pcbi.1006816.e145"><alternatives><graphic id="pcbi.1006816.e145g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e145" xlink:type="simple"/><mml:math display="block" id="M145"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Penalty</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>η</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mi>T</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>‖</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="2pt"/><mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>;</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>200</mml:mn> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>‖</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula>
where <italic>m</italic> is the size of the mini-batch in the gradient descent, <italic>T</italic> = (<italic>t</italic><sub>2</sub> − <italic>t</italic><sub>1</sub>)/Δ<italic>t</italic> is the time window for calculating the sustained response <xref ref-type="disp-formula" rid="pcbi.1006816.e137">Eq (30)</xref>, and <italic>η</italic> = 100 is the weight of the penalty relative to the WGAN generator loss. This penalty, together with the modified neural input-output nonlinearity descrbied in (<xref ref-type="disp-formula" rid="pcbi.1006816.e118">Eq (23)</xref>), ameliorated diverging solutions of SSN dynamics and the resulting extremely large or small generator gradients.</p>
<p>Once the SSN network produces large output rates, it disrupts the learning in the discriminator. Furthermore, <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref> alone can fix such behavior of SSN without relying on the discriminator to learn to adapt to new extremely strong inputs (which are the SSN’s output rates). Thus, we find that it is better to skip such generator output samples for stabilizing learning through the GAN framework. Namely, we update the discriminator parameters for a mini-batch only if
<disp-formula id="pcbi.1006816.e146"><alternatives><graphic id="pcbi.1006816.e146g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e146" xlink:type="simple"/><mml:math display="block" id="M146"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Penalty</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula>
where the bound 1 is rather arbitrary. We observed that for many successful trainings, such large firing rates never occur.</p>
<p>In order to encourage convergence to a fixed point, we also tried penalizing large absolute values of the time derivative <italic>d</italic><bold>r</bold>/<italic>dt</italic> in the window [<italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>]. However, we found empirically that the penalty <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref> was sufficient to allow the training algorithms to find parameters for which the network converged with high probability to a fixed point.</p>
</sec>
<sec id="sec021">
<title>Goodness of fit analysis</title>
<p>We looked at the goodness of fit of model outputs by comparing the distributions of several scalar functions or “summary statistics” of tuning curves. We give the precise definitions of these statistics here.</p>
<p>For the feedforward network model of M1 tuning curves, in Experiment 1 we compared the data and model histograms of four test statistics or measures characterizing the hand-location tuning curves, defined as follows. Let <inline-formula id="pcbi.1006816.e147"><alternatives><graphic id="pcbi.1006816.e147g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e147" xlink:type="simple"/><mml:math display="inline" id="M147"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote the tuning curve, <italic>i</italic>.<italic>e</italic>., the trial average firing rate in condition <italic>s</italic>, with <italic>s</italic> ∈ {1, ⋯, 27} indexing the hand location from among the 3 × 3 × 3 cubic grid of target locations in the experiment of Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>]). The average firing rate was simply <inline-formula id="pcbi.1006816.e148"><alternatives><graphic id="pcbi.1006816.e148g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e148" xlink:type="simple"/><mml:math display="inline" id="M148"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>27</mml:mn></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>27</mml:mn></mml:msubsup> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The coding level of a tuning curve was defined as
<disp-formula id="pcbi.1006816.e149"><alternatives><graphic id="pcbi.1006816.e149g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e149" xlink:type="simple"/><mml:math display="block" id="M149"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Coding</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Level</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>27</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>27</mml:mn></mml:munderover> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>5</mml:mn> <mml:mspace width="2pt"/><mml:mtext>Hz</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula> <italic>i</italic>.<italic>e</italic>., the fraction of conditions with rate larger than 5 Hz (Θ(⋅) denotes the Heaviside function). The <italic>R</italic><sup>2</sup> denoted the coefficient of determination of the optimal linear fit to the tuning curve, <italic>i</italic>.<italic>e</italic>.,
<disp-formula id="pcbi.1006816.e150"><alternatives><graphic id="pcbi.1006816.e150g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e150" xlink:type="simple"/><mml:math display="block" id="M150"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>R</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>27</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>27</mml:mn></mml:msubsup> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula>
where <italic>L</italic>(<italic>s</italic>) = <italic>r</italic><sub>0</sub> + <bold>m</bold> ⋅ <bold>x</bold><sub><italic>s</italic></sub> is the optimal linear approximation to <inline-formula id="pcbi.1006816.e151"><alternatives><graphic id="pcbi.1006816.e151g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e151" xlink:type="simple"/><mml:math display="inline" id="M151"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> (where <italic>r</italic><sub>0</sub> and <bold>m</bold> are the coefficients of the linear regression). Finally, following Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref004">4</xref>], the complexity score of a tuning curve was defined as in <xref ref-type="disp-formula" rid="pcbi.1006816.e152">Eq (36)</xref>.
<disp-formula id="pcbi.1006816.e152"><alternatives><graphic id="pcbi.1006816.e152g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e152" xlink:type="simple"/><mml:math display="block" id="M152"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Complexity</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Score</mml:mtext> <mml:mo>=</mml:mo> <mml:mtext>SD</mml:mtext> <mml:mspace width="2pt"/><mml:mo>[</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mo form="prefix" movablelimits="true">min</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>|</mml:mo> <mml:mspace width="2pt"/><mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:msup><mml:mi>s</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula>
where SD denotes standard deviations.</p>
<p>To quantify the goodness of fit between the outputs of the ground truth and trained SSN models in Experiment 2, we compared the distributions of four test statistics characterizing the size tuning curves: preferred stimulus size, maximum firing rate, the suppression index, and the normalized participation ration, as defined below. While to fit the model we used size tuning curves containing responses to stimuli with <italic>S</italic> = 8 different sizes, <italic>b</italic><sub><italic>s</italic></sub>, in the set (0, 1/16, 1/8, 3/16, 1/4, 1/2, 3/4, 1), for testing purposes and to evaluate the above measures, we generated tuning curves from the trained SSN using a larger set of stimulus sizes (denoted by <italic>b</italic> below). Letting <inline-formula id="pcbi.1006816.e153"><alternatives><graphic id="pcbi.1006816.e153g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e153" xlink:type="simple"/><mml:math display="inline" id="M153"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote the size tuning curve (<italic>i</italic>.<italic>e</italic>., <inline-formula id="pcbi.1006816.e154"><alternatives><graphic id="pcbi.1006816.e154g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e154" xlink:type="simple"/><mml:math display="inline" id="M154"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the sustained response of the center excitatory neuron to the stimulus with size <italic>b</italic>), the maximum firing rate is max<sub><italic>b</italic></sub> <inline-formula id="pcbi.1006816.e155"><alternatives><graphic id="pcbi.1006816.e155g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e155" xlink:type="simple"/><mml:math display="inline" id="M155"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the preferred size is arg max<sub><italic>b</italic></sub> <inline-formula id="pcbi.1006816.e156"><alternatives><graphic id="pcbi.1006816.e156g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e156" xlink:type="simple"/><mml:math display="inline" id="M156"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The suppression index is defined by
<disp-formula id="pcbi.1006816.e157"><alternatives><graphic id="pcbi.1006816.e157g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e157" xlink:type="simple"/><mml:math display="block" id="M157"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Suppression</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Index</mml:mtext> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mi>b</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
and measures the strength of surround suppression. Finally, the normalized participation ratio (related to the inverse participation ratio [<xref ref-type="bibr" rid="pcbi.1006816.ref059">59</xref>]) is defined by
<disp-formula id="pcbi.1006816.e158"><alternatives><graphic id="pcbi.1006816.e158g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e158" xlink:type="simple"/><mml:math display="block" id="M158"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Normalized</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Participation</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>Ratio</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mi>b</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>b</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>b</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula>
and measures the fraction of all tested sizes that elicited responses comparable to the maximum response.</p>
</sec>
<sec id="sec022">
<title>Discriminator networks</title>
<p>The most studied application of GANs is in producing highly structured, high-dimensional output such as images, videos, and audio. In those applications, mathematical structures such as translational symmetry in the data space (for images) is exploited to design complex and structured discriminators such as deep convolutional networks. It has also been noted that the discriminator network should be sufficiently powerful so that it is capable of fully capturing the data and model distributions [<xref ref-type="bibr" rid="pcbi.1006816.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006816.ref044">44</xref>]. In our application, the outputs of the generator are comparatively lower-dimensional objects, with less complex distributions. Furthermore, developing a new discriminator architecture exploiting mathematical structure in the tuning curve space such as metric and ordering in the stimulus space is beyond the scope of this paper. In this work we used relatively simple discriminator networks. Nevertheless care is needed in designing discriminators; a function <inline-formula id="pcbi.1006816.e159"><alternatives><graphic id="pcbi.1006816.e159g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e159" xlink:type="simple"/><mml:math display="inline" id="M159"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> that is too simple can preclude the fitting of important aspects of the distribution. For example, if a linear function <inline-formula id="pcbi.1006816.e160"><alternatives><graphic id="pcbi.1006816.e160g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e160" xlink:type="simple"/><mml:math display="inline" id="M160"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> were used in the WGAN approach it would result in a fit that matches only the average tuning curve between model and data, and ignores tuning curve variability altogether.</p>
<p>For the M1 feedforward model, we used a dense feedforward neural network as the discriminator <inline-formula id="pcbi.1006816.e161"><alternatives><graphic id="pcbi.1006816.e161g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e161" xlink:type="simple"/><mml:math display="inline" id="M161"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula>, with four hidden layers of 128 rectified linear units and a single linear readout unit in the final layer. The discriminator network weights were initialized to uniformly random weights with Glorot normalization [<xref ref-type="bibr" rid="pcbi.1006816.ref060">60</xref>]. We do not use any kind of normalization or parameter regularization other than the WGAN penalty term in <xref ref-type="disp-formula" rid="pcbi.1006816.e037">Eq (4)</xref> (i.e. we set Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) to zero in this example).</p>
<p>For the SSN recurrent model, we used dense feedforward neural networks with four hidden layers and with layer normalization [<xref ref-type="bibr" rid="pcbi.1006816.ref061">61</xref>] as recommended for WGAN in Ref. [<xref ref-type="bibr" rid="pcbi.1006816.ref026">26</xref>]. The discriminator network used in the experiments of Figs <xref ref-type="fig" rid="pcbi.1006816.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1006816.g005">5</xref> had 128 and 64 neurons in each hidden layer, respectively. In the training experiments underlying <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref>, we used networks with 32 to 128 neurons in each layer; as indicated by the WGAN histogram, all choices consistently performed well. We note that simple dense feedforward neural networks without normalization do not work well for SSN due to numerical instability in long-running training. It was important, however, <italic>not</italic> to apply the layer normalization in the first (input) layer, as the mean and variance across stimulus parameters of the turning curves are valuable information for the discriminator which would be discarded by such a normalization. We also used weight decay [<xref ref-type="bibr" rid="pcbi.1006816.ref062">62</xref>] for all parameters to stabilize the learning. We initialized all neural biases to 0 and initialized all weights as independent standard normal random variables, except in the input layer. For the input layers, we used the same initialization as in the M1 feedforward model’s discriminator.</p>
</sec>
<sec id="sec023">
<title>Moment matching</title>
<p>To provide a benchmark for our proposed GAN-based method, we also fit the SSN using moment matching [<xref ref-type="bibr" rid="pcbi.1006816.ref063">63</xref>]. We define a generic moment matching loss as
<disp-formula id="pcbi.1006816.e162"><alternatives><graphic id="pcbi.1006816.e162g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e162" xlink:type="simple"/><mml:math display="block" id="M162"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>D</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>D</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mspace width="2pt"/><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Here, <italic>D</italic> = <italic>S</italic> × <italic>O</italic>, where <italic>S</italic> is the total number of stimulus conditions and <italic>O</italic> the number of neurons whose firing rates are probed in the SSN, and <italic>d</italic> indexes the combination of stimulus condition and probe neuron. <italic>m</italic><sub><italic>d</italic></sub>(<bold><italic>θ</italic></bold>) and <italic>s</italic><sub><italic>d</italic></sub>(<bold><italic>θ</italic></bold>) are the mean and variance of response in combination <italic>d</italic>, across a mini-batch of 32 model-generated sample tuning curves, respectively, while <italic>μ</italic><sub><italic>d</italic></sub> and <italic>σ</italic><sub><italic>d</italic></sub> are the empirical mean and variance of this response, across the full training dataset of 2048 size tuning curves. The <italic>w</italic><sub><italic>i</italic>,<italic>d</italic></sub> are the weights given to each moment, and are hyperparameters of the moment matching method. We tried the following options
<disp-formula id="pcbi.1006816.e163"><alternatives><graphic id="pcbi.1006816.e163g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e163" xlink:type="simple"/><mml:math display="block" id="M163"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>uniform</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>scaling</mml:mtext> <mml:mo>:</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd columnalign="center"><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>D</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>D</mml:mi></mml:munderover> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>ℓ</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula>
<disp-formula id="pcbi.1006816.e164"><alternatives><graphic id="pcbi.1006816.e164g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e164" xlink:type="simple"/><mml:math display="block" id="M164"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>element-wise</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>scaling</mml:mtext> <mml:mo>:</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd columnalign="center"><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ε</mml:mi> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>ℓ</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula>
<disp-formula id="pcbi.1006816.e165"><alternatives><graphic id="pcbi.1006816.e165g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e165" xlink:type="simple"/><mml:math display="block" id="M165"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>relative</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>scaling</mml:mtext> <mml:mo>:</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd columnalign="center"><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ε</mml:mi> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>=</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ε</mml:mi> <mml:msup><mml:mo>)</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(40)</label></disp-formula>
where <italic>ℓ</italic> controls weight of the variance with respect to the mean and <italic>ε</italic> = 10<sup>−3</sup> is a regularization constant to avoid division by zero. In our preliminary experiments, we found that element-wise <xref ref-type="disp-formula" rid="pcbi.1006816.e164">Eq (39)</xref> and relative <xref ref-type="disp-formula" rid="pcbi.1006816.e165">Eq (40)</xref> scalings are better than uniform scaling <xref ref-type="disp-formula" rid="pcbi.1006816.e163">Eq (38)</xref>. Thus, we only used element-wise <xref ref-type="disp-formula" rid="pcbi.1006816.e164">Eq (39)</xref> and relative <xref ref-type="disp-formula" rid="pcbi.1006816.e165">Eq (40)</xref> scalings in results presented here. For fitting the SSN, the same reasons for encouraging dynamical stability as in the WGAN case hold. Thus, we added the penalty term as defined in <xref ref-type="disp-formula" rid="pcbi.1006816.e145">Eq (32)</xref> and minimized the loss
<disp-formula id="pcbi.1006816.e166"><alternatives><graphic id="pcbi.1006816.e166g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e166" xlink:type="simple"/><mml:math display="block" id="M166"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>Penalty</mml:mtext> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula>
For most of the trainings, large firing rates yielding Penalty<sub><italic>G</italic></sub>(<bold><italic>θ</italic></bold>) &gt; 0 rarely occured. The generator parameters are updated using Adam (a variant of stochastic gradient descent) with the hyperparameters <italic>β</italic><sub>1</sub> = 0.5, <italic>β</italic><sub>2</sub> = 0.9, <italic>ϵ</italic> = 10<sup>−8</sup> as in Algorithm 1. We use the learning rate 0.001 unless specified.</p>
</sec>
<sec id="sec024">
<title>Stopping criterion and performance metric</title>
<p>To compare learned results between the GAN and moment matching and across different hyperparameters on an equal footing, we defined a condition for terminating learning as follows. First, we stop training at the first generator update step, <italic>n</italic><sub>0</sub>, in which the speed-of-change of the generator parameters, as evaluated by
<disp-formula id="pcbi.1006816.e167"><alternatives><graphic id="pcbi.1006816.e167g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e167" xlink:type="simple"/><mml:math display="block" id="M167"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mo>‖</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ν</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:munderover> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mi>κ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>κ</mml:mi></mml:mfrac> <mml:msub><mml:mo>‖</mml:mo> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula>
becomes smaller than a tolerance threshold of 0.01. Here <bold><italic>θ</italic></bold>(<italic>n</italic>) is the vector of generator parameters at the <italic>n</italic><sup>th</sup> generator update, <italic>κ</italic> controls the timescale at which the speed is computed, and <italic>ν</italic> is the size of the moving average window. ‖<bold>x</bold>‖<sub>1</sub> denotes the <italic>L</italic><sub>1</sub>-norm of the vector <bold>x</bold>, i.e., the sum of absolute values of its components; thus the condition <xref ref-type="disp-formula" rid="pcbi.1006816.e167">Eq (42)</xref> ensures that the speed-of-change of all model parameters are small, i.e., they have approximately converged. To obtain the final result (estimated parameters) of learning, we then compute the average of the generator parameter <bold><italic>θ</italic></bold>(<italic>n</italic>) in the <italic>ω</italic> steps leading to step <italic>n</italic><sub>0</sub> <disp-formula id="pcbi.1006816.e168"><alternatives><graphic id="pcbi.1006816.e168g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e168" xlink:type="simple"/><mml:math display="block" id="M168"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ω</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:munderover> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula>
For comparing results across different generator learning rates, <italic>α</italic><sub><italic>G</italic></sub>, we used <inline-formula id="pcbi.1006816.e169"><alternatives><graphic id="pcbi.1006816.e169g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e169" xlink:type="simple"/><mml:math display="inline" id="M169"><mml:mrow><mml:mi>κ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>=</mml:mo> <mml:mi>ω</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>α</mml:mi> <mml:mrow><mml:mi>G</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>As the metric of performance, we use the so-called symmetric mean average percent error (sMAPE) of the estimated generator parameters <inline-formula id="pcbi.1006816.e170"><alternatives><graphic id="pcbi.1006816.e170g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e170" xlink:type="simple"/><mml:math display="inline" id="M170"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> relative to the ground truth parameters <bold><italic>θ</italic></bold><sup>truth</sup> which were used to generate the training dataset. That is we let
<disp-formula id="pcbi.1006816.e171"><alternatives><graphic id="pcbi.1006816.e171g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e171" xlink:type="simple"/><mml:math display="block" id="M171"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>sMAPE</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>100</mml:mn> <mml:mo>%</mml:mo> <mml:mspace width="2pt"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>13</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>13</mml:mn></mml:munderover> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>truth</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>truth</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(44)</label></disp-formula>
where 13 is the number of parameters. The performance in any learning run is then computed by the <inline-formula id="pcbi.1006816.e172"><alternatives><graphic id="pcbi.1006816.e172g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006816.e172" xlink:type="simple"/><mml:math display="inline" id="M172"><mml:mrow><mml:mtext>sMAPE</mml:mtext> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the final result, <xref ref-type="disp-formula" rid="pcbi.1006816.e168">Eq (43)</xref>, of that run obtained using the above termination procedure <xref ref-type="disp-formula" rid="pcbi.1006816.e167">Eq (42)</xref>. Note that this termination criterion is used only for <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref> and not in <xref ref-type="fig" rid="pcbi.1006816.g005">Fig 5</xref>, where we plotted the learning curve over a broader range for demonstration purposes.</p>
</sec>
<sec id="sec025">
<title>Hyperparameters for parameter identification experiment</title>
<p>For the WGAN-based fits shown in <xref ref-type="fig" rid="pcbi.1006816.g006">Fig 6</xref> we picked combinations of hyperparameters from the following choices: the generator learning rate was <italic>α</italic><sub><italic>G</italic></sub> = 10<sup>−4</sup>, 2 × 10<sup>−4</sup>, or 4 × 10<sup>−4</sup>, the number of neurons in each discriminator hidden layer was 32, 64 or 128, discriminator update rule was Adam or RMSprop. For the moment matching fits the hyperparameter choices were: the learning rate was 10<sup>−4</sup>, 2 × 10<sup>−4</sup>, 4 × 10<sup>−4</sup>, 10<sup>−3</sup>, 2 × 10<sup>−3</sup>, or 4 × 10<sup>−3</sup>, the weight of variance λ = 0.01, 0.1, 1, and the moment scaling was element-wise <xref ref-type="disp-formula" rid="pcbi.1006816.e164">Eq (39)</xref> or relative <xref ref-type="disp-formula" rid="pcbi.1006816.e165">Eq (40)</xref>.</p>
</sec>
<sec id="sec026">
<title>Implementation</title>
<p>We implemented our GAN-based method and moment matching in Python using Theano [<xref ref-type="bibr" rid="pcbi.1006816.ref064">64</xref>] and Lasagne [<xref ref-type="bibr" rid="pcbi.1006816.ref065">65</xref>]. Our implementation is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/ahmadianlab/tc-gan" xlink:type="simple">https://github.com/ahmadianlab/tc-gan</ext-link> under the MIT license.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>The authors thank Daniel Lowd and Caleb Holt for inspiring conversations.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006816.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>. <article-title>Orientation selectivity in macaque V1: diversity and laminar dependence</article-title>. <source>J Neurosci</source>. <year>2002</year>;<volume>22</volume>(<issue>13</issue>):<fpage>5639</fpage>–<lpage>5651</lpage>. <object-id pub-id-type="pmid">12097515</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Persi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nowak</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Barone</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>van Vreeswijk</surname> <given-names>C</given-names></name>. <article-title>Power-Law Input-Output Transfer Functions Explain the Contrast-Response and Tuning Properties of Neurons in Visual Cortex</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1001078" xlink:type="simple">10.1371/journal.pcbi.1001078</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <etal>et al</etal>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>(<issue>7451</issue>):<fpage>585</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lalazar</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>. <article-title>Tuning Curves for Arm Posture Control in Motor Cortex Are Consistent with Random Connectivity</article-title>. <source>PLOS Comp Bio</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004910" xlink:type="simple">10.1371/journal.pcbi.1004910</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>9</issue>):<fpage>3844</fpage>–<lpage>3856</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2753-12.2013" xlink:type="simple">10.1523/JNEUROSCI.2753-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23447596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roxin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mongillo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Van Vreeswijk</surname> <given-names>C</given-names></name>. <article-title>On the Distribution of Firing Rates in Networks of Cortical Neurons</article-title>. <source>The Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>45</issue>):<fpage>16217</fpage>–<lpage>16226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1677-11.2011" xlink:type="simple">10.1523/JNEUROSCI.1677-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22072673</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title>. <source>Nature Neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>11</issue>):<fpage>1498</fpage>–<lpage>1505</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3220" xlink:type="simple">10.1038/nn.3220</ext-link></comment> <object-id pub-id-type="pmid">23001062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Van Hooser</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>85</volume>(<issue>2</issue>):<fpage>402</fpage>–<lpage>417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.026" xlink:type="simple">10.1016/j.neuron.2014.12.026</ext-link></comment> <object-id pub-id-type="pmid">25611511</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Fumarola</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>Properties of networks with partially structured and partially random connectivity</article-title>. <source>Phys Rev E Stat Nonlin Soft Matter Phys</source>. <year>2015</year>;<volume>91</volume>(<issue>1</issue>):<fpage>012820</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.91.012820" xlink:type="simple">10.1103/PhysRevE.91.012820</ext-link></comment> <object-id pub-id-type="pmid">25679669</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>van Vreeswijk</surname> <given-names>C</given-names></name>. <article-title>The mechanism of orientation selectivity in primary visual cortex without a functional map</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>12</issue>):<fpage>4049</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.6284-11.2012" xlink:type="simple">10.1523/JNEUROSCI.6284-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22442071</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source>. <year>2006</year>;<volume>440</volume>:<fpage>1007</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04701" xlink:type="simple">10.1038/nature04701</ext-link></comment> <object-id pub-id-type="pmid">16625187</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Grivich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Petrusca</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The Structure of Multi-Neuron Firing Patterns in Primate Retina</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>(<issue>32</issue>):<fpage>8254</fpage>–<lpage>8266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1282-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1282-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16899720</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>Spatiotemporal correlations and visual signaling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>:<fpage>995</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hobbs</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Patel</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>A Maximum Entropy Model Applied to Spatial and Temporal Correlations from Cortical Networks In Vitro</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>2</issue>):<fpage>505</fpage>–<lpage>518</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3359-07.2008" xlink:type="simple">10.1523/JNEUROSCI.3359-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18184793</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Greschner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>The structure of large-scale synchronized firing in primate retina</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>15</issue>):<fpage>5022</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5187-08.2009" xlink:type="simple">10.1523/JNEUROSCI.5187-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19369571</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yatsenko</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Josic</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Froudarakis</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Improved estimation and interpretation of correlations in neural circuits</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>3</issue>):<fpage>e1004083</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004083" xlink:type="simple">10.1371/journal.pcbi.1004083</ext-link></comment> <object-id pub-id-type="pmid">25826696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Berger</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>A synaptic organizing principle for cortical neuronal groups</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>; p. <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ko</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hofer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pichler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Buchanan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>T</given-names></name>. <article-title>Functional specificity of local synaptic connections in neocortical networks</article-title>. <source>Nature</source>. <year>2011</year>;<volume>473</volume>(<issue>7345</issue>):<fpage>87</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09880" xlink:type="simple">10.1038/nature09880</ext-link></comment> <object-id pub-id-type="pmid">21478872</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>D</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS biology</source>. <year>2005</year>;<volume>3</volume>(<issue>3</issue>):<fpage>e68</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment> <object-id pub-id-type="pmid">15737062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marin</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Pudlo</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Robert</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Ryder</surname> <given-names>RJ</given-names></name>. <article-title>Approximate Bayesian computational methods</article-title>. <source>Statistics and Computing</source>. <year>2012</year>;<volume>22</volume>(<issue>6</issue>):<fpage>1167</fpage>–<lpage>1180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11222-011-9288-2" xlink:type="simple">10.1007/s11222-011-9288-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>. <article-title>Approximate Bayesian Computation in Evolution and Ecology</article-title>. <source>Annu Rev Ecol Evol Syst</source>. <year>2010</year>;<volume>41</volume>:<fpage>379</fpage>–<lpage>406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-ecolsys-102209-144621" xlink:type="simple">10.1146/annurev-ecolsys-102209-144621</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma DP, Welling M. Auto-Encoding Variational Bayes; 2013. Available from: arXiv:1312.6114.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative Adversarial Networks; 2014. Available from: arXiv:1406.2661.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Tran D, Ranganath R, Blei DM. Hierarchical Implicit Models and Likelihood-Free Variational Inference; 2017. Available from: arXiv:1702.08896.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Arjovsky M, Chintala S, Bottou L. Wasserstein GAN; 2017. Available from: arXiv:1701.07875.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville AC. Improved Training of Wasserstein GANs; 2017. Available from: arXiv:1704.00028.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Crisanti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sommers</surname> <given-names>HJ</given-names></name>. <article-title>Chaos in Random Neural Networks</article-title>. <source>Physical Review Letters</source>. <year>1988</year>;<volume>61</volume>(<issue>3</issue>):<fpage>259</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.61.259" xlink:type="simple">10.1103/PhysRevLett.61.259</ext-link></comment> <object-id pub-id-type="pmid">10039285</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Stimulus-dependent suppression of chaos in recurrent neural networks</article-title>. <source>Physical Review E</source>. <year>2010</year>;<volume>82</volume>(<issue>1</issue>):<fpage>011903</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.82.011903" xlink:type="simple">10.1103/PhysRevE.82.011903</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Goodfellow IJ. NIPS 2016 Tutorial: Generative Adversarial Networks; 2017. Available from: arXiv:1701.00160.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Radford A, Metz L, Chintala S. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks; 2015. Available from: arXiv:1511.06434.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref031">
<label>31</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Villani</surname> <given-names>C</given-names></name>. <source>Optimal Transport: Old and New</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Li Y, Swersky K, Zemel R. Generative moment matching networks. In: International Conference on Machine Learning; 2015. p. 1718–1727.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Mirza M, Osindero S. Conditional Generative Adversarial Nets; 2014. Available from: arXiv:1411.1784.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Georgopoulos</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Caminiti</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Massey</surname> <given-names>JT</given-names></name>. <article-title>On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex</article-title>. <source>J Neurosci</source>. <year>1982</year>;<volume>2</volume>(<issue>11</issue>):<fpage>1527</fpage>–<lpage>1537</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.02-11-01527.1982" xlink:type="simple">10.1523/JNEUROSCI.02-11-01527.1982</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>Analysis of the stabilized supralinear network</article-title>. <source>Neural Comput</source>. <year>2013</year>;<volume>25</volume>(<issue>8</issue>):<fpage>1994</fpage>–<lpage>2037</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00472" xlink:type="simple">10.1162/NECO_a_00472</ext-link></comment> <object-id pub-id-type="pmid">23663149</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>The Dynamical Regime of Sensory Cortex: Stable Dynamics around a Single Stimulus-Tuned Attractor Account for Patterns of Noise Variability</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>98</volume>(<issue>4</issue>):<fpage>846</fpage>–<lpage>860</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.04.017" xlink:type="simple">10.1016/j.neuron.2018.04.017</ext-link></comment> <object-id pub-id-type="pmid">29772203</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat</article-title>. <source>J Neurophysiol</source>. <year>1965</year>;<volume>28</volume>:<fpage>229</fpage>–<lpage>289</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1965.28.2.229" xlink:type="simple">10.1152/jn.1965.28.2.229</ext-link></comment> <object-id pub-id-type="pmid">14283058</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Adesnik</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bruns</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Taniguchi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>A neural circuit for spatial summation in visual cortex</article-title>. <source>Nature</source>. <year>2012</year>;<volume>490</volume>(<issue>7419</issue>):<fpage>226</fpage>–<lpage>231</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11526" xlink:type="simple">10.1038/nature11526</ext-link></comment> <object-id pub-id-type="pmid">23060193</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dale</surname> <given-names>H</given-names></name>. <article-title>Pharmacology and Nerve-endings (Walter Ernest Dixon Memorial Lecture): (Section of Therapeutics and Pharmacology)</article-title>. <source>Proc R Soc Med</source>. <year>1935</year>;<volume>28</volume>(<issue>3</issue>):<fpage>319</fpage>–<lpage>332</lpage>. <object-id pub-id-type="pmid">19990108</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Strata</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Harvey</surname> <given-names>R</given-names></name>. <article-title>Dale’s principle</article-title>. <source>Brain Res Bull</source>. <year>1999</year>;<volume>50</volume>(<issue>5-6</issue>):<fpage>349</fpage>–<lpage>350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0361-9230(99)00100-8" xlink:type="simple">10.1016/S0361-9230(99)00100-8</ext-link></comment> <object-id pub-id-type="pmid">10643431</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Nagarajan V, Kolter JZ. Gradient descent GAN optimization is locally stable; 2017. Available from: arXiv:1706.04156.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Heusel M, Ramsauer H, Unterthiner T, Nessler B, Klambauer G, Hochreiter S. GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium; 2017. Available from: arXiv:1706.08500.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Arora S, Zhang Y. Do GANs actually learn the distribution? An empirical study; 2017. Available from: arXiv:1706.08224.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Arora S, Ge R, Liang Y, Ma T, Zhang Y. Generalization and Equilibrium in Generative Adversarial Nets (GANs). In: Precup D, Teh YW, editors. Proceedings of the 34th International Conference on Machine Learning. vol. 70 of Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR; 2017. p. 224–232. Available from: <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v70/arora17a.html" xlink:type="simple">http://proceedings.mlr.press/v70/arora17a.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Belghazi I, Rajeswar S, Baratin A, Hjelm RD, Courville AC. MINE: Mutual Information Neural Estimation; 2018. Available from: arXiv:1801.04062.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref046">
<label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Lin Z, Khetan A, Fanti GC, Oh S. PacGAN: The power of two samples in generative adversarial networks; 2017. Available from: arXiv:1712.04086.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Doersch C. Tutorial on Variational Autoencoders; 2016. Available from: arXiv:1606.05908.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Mescheder LM, Nowozin S, Geiger A. Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks; 2017. Available from: arXiv:1701.04722.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref049">
<label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Hu Z, Yang Z, Salakhutdinov R, Xing EP. On Unifying Deep Generative Models; 2017. Available from: arXiv:1706.00550.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Rosca M, Lakshminarayanan B, Warde-Farley D, Mohamed S. Variational Approaches for Auto-Encoding Generative Adversarial Networks; 2017. Available from: arXiv:1706.04987.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chopra</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hadsell</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ranzato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>F</given-names></name>. <article-title>A tutorial on energy-based learning</article-title>. <source>Predicting structured data</source>. <year>2006</year>;<volume>1</volume>(<issue>0</issue>).</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref052">
<label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Ranzato M, Boureau YL, Chopra S, LeCun Y. A unified energy-based framework for unsupervised learning. In: Artificial Intelligence and Statistics; 2007. p. 371–379.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref053">
<label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Nowozin S, Cseke B, Tomioka R. f-gan: Training generative neural samplers using variational divergence minimization. In: Advances in Neural Information Processing Systems; 2016. p. 271–279.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref054">
<label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Uehara M, Sato I, Suzuki M, Nakayama K, Matsuo Y. Generative adversarial nets from a density ratio estimation perspective; 2016. Available from: arXiv:1610.02920.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref055">
<label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Mohamed S, Lakshminarayanan B. Learning in implicit generative models; 2016. Available from: arXiv:1610.03483.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref056">
<label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Poole B, Alemi AA, Sohl-Dickstein J, Angelova A. Improved generator objectives for GANs; 2016. Available from: arXiv:1612.02780.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref057">
<label>57</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref058">
<label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhao J, Mathieu M, LeCun Y. Energy-Based Generative Adversarial Network; 2016. Available from: arXiv:1609.03126.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mirlin</surname> <given-names>AD</given-names></name>. <article-title>Statistics of energy levels and eigenfunctions in disordered systems</article-title>. <source>Physics Reports</source>. <year>2000</year>;<volume>326</volume>(<issue>5–6</issue>):<fpage>259</fpage>–<lpage>382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0370-1573(99)00091-5" xlink:type="simple">10.1016/S0370-1573(99)00091-5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006816.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Teh YW, Titterington M, editors. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. vol. 9 of Proceedings of Machine Learning Research. Chia Laguna Resort, Sardinia, Italy: PMLR; 2010. p. 249–256. Available from: <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v9/glorot10a.html" xlink:type="simple">http://proceedings.mlr.press/v9/glorot10a.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref061">
<label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Lei Ba J, Kiros JR, Hinton GE. Layer Normalization; 2016. Available from: arXiv:1607.06450.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref062">
<label>62</label>
<mixed-citation publication-type="other" xlink:type="simple">Loshchilov I, Hutter F. Fixing Weight Decay Regularization in Adam; 2017. Available from: arXiv:1711.05101.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref063">
<label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">Lindsay BG. Method of Moments. John Wiley and Sons, Ltd; 2014. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/9781118445112.stat05908" xlink:type="simple">http://dx.doi.org/10.1002/9781118445112.stat05908</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref064">
<label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions; 2016. Available from: arXiv:1605.02688.</mixed-citation>
</ref>
<ref id="pcbi.1006816.ref065">
<label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Dieleman S, Schlüter J, Raffel C, Olson E, Sønderby SK, Nouri D, et al. Lasagne: First release.; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.27878" xlink:type="simple">http://dx.doi.org/10.5281/zenodo.27878</ext-link>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>