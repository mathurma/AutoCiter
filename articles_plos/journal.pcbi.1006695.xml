<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006695</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-02028</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject><subj-group><subject>Eye movements</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Evolutionary processes</subject><subj-group><subject>Evolutionary adaptation</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical models</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular system</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Mathematical functions</subject><subj-group><subject>Sine waves</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A generative learning model for saccade adaptation</article-title>
<alt-title alt-title-type="running-head">Generative learning model for saccade adaptation</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1591-8802</contrib-id>
<name name-style="western">
<surname>Cassanello</surname>
<given-names>Carlos R.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ostendorf</surname>
<given-names>Florian</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8214-8556</contrib-id>
<name name-style="western">
<surname>Rolfs</surname>
<given-names>Martin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Bernstein Center for Computational Neuroscience, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Neurology, Charité – University Medicine Berlin, Berlin, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname>
<given-names>Wolfgang</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Technische Universitat Chemnitz, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: Lise-Meitner-Group for Environmental Neuroscience, Max Planck Institute for Human Development, Berlin, Germany</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">cassanello@mpib-berlin.mpg.de</email> (CRC); <email xlink:type="simple">martin.rolfs@hu-berlin.de</email> (MR)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>9</day>
<month>8</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>8</issue>
<elocation-id>e1006695</elocation-id>
<history>
<date date-type="received">
<day>1</day>
<month>12</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>6</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Cassanello et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006695"/>
<abstract>
<p>Plasticity in the oculomotor system ensures that saccadic eye movements reliably meet their visual goals—to bring regions of interest into foveal, high-acuity vision. Here, we present a comprehensive description of sensorimotor learning in saccades. We induced continuous adaptation of saccade amplitudes using a double-step paradigm, in which participants saccade to a peripheral target stimulus, which then undergoes a surreptitious, intra-saccadic shift (ISS) as the eyes are in flight. In our experiments, the ISS followed a systematic variation, increasing or decreasing from one saccade to the next as a sinusoidal function of the trial number. Over a large range of frequencies, we confirm that adaptation gain shows (1) a periodic response, reflecting the frequency of the ISS with a delay of a number of trials, and (2) a simultaneous drift towards lower saccade gains. We then show that state-space-based linear time-invariant systems (LTIS) represent suitable generative models for this evolution of saccade gain over time. This <italic>state-equation</italic> algorithm computes the prediction of an internal (or hidden state-) variable by learning from recent feedback errors, and it can be compared to experimentally observed adaptation gain. The algorithm also includes a forgetting rate that quantifies per-trial leaks in the adaptation gain, as well as a systematic, non-error-based bias. Finally, we study how the parameters of the generative models depend on features of the ISS. Driven by a sinusoidal disturbance, the state-equation admits an exact analytical solution that expresses the parameters of the phenomenological description as functions of those of the generative model. Together with statistical model selection criteria, we use these correspondences to characterize and refine the structure of compatible state-equation models. We discuss the relation of these findings to established results and suggest that they may guide further design of experimental research across domains of sensorimotor adaptation.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Constant adjustments of saccade metrics maintain oculomotor accuracy under changing environments. This error-driven learning can be induced experimentally by manipulating the targeting error of eye movements. Here, we investigate oculomotor learning in healthy participants in response to a sinusoidally evolving error. We then fit a class of generative models to the observed dynamics of oculomotor adaptation under this new learning regime. Formal model comparison suggests a richer model parameterization for such a sinusoidal error variation than proposed so far in the context of classical, step-like disturbances. We identify and fit the parameters of a generative model as underlying those of a phenomenological description of adaptation dynamics and provide an explicit link of this generative model to more established state equations for motor learning. The joint use of the sinusoidal adaption regime and consecutive model fit may provide a powerful approach to assess interindividual differences in adaptation across healthy individuals and to evaluate changes in learning dynamics in altered brain states, such as sustained by injuries, diseases, or aging.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>RO 3579/2-1, RO 3579/3-1, and RO 3579/8-1</award-id>
<principal-award-recipient>Martin Rolfs, Ph.D.</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Deutsche Forschungsgemeinschaft (DFG) grants to MR (RO 3579/2-1, RO 3579/3-1, and RO 3579/8-1). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="35"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-08-21</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The recorded datasets are publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/r7xum" xlink:type="simple">https://osf.io/r7xum</ext-link>, and can be cited under DOI <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/R7XUM" xlink:type="simple">10.17605/OSF.IO/R7XUM</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The accuracy of saccadic eye movement is maintained through mechanisms of saccade adaptation, which adjust the amplitude [<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref003">3</xref>] or direction [<xref ref-type="bibr" rid="pcbi.1006695.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref006">6</xref>] of subsequent movements in response to targeting errors. As online visual feedback cannot be used to correct the ongoing movement, saccadic eye movements need to be preprogrammed and adaptation must largely rely on past experience and active predictions [<xref ref-type="bibr" rid="pcbi.1006695.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>] rather than closed-loop sensory information.</p>
<p>To induce saccade adaptation in the laboratory [<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>], participants are instructed to follow a step of a target stimulus with their eyes and this visual cue is then displaced further during the saccade eye movement. Typically, this second, intra-saccadic step (ISS) is constant across trials and directed along the initial target vector towards smaller or larger saccade amplitudes. Although the ISS is visually imperceptible [<xref ref-type="bibr" rid="pcbi.1006695.ref009">9</xref>], saccades adjust their amplitude to compensate for the induced error. In phenomenological analyses of such saccade adaptation data, the amount of adaptation is usually quantified by comparing saccade gain values before and after the adapting block and interpolating an exponential fit in between [<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref010">10</xref>].</p>
<p>We recently presented a version of this paradigm in which the ISS (the disturbance responsible for inducing adaptation) follows a sinusoidal variation as a function of trial number ([<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref012">12</xref>]; see also [<xref ref-type="bibr" rid="pcbi.1006695.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref014">14</xref>]). We reported that gain changes were well described by a parametric functional form consisting of two additive components. One component was a <italic>periodic response</italic> reflecting the frequency of the ISS that was adequately fitted with a lagged but otherwise undistorted sinusoid. The second component constituted a <italic>drift of the baseline</italic> toward lower saccade gain (larger hypometria) that was appropriately accounted for using an exponential dependence.</p>
<p>Here, we investigate whether a generative algorithm that models saccade gain modifications on a trial-by-trial basis by learning from errors made on previous trials can account for this response. To this end, we implemented and fit a series of state-space models in which a modified delta-rule algorithm updates a hidden or latent variable (for which the experimentally observed adaptation gain is a proxy) by weighting the last experienced visual error, in addition to other error-based and non-error based learning components [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref023">23</xref>].</p>
<p>We adopt the approach that these algorithms are linear time-invariant systems (LTIS), in that their coefficients are time and trial-independent. LTIS models, also known as linear dynamical systems (LDS) have been successfully used in a number of motor adaptation studies [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref027">27</xref>]. Applied to saccade adaptation, they may predict the dynamics of the saccade amplitude itself as well as various forms of movement gain typically used in describing adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>]. Our first goal was to establish empirically whether LTIS models could fit the data recorded with a sinusoidal adaptation paradigm, as efficiently as when using a constant (fixed) ISS. Once we have established this point, we will explore the relation between the predicted phenomenological parameters [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref012">12</xref>] and the learning parameters of the underlying generative model, as well as their potential dependence on the perturbation dynamics.</p>
<p>We first analyze the ability of a family of generative models to describe experimental recordings of saccade adaptation by fitting the relevant learning parameters. We then perform statistical model-selection analysis to determine those that best fitted the same data in the various experimental conditions. We fitted models to two data sets, a previously published one [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>] and a variation of that paradigm that extended the range of frequencies of the sinusoidal variation of the ISS. Both data sets contrasted two established saccadic adaptation protocols [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref012">12</xref>]: Two-way adaptation (i.e., bidirectional adaptation along the saccade vector of saccades executed along the horizontal meridian) and Global adaptation (i.e., adaptation along the saccade vector of saccades executed in random directions). We then explore consequences for current models of motor learning and suggest possible modifications that may be required to generate a suitable description of sensorimotor learning during sinusoidal saccadic adaptation. In conducting this selection, we confirm that a single learning parameter model (a state-equation with just an error-based learning term; cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>]) does not suffice to fit the data. We then demonstrate that including an extra term that weights the next-to-last trial’s error provides a better fit for the Two-way type of adaptation. This learning rate has the intriguing feature that it has negative values for all frequencies, suggesting an active unlearning of the next-to-last trial’s feedback error, close, but not equal in magnitude to the learning rate of the last trial’s error. We discuss possible functional roles of these processes for oculomotor adaptation in natural situations, where saccadic accuracy is expected to exhibit slow dynamic changes across time.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Ethics statement</title>
<p>The Ethics Committee of the German Society for Psychology (DGPs) approved our protocols. We obtained written informed consent from all participants prior to the inclusion in the study. The present study conformed to the Declaration of Helsinki (2008).</p>
</sec>
<sec id="sec004">
<title>Procedure</title>
<p>We re-analyzed the data we recently collected using a fast-paced saccade adaptation paradigm with a sinusoidal disturbance. We had previously described these data by fitting a phenomenological model that we identified using statistical model selection. For details on the experimental procedures pertaining to this original data set (henceforth, ORIG) and to the selection of the functional form of this phenomenological model, please refer to our former communication [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>].</p>
<p>We applied the same experimental procedure in collecting further data with an enhanced range of frequencies. In this case, thirteen participants ran two sessions with similar Two-way and Global adaptation protocols as used in previous reports [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref012">12</xref>]. In short, Two-way adaptation refers to bidirectional adaptation along the saccade vector of saccades executed along the horizontal meridian. In turn, Global adaptation refers to adaptation along the saccade vector of saccades executed in random directions.</p>
<p>In collecting this dataset (henceforth, FREQ), each session had 2370 trials divided in 11 blocks. Odd numbered blocks had 75 no-adaptation trials (zero ISS). The five even-numbered blocks consisted of 384 trials each with a sinusoidal disturbance similar to that used before but with frequencies of 1, 3, 6, 12 and 24 cycles per block (i.e., 384, 128, 64, 32, and 16 saccades per cycle, respectively). The order of adaptation blocks was randomly interleaved for each observer and type of adaptation. The program was paused after each adaptation block, giving participants some resting time, and we calibrated eye position routinely at the beginning of each non-adapting (odd-numbered) block. In each trial, the pre-saccadic target step was fixed at 8 degrees of visual angle (dva). The subsequent second step (ISS) then ranged between –25% and +25% of the first step, changing size according to a sine function of trial number.</p>
</sec>
<sec id="sec005">
<title>Data analysis and phenomenological model</title>
<sec id="sec006">
<title>Modeling of the saccadic response</title>
<p>In a double-step adaptation paradigm [<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>], after a fixation interval the fixation target <italic>FP</italic>(<italic>n</italic>) undergoes a first step to become the target of a saccade, displayed at the pre-saccadic location <italic>TP</italic>1(<italic>n</italic>). Because the eyes might have been stationed at a location <italic>EP</italic>1(<italic>n</italic>) close to but different than <italic>FP</italic>(<italic>n</italic>), we define the pre-saccadic target amplitude <italic>preTP</italic>(<italic>n</italic>) = <italic>TP</italic>1(<italic>n</italic>) − <italic>EP</italic>1(<italic>n</italic>), with origin at <italic>EP</italic>1(<italic>n</italic>) rather than <italic>FP</italic>(<italic>n</italic>) and keep this convention throughout the study.</p>
<p>The second step of the McLaughlin paradigm (i.e., the target displacement inducing a feedback error) then shifts the target during the saccade to a position <italic>TP</italic>2(<italic>n</italic>) (so that <italic>ISS</italic>(<italic>n</italic>) = <italic>TP</italic>2(<italic>n</italic>) − <italic>TP</italic>1(<italic>n</italic>)). Therefore, the post-saccadic target amplitude (at or immediately after saccade landing) is given by the identity: <italic>postTP</italic>(<italic>n</italic>) = <italic>preTP</italic>(<italic>n</italic>) + <italic>ISS</italic>(<italic>n</italic>). For convenience, we will define a <italic>target gain</italic>, <italic>t</italic>(<italic>n</italic>), as the ratio of the post-saccadic target amplitude to the pre-saccadic one, as well as a <italic>disturbance gain</italic>, <italic>d</italic>(<italic>n</italic>), as the ratio of the second to the first target steps, i.e., the ratio of the <italic>ISS</italic> to the saccade proxy:
<disp-formula id="pcbi.1006695.e001">
<alternatives>
<graphic id="pcbi.1006695.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>In the general case, there would be a constant and a variable component in the second target step, <italic>ISS</italic>(<italic>n</italic>) = <italic>C</italic> + <italic>V</italic>(<italic>n</italic>). In our sinusoidal adaptation paradigms, <italic>C</italic> = 0 and <inline-formula id="pcbi.1006695.e002"><alternatives><graphic id="pcbi.1006695.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a sine function of the trial number so that:
<disp-formula id="pcbi.1006695.e003">
<alternatives>
<graphic id="pcbi.1006695.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="2pt"/><mml:mo>+</mml:mo><mml:mspace width="2pt"/><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mspace width="2pt"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>c</italic> and <italic>v</italic>(<italic>n</italic>) are the ratios of the constant and variable part of the <italic>ISS</italic> to the pre-saccadic target amplitude. In the sinusoidal paradigms, <italic>f</italic> is the frequency of the sinusoid in cycles per block, <italic>N</italic> is the number of trials in an adaptation block, and <italic>n</italic> is the index of the current trial. At fixed amplitude, the dynamics of the disturbance is fully determined by its angular frequency <inline-formula id="pcbi.1006695.e004"><alternatives><graphic id="pcbi.1006695.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, that characterizes the rate of change of the sinusoid in each trial. <italic>P</italic> is the maximum absolute magnitude of the variable part <italic>V</italic>(<italic>n</italic>), i.e., the ‘amplitude’ of the sinusoid that defines the <italic>ISS</italic>. It was fixed at 2 dva throughout all sinusoidal adaptation datasets. Therefore, <italic>ISS</italic>(<italic>n</italic>) changed in magnitude periodically and in a sinusoidal fashion between approximately –25% and +25% of the magnitude of the pre-saccadic target eccentricity (<italic>preTP</italic>(<italic>n</italic>)), which was held approximately fixed at 8 dva in all datasets. Finally, <italic>p</italic>(<italic>n</italic>) is the ratio of <italic>P</italic> and <italic>preTP</italic>(<italic>n</italic>), and had an approximately constant value of 0.25 across the sinusoidal datasets (the slight dependence on the trial number was a consequence of the slight dependence of the normalizing factor <italic>preTP</italic>(<italic>n</italic>) on the trial number; in actuality, the magnitude held constant at 8 dva across the experiment was <italic>TP</italic>1(<italic>n</italic>) − <italic>FP</italic>(<italic>n</italic>), which differed slightly but not systematically from <italic>TP</italic>1(<italic>n</italic>) − <italic>EP</italic>1(<italic>n</italic>)). Given that we used integer number of cycles across all sinusoidal adaptation experiments, we expressed the frequency in cycles per block (cpb). We set the initial phase to zero, which means that the magnitude of the ISS starts at zero in the direction of positive <italic>ISS</italic> (outward second-steps of the saccade target) first. <xref ref-type="disp-formula" rid="pcbi.1006695.e003">Eq 1</xref> provides a complete description of the stimulus that we used. Yet, for the analyses pursued here and to make closer contact with our phenomenological characterization of oculomotor responses in sinusoidal adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>], we will further define a <italic>stimulus gain</italic>, <italic>s</italic>(<italic>n</italic>), to be the disturbance gain normalized to (i.e., divided by) its maximum absolute value. Therefore, <italic>s</italic>(<italic>n</italic>) would range within ±1 in units of its maximum amplitude following a sinusoidal variation with trial number:
<disp-formula id="pcbi.1006695.e005">
<alternatives>
<graphic id="pcbi.1006695.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>Saccade amplitude adaptation is usually described in terms of the changes in <italic>saccade gain</italic> (<italic>SG</italic>(<italic>n</italic>)), defined as the ratio of the saccade amplitude (<italic>SA</italic>(<italic>n</italic>)) to the pre-saccadic position error (<italic>preTP</italic>(<italic>n</italic>)). During non-adapting trials and at the beginning of the adaptation blocks, <italic>SG</italic>(<italic>n</italic>) is typically slightly smaller than 1, which means that the saccade undershoots the target. Since we are interested in keeping track of the excursions of the saccade gain with respect to a perfect completion of the saccade that matches <italic>preTP</italic>(<italic>n</italic>) exactly, we shall define an <italic>adaptation gain</italic> subtracting one from the usual saccade gain and normalized to the maximum absolute value of the <italic>ISS</italic>,
<disp-formula id="pcbi.1006695.e006">
<alternatives>
<graphic id="pcbi.1006695.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>The adaptation gain represents the residual of the saccade gain with respect to perfect landing. When a saccade lands exactly on the first target step (a perfectly accurate saccade), the saccade gain will be one while the adaptation gain will be zero. Therefore, the adaptation gain uses perfect landing as the origin of coordinates and quantifies departures from this ideal goal state. Clearly, in both descriptions the reference represents a state of no adaptation. The adaptation gain description may be viewed as following the evolution of the error rather than that of the full eye movement. As long as the true underlying learning model is strictly linear, both descriptions must be equivalent since they relate to each other by a shift. We used the adaptation gain, <italic>g</italic>(<italic>n</italic>), in our previous reports [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref012">12</xref>] to provide phenomenological parametric description of sinusoidal adaptation data and it is also commonly used within motor control research. Throughout the manuscript we shall use <italic>SG</italic>(<italic>n</italic>) or <italic>g</italic>(<italic>n</italic>) as the relevant behavioral variables describing the data, which are computed directly from the experimental measurements of the eye and target positions in each trial.</p>
</sec>
<sec id="sec007">
<title>Assessment of the evidence in favor of a model</title>
<p>In implementing the phenomenological parameter estimation, we adopted a Gaussian likelihood for the data given the model. This likelihood can be maximized with respect to the parameters at a fixed but unknown width. Instead we adopted the following procedure [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref013">13</xref>]. Using Bayes theorem, priors for the parameters to be estimated, and assuming a constant prior probability for the data, we can obtain a joint probability amplitude for all parameters that can be marginalized to extract individual probability amplitudes for each parameter. In this process, the width of the Gaussian likelihood is a nuisance parameter that we integrate out using a non-informative prior [<xref ref-type="bibr" rid="pcbi.1006695.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref028">28</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref029">29</xref>]. Once such integration is conducted, the volume of the resulting probability density (given the data) provides an estimate of the odds that the model would provide a reasonable description of the data. Here we provide a full model consisting of six parameters (sinusoidal entraining of the oculomotor response riding over a baseline drift) that we want to compare to a partial model (the drift of the baseline alone) and to a minimal model consisting of the mean of the adapting block with variance equal to the variance of the recorded data over that block. To establish which situation is more likely across different number of parameters, we take the log of the ratio of the odds across the models. The resulting magnitude is the <italic>evidence</italic> that the data are in favor of a particular model and is measured in <italic>decibels</italic> (db). When this magnitude is positive, the odds favor the model in the numerator, with evidence higher than 3 db indicating that this model is significantly favored to the one in the denominator. We use this metric to assess the quality of our parameter estimation.</p>
</sec>
<sec id="sec008">
<title>Statistics</title>
<p>Throughout the manuscript we report results as mean ± SD for individual data and mean ± SEM when we discuss group data. In the phenomenological fittings, to determine average parameters from the parameter estimation other than the frequency, we computed the mean and variance for each parameter and participant as the first two moments of the corresponding posterior probability distribution and took the average of the means weighted by their standard deviations (square root of the estimated variance) to generate each point on the population plot. Alternative estimators (e.g., the modes of the posterior distributions, with and without weighting) gave qualitatively similar results.</p>
</sec>
</sec>
<sec id="sec009">
<title>Modeling of the sensorimotor learning process: The modified delta-rule state equation</title>
<p>To investigate generative models, we adopt the following rationale. In each trial, the oculomotor system must generate a motor command to produce the impending saccade. This needs to be calibrated against the actual physical size of the required movement [<xref ref-type="bibr" rid="pcbi.1006695.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref031">31</xref>].</p>
<p>If the saccade fails to land on target, the motor command needs to be recalibrated based on preexisting calibrations, and we will hypothesize that those changes take place in an obligatory manner (cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>]) through additive, error-based modifications attempting to ameliorate post-saccadic mismatches between the eyes’ landing position and target location.</p>
<p>We model the underlying sensorimotor learning using linear time-invariant systems (LTIS). The model parameters (or the learning coefficients) are time independent in each experimental block, although they can vary across experimental conditions or phases [<xref ref-type="bibr" rid="pcbi.1006695.ref032">32</xref>]. These models are closely related to linear dynamical systems (LDS; cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref022">22</xref>]), except that here we only address noise-free models.</p>
<p>Because saccades are extremely rapid movements that do not admit reprogramming in mid-flight, it is assumed that all gain changes take place in between saccades. In our models, therefore, the error-based correction terms weight errors that were experienced in previous saccades. As a consequence, in the estimation of the forthcoming event, the post-saccadic stimulus gain is not compared against the adaptation gain measured for that trial but against the previous estimate of the gain. To justify these assumptions, it is usually assumed that the motor system sends an <italic>efference copy</italic> of the motor command to the sensory areas, which enables prediction of the sensory consequences of the movement and therefore avails comparison to experienced post-saccadic feedback [<xref ref-type="bibr" rid="pcbi.1006695.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref035">35</xref>].</p>
<p>We will assume that the values of saccade and adaptation gains observed and extracted from the recorded data (i.e., <inline-formula id="pcbi.1006695.e007"><alternatives><graphic id="pcbi.1006695.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006695.e008"><alternatives><graphic id="pcbi.1006695.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>) are adequate proxies of that motor calibration process. Yet the calibration itself is an internal feature of the brain and therefore the <italic>adaptation gain</italic> that enters the generative algorithm (the state-equation) that we intend to study is a <italic>hidden variable</italic> representing the <italic>internal state</italic> of the system. A model providing its temporal evolution can then be fitted to the data; yet the variable itself is not experimentally accessible. We denote the internal variable associated to the saccade gain by <italic>z</italic>(<italic>n</italic>). To describe the evolution of this <italic>state</italic> variable we introduce the state-equation:
<disp-formula id="pcbi.1006695.e009">
<alternatives>
<graphic id="pcbi.1006695.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∙</mml:mo><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mspace width="2pt"/><mml:mi>D</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
supplemented with an initial condition that sets the initial value <italic>z</italic>(1) = <italic>G</italic> · <italic>t</italic>(1). Here, the target gain <italic>t</italic>(<italic>n</italic>) is available from recordings in each trial and we shall assess how well the <italic>prediction</italic> of the saccade gain (<italic>z</italic>(<italic>n</italic> + 1), provided by <xref ref-type="disp-formula" rid="pcbi.1006695.e009">Eq 4</xref>) fits the recorded data <italic>SG</italic>(<italic>n</italic>). The first term on the RHS of the equation is a persistence term. The persistence rate <italic>A</italic> determines how much of the estimate of the state variable at trial <italic>n</italic> is transferred to the estimate at the next trial [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref036">36</xref>]. Therefore, its magnitude is expected to be typically slightly smaller than 1 and it is set to be 1 in the models that do not include its effect. The second term weights the discrepancy between the gain of the target at trial <italic>n</italic> and the predicted gain of the movement under the underlying assumption that the size of the state variable is an adequate proxy for the (sensory) consequences of the movement. The weighting coefficient <italic>K</italic> is called <italic>learning rate</italic>. <italic>M</italic> embodies any systematic effect (drift or bias) that takes place in each trial but it is not directly determined by the sensory feedback [<xref ref-type="bibr" rid="pcbi.1006695.ref037">37</xref>]; we shall call it a drift parameter. The last term is a second error-based correction term that weights the discrepancy between the gain of the target and the estimate of the movement at a trial other than the last error with an additional (distal) learning rate <italic>D</italic>. For concreteness we shall assume that this correction is based on the sensory feedback arising from the next-to-last trial. However, we shall return to this specific assumption further in the <bold>Discussion</bold>. Note that with the inclusion of this hypothetical double error sampling the full model of <xref ref-type="disp-formula" rid="pcbi.1006695.e009">Eq 4</xref> (and <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> below) becomes an algorithm that coherently uses two delayed feedbacks to estimate the state of a single internal variable that models the sensory consequences of the intended motion.</p>
<sec id="sec010">
<title>Formatting of the data for fittings of the learning model</title>
<p>To be able to consistently compare results from this manuscript with the phenomenological analyses of the data presented in our earlier report, we will write the generative model in terms of a state variable associated to the <italic>adaptation gain</italic> of <xref ref-type="disp-formula" rid="pcbi.1006695.e006">Eq 3</xref> (cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>], and therefore naturally defined as <inline-formula id="pcbi.1006695.e010"><alternatives><graphic id="pcbi.1006695.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Applying these changes, we obtain:
<disp-formula id="pcbi.1006695.e011">
<alternatives>
<graphic id="pcbi.1006695.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∙</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>∙</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>As suggested by Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e009">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1006695.e011">5</xref>, a sensorimotor learning model can be written in terms of hidden variables that would be naturally associated with the saccade gain or the adaptation gain defined in Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e005">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006695.e006">3</xref> respectively. When transitioning from the saccade gain to the adaptation gain description in this linear model, the only parameter of <xref ref-type="disp-formula" rid="pcbi.1006695.e009">Eq 4</xref> susceptible to changes is <italic>M</italic>, which we indicated in <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> using the lower-case <italic>m</italic> instead. Throughout the manuscript we adopt the adaptation gain (defined above) as the state variable to characterize the internal model and <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> as its relevant state-equation. In this description, the stimulus gain reduces to a pure sinusoidal disturbance with zero mean (i.e., with no static component), which minimizes confounds between the effects of the retention rate <italic>A</italic> and the drift parameter <italic>m</italic>.</p>
<p>Because movement gains are computed from experimental observations, models of motor control often include a second equation that maps the estimates of the hypothesized internal variable to real-world observations (see, e.g., [<xref ref-type="bibr" rid="pcbi.1006695.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref021">21</xref>]). In our simplified analyses and again invoking the pre-programmed nature and accuracy of saccades, we set this second (observation) stage to be an identity.</p>
</sec>
</sec>
<sec id="sec011">
<title>Estimation of the learning parameters, model classification and model selection</title>
<p>We conducted our analyses using the full form of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. We were interested in determining which model suffices to account for the data with the least number of parameters. The magnitude being learned is <italic>x</italic>, the internal representation of the adaptation gain of the imminent saccade. This gain has value zero upon the ideal outcome of perfect movement accuracy and in that respect, it can be interpreted as the gain of an internal prediction error. Using <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>, we generated the predicted values of <italic>x</italic>(<italic>n</italic>) in each condition and for each participant, and then fitted a number of models that differed from each other in which parameters were estimated. When a parameter among <italic>K</italic>, <italic>m</italic>, or <italic>D</italic> was not present, the corresponding term was removed from <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. Note however, that when the parameter <italic>A</italic> was not included as a fitting parameter, its value was set to unity (i.e., <italic>A</italic> = 1). In the case of the initial value <italic>G</italic>, we obtained an estimate by taking the average of the first five values of the gain. We proceeded in this way because the initial value of the state of the system is unknown and, while the first recorded value of the gain could be considered a proxy for such initial state, execution and motor errors could yield a value of the gain significantly different than the actual initial state of the system; we averaged over 5 trials to alleviate this problem. In models where the initial value of the gain was left free to become a fitting parameter, this average over the first five saccades was used as an initial value for the fitting routine for that particular parameter. Improvements can be achieved by letting the initial condition become an extra parameter. We discuss below the interpretation of using the initial condition as a fitting parameter of the model.</p>
<p>In view of these features of the generative model, a natural classification of the models tested arises as follows: given the parameters <italic>K</italic>, <italic>A</italic>, <italic>m</italic>, <italic>D</italic><sub>1</sub>,⋯, <italic>D</italic><sub><italic>w</italic></sub>, <italic>G</italic>, we will 1) include <italic>K</italic> in every model because we are modeling intrinsic learning where we assume that learning from the last experienced feedback is always present as well as obligatory [<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref031">31</xref>]; 2) models will be generated by adding successively the parameters <italic>A</italic>, <italic>m</italic>, and <italic>D</italic>, of which one or more could be present but in this study we restrict ourselves to learning possibly from only one extra feedback in the past; 3) <italic>G</italic> is an optional parameter that is included in an attempt to alleviate extreme effects of the initial condition(s) as explained above. By applying points 1) through 3), sixteen different models can be generated. For reasons to become clear below we would group them in four families according to whether or not they contain the bias term (<italic>m</italic>) and the additional error term (with learning rate <italic>D</italic>): <italic>K</italic> only (although with <italic>A</italic> = 1 when omitted), <italic>KA</italic>, <italic>KG</italic>, <italic>KAG</italic> feature zero bias and a single error term; <italic>Km</italic>, <italic>KAm</italic>, <italic>KmG</italic>, <italic>KAmG</italic> are models with a single error term that allow bias; <italic>KD</italic>, <italic>KAD</italic>, <italic>KDG</italic>, <italic>KADG</italic> have no bias term but sample two errors, and <italic>KmD</italic>, <italic>KAmD</italic>, <italic>KmDG</italic>, <italic>KAmDG</italic> feature both a bias term and learn based on double error sampling. Therefore, the simplest model had a single fitting parameter (the learning rate <italic>K</italic>, cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>]) and was obtained by setting <italic>A</italic> = 1, removing the terms that involved <italic>m</italic> and <italic>D</italic>, and setting the initial value <italic>G</italic> to be the mean of the first five values of the gain in the block. The full model had all five as fitting parameters.</p>
<p>All parameters of the generative models were estimated by fitting the model to the experimental data using MATLAB function nlinfit; 95% confidence intervals for the fitted parameters were computed using MATLAB function nlparci and predicted responses for the hidden variable <italic>x</italic> with its corresponding 95% confidence intervals were obtained from MATLAB function nlpredci.</p>
<p>All 16 models were fitted to data from each individual participant parameters were extracted for each model, and models were compared using the Akaike information criterion (AIC; [<xref ref-type="bibr" rid="pcbi.1006695.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref041">41</xref>]) by computing Akaike weights across models for each participant. Finally, these weights were averaged across participants for each model in each condition.</p>
</sec>
<sec id="sec012">
<title>Using the generative model to predict the parameters of the phenomenological description of the adaptation gain</title>
<p>The adaptation gain of the oculomotor response to a sinusoidal disturbance is best described by a phenomenological function consisting of a decaying exponential added to a lagged but otherwise undistorted sinusoid [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>]. The sinusoidal component of the response onsets at the beginning of the adaptation block but all fittings include the pre-adaptation block as well. The frequency of the stimulus disturbance is matched closely by the gain. To fully describe the response, five extra phenomenological parameters are required: amplitude (<italic>a</italic>) and lag (<italic>ϕ</italic>) of the periodic part of the error gain complete the description of the periodic part. The exponential decaying component that describes the baseline on which the periodic response rides requires other three: an asymptotic value (<italic>B</italic><sub>0</sub>) where the baseline stabilizes at large trial number, a timescale (<italic>λ</italic>) in which the baseline reaches 1/<italic>e</italic> of the full decay, and the amplitude of the decay (<italic>B</italic>):
<disp-formula id="pcbi.1006695.e012">
<alternatives>
<graphic id="pcbi.1006695.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>∙</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mtext>with</mml:mtext><mml:mspace width="2em"/><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>We use here the same denominations used in our previous report [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>], except for changing the name of the timescale to <italic>λ</italic> to prevent confusion with the amplitude of the periodic component <italic>a</italic>. To estimate parameters of the phenomenological functional form that best fits the data we used the same general procedure and parameter estimation algorithm implemented in our earlier contributions [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref013">13</xref>]. Solving the state-equation via iteration in the simpler case where the system learns only from the last experienced feedback (cf. <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>), or borrowing techniques from the theory of LTIS reveals a correspondence between these phenomenological parameters and the coefficients of the generative model of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. (A complete derivation of the phenomenological parameters as functions of the generative ones is not presented here due to space limitations; details about the analytical procedures adopted can be found in [<xref ref-type="bibr" rid="pcbi.1006695.ref042">42</xref>]). Depending on the parameters that each generative model includes, the functional form and value of the phenomenological coefficients may change. Here we are interested in assessing which theoretical prediction of the relation among phenomenological and generative model parameters matches the data best as a way to validate the underlying sensorimotor learning algorithm.</p>
</sec>
<sec id="sec013">
<title>Lag and amplitude of the periodic response</title>
<p>The lag of the periodic response of the error gain derived from the (full version of the) generative model of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> including the next-to-last feedback-error term is given by:
<disp-formula id="pcbi.1006695.e013">
<alternatives>
<graphic id="pcbi.1006695.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>In models without next-to-last feedback term <italic>D</italic> should be set to zero; in models that do not have <italic>A</italic> as a fitting parameter, its value should be set to 1 in <xref ref-type="disp-formula" rid="pcbi.1006695.e013">Eq 7</xref>.</p>
<p>The periodic component of the response to a sinusoidal disturbance in models where the next-to-last feedback is included can be written as:
<disp-formula id="pcbi.1006695.e014">
<alternatives>
<graphic id="pcbi.1006695.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="2pt"/></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where <inline-formula id="pcbi.1006695.e015"><alternatives><graphic id="pcbi.1006695.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:math></alternatives></inline-formula>.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1006695.e014">Eq 8</xref> shows that if <italic>D</italic> = 0 we recover the solution expected by iteration when there is learning from the last error only. Then the amplitude of the periodic component (<italic>a</italic>) in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> can be read out directly to be <inline-formula id="pcbi.1006695.e016"><alternatives><graphic id="pcbi.1006695.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. When <italic>D</italic> ≠ 0 we need to re-write <xref ref-type="disp-formula" rid="pcbi.1006695.e014">Eq 8</xref> so that it matches the periodic part of <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>. After some algebra <xref ref-type="disp-formula" rid="pcbi.1006695.e014">Eq 8</xref> can be recast as:
<disp-formula id="pcbi.1006695.e017">
<alternatives>
<graphic id="pcbi.1006695.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>∙</mml:mo><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ω</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>+</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where
<disp-formula id="pcbi.1006695.e018">
<alternatives>
<graphic id="pcbi.1006695.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>,</mml:mo><mml:mspace width="2pt"/><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="4pt"/><mml:mtext>cos</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="2pt"/><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mspace width="2pt"/><mml:mtext>sin</mml:mtext></mml:mrow><mml:mspace width="2pt"/><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="2pt"/><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula></p>
<p>Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref> to <xref ref-type="disp-formula" rid="pcbi.1006695.e018">10</xref> clarify the effect of the presence of the next-to-last error learning rate <italic>D</italic>. <xref ref-type="disp-formula" rid="pcbi.1006695.e013">Eq 7</xref> shows how the <italic>bare</italic> lag <italic>ϕ</italic> changes when <italic>D</italic> is present. Yet, it would be incorrect to compare the fitted values of the phenomenological lag to <xref ref-type="disp-formula" rid="pcbi.1006695.e013">Eq 7</xref>. The reason is that the second contribution in <xref ref-type="disp-formula" rid="pcbi.1006695.e014">Eq 8</xref> modifies not only the amplitude of the periodic component to the new value <inline-formula id="pcbi.1006695.e019"><alternatives><graphic id="pcbi.1006695.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, but it also adds the shift <italic>φ</italic> to the lag. Therefore, if there were also learning from the next-to-last error, the observed (behavioral) lag should be compared to <italic>ϕ</italic> + <italic>φ</italic>.</p>
</sec>
<sec id="sec014">
<title>Baseline drift parameters</title>
<p>Following a sinusoidal disturbance, the baseline of the error gain will approach an asymptote at large trial number that can be written as a function of parameters of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> as (see also <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>):
<disp-formula id="pcbi.1006695.e020">
<alternatives>
<graphic id="pcbi.1006695.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>The timescale <italic>λ</italic> for the decay of the baseline, has units of 1/trials and it is defined by:
<disp-formula id="pcbi.1006695.e021">
<alternatives>
<graphic id="pcbi.1006695.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>±</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>4</mml:mn><mml:mi>D</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p><xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref> provides the weights of the <italic>impulse response</italic> that generates the integral solution by convolving the stimulus (i.e., <italic>s</italic>(<italic>n</italic>); cf. <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>, [<xref ref-type="bibr" rid="pcbi.1006695.ref042">42</xref>]). The inverse of the timescale parameter <italic>λ</italic> gives the number of trials over which the stimulus is integrated. Beyond this <italic>window of integration</italic>, the weighting of the stimulus would have reduced enough to ignore further contributions. When <italic>D</italic> = 0, the integration weight becomes <italic>e</italic><sup>−<italic>λ</italic></sup> = (<italic>A</italic> − <italic>K</italic>), which is positive and smaller than 1, provided that the learning rate <italic>K</italic> &lt; 1 and <italic>A</italic> ~ 1. When <italic>D</italic> ≠ 0, <xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref> provides timescales for two modes that compose the integral solution of the state-equation. These result from the addition or subtraction of the second term in braces. If the parameter <italic>D</italic> is negative, the second term inside the braces becomes slightly larger than the first. The timescale resulting from the addition is positive and can be expressed as a decaying exponential. The subtraction solution is negative and of small magnitude and, therefore, it will decay much faster when raised to the trial number. It introduces small additive fluctuations to the exponential decay of the addition solution without changing its overall behavior. Critically, diverse sizes of the learning parameters may result in smaller or larger timescales in models with <italic>D</italic> ≠ 0 compared to models where <italic>D</italic> = 0 (cf. <xref ref-type="sec" rid="sec015">Results</xref> section and <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>).</p>
<p>To recap, <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> has four phenomenological parameters that we shall explore in further detail: <italic>B</italic><sub>0</sub>, <italic>λ</italic>, <italic>a</italic>, and <italic>ϕ</italic>. The former two parameters are already familiar from phenomenological descriptions of data in paradigms using fixed-sized second-step for the target. The latter are new, arising in paradigms with sinusoidal disturbances.</p>
<p>The amplitude of the decay of the baseline also bears dependence on the learning rates as well as on the initial condition. Because of the strong influence of the initial condition on this parameter, we refrain from a comparison of the behavioral fittings to the predictions from the generative model for this case.</p>
<p>Part of the material discussed in this contribution have been presented in the form of posters or slide presentations [<xref ref-type="bibr" rid="pcbi.1006695.ref043">43</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref044">44</xref>].</p>
</sec>
</sec>
<sec id="sec015" sec-type="results">
<title>Results</title>
<sec id="sec016">
<title>Analysis of the data at the phenomenological level</title>
<p>To obtain a general idea of patterns present in the data, we first collapsed the data for each stimulus frequency and adaptation type across participants (group data). We fit these data using a piecewise continuous function given by the addition of a monotonic (exponential) decay of the baseline–spanning both pre-adaptation and adapting trials- and a periodic entraining of the oculomotor response to the sinusoidal stimulus that begins at the onset of the adaptation block. This choice was supported by the fact that we had confirmed using statistical model selection criteria (i.e., AIC and BIC, [<xref ref-type="bibr" rid="pcbi.1006695.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref045">45</xref>]) that this functional dependence was the best descriptor of the oculomotor response among the set of models tested in Cassanello et al. [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>]. For illustration purposes only, <xref ref-type="fig" rid="pcbi.1006695.g001">Fig 1</xref> shows the group data in each dataset, along with the fits resulting from the parameter estimation based on the phenomenological model of <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>. The same parameterization was used to fit each participant’s run. Figs <xref ref-type="fig" rid="pcbi.1006695.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1006695.g003">3</xref> summarize the estimation of the phenomenological parameters entering <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>. <xref ref-type="fig" rid="pcbi.1006695.g002">Fig 2</xref> shows the values of mean ± SEM of the parameters estimated from every individual dataset for each frequency and adaptation type.</p>
<fig id="pcbi.1006695.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Fits of the phenomenological model to the experimental data.</title>
<p>The plots show adaptation gain (colored lines) averaged over individuals in the (<bold>a</bold>) Two-way adaptation and (<bold>b</bold>) Global adaptation condition of the ORIG data set (reported in [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>]), as well as the (<bold>c</bold>) Two-way adaptation and (<bold>d</bold>) Global adaptation condition of the FREQ data set, using the same paradigm over an extended range of frequencies. The fit (black line) is based on <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>. The same equation was fitted to data from each participant in each condition and experiment, to estimate phenomenological parameters on an individual basis. For illustration purposes only, the figure depicts fittings done over the averages along with 95% confidence intervals (gray shaded areas). The black dotted lines indicate the time evolution of the baseline if the amplitude of the periodic response were zero, corresponding to a drift only model. The solid black lines indicate the approximate middle-point locations of the periodic component.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g001" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006695.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Phenomenological parameters as a function of ISS frequency, estimated from both datasets (ORIG, diamonds, and FREQ, circles).</title>
<p>(<bold>a</bold>,<bold>b</bold>) Amplitude (<bold>a</bold>) and Lag (<bold>b</bold>) parameters of the periodic (sinusoidal) component of the response. (<bold>c</bold>,<bold>d</bold>) Asymptote (<bold>c</bold>) and timescale (<bold>d</bold>) parameters of the monotonic drift of the baseline toward greater hypometria. Each point is a condition defined by type of adaptation and ISS frequency. Blue and red colors correspond to horizontal Two-way and Global adaptation, respectively. Error bars are SEM across participants. These four parameters are further compared to the values predicted by the solution to the generative models tested.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006695.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Assessment of the quality of the fits of the parametric phenomenological model to the group data.</title>
<p>(<bold>a</bold>,<bold>c</bold>) Each bar is split into the log of the odds ratio of the full model to a drift only model that lacks the sinusoidal component (darker tone of the bars) added to the log of the odds ratio of the drift only model to the noise only model described above (lighter tone of the bars). For all but one condition (Global adaptation, 24 cpd), the full model provides the best account of the data. (<bold>b</bold>,<bold>d</bold>) Estimates of the frequency of the periodic component of the oculomotor response, for dataset ORIG (<bold>b</bold>) and dataset FREQ (<bold>d</bold>). Error bars are SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g003" xlink:type="simple"/>
</fig>
<p>Some features are readily apparent from these plots. First, the frequency of the ISS is reliably estimated (cf. <xref ref-type="fig" rid="pcbi.1006695.g003">Fig 3b and 3d</xref>). Second, the amplitude and the lag of the periodic components of the adaptation gain decay with increasing frequency of the stimulus (<xref ref-type="fig" rid="pcbi.1006695.g002">Fig 2a and 2b</xref>). The amplitudes of the periodic component are systematically larger in Two-way adaptation, while the lags observed in global adaptation are systematically larger than in the Two-way case. The systematic decay of the values of the lag with increasing frequency does not seem to extend to the smallest frequency (1 cpb in the new dataset). This may be related to the fact that at such low frequency the stimulus resembles more the behavior of a ramp that then turns rather than a truly periodic disturbance.</p>
<p>The parameters that affect the observed drift in the baseline (i.e., asymptote and timescale, <xref ref-type="fig" rid="pcbi.1006695.g002">Fig 2c and 2d</xref>) remain rather independent of the experimental condition. This feature is more apparent in the ORIG dataset, but it still seems to hold in the FREQ dataset. An exception arises at the lower frequency (1 cpb) tested in the FREQ dataset. However, the case of frequency one is rather special and should possibly be considered as transitional between periodic and non-periodic stimuli.</p>
<p><xref ref-type="fig" rid="pcbi.1006695.g003">Fig 3</xref> provides an idea of the quality of the fits by showing the <italic>evidence</italic> of the data in favor of the models tested (cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref028">28</xref>]). Upper and lower rows correspond to Two-way and Global adaptation type respectively. For dataset ORIG, <xref ref-type="fig" rid="pcbi.1006695.g003">Fig 3a</xref> shows the logs of the odds ratio of the parametric model of <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> against a noise only model consisting of the block mean with variance similar to that of the data. Each bar is split into the log of the odds ratio of the full model to a drift only model that lacks the sinusoidal component (darker tone of the bars) added to the log of the odds ratio of the drift only model to the noise only model described above (lighter tone of the bars). This separation is possible because the models are nested so that the simpler models can be obtained from the full model by eliminating parameters. The evidence then compares the density of models likely to fit the data. <xref ref-type="fig" rid="pcbi.1006695.g003">Fig 3b</xref> shows the estimation of the frequency of the oculomotor response against the actual frequency of the stimulus for the three frequency values tested in dataset ORIG (3, 4, and 6 cpb). <xref ref-type="fig" rid="pcbi.1006695.g003">Fig 3c and 3d</xref> shows the evidence and the agreement of the response with the five frequencies used in dataset FREQ (1, 3, 6, 12, and 24 cpb).</p>
</sec>
<sec id="sec017">
<title>State-equation fittings and model selection</title>
<p>To assess the generative model, we fit <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> to all data available. For illustration puposes only, we first show that the model provides a reasonable overall fit to the group data. <xref ref-type="fig" rid="pcbi.1006695.g004">Fig 4</xref> shows fits of the oculomotor response predicted by the full form of the generative model given by <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> with all five parameters described in the <bold>Methods</bold> section: <italic>K</italic>, <italic>A</italic>, <italic>m</italic>, <italic>D</italic> as well as the initial condition <italic>G</italic>. As before the qualitative agreement of the fits and the data is evident in both datasets. As we did with the phenomenological fits, we included the pre-adaptation blocks in each condition in each dataset.</p>
<fig id="pcbi.1006695.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Fits of the oculomotor response predicted by the state-equation (<xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>) with all five parameters.</title>
<p>The plots show adaptation gain (colored lines), averaged over individuals in the (<bold>a</bold>) Two-way adaptation and (<bold>b</bold>) Global adaptation condition of the ORIG data set (reported in [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>]), as well as the (<bold>c</bold>) Two-way adaptation and (<bold>d</bold>) Global adaptation condition of the FREQ data set, using the same paradigm over an extended range of frequencies. The stimuli input in the model fits is <bold>s</bold>(<bold><italic>n</italic></bold>) (cf. Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e003">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1006695.e006">3</xref>), which is zero in the preadaptation block. The same equation was fitted to data from each participant in each condition and experiment, to estimate parameters of the generative model on an individual basis. For illustration purposes only, the figure depicts fittings done over the averages along with 95% confidence intervals (gray shaded areas).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g004" xlink:type="simple"/>
</fig>
<p>For all subsequent analyses, we fitted models to individual data. In particular, we compared 16 different models that differed from each other depending on which parameters were fitted (see <xref ref-type="sec" rid="sec002">Methods</xref> for details). We used Akaike’s information criterion (AIC) to explore statistical selection among these models. Akaike weights (cf. section II of [<xref ref-type="bibr" rid="pcbi.1006695.ref040">40</xref>]) are shown in <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref> segregated by model and condition, for datasets ORIG and FREQ, respectively. In each condition (identified by adaptation type and stimulus frequency), we computed a matrix of weights in the following way. Because the best fitted model may differ between individuals, we first computed the AIC weights among the 16 models for each participant and condition. Then we averaged the resulting individual weights across participants. Results from this procedure are shown in <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref>.</p>
<fig id="pcbi.1006695.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Akaike weights [<xref ref-type="bibr" rid="pcbi.1006695.ref040">40</xref>] for the 16 versions of the generative model, segregated by condition (frequency and type of adaptation).</title>
<p>The label along the middle y-axis indicates the model for the weight displayed in the horizontal bars. Results from dataset ORIG (<bold>a</bold>) and FREQ (<bold>b</bold>). Weights for each of the three frequencies for each type of adaptation (blue tones for Two-way expanding to the right, red tones for Global to the left) are stacked for each model and color-coded as in <xref ref-type="fig" rid="pcbi.1006695.g001">Fig 1</xref>. The models are grouped according to the criterion described in the subsection <italic>Rationale for generative model building and parameter exploration</italic> in the <bold>Discussion</bold> section (see text for further details). Gray areas in the background indicate the average weight of the corresponding model group.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g005" xlink:type="simple"/>
</fig>
<p>Inspection of <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref> suggests clear overall preference for models in groups II (which include <italic>m</italic> but not <italic>D</italic>) and IV (featuring both <italic>m</italic> and <italic>D</italic>). We discuss below why this is expected on theoretical grounds given the features of the data. Models from group IV that learn based on two error samples, are preferred in Two-way adaptation, specifically the full model (<italic>KAmDG</italic>) and the model in which <italic>A</italic> was set to unity (<italic>KmDG</italic>). Models in group II that feature a single learning rate (error-correcting based only on the last experienced feedback), specifically <italic>KAm</italic> and <italic>KAmG</italic>, have an edge in Global adaptation. In what follows, we will focus on a comparison of these four models.</p>
<p><xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6</xref> shows the values of the generative parameters (Mean ± SEM, N = 10 for dataset ORIG, N = 13 for dataset FREQ) of the best models that learn only from the last experienced feedback error (<italic>KAm</italic>, <italic>KAmG</italic>). Left and right columns correspond to datasets ORIG and FREQ respectively. Learning rate <italic>K</italic>, persistence rate <italic>A</italic> and drift parameter <italic>m</italic> are shown in <xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6a, 6b and 6c</xref>. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7</xref> reports the parameters of the best models that update their hidden variable based on double error sampling. Those models are <italic>KmDG</italic> and <italic>KAmDG</italic>. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7a and 7d</xref> show respectively the learning rates <italic>K</italic> and <italic>D</italic> that weight the contributions of last and next-to-last feedback, <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7b and 7c</xref> show the persistence rate <italic>A</italic> and the drift parameter <italic>m</italic>. Note that all models include the drift parameter <italic>m</italic> as a fitting parameter. We shall explain below why this should be expected.</p>
<fig id="pcbi.1006695.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Average over individual parameters of generative models <italic>KAm</italic>, <italic>KAmG</italic>, the best among those that learn from the last feedback only (cf. <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> with <italic>D</italic> = 0).</title>
<p>Blue and red colors correspond to horizontal Two-way and Global adaptation, respectively. (<bold>a</bold>) Learning rate <bold><italic>K</italic></bold>, (<bold>b</bold>) Persistence rate <bold><italic>A</italic></bold>, and (<bold>c</bold>) bias or drift-parameter <bold><italic>m</italic></bold> are plotted as a function of condition. Both favored models feature <bold><italic>A</italic></bold> and <bold><italic>m</italic></bold> as fitting parameters. Note the variability in their fitted values across conditions, in particular for dataset FREQ. Error bars are ±SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g006" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006695.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Average over individual parameters of generative models <italic>KmDG</italic>, <italic>KAmDG</italic>, the best among those learning from last and next-to-last feedbacks (double-error-sampling model; cf. full <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>).</title>
<p>Blue and red colors correspond to horizontal Two-way and Global adaptation, respectively. (<bold>a</bold>) Learning rate <bold><italic>K</italic></bold>, (<bold>d</bold>) Learning rate <bold><italic>D</italic></bold>, (<bold>b</bold>) Persistence rate <bold><italic>A</italic></bold>, and (<bold>c</bold>) bias or drift-parameter <bold><italic>m</italic></bold> are plotted as a function of condition. Both favored models feature <bold><italic>D</italic></bold> and <bold><italic>m</italic></bold> as fitting parameters. Note that in both models, the bias parameter <bold><italic>m</italic></bold>, and the persistence rate <bold><italic>A</italic></bold> in model <bold><italic>KAmDG</italic></bold>, display much lower variability in their fitted values across conditions when compared to that of <xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6</xref>. (<bold>e</bold>,<bold>f</bold>) Addition and difference of both learning rates, <bold><italic>κ</italic></bold> = <bold><italic>K</italic></bold> + <bold><italic>D</italic></bold> and <bold><italic>η</italic></bold> = <bold><italic>K</italic></bold> − <bold><italic>D</italic></bold> (see text for discussion).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g007" xlink:type="simple"/>
</fig>
<p>Again, several features are readily apparent from these plots. The learning rates (<italic>K</italic> and <italic>D</italic>) obtained from ORIG [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>], show a rather clear segregation between Two-way adaptation and Global adaptation: <italic>K</italic> and <italic>D</italic> are larger for Two-way (blue colors) than for the Global case (red colors) suggesting that the extra variability brought upon by the random directions of the subsequent saccades characteristic of Global adaptation has a detrimental effect on all learning rates. They do not show a strong dependence on the frequency but the range of values used in that experiment was rather narrow, ranging from 3 to 6 cpb. This segregation in the learning rates between Two-way and Global adaptation is also clearly present in the best models fitted to dataset FREQ.</p>
<p>A feature observed in all cases is that in models that learn only from the last experienced error, the (single) learning rate (<italic>K</italic>) shows a mild increase with the frequency (cf. <xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6a</xref>). This changes substantially if learning from the next-to-last feedback is included. In all of these models, the following features are observed. First, the magnitude of <italic>K</italic>, the learning rate of the last-feedback error-correction term increases by about an order of magnitude with respect to the models that do not have next-to-last error-correction (compare the scales of Figs <xref ref-type="fig" rid="pcbi.1006695.g006">6a</xref> and <xref ref-type="fig" rid="pcbi.1006695.g007">7a</xref>). Second, the magnitude of the next-to-last error learning rate (<italic>D</italic>) is similar to that of the last error (<italic>K</italic>) but with opposite sign. This seems to suggest that the next-to-last error is weighted negatively (or actively attempted to be forgotten) in the algorithm. Third, the discrepancy in magnitude between <italic>K</italic> and <italic>D</italic> is consistently larger for Two-way than for Global adaptation (compare the separation between corresponding blue and red lines in <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7a and 7d</xref>). Fourth, the learning rate <italic>K</italic> reverses its dependence with the frequency of the stimulus with respect to the models without <italic>D</italic>, and now decreases monotonically as the frequency of the stimulus increases. At the same time, the magnitude of <italic>D</italic> also decreases with the frequency. As a consequence, the discrepancy in magnitude between <italic>K</italic> and <italic>D</italic> is such that the addition of both learning rates approximately matches the range of the values of <italic>K</italic> fitted in the models that learn only from the last feedback (compare the values plotted in Figs <xref ref-type="fig" rid="pcbi.1006695.g006">6a</xref> and <xref ref-type="fig" rid="pcbi.1006695.g007">7e</xref>). This suggests that when the additional error learning is not part of the model, the only learning rate fitted may represent an average across subprocesses.</p>
<p>The values of the parameters fitted with the best four models are shown in <xref ref-type="supplementary-material" rid="pcbi.1006695.s001">S1 Table</xref> (Mean ± SEM, N = 10 for ORIG, N = 13 for FREQ). To assess dependence of the generative parameters on the experimental conditions we run 2 X 3 (ORIG) and 2 X 5 (FREQ) repeated-measures ANOVA on the fitted values using as regressors type of adaptation (Two-way and Global) and ISS frequency. Results are shown in <xref ref-type="supplementary-material" rid="pcbi.1006695.s002">S2 Table</xref> for the parameters given in <xref ref-type="supplementary-material" rid="pcbi.1006695.s001">S1 Table</xref>. We regard as more representative the results from dataset FREQ due to the more extended range of frequencies tested. Consistent with the qualitative observations mentioned above, while type of adaptation is highly significant for the learning rates in every model, frequency shows significance for <italic>K</italic> and <italic>D</italic> only in the models that feature double error sampling (<italic>KmDG</italic>, for both datasets, <italic>KAmDG</italic> only for FREQ) but not in those learning just from the last feedback (<italic>KAm</italic>, <italic>KAmG</italic>). As for the persistence rate, frequency is never significant suggesting that it can be kept fixed as in model <italic>KmDG</italic>. Type of adaptation is significant in <italic>KAm</italic> and <italic>KAmG</italic> but such significance disappears in <italic>KAmDG</italic>.</p>
</sec>
<sec id="sec018">
<title>Analytical solution of the generative model: Predicting the phenomenological parameters</title>
<p>The iteration of state-equations that learn from the last feedback already qualitatively predicts both components of the phenomenological response. In general, the complete response can be interpreted as a convolution of the stimulus with a <italic>response function</italic>. This response function integrates the stimulus by weighting the disturbance over a temporal window, the size of which depends on the magnitude of the learning and persistence rates that combine to assemble the weights (cf. <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>). Contributions from constant components of the disturbance that arise either from constant features in the stimulus (as in the traditional fixed-ISS paradigm [<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>]) or from intrinsic biases that may not be strictly error-based in nature (e.g., in our case represented by the drift parameter; cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref037">37</xref>]) accumulate across trials, changing saccade gain in a monotonic fashion akin to a drift of the baseline towards an asymptote. Iteration of the systematically varying part of the disturbance results in its convolution with similar weights but the trial-by-trial variation usually prevents finding a closed form for the series re-summation. However, a sinusoidal disturbance avails a closed analytical integral solution, it is periodic with the same frequency, lagging the stimulus by a number of trials. Two new phenomenological parameters of this periodic response—its amplitude and lag—bare characteristic dependences on the learning parameters.</p>
<p>Above, we fitted the extended version of <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> to the data and obtained and reported estimates for its phenomenological parameters (i.e., frequency <italic>ν</italic>, amplitude <italic>a</italic>, lag <italic>ϕ</italic>, asymptote <italic>B</italic><sub>0</sub>, timescale <italic>λ</italic> and decay amplitude <italic>B</italic>; cf. <xref ref-type="fig" rid="pcbi.1006695.g002">Fig 2</xref> above). Similarly, we fitted the generative parameters for all generative models using the corresponding versions of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. Figs <xref ref-type="fig" rid="pcbi.1006695.g006">6</xref> and <xref ref-type="fig" rid="pcbi.1006695.g007">7</xref> display those estimates for the four models that provided the best fits (excluding <italic>D</italic>: <italic>KAm</italic> and <italic>KAmG</italic> and including <italic>D</italic>: <italic>KAmDG</italic> and <italic>KmDG</italic> respectively).</p>
<p>When the learning algorithm includes several error-based terms, <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> can be integrated using techniques standard within the theory of LTIS [<xref ref-type="bibr" rid="pcbi.1006695.ref042">42</xref>]. This integration provides analytical predictions of the phenomenological parameters as functions of the learning parameters fitted with the generative models (Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref> through <xref ref-type="disp-formula" rid="pcbi.1006695.e021">12</xref>). We attempt matching these predictions to the values fitted using the phenomenological parameter estimation implemented before (see <xref ref-type="fig" rid="pcbi.1006695.g002">Fig 2</xref>). It should be pointed out, however, that the phenomenological parameter values have also been obtained from fits to the data and therefore should only be regarded as indicative reference values to guide intuition, not as ground truth. Validation of the actual underlying structure of the learning model relies ultimately on statistical model selection. Yet, a direct comparison between the fitted phenomenological parameters and analytical predictions evaluated on the fitted generative parameters is informative because a given value of a phenomenological parameter has to be compared to diverse combinations of the generative parameters that in turn depend on the specific structure of the learning model.</p>
<p>We start with <xref ref-type="disp-formula" rid="pcbi.1006695.e020">Eq 11</xref> that provides a relationship between the expected asymptote of the adaptation gain at large trial number and the generative model parameters.</p>
<disp-formula id="pcbi.1006695.e022">
<alternatives>
<graphic id="pcbi.1006695.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
<p>A first significant observation about this expression is that in order to observe a drift in the baseline of the adaptation gain (i.e., in order to have an asymptote <italic>B</italic><sub>0</sub> ≠ 0), a finite value of the drift parameter <italic>m</italic> is strictly necessary. If <italic>m</italic> vanishes, the adaptation gain would maintain a baseline pinned at zero regardless of the values of <italic>K</italic>, <italic>A</italic> or <italic>D</italic>. In addition, in a situation where <italic>A</italic> ~ 1, <inline-formula id="pcbi.1006695.e023"><alternatives><graphic id="pcbi.1006695.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> or <inline-formula id="pcbi.1006695.e024"><alternatives><graphic id="pcbi.1006695.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mfrac><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> in models where <italic>D</italic> = 0. Note that these are all signed magnitudes, not absolute values. In other words, a small learning rate <italic>K</italic> or a small number resulting from the addition <italic>K</italic> + <italic>D</italic> will modulate the size of the asymptote and will determine its sign (i.e., will modify the degree of hypometria or hypermetria). Still a finite value for <italic>m</italic> is strictly needed to have non-zero asymptote. Recall that when <italic>m</italic> is not a fitting parameter, its value is set to zero. Due to the pervasive baseline drift across all of our data, all models favored under statistical model selection contain <italic>m</italic> as a fitting parameter. This is why model groups II and IV (cf. <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref>) are preferred, as pointed out above and in the <bold>Discussion</bold>. Note, in addition, that the smaller the learning rate (<italic>K</italic> or <italic>K</italic> + <italic>D</italic>), the larger the size of the asymptote <italic>B</italic><sub>0</sub>.</p>
<p>Experimentally, we observed drifts towards higher hypometria in all averages and in most of the individual data. Note that formatting the data in terms of <italic>adaptation gain</italic> instead of <italic>saccade gain</italic> allows us to remove confounds coming from constant contributions from the stimulus and therefore the parameter <italic>m</italic> should be regarded as intrinsic to the system. In other words, <italic>m</italic> characterizes or quantifies learning that would occur in absence of stimulus disturbance (i.e., with zero ISS), as if the system has an intrinsic propensity to modify its gain by virtue of environmental or experimental conditions not necessarily linked to an error.</p>
<p><xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8a</xref> displays the matching of the analytical predictions of the asymptotes computed by inserting the fitted values of <italic>m</italic>, <italic>A</italic>, <italic>K</italic> and <italic>D</italic> into <xref ref-type="disp-formula" rid="pcbi.1006695.e020">Eq 11</xref> for each participant’s data, to the phenomenological estimation of <italic>B</italic><sub>0</sub> obtained from <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> and the parameter estimation of the phenomenological fits of the data for both datasets and both adaptation types.</p>
<fig id="pcbi.1006695.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006695.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Comparison of phenomenologically fitted parameters to their theoretical predictions based on the generative model.</title>
<p>y-axes show the values obtained with the phenomenological parameter estimation (<xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>); x-axes show predictions obtained by inserting the best estimated values of the generative parameters into the analytical expressions of Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006695.e021">12</xref> (cf. ‘<italic>Using the generative model to predict the parameters of the phenomenological description of the adaptation gain’</italic> in the <xref ref-type="sec" rid="sec002">Methods</xref> section). Each row corresponds to one of the four best generative models. Each point is a single participant in a given condition and experiment. Data from the condition with a frequency of 1 cpb has been omitted because the predictions were poor for all models (in particular for the lag valiable, see the text for a discussion of this point). (<bold>a</bold>) Asymptotes of the gain at large trial numbers (<bold><italic>B</italic></bold><sub>0</sub> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> vs predictions by <xref ref-type="disp-formula" rid="pcbi.1006695.e020">Eq 11</xref>). (<bold>b</bold>) Timescale of the decay of the baseline (<bold><italic>λ</italic></bold> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> vs predictions by <xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref>). Note the wide spread of the predicted values for models <bold><italic>KAm</italic></bold>, and <bold><italic>KAmG</italic></bold> that also results in several outliers beyond the limits of the subplot. In contrast, models <bold><italic>KmDG</italic></bold> tend to underestimate the phenomenological timescales. Model <bold><italic>KAmDG</italic></bold> provides the best prediction of the phenomenologically fitted values with only two points just beyond the limits of the plot. (<bold>c</bold>) Amplitude of the periodic component of the gain (<bold><italic>a</italic></bold> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> vs predictions by Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e014">8</xref>–<xref ref-type="disp-formula" rid="pcbi.1006695.e018">10</xref>). Note that models <bold><italic>KAm</italic></bold> and <bold><italic>KAmG</italic></bold> tend to underestimate the observed amplitudes of the peridic component of the gain. (<bold>d</bold>) Lag of the periodic component of the gain (<bold><italic>ϕ</italic></bold> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> vs predictions by Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006695.e018">10</xref>). The plots reveal a slight tendency for models <bold><italic>KAm</italic></bold> and <bold><italic>KAmG</italic></bold> to overestimate the length of the lag with respect to the predictions of the models including double error sampling (<bold><italic>KmDG</italic></bold> and <bold><italic>KAmDG</italic></bold>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.g008" xlink:type="simple"/>
</fig>
<p>A second parameter characteristic of the baseline drift is given by the timescale. <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8b</xref> shows predicted values for the timescales that result when values of <italic>m</italic>, <italic>A</italic>, <italic>K</italic> and <italic>D</italic> fitted with the state-equation are inserted in <xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref>. The first two rows of <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8b</xref> show a clear overestimation of the baseline timescale in models that do not feature double error sampling (i.e., <italic>KAm</italic> and <italic>KAmG</italic>) as several individual data points fall outside the boundaries of the plot. Yet, models that include corrections based on the next-to-last error term, seem to underestimate the timescale (in particular model <italic>KmDG</italic>). When introducing <xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref>, we pointed out that if the second error learning rate <italic>D</italic> is negative, the dominant mode in the solution still features a monotonic decay that can fit the phenomenologically observed exponential baseline drift of the gain. This is indeed the case in the majority of fits to the individual participants’ runs: Across models in group IV, <italic>D</italic> was non-negative in only 13% of the individual runs; 6% for Two-way adaptation and 21% for Global adaptation data. For model <italic>KmDG</italic>, <italic>D</italic> was non-negative in 7% of all runs; with only 1% (1 run out of 95) for Two-way adaptation and 14% for Global adaptation. Furthermore, when estimating the timescales of models that include double error-correction, <xref ref-type="disp-formula" rid="pcbi.1006695.e021">Eq 12</xref> consistently gives smaller values than for models without the second error term (compare subplots of <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8b</xref> for the corresponding models, cf. <bold>Fig A</bold>, <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>). This ordering relation between the timescales of models with and without <italic>D</italic> was unknown before conducting the fits. Thus, data collected using a sinusoidal adaptation paradigm suggests that including a second error-correction term yields a significant decrease in the timescale with respect to models featuring a single error-correction term. Therefore, the integration windows (i.e., the inverse of the timescale) of models with double error-correction grow significantly larger compared to those that lack the second error sampling.</p>
<p>Asymptote and timescale are parameters traditionally investigated and reported in adaptation to fixed-step disturbances. Sinusoidal adaptation paradigms provide two additional parameters associated to the periodic component of the adaptation gain observed in these protocols. <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8c and 8d</xref> compare predictions for the amplitude and the lag of the periodic component of the gain obtained by using Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref> through <xref ref-type="disp-formula" rid="pcbi.1006695.e018">10</xref> above. Data from both datasets suggest that models that do not feature double error sampling underestimate the magnitude of the amplitude of the periodic component of the oculomotor response (cf. predictions from these models in <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8c</xref>). This feature in fact is common to all models that learn from a single feedback and include <italic>m</italic> (besides models <italic>KAm</italic> and <italic>KAmG</italic>), but the inclusion of <italic>D</italic> helps mitigating misestimation of this amplitude.</p>
<p>The last comparison is provided by the lag of the periodic component. <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8d</xref> compares predictions based on the state-equation learner (Eqs <xref ref-type="disp-formula" rid="pcbi.1006695.e013">7</xref> and <xref ref-type="disp-formula" rid="pcbi.1006695.e018">10</xref> furnish predictions for the components of the lag <italic>ϕ</italic> and <italic>φ</italic> after inserting the parameter values fitted with <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>) and the phenomenology (parameter <italic>ϕ</italic> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>). From <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8c and 8d</xref> it is apparent that the models that include both <italic>m</italic> and <italic>D</italic> as fitting parameters provide better predictions, also displaying less variability across participants, in particular for the Two-way adaptation type. Among models with <italic>D</italic> = 0, again models <italic>KAm</italic> and <italic>KAmG</italic> fit best. <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8d</xref> shows, however, that these models appear to overestimate the lag (cf. compare corresponding subplots in the figure), while models that have a second learning rate <italic>D</italic> match better the empirically observed lag. In addition, all models fail the estimation of the lag for a disturbance of frequency one as they all significantly overestimate the lag observed. Even though the predictions of the other phenomenological parameters are reasonable (the amplitude of the periodic component, timescale and asymptote), predictions for the 1cpb condition for both Two-way and Global adaptation have been omitted altogether in <xref ref-type="fig" rid="pcbi.1006695.g008">Fig 8</xref>. This mismatch between the direct phenomenological estimation of the lag from the data and the analytical predictions arising from the integration of the state-equation for the case of the 1cpb condition, may be rooted in the fact that the functional dependence of the phenomenological parameters on the generative ones is determined by the specific sinusoidal dynamics of the driving stimulus, while the case of a 1-cpb frequency is the least periodic condition among all tested.</p>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>We used a modified version of the traditional two-step saccade adaptation paradigm ([<xref ref-type="bibr" rid="pcbi.1006695.ref001">1</xref>]; see [<xref ref-type="bibr" rid="pcbi.1006695.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref046">46</xref>] for reviews) in which the size of the second step varied as a sinusoidal function of trial number with an amplitude of 25% of a fixed pre-saccadic target amplitude. We recorded observers’ eye movements at a total of six different frequencies and applied the sinusoidal disturbance always along the saccade vector which was aligned either in a horizontal bi-directional fashion (Two-way adaptation) or in random directions drawn from a uniform circular distribution (Global adaptation). The oculomotor response, quantified by the adaptation gain, followed the disturbance variation with comparable frequency, an amplitude ranging between 10 and 30% of that of the stimulus (i.e., 2.5 to 7.5% of the saccade amplitude), and lagging the stimulus by a few trials. In addition, it developed a systematic drift of the baseline towards larger hypometria that reached asymptotes of around 40% of the disturbance amplitude (i.e., 10% of the saccade amplitude) and was largely comparable across conditions. The phenomenological description in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>—composed of a periodic response and an exponential decay—captured this behavior well and we estimated all six parameters pertaining to that description.</p>
<p>The present study explored whether the phenomenology described by <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> can be modeled with a state-equation, i.e., a generative rather than descriptive model of the underlying sensorimotor learning. We clearly show that the recorded saccade adaptation data is indeed predictable in a robust and stable way using a linear time invariant state-equation similar but not identical to those proposed before in the literature. Moreover, in previous accounts, simulations based on generative models as well as ad-hoc fittings (mostly exponential or monotonic) of the temporal evolution of the gain were provided without specifying a pathway of how to evolve from one description to the other. We suggest that connection here and provide results of the derivation involved in transitioning between these descriptions.</p>
<sec id="sec020">
<title>Rationale for generative model building and parameter exploration</title>
<p>In mathematical terms, the functional form in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref> is the integral solution of a family of LTISs of which <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> is a particular example. It is referred to as a state-equation or state-space model because the internal variable <italic>x</italic> characterizes the gain or state of adaptation of the system. This algorithm is <italic>generative</italic> because it estimates the value of <italic>x</italic> at trial <italic>n</italic> + 1 by modifying its estimate at the previous trial including possible effects of systematic biases and correcting the former value by weighting sensory feedback resulting from movement execution [<xref ref-type="bibr" rid="pcbi.1006695.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref048">48</xref>] (see also [<xref ref-type="bibr" rid="pcbi.1006695.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref032">32</xref>]; for further details on our specific use see the <xref ref-type="sec" rid="sec002">Methods</xref> section). Here we limit our discussion to <italic>noise-free</italic> generative models in that <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> does not include any noise term. Yet, <xref ref-type="fig" rid="pcbi.1006695.g001">Fig 1</xref> together with <xref ref-type="fig" rid="pcbi.1006695.g004">Fig 4</xref> suggest that integral solutions as well as numerical outcomes of noise-free generative models survive ensuing variability, at least for the paradigm, type of stimulus and within the ranges of the conditions tested.</p>
<p>We analyzed 16 models that differed in the specific parameters that were fitted and then used Akaike’s information criteria to attempt model selection. Since we were primarily modeling intrinsic error-based sensorimotor learning, the learning rate <italic>K</italic>—that weights the impact of the last feedback error on the state of adaptation—was present in every model. Second, we included the initial condition <italic>G</italic> as a fitting parameter in half of the models. This parameter is not part of the trial-by-trial learning algorithm and its effect should decay as the trial number increases (cf. <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>). However, the initial condition affects the amplitude of decay of the baseline drift (cf. <italic>B</italic> in <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>). Because the argument of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref> is an internal variable not directly experimentally accessible, a proxy for its initial value can only be approximated (for example, by averaging the first five gain values in the block) or included as a fitting parameter. Third, we included a persistence rate <italic>A</italic> that weighted how much of the estimate from the previous movement remained in the subsequent one. The fourth parameter, <italic>m</italic>, captured systematic effects, that are not error-based in nature, and gave origin to drifts in the baseline that were pervasive across all conditions. Finally, we considered the plausibility and study the effects of a second learning rate <italic>D</italic> that tracks errors other than the most recent (here, the next-to-last feedback error).</p>
<p>To further discuss the effect of the generative parameters, we split the 16 models into four groups:</p>
<list list-type="simple">
<list-item>
<label>I</label>
<p>Models that neither included terms depending on the second learning rate <italic>D</italic> nor the drift term <italic>m</italic> (<italic>K</italic>, <italic>KG</italic>, <italic>KA</italic>, <italic>KAG</italic>);</p></list-item>
<list-item>
<label>II</label>
<p>Models without terms depending on <italic>D</italic> but including <italic>m</italic> (<italic>Km</italic>, <italic>KmG</italic>, <italic>KAm</italic>, <italic>KAmG</italic>);</p></list-item>
<list-item>
<label>III</label>
<p>Models including terms depending on <italic>D</italic> but excluding <italic>m</italic> (<italic>KD</italic>, <italic>KDG</italic>, <italic>KAD</italic>, <italic>KADG</italic>);</p></list-item>
<list-item>
<label>IV</label>
<p>Models with both <italic>D</italic> and <italic>m</italic> terms included (<italic>KmD</italic>, <italic>KAmD</italic>, <italic>KmDG</italic>, <italic>KAmDG</italic>).</p></list-item>
</list>
<p>We recall that in models where <italic>A</italic> is not a fitting parameter, <italic>A</italic> = 1. The groups are listed on the left side of <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref>. Models within group I consistently fitted worst. Moreover, models that do not include <italic>m</italic> (groups I and III) cannot capture an evolution of the gain into a stationary asymptotic value because the state equation does not admit a solution featuring that behavior (that is, if the stimulus has no constant term). These models, however, may be useful in experimental paradigms where a stable state of adaptation is not clearly reached either because the length of the adaptation block used may be too short or because the driving disturbance is unbounded (for example a linear ramp). On the other hand, models that include sampling from two errors (cf. groups III and IV) will likely be better suited to extract correlations built into the stimulus as it is the case of a sinusoidal ISS.</p>
<p>The fits of the phenomenological model (<xref ref-type="fig" rid="pcbi.1006695.g001">Fig 1</xref>; <xref ref-type="disp-formula" rid="pcbi.1006695.e012">Eq 6</xref>) suggest that asymptotic behavior of the baseline and reflection of the stimulus self-correlation (entraining) were clear structural properties of the oculomotor response. The analytical solutions of models in both groups II and IV are consistent with this phenomenology. <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref> summarizes the AIC weights emerging from the fits to the individual participants’ data. The weights shown in the horizontal bars are averages over individual participants’ weights for each condition and color coded by the frequency of the stimulus. Data from Two-way adaptation is depicted with blue tones in bars increasing towards the right. Global adaptation is shown with bars spanning to the left in red tones. The average weight for each model family is shown by the gray background behind the corresponding group. While models in group II already generate responses in qualitatively good agreement with the evolution of the adaptation gain, it remains to be decided whether corrections based on the memory of more than a single error provide for a better fit. AIC weights show that group IV clearly outperforms all others in Two-way adaptation in both datasets, suggesting that the best generative model to describe this type of adaptation includes all four parameters <italic>K</italic>, <italic>A</italic>, <italic>m</italic> and <italic>D</italic>. In Global adaptation, models from group II either match or slightly outperform those of group IV. Model comparison showed that a state-equation including a single parameter or any combination of only two of the four parameters <italic>K</italic>, <italic>A</italic>, <italic>m</italic> and <italic>D</italic> could not adequately account for our data (cf. <xref ref-type="fig" rid="pcbi.1006695.g005">Fig 5</xref>). In addition, an inspection of actual values of the parameters fitted across the population suggests that the parameters <italic>A</italic> and <italic>m</italic> may be set to constant values, that is, to almost one for the former and to a very small and negative number for the latter (cf. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7c and 7d</xref>), at least within the range of frequencies tested in these experiments. Overall, the drift parameter <italic>m</italic> and the second learning rate <italic>D</italic> proved useful and necessary to account for systematic effects in our data, suggesting (1) that some changes in the adaptation state are not error-based and (2) that—at least under specific circumstances—the brain keeps track of at least one extra occurrence of the error besides the last experienced one. Three-parameter models that did not involve <italic>D</italic> (specifically <italic>KAm</italic>) were most successful in Global adaptation and in the high frequencies of Two-way adaptation. This could be simply a reflection of increased levels of measurement noise in these conditions giving an upper hand to models with fewer parameters. More interestingly, it could point to an architecture that samples two errors only under certain conditions, for instance, when errors are repeatedly experienced for the same saccadic vector, or, when the variation of the feedback error has a high signal-to-noise ratio. We speculate that overtraining along a given direction, understood as the repetitive experience of consistent error along similar saccade vectors in Two-way adaptation (note that in our paradigm Two-way adaptation stimulates only two retinal locations) may give rise to vector specificity and, consequently, to the adaptation fields typically observed with fixed-step paradigms. Indeed, Rolfs and collaborators [<xref ref-type="bibr" rid="pcbi.1006695.ref018">18</xref>] suggested that Global adaptation, featuring apparent full transfer across random directions, appears to onset ahead of the development of vector-specific adaptation fields. This appears consistent with the present finding that models that rely on a single error-correction show timescales corresponding to faster evolution of the baseline drift (although with longer lags in the sinusoidal component) as compared to those of Two-way adaptation (featuring shorter lags in the sinusoidal component consistent with tracking the stimulus more closely due to the repetitive training in a specific direction).</p>
</sec>
<sec id="sec021">
<title>Drift in the baseline and the meaning of <italic>m</italic></title>
<p>The persistent drift of the baseline towards higher hypometria is a distinctive feature in our data that cannot be accounted for on the basis of motor adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref049">49</xref>]. We included an extra parameter <italic>m</italic> to account for this drift in mean adaptation gain towards an asymptote differing from the mean of the stimulus (cf. <xref ref-type="disp-formula" rid="pcbi.1006695.e020">Eq 11</xref>). This parameter is conceptually novel, distinct and independent of the persistence rate <italic>A</italic>, and determines the presence of a non-zero asymptote via <xref ref-type="disp-formula" rid="pcbi.1006695.e020">Eq 11</xref>. Because in our paradigm the goal of the task was to land on the target as close as possible, and because the sinusoidal ISS introduced a continuously changing prediction error, the best expected outcome would be to track the disturbance within the levels of error typical of trials without disturbance. With respect to that goal, the presence of a baseline drift introduces an additional discrepancy that does not, however, hinder successful adaptation to the disturbance.</p>
<p>Saccadic eye movements slightly undershoot their target on average [<xref ref-type="bibr" rid="pcbi.1006695.ref050">50</xref>] and this systematic offset corresponds to the internally predicted visual outcome of a saccadic eye movement [<xref ref-type="bibr" rid="pcbi.1006695.ref051">51</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref052">52</xref>]. We surmise that our paradigm may have yielded a re-calibration of this desired offset [<xref ref-type="bibr" rid="pcbi.1006695.ref053">53</xref>] over the course of an experimental run. This recalibration towards a larger undershoot may result from the high probability of a quick return saccade after every eye movement in our fast-pace paradigm, reducing the utility of maintaining a saccade gain close to one. We note that this systematic decrease in saccade gain may in general—albeit to different degrees—pervade the study of saccadic adaptation (but see [<xref ref-type="bibr" rid="pcbi.1006695.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref054">54</xref>]). In fixed-step paradigms (as opposed to the sinusoidal paradigm employed here) it would have been obscured as the error-based correction for the surreptitious target displacement undergoes similar dynamics as the drift reported here.</p>
<p>On the other hand, from the point of view of the internal model of the movement that the brain may implement [<xref ref-type="bibr" rid="pcbi.1006695.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref035">35</xref>], this bias parameter <italic>m</italic> may hint to a discrepancy between the experimental coordinate system where measurements are acquired and the coordinate system in which the internal model is represented.</p>
<p>On a neurophysiological level, the small systematic bias that gives rise to the drift of the baseline may originate from the dynamics of the responses in the neuronal substrates involved with saccade adaptation ([<xref ref-type="bibr" rid="pcbi.1006695.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref060">60</xref>], R. Shadmehr, <italic>personal communication</italic>, July 12, 2018). It is also possible that the fast-pacing used in our paradigm exacerbates effects that generate a small and negative bias parameter, <italic>m</italic>, which appeared to onset already at the pre-adaptation block. That would further suggests that the magnitude of <italic>m</italic> may depend on the inter-saccade interval as well as on the precise timing of the ISS onset, which should be addressed in future studies.</p>
</sec>
<sec id="sec022">
<title>Consequences of learning from double error sampling (<italic>D</italic> parameter): Two learners?</title>
<p>The models that best explained the data featured a double error sampling, learning not only from the feedback experienced after the last saccade but also from the movement that occurred in a trial before that. Hence, the best models used a feedback reaching further back in time through the <italic>K</italic>- and <italic>D</italic>-terms of <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. Yet, does the oculomotor system actually implement this double error sampling that may coherently participate in a single internal model prediction? We suggest that the brain may attempt to approximate the performance achieved by the double-error-sampling algorithm by using two single-feedback learners operating on appropriate combinations of the stimulus sampled at two different times.</p>
<p>To understand that, we return to <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>. For simplicity, we will assume that <italic>m</italic> = 0.
<disp-formula id="pcbi.1006695.e025">
<alternatives>
<graphic id="pcbi.1006695.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>D</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
and write a transformation among state variables sampled at two different trials as,
<disp-formula id="pcbi.1006695.e026">
<alternatives>
<graphic id="pcbi.1006695.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="2pt"/><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
that can be substituted in the RHS of <xref ref-type="disp-formula" rid="pcbi.1006695.e025">Eq 13</xref> using the inverse relations:
<disp-formula id="pcbi.1006695.e027">
<alternatives>
<graphic id="pcbi.1006695.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="2pt"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="2pt"/><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>We can re-write <xref ref-type="disp-formula" rid="pcbi.1006695.e025">Eq 13</xref> in terms of these alternative state variables <italic>X</italic><sub>+</sub> and <italic>X</italic><sub>−</sub>:
<disp-formula id="pcbi.1006695.e028">
<alternatives>
<graphic id="pcbi.1006695.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>-</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
where we adopted the definitions of <italic>κ</italic> = <italic>K</italic> + <italic>D</italic>, <italic>η</italic> = <italic>K</italic> − <italic>D</italic>, <inline-formula id="pcbi.1006695.e029"><alternatives><graphic id="pcbi.1006695.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006695.e030"><alternatives><graphic id="pcbi.1006695.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. <xref ref-type="disp-formula" rid="pcbi.1006695.e028">Eq 14</xref> avails the interpretation of the generative model as selectively learning into two component channels that learn from a single feedback error taken from different sources. The source for the learner <italic>X</italic><sub>+</sub> is the mean of the two samplings of the stimulus, i.e., <inline-formula id="pcbi.1006695.e031"><alternatives><graphic id="pcbi.1006695.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. The source for the second learner is the rate of change of the stimulus across the sampling events given by <inline-formula id="pcbi.1006695.e032"><alternatives><graphic id="pcbi.1006695.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006695.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> which, when the samplings occur on successive trials, it could be interpreted as the discrete time derivative of the stimulus taking the elementary timestep as the (average) inter-trial interval.</p>
<p>Note that the representation in terms of these alternative internal variables would significantly alter the underlying structure of the noise-free learning model. But if we insist on keeping a close connection to the parameters extracted using the double-error-sampling algorithm, we would expect that the learning rate for learner <italic>X</italic><sub>+</sub> would be the addition of the rates for the two errors, <italic>κ</italic> = <italic>K</italic> + <italic>D</italic>, while for learner <italic>X</italic><sub>−</sub> it would be <italic>η</italic> = <italic>K</italic> − <italic>D</italic> (cf. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7e and 7f</xref>). In all our fittings using the double error sampling, <italic>K</italic> and <italic>D</italic> were very close in magnitude but carried opposite sign. Furthermore, <italic>κ</italic> was small and similar in magnitude to the learning rate <italic>K</italic> obtained for models that learned only from the last error. Because <italic>D</italic> was negative, the learning rate <italic>η</italic> for the second learner became also positive but much larger than <italic>κ</italic>, in fact about an order of magnitude larger (<xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7e and 7f</xref>) effectively enhancing the overall gain of the process without driving the system unstable [<xref ref-type="bibr" rid="pcbi.1006695.ref061">61</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref063">63</xref>]. As a consequence, (<italic>A</italic> − <italic>κ</italic>), which can be thought of as an <italic>effective A</italic><sub>+</sub> will be much closer to unity than <italic>A</italic><sub>−</sub> = (<italic>A</italic> − <italic>η</italic>). Therefore, <italic>X</italic><sub>+</sub> will learn and forget much slower than <italic>X</italic><sub>−</sub>.</p>
<p>Using this double error sampling, the oculomotor system could track the rate of change of the stimulus from one saccade to the next, besides just its last change in size and it would approximate the learning efficiency of the double-error-sampling algorithm. The new internal learning variables (<italic>X</italic><sub>+</sub> and <italic>X</italic><sub>−</sub>) would learn from smoothed-out versions of the disturbance resulting from the average sum and difference of the two sampled inputs. Whether this constitutes an advantage over learning exclusively from the last feedback depends on the nature of stimulus. If the disturbance is constant or fully random there would be very little advantage in performing the double error sampling. In the former case, the inter-sampling variation is zero leaving nothing to learn. In the latter, the inter-sampling variation would be another random magnitude and there would be little advantage in learning from the variation in the feedback. However, if the mean of the disturbance varies in a systematic way—as it does during sinusoidal adaptation, and presumably in natural scenarios—learning from its rate of variation would be advantageous and could well justify a large learning rate. In the representation of the double-error-sampling model, unlearning actively the next-to-last sampled feedback error (i.e., with a large and negative <italic>D</italic> subtracted from an enhanced <italic>K</italic>) would materialize this advantage with little extra investment. However, a negative learning rate feels counter-intuitive as learning is believed to follow the direction of the correction suggested by the feedback. Segregation of the learning underlying motor (or saccade) adaptation into two learners displaying similar characteristics to those suggested here have indeed been proposed in other contexts [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref064">64</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref065">65</xref>]. The argument presented above suggests a mechanistic way to construct a two-learner system, in which the components <italic>X</italic><sub>+</sub> and <italic>X</italic><sub>−</sub> can be considered statistics in counterphase. To approximate the double-error-sampling learner, the system may hold in memory both samples, compute mean sum and differences between the samples and implement two learners based on those statistics rather than from bare values of errors or stimulus occurrences. To achieve that, the oculomotor system would need to keep memory and weight prediction errors from a former time scale besides the last feedback [<xref ref-type="bibr" rid="pcbi.1006695.ref065">65</xref>].</p>
<p>An important point to notice is that, even if there is double error sampling, it does not need to be strictly the next-to-last error. It would be enough that the brain keeps a correlation of errors over two different trials (cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref066">66</xref>]) although it would be reasonable that they are spaced only by a short delay [<xref ref-type="bibr" rid="pcbi.1006695.ref061">61</xref>]. This is a reasonable generalization since the inter-trial interval is rather arbitrarily set by the pacing of the task that may or may not match a possible internal sampling frequency by the brain. The frequency of the stimulus then determines to what degree differences in the stimulus can be sampled, which may explain the dependence of the amplitude and lag of the periodic component of the response with the frequency as well as the fact that the evidence for the full model seems to peak at intermediate frequencies. In other words, it may be easier to learn at certain frequencies (for a fixed amplitude) or at certain effective rates of change of the stimulus.</p>
</sec>
<sec id="sec023">
<title>Dependency of learning rate on perturbation dynamics: Linear but not strictly time invariant systems</title>
<p>We further explored whether the values of the generative parameters exhibited dependence on the experimental condition, specifically with the type of adaptation and the frequency of the disturbance. The parameters of our models remained time-invariant across pre-adaptation and adaptation blocks. However, we did not rule out that these parameters may change with adaptation type and stimulus frequency. In fact, LTIS models with parameters not strictly time-invariant have been invoked to model (meta-learning in) savings in adaptation to visuomotor rotations [<xref ref-type="bibr" rid="pcbi.1006695.ref032">32</xref>]. Strict LTIS models with two learners had been able to successfully account for savings in long-term saccade adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref064">64</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref067">67</xref>]) but were not able to fit differences in the dynamics of the adaptation, extinction and re-adaptation phases observed using counter-adaptation and wash-out paradigms in adaptation to visuomotor rotations without letting the rates change across the phases [<xref ref-type="bibr" rid="pcbi.1006695.ref032">32</xref>].</p>
<p>We limit our discussion to the best four generative models selected in the <bold>Results</bold> section. In models <italic>KAm</italic> and <italic>KAmG</italic> (and in general in all models of groups I and II), the (only) learning rate <italic>K</italic> remained relatively independent of, or exhibited a tendency to grow with the frequency of the stimulus (<xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6a</xref>). Learning rates for Two-way adaptation roughly ranged between 0.01 and 0.035 fraction of the error across the frequencies tested. The same parameter in Global adaptation was smaller and remained within the range 0.005 to 0.015 (cf. <xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6a</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006695.s001">S1 Table</xref>). These observations were confirmed by ANOVAs run on the fitted values of the parameter <italic>K</italic> in models <italic>KAm</italic> and <italic>KAmG</italic> in that type of adaptation was always a significant factor while ISS frequency never was (<xref ref-type="supplementary-material" rid="pcbi.1006695.s002">S2 Table</xref>). These values of <italic>K</italic> compare reasonably well with the magnitude of learning rates previously reported in the literature (cf. [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref019">19</xref>]). The dependence of the learning rate on the frequency of the disturbance seems in qualitative agreement with results from reaching experiments in which subjects learned to track a target undergoing surreptitious displacement that followed a random walk [<xref ref-type="bibr" rid="pcbi.1006695.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref047">47</xref>]. Using a Kalman filter to estimate corrections to the learning rate due to various types of variability Burge and collaborators [<xref ref-type="bibr" rid="pcbi.1006695.ref030">30</xref>] argued that the learning rate increased as the drift of the walker increased. In the sinusoidal adaptation paradigm where the amplitude of the sine function that produces the ISS is of fixed amplitude, this situation occurs when the frequency increases because its size from one trial to the next changes faster. However, this suggestion seems at odds with the intuition that a more consistent stimulus should drive more efficient adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref069">69</xref>]. In particular, it has been reported that a smooth gradual variation results in more efficient adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref070">70</xref>]. If this were the case and reflected onto the model parameters, the learning rate should be higher for smaller frequencies.</p>
<p>However, the dependence of the learning rate(s) on the frequency described above changed rather dramatically when double error sampling was included (cf. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7a and 7d</xref>). Interestingly, in models that feature double error sampling, the learning rate of the most-recent error-term (<italic>K</italic>) reversed its tendency and decreased as the frequency increases, achieving its highest values in the conditions of lower frequency, this is, in situations where the stimulus displayed higher consistency. Concurrently, the learning rate for the next-to-last feedback (<italic>D</italic>) achieved its most negative values at lower frequencies and grew less negative as the stimulus frequency increased. In the alternative scenario of two additive learners with single error correcting terms that learned respectively from the half-sum and the half-difference of the two sampled errors suggested in the previous sub-section, the learning rates <italic>κ</italic> and <italic>η</italic> also showed a distinct dependency on the ISS frequency. The <italic>slow-learner</italic> (with learning rate <italic>κ</italic>) would only have corrected up to 1% of the average of the two errors sampled while the <italic>fast-learner</italic> (with rate <italic>η</italic>) would have produced corrections of up to 40% of the change experienced between the two sampled errors (cf. <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7e and 7f</xref>). Note that this massive change in the dependence of the learning rates on the frequency was a consequence of changing the hypothesized structure of the model and not of correcting the magnitude of the rates for effects of variability. Once again, ANOVAs confirmed that not only the type of adaptation but also the stimulus frequency had significant impact on the learning rates (<italic>K</italic> and <italic>D</italic>, as well as <italic>κ</italic> and <italic>η</italic>) in models <italic>KmDG</italic> and <italic>KAmDG</italic> as well as all models of group IV (cf. <xref ref-type="supplementary-material" rid="pcbi.1006695.s002">S2 Table</xref>).</p>
<p>In contradistinction, the retention rate <italic>A</italic> (Figs <xref ref-type="fig" rid="pcbi.1006695.g006">6b</xref> and <xref ref-type="fig" rid="pcbi.1006695.g007">7b</xref>) and the bias parameter <italic>m</italic> (Figs <xref ref-type="fig" rid="pcbi.1006695.g006">6c</xref> and <xref ref-type="fig" rid="pcbi.1006695.g007">7c</xref>) remained relatively independent of the frequency under such changes, although their overall variability was clearly reduced in the models featuring double error sampling (contrast the value ranges of <italic>m</italic> and <italic>A</italic> in <xref ref-type="fig" rid="pcbi.1006695.g006">Fig 6</xref>, against the corresponding ones in <xref ref-type="fig" rid="pcbi.1006695.g007">Fig 7</xref>, aside from model <italic>KmDG</italic> in which <italic>A</italic> = 1; see also corresponding entries in <xref ref-type="supplementary-material" rid="pcbi.1006695.s001">S1 Table</xref>). ANOVAs run over these parameters further confirmed non-significance of the frequency except for model <italic>KmDG</italic> on <italic>m</italic> in dataset ORIG (<xref ref-type="supplementary-material" rid="pcbi.1006695.s002">S2 Table</xref>). Type of adaptation occasionally modulated <italic>A</italic> in dataset FREQ in models with a single error term. Taken altogether this suggests that both <italic>A</italic> and <italic>m</italic> may be largely frequency independent and can be modeled as constant values maybe differing in value for Two-way and Global types.</p>
<p>In summary, introducing a second error term increased the magnitude of both learning rates (<italic>K</italic> and <italic>D</italic>) by an order of magnitude with opposing signs. The learning rates of these models showed a clear dependence on the frequency of the disturbance: higher stimulus consistency (i.e., lower stimulus frequencies) correlated with higher adaptation efficiency. At the same time, the inclusion of the double error sampling reduced variability in the estimates of the persistence rate <italic>A</italic> and the drift parameter <italic>m</italic>, indicating that their estimates were not affected by the ISS frequency, and could thus be set to appropriate constant values.</p>
</sec>
<sec id="sec024">
<title>Relation to previous work on sensorimotor control and adaptation</title>
<p>Multiple distinct learning processes contribute to sensorimotor adaptation [<xref ref-type="bibr" rid="pcbi.1006695.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref064">64</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref066">66</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref071">71</xref>]. Recent research conducted primarily within adaptation to visuomotor rotations or in reaching movements, suggests that adaptation can be decomposed into two fundamental processes that may operate in parallel: one that would be responsible for implicit learning that progresses slowly and can be described mechanistically by a state-equation [<xref ref-type="bibr" rid="pcbi.1006695.ref049">49</xref>]. This slow learning process is relatively stable over breaks, takes place with automatic, reflex-like behavior and its properties tend to be sturdy and do not change fast with recent experience. A second, parallel process, in turn, learns explicitly, is faster although it may require longer reaction time and possibly voluntary behavior to be engaged. This faster process would exhibit longer term memory of prior learning [<xref ref-type="bibr" rid="pcbi.1006695.ref071">71</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref074">74</xref>].</p>
<p>We believe that our paradigm taps only the first, implicit component. Yet, we suggest that our analyses provide evidence for two separable subcomponents, although both would be intrinsic in nature [<xref ref-type="bibr" rid="pcbi.1006695.ref075">75</xref>]. In fact, a key difference between our oculomotor learning and learning that occurs in adaptation to visuomotor rotations and during reaching in force fields is that our participants were primarily unaware of the inducing disturbance. In contrast, in the aforementioned paradigms, participants immediately notice a disturbance even when they may not be fully aware of the exact effect. In this sense, our paradigm could be considered qualitatively closer to that used by Cheng and Sabes [<xref ref-type="bibr" rid="pcbi.1006695.ref022">22</xref>] who studied calibration of visually guided reaching in participants fully unaware of the stimulus manipulation. Their paradigm used a random, time-varying sequence of feedback shifts. They found that a linear dynamical system (LDS) with a single error term and trial-by-trial state update for variability implemented with an estimation-maximization algorithm successfully described mean reach point and the temporal correlation between reaching errors and visual shifts. They further argued that the learning taking place under a random stimulus generalizes to a situation of constant shifts in a block paradigm and, therefore, that adaptation dynamics does not rely on the sequence (or correlation) of feedback shifts but can be generally described with the LDS model. In contrast to random or block constant ISS, our paradigm featured a disturbance that was fully self-correlated since it followed a sine variation with the trial number. Therefore, it may prove advantageous for the oculomotor system to extract correlations embedded in the disturbance because they would help tracking the target. As pointed out, including double error sampling would serve this purpose.</p>
<p>We believe that the presence of a systematically varying disturbance enables a further decomposition of the implicit component of adaptation, perhaps into a primary one, that attempts to mitigate the end-point discrepancy regardless of self-correlations in the disturbance, and a second one that attempts to extract (and use) such correlations. It remains an open question how these putative subprocesses may map on distinct or overlapping anatomical structures, such as cerebellar cortices, deep cerebellar nuclei and extracerebellar structures [<xref ref-type="bibr" rid="pcbi.1006695.ref055">55</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref057">57</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref059">59</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref060">60</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref064">64</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref076">76</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref080">80</xref>].</p>
<p>A recent study suggested that learning in dynamic environments may be adequately modeled with an algorithm popular in industrial process control, the proportional-integral-derivative (PID) controller [<xref ref-type="bibr" rid="pcbi.1006695.ref081">81</xref>]. The algorithm generates a control signal adding three error-related contributions: a term proportional to the current error that resembles a usual delta-rule (the P-term), a term that integrates over a history of errors experienced before the current one, and a derivative term estimated from the difference between the last two errors. The model shares some features with ours, in particular that the learning rate for the next-to-last error needs to be negative to approximate the derivative term. The PID controller acts on the actual recorded errors (the equivalent of the visual errors observed after each saccade is executed) and contains no internal state estimation, whereas our model operates on an internal variable that contains the state estimation of the prediction error that would result from the movement execution. Our state variable in fact accumulates and retains a substantial portion of the history of previous error (the persistence term in <xref ref-type="disp-formula" rid="pcbi.1006695.e011">Eq 5</xref>, see also the example given in <xref ref-type="supplementary-material" rid="pcbi.1006695.s003">S1 Appendix</xref>), which is updated on a trial-by-trial basis by the term proportional to the latest prediction error (the delta-rule term). The inclusion of an extra error in our state-equation (specifically that of the previous to last one) effectively brings into play a contribution similar to the derivative term of the PID model. In short, our <italic>D</italic>-term enables a systematic correction to the integral term (our <italic>A</italic>-term) that otherwise would be determined rigidly by the iteration of the equation. In that respect, keeping track of former errors enables a structural correction that acts at a global level even when it is introduced on a trial-by-trial basis, lending both robustness and flexibility to the algorithm. Ritz and collaborators [<xref ref-type="bibr" rid="pcbi.1006695.ref081">81</xref>] further compared the performance of the PID model to a Kalman filter used to update a state variable in presence of noise applied on the single error structure of the usual delta-rule and found that the PID controller performs better. A further similarity with the aforementioned work lies in their observation that models with a derivative term are usually not readily selected under statistical model selection even when they may display significant improvement in the description of the behavior (see [<xref ref-type="bibr" rid="pcbi.1006695.ref081">81</xref>] for a longer discussion on this point).</p>
</sec>
</sec>
<sec id="sec025" sec-type="conclusions">
<title>Conclusions</title>
<p>Having adequate generative models that describe eye movements have been stressed before [<xref ref-type="bibr" rid="pcbi.1006695.ref080">80</xref>,<xref ref-type="bibr" rid="pcbi.1006695.ref082">82</xref>–<xref ref-type="bibr" rid="pcbi.1006695.ref086">86</xref>] as an important tool to assess, at the single patient level, a variety of movement abnormalities that have been identified as markers of neurological conditions or disorders at a group level. In this study, elaborating on the idea of tracking a memory of errors [<xref ref-type="bibr" rid="pcbi.1006695.ref065">65</xref>], we attempted to identify and constrain a relatively minimal set of requirements that would suffice to model saccade adaptation data collected under the paradigm and stimulus that we recently implemented [<xref ref-type="bibr" rid="pcbi.1006695.ref011">11</xref>] but that would also include previous accounts of the phenomenon under other known paradigms. While certainly many refinements are still due, we unveiled features of an algorithm that seems suitable to account for the sensorimotor learning observed in our data. We hope it can be generalized, extended and adapted for use in future research.</p>
</sec>
<sec id="sec026">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006695.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Generative parameters fitted with the best four models.</title>
<p>Model name is shown at the top. The corresponding datasets can be identified by the stimulus frequencies tested: ORIG: 3, 4 and 6cpb. FREQ: 1, 3, 6, 12 and 24cpb.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006695.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.s002" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>ANOVA results on the generative parameters fitted with the best four models.</title>
<p>Repeated-measures ANOVA (2 X 3 on data from ORIG; 2 X 5 on data from FREQ) with factors type of adaptation and stimulus frequency was run on each of the four best models. Model name is shown at the side of the table and parameter names are on the top. The dataset is indicated in the cell at the upper left corner next the the parameter names. Highlights indicate the cases where the corresponding factor shows significant effects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006695.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006695.s003" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Predictions of a state-equation with a sigle, most-recent error-based correction term. Effects of including next-to-last error-sampling.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Thérèse Collins and members of the Rolfs lab for insightful discussions and help with data collection.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006695.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McLaughlin</surname> <given-names>SC</given-names></name>. <article-title>Parametric adjustment in saccadic eye movements</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1967</year>;<volume>2</volume>: <fpage>359</fpage>–<lpage>362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03210071" xlink:type="simple">10.3758/BF03210071</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopp</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Fuchs</surname> <given-names>AF</given-names></name>. <article-title>The characteristics and neuronal substrate of saccadic eye movement plasticity</article-title>. <source>Prog Neurobiol</source>. <year>2004</year>;<volume>72</volume>: <fpage>27</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2003.12.002" xlink:type="simple">10.1016/j.pneurobio.2003.12.002</ext-link></comment> <object-id pub-id-type="pmid">15019175</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herman</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Cloud</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Wallman</surname> <given-names>J</given-names></name>. <article-title>End-point variability is not noise in saccade adaptation</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>: <fpage>e59731</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0059731" xlink:type="simple">10.1371/journal.pone.0059731</ext-link></comment> <object-id pub-id-type="pmid">23555763</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref004"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Harwood MR, Wallman J. Temporal dynamics and strategy in saccade adaptation. Program No 99011 Neuroscience Meeting Planner San Diego, CA: Society for Neuroscience, Online 2004.</mixed-citation></ref>
<ref id="pcbi.1006695.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Heed</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Röder</surname> <given-names>B</given-names></name>. <article-title>Eye-movement-driven changes in the perception of auditory space</article-title>. <source>Atten Percept Psychophys</source>. <year>2010</year>;<volume>72</volume>: <fpage>736</fpage>–<lpage>746</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/APP.72.3.736" xlink:type="simple">10.3758/APP.72.3.736</ext-link></comment> <object-id pub-id-type="pmid">20348579</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Azadi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Harwood</surname> <given-names>MR</given-names></name>. <article-title>Visual cues that are effective for contextual saccade adaptation</article-title>. <source>Journal of Neurophysiology</source>. <year>2014</year>;<volume>111</volume>: <fpage>2307</fpage>–<lpage>2319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00894.2013" xlink:type="simple">10.1152/jn.00894.2013</ext-link></comment> <object-id pub-id-type="pmid">24647429</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen-Harris</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Joiner</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Ethier</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Zee</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Adaptive control of saccades via internal feedback</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>: <fpage>2804</fpage>–<lpage>2813</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5300-07.2008" xlink:type="simple">10.1523/JNEUROSCI.5300-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18337410</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ethier</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Zee</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Spontaneous recovery of motor memory during saccade adaptation</article-title>. <source>Journal of Neurophysiology</source>. <year>2008</year>;<volume>99</volume>: <fpage>2577</fpage>–<lpage>2583</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00015.2008" xlink:type="simple">10.1152/jn.00015.2008</ext-link></comment> <object-id pub-id-type="pmid">18353917</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bridgeman</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hendry</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stark</surname> <given-names>L</given-names></name>. <article-title>Failure to detect displacement of the visual world during saccadic eye movements</article-title>. <source>Vision Research</source>. <year>1975</year>;<volume>15</volume>: <fpage>719</fpage>–<lpage>722</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0042-6989(75)90290-4" xlink:type="simple">10.1016/0042-6989(75)90290-4</ext-link></comment> <object-id pub-id-type="pmid">1138489</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Noto</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Bevans</surname> <given-names>SE</given-names></name>. <article-title>Effect of Visual Error Size on Saccade Adaptation in Monkey</article-title>. <source>Journal of Neurophysiology. American Physiological Society</source>; <year>2003</year>;<volume>90</volume>: <fpage>1235</fpage>–<lpage>1244</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00656.2002" xlink:type="simple">10.1152/jn.00656.2002</ext-link></comment> <object-id pub-id-type="pmid">12711711</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cassanello</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Ohl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>. <article-title>Saccadic adaptation to a systematically varying disturbance</article-title>. <source>Journal of Neurophysiology</source>. <year>2016</year>;<volume>116</volume>: <fpage>336</fpage>–<lpage>350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00206.2016" xlink:type="simple">10.1152/jn.00206.2016</ext-link></comment> <object-id pub-id-type="pmid">27098027</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cassanello</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ohl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>. <article-title>Saccadic plasticity induced by a periodic disturbance of visual feedback</article-title>. <source>Journal of Vision. The Association for Research in Vision and Ophthalmology</source>; <year>2014</year>;<volume>14</volume>: <fpage>740</fpage>–<lpage>740</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.10.740" xlink:type="simple">10.1167/14.10.740</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hudson</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Measuring adaptation with a sinusoidal perturbation function</article-title>. <source>Journal of Neuroscience Methods</source>. <year>2012</year>;<volume>208</volume>: <fpage>48</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2012.04.001" xlink:type="simple">10.1016/j.jneumeth.2012.04.001</ext-link></comment> <object-id pub-id-type="pmid">22565135</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gray</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Blangero</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Herman</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Wallman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Harwood</surname> <given-names>MR</given-names></name>. <article-title>Adaptation of naturally paced saccades</article-title>. <source>Journal of Neurophysiology</source>. <year>2014</year>;<volume>111</volume>: <fpage>2343</fpage>–<lpage>2354</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00905.2013" xlink:type="simple">10.1152/jn.00905.2013</ext-link></comment> <object-id pub-id-type="pmid">24623511</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baddeley</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Ingram</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>. <article-title>System identification applied to a visuomotor task: near-optimal human performance in a noisy changing task</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>: <fpage>3066</fpage>–<lpage>3075</lpage>. <object-id pub-id-type="pmid">12684493</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Heasly</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>: <fpage>12366</fpage>–<lpage>12378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0822-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0822-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20844132</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref017"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Rescorla RA, Wagner AR. A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforment. Prokasy WF, editor. Classical conditioning II: Current research and theory; 1972.</mixed-citation></ref>
<ref id="pcbi.1006695.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Knapen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Cavanagh</surname> <given-names>P</given-names></name>. <article-title>Global saccadic adaptation</article-title>. <source>Vision Research. Elsevier Ltd</source>; <year>2010</year>;<volume>50</volume>: <fpage>1882</fpage>–<lpage>1890</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2010.06.010" xlink:type="simple">10.1016/j.visres.2010.06.010</ext-link></comment> <object-id pub-id-type="pmid">20600235</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srimal</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ryklin</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Curtis</surname> <given-names>CE</given-names></name>. <article-title>Obligatory Adaptation of Saccade Gains</article-title>. <source>Journal of Neurophysiology</source>. <year>2008</year>;<volume>99</volume>: <fpage>1554</fpage>–<lpage>1558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01024.2007" xlink:type="simple">10.1152/jn.01024.2007</ext-link></comment> <object-id pub-id-type="pmid">18234985</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donchin</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>: <fpage>9032</fpage>–<lpage>9045</lpage>. <object-id pub-id-type="pmid">14534237</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cheng</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sabes</surname> <given-names>PN</given-names></name>. <article-title>Modeling sensorimotor learning with linear dynamical systems</article-title>. <source>Neural Computation</source>. <year>2006</year>;<volume>18</volume>: <fpage>760</fpage>–<lpage>793</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976606775774651" xlink:type="simple">10.1162/089976606775774651</ext-link></comment> <object-id pub-id-type="pmid">16494690</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cheng</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sabes</surname> <given-names>PN</given-names></name>. <article-title>Calibration of visually guided reaching is driven by error-corrective learning and internal dynamics</article-title>. <source>Journal of Neurophysiology. American Physiological Society</source>; <year>2007</year>;<volume>97</volume>: <fpage>3057</fpage>–<lpage>3069</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00897.2006" xlink:type="simple">10.1152/jn.00897.2006</ext-link></comment> <object-id pub-id-type="pmid">17202230</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Porr</surname> <given-names>B</given-names></name>. <article-title>Temporal sequence learning, prediction, and control: a review of different models and their relation to biological mechanisms</article-title>. <source>Neural Computation</source>. <year>2005</year>;<volume>17</volume>: <fpage>245</fpage>–<lpage>319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0899766053011555" xlink:type="simple">10.1162/0899766053011555</ext-link></comment> <object-id pub-id-type="pmid">15720770</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hashambhoy</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rane</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Neural correlates of reach errors</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>9919</fpage>–<lpage>9931</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1874-05.2005" xlink:type="simple">10.1523/JNEUROSCI.1874-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16251440</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Ghazizadeh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Interacting adaptive processes with different timescales underlie short-term motor learning</article-title>. <source>PLoS Biol</source>. <year>2006</year>;<volume>4</volume>: <fpage>1035</fpage>–<lpage>1043</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0040179" xlink:type="simple">10.1371/journal.pbio.0040179</ext-link></comment> <object-id pub-id-type="pmid">16700627</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vaswani</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Decay of motor memories in the absence of error</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>: <fpage>7700</fpage>–<lpage>7709</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0124-13.2013" xlink:type="simple">10.1523/JNEUROSCI.0124-13.2013</ext-link></comment> <object-id pub-id-type="pmid">23637163</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vaswani</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Shmuelof</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Delnicki</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Persistent residual errors in motor adaptation tasks: reversion to baseline and exploratory escape</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>6969</fpage>–<lpage>6977</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2656-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2656-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25926471</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name>. <article-title>Bayes factors</article-title>. <source>Journal of the American Statistical Association</source>. <year>1995</year>;<volume>9</volume>: <fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1995.10476572" xlink:type="simple">10.1080/01621459.1995.10476572</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jeffreys</surname> <given-names>H</given-names></name>. <article-title>An Invariant Form for the Prior Probability in Estimation Problems</article-title>. <source>Proc R Soc Lond A Math Phys Sci. The Royal Society</source>; <year>1946</year>;<volume>186</volume>: <fpage>453</fpage>–<lpage>461</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/97883?ref=no-x-route:2f182c5222c53409df89751cc556fb6f" xlink:type="simple">10.2307/97883?ref=no-x-route:2f182c5222c53409df89751cc556fb6f</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>The statistical determinants of adaptation rate in human reaching</article-title>. <source>Journal of Vision</source>. <year>2008</year>;<volume>8</volume>: <fpage>20.1</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/8.4.20" xlink:type="simple">10.1167/8.4.20</ext-link></comment> <object-id pub-id-type="pmid">18484859</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thoroughman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Learning of action through adaptive combination of motor primitives</article-title>. <source>Nature. Nature Publishing Group</source>; <year>2000</year>;<volume>407</volume>: <fpage>742</fpage>–<lpage>747</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35037588" xlink:type="simple">10.1038/35037588</ext-link></comment> <object-id pub-id-type="pmid">11048720</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zarahn</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Weston</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Explaining savings for visuomotor adaptation: linear time-invariant state-space models are not sufficient</article-title>. <source>Journal of Neurophysiology. American Physiological Society</source>; <year>2008</year>;<volume>100</volume>: <fpage>2537</fpage>–<lpage>2548</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.90529.2008" xlink:type="simple">10.1152/jn.90529.2008</ext-link></comment> <object-id pub-id-type="pmid">18596178</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>. <article-title>Internal models for motor control and trajectory planning</article-title>. <source>Current Opinion in Neurobiology</source>. <year>1999</year>;<volume>9</volume>: <fpage>718</fpage>–<lpage>727</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/s0959-4388(99)00028-8" xlink:type="simple">10.1016/s0959-4388(99)00028-8</ext-link></comment> <object-id pub-id-type="pmid">10607637</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>. <article-title>Internal models in the cerebellum</article-title>. <source>Trends Cogn Sci (Regul Ed)</source>. <year>1998</year>;<volume>2</volume>: <fpage>338</fpage>–<lpage>347</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1364-6613(98)01221-2" xlink:type="simple">10.1016/S1364-6613(98)01221-2</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>. <article-title>Forward Models: Supervised Learning with a Distal Teacher</article-title>. <source>Cognitive Science</source>. <year>1992</year>;<volume>16</volume>: <fpage>307</fpage>–<lpage>354</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1207/s15516709cog1603_1" xlink:type="simple">10.1207/s15516709cog1603_1</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Kooij</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>van Beers</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Smeets</surname> <given-names>JBJ</given-names></name>. <article-title>Visuomotor adaptation: how forgetting keeps us conservative</article-title>. <name name-style="western"><surname>Balasubramaniam</surname> <given-names>R</given-names></name>, editor. <source>PLoS ONE</source>. <year>2015</year>;<volume>10</volume>: <fpage>e0117901</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0117901" xlink:type="simple">10.1371/journal.pone.0117901</ext-link></comment> <object-id pub-id-type="pmid">25723763</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cameron</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>la Malla de</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>López-Moliner</surname> <given-names>J</given-names></name>. <article-title>Why do movements drift in the dark? Passive versus active mechanisms of error accumulation</article-title>. <source>Journal of Neurophysiology</source>. <year>2015</year>;<volume>114</volume>: <fpage>390</fpage>–<lpage>399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00032.2015" xlink:type="simple">10.1152/jn.00032.2015</ext-link></comment> <object-id pub-id-type="pmid">25925317</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>An information criterion (AIC)</article-title>. <source>Math Sci. Math Sci</source>; <year>1976</year>;<volume>14</volume>: <fpage>5</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006695.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>A new look at the Bayes procedure</article-title>. <source>Biometrika</source>. <year>1978</year>;<volume>65</volume>: <fpage>53</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/65.1.53" xlink:type="simple">10.1093/biomet/65.1.53</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burnham</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>DR</given-names></name>. <article-title>Multimodel inference: Understanding AIC and BIC in model selection</article-title>. <source>Sociological Methods and Research</source>. <year>2004</year>;<volume>33</volume>: <fpage>261</fpage>–<lpage>304</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0049124104268644" xlink:type="simple">10.1177/0049124104268644</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bozdogan</surname> <given-names>H</given-names></name>. <article-title>Model selection and Akaike’s Information Criterion (AIC): The general theory and its analytical extensions</article-title>. <source>Psychometrika. Second Edition. Springer-Verlag</source>; <year>1987</year>;<volume>52</volume>: <fpage>345</fpage>–<lpage>370</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02294361" xlink:type="simple">10.1007/BF02294361</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref042"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Proakis JG, Manolakis DG. Digital Signal Processing. 2007.</mixed-citation></ref>
<ref id="pcbi.1006695.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cassanello</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ostendorf</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>. <article-title>Oculomotor entraining and persistent baseline drift in saccadic adaptation to a sinusoidal disturbance</article-title>. <source>Journal of Vision. The Association for Research in Vision and Ophthalmology</source>; <year>2016</year>;<volume>16</volume>: <fpage>379</fpage>–<lpage>379</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/16.12.379" xlink:type="simple">10.1167/16.12.379</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cassanello</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ostendorf</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>. <article-title>State-equation learning model for saccade adaptation</article-title>. <source>Journal of Vision. The Association for Research in Vision and Ophthalmology</source>; <year>2017</year>;<volume>17</volume>: <fpage>1142</fpage>–<lpage>1142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/17.10.1142" xlink:type="simple">10.1167/17.10.1142</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>. <article-title>Estimating the dimension of a model</article-title>. <source>Ann Statist. Institute of Mathematical Statistics</source>; <year>1978</year>;<volume>6</volume>: <fpage>461</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2958889?ref=search-gateway:b77d54fcff1a48c3e08bcd0ba4ebc395" xlink:type="simple">10.2307/2958889?ref=search-gateway:b77d54fcff1a48c3e08bcd0ba4ebc395</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelisson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Alahyane</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Panouillères</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tilikete</surname> <given-names>C</given-names></name>. <article-title>Sensorimotor adaptation of saccadic eye movements</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2010</year>;<volume>34</volume>: <fpage>1103</fpage>–<lpage>1120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2009.12.010" xlink:type="simple">10.1016/j.neubiorev.2009.12.010</ext-link></comment> <object-id pub-id-type="pmid">20026351</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baddeley</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Ingram</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>. <article-title>System identification applied to a visuomotor task: near-optimal human performance in a noisy changing task</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>: <fpage>3066</fpage>–<lpage>3075</lpage>. <object-id pub-id-type="pmid">12684493</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Beers</surname> <given-names>RJ</given-names></name>. <article-title>How Does Our Motor System Determine Its Learning Rate?</article-title> <source>PLoS ONE</source>. <year>2012</year>;<volume>7</volume>: <fpage>e49373</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0049373" xlink:type="simple">10.1371/journal.pone.0049373</ext-link></comment> <object-id pub-id-type="pmid">23152899</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Error correction, sensory prediction, and adaptation in motor control</article-title>. <source>Annu Rev Neurosci</source>. <year>2010</year>;<volume>33</volume>: <fpage>89</fpage>–<lpage>108</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-060909-153135" xlink:type="simple">10.1146/annurev-neuro-060909-153135</ext-link></comment> <object-id pub-id-type="pmid">20367317</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Becker</surname> <given-names>W</given-names></name>. <article-title>The neurobiology of saccadic eye movements</article-title>. <source>Metrics. Reviews of Oculomotor Research</source>. <year>1989</year>;<volume>3</volume>: <fpage>13</fpage>–<lpage>67</lpage>. <object-id pub-id-type="pmid">2486323</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henson</surname> <given-names>DB</given-names></name>. <article-title>Corrective saccades: Effects of altering visual feedback</article-title>. <source>Vision Research</source>. <year>1978</year>;<volume>18</volume>: <fpage>63</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0042-6989(78)90078-0" xlink:type="simple">10.1016/0042-6989(78)90078-0</ext-link></comment> <object-id pub-id-type="pmid">664277</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Shelhamer</surname> <given-names>M</given-names></name>. <article-title>Sensorimotor adaptation error signals are derived from realistic predictions of movement outcomes</article-title>. <source>Journal of Neurophysiology</source>. <year>2011</year>;<volume>105</volume>: <fpage>1130</fpage>–<lpage>1140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00394.2010" xlink:type="simple">10.1152/jn.00394.2010</ext-link></comment> <object-id pub-id-type="pmid">21123665</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Deubel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cavanagh</surname> <given-names>P</given-names></name>. <article-title>Post-saccadic location judgments reveal remapping of saccade targets to non-foveal locations</article-title>. <source>Journal of Vision</source>. <year>2009</year>;<volume>9</volume>: <fpage>29.1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.5.29" xlink:type="simple">10.1167/9.5.29</ext-link></comment> <object-id pub-id-type="pmid">19757907</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Golla</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tziridis</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Haarmeier</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Catz</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Barash</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thier</surname> <given-names>P</given-names></name>. <article-title>Reduced saccadic resilience and impaired saccadic adaptation due to cerebellar disease</article-title>. <source>Eur J Neurosci</source>. <year>2008</year>;<volume>27</volume>: <fpage>132</fpage>–<lpage>144</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2007.05996.x" xlink:type="simple">10.1111/j.1460-9568.2007.05996.x</ext-link></comment> <object-id pub-id-type="pmid">18184318</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herzfeld</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Kojima</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Soetedjo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Encoding of action by the Purkinje cells of the cerebellum</article-title>. <source>Nature</source>. <year>2015</year>;<volume>526</volume>: <fpage>439</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature15693" xlink:type="simple">10.1038/nature15693</ext-link></comment> <object-id pub-id-type="pmid">26469054</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Herzfeld DJ, Kojima Y, Soetedjo R, Shadmehr R. Sensory prediction errors during saccade adaptation drive cerebellar complex spikes and learning. Program No 715.16 Neuroscience Planner, San Diego, CA: Society for Neuroscience, Online, 2016. pp. 1–1.</mixed-citation></ref>
<ref id="pcbi.1006695.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herzfeld</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Kojima</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Soetedjo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Encoding of error and learning to correct that error by the Purkinje cells of the cerebellum</article-title>. <source>Nat Neurosci. Nature Publishing Group</source>; <year>2018</year>;<volume>21</volume>: <fpage>736</fpage>–<lpage>743</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-018-0136-y" xlink:type="simple">10.1038/s41593-018-0136-y</ext-link></comment> <object-id pub-id-type="pmid">29662213</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Junker</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Endres</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>ZP</given-names></name>, <name name-style="western"><surname>Dicke</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Giese</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Thier</surname> <given-names>P</given-names></name>. <article-title>Learning from the past: A reverberation of past errors in the cerebellar climbing fiber signal</article-title>. <source>PLoS Biol</source>. <year>2018</year>;<volume>16</volume>: <fpage>e2004344</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.2004344" xlink:type="simple">10.1371/journal.pbio.2004344</ext-link></comment> <object-id pub-id-type="pmid">30067764</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rössert</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Porrill</surname> <given-names>J</given-names></name>. <article-title>At the Edge of Chaos: How Cerebellar Granular Layer Network Dynamics Can Provide the Basis for Temporal Filters. Graham LJ, editor</article-title>. <source>PLoS Comput Biol. Public Library of Science</source>; <year>2015</year>;<volume>11</volume>: <fpage>e1004515</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004515" xlink:type="simple">10.1371/journal.pcbi.1004515</ext-link></comment> <object-id pub-id-type="pmid">26484859</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalidindi</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>George Thuruthel</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Laschi</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Falotico</surname> <given-names>E</given-names></name>. <article-title>Modeling the Encoding of Saccade Kinematic Metrics in the Purkinje Cell Layer of the Cerebellar Vermis</article-title>. <source>Front Comput Neurosci</source>. <year>2019</year>;<volume>12</volume>: <fpage>293</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2018.00108" xlink:type="simple">10.3389/fncom.2018.00108</ext-link></comment> <object-id pub-id-type="pmid">30687055</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Weir</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>JF</given-names></name>. <article-title>Is the cerebellum a smith predictor?</article-title> <source>J Mot Behav</source>. <year>1993</year>;<volume>25</volume>: <fpage>203</fpage>–<lpage>216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00222895.1993.9942050" xlink:type="simple">10.1080/00222895.1993.9942050</ext-link></comment> <object-id pub-id-type="pmid">12581990</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Forward models for physiological motor control</article-title>. <source>Neural Networks</source>. <year>1996</year>;<volume>9</volume>: <fpage>1265</fpage>–<lpage>1279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(96)00035-4" xlink:type="simple">10.1016/S0893-6080(96)00035-4</ext-link></comment> <object-id pub-id-type="pmid">12662535</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref063"><label>63</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Miall</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <chapter-title>The Cerebellum as a Predictive Model of the Motor System: A Smith Predictor Hypothesis</chapter-title>. <source>Neural Control of Movement</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer</publisher-name>, Boston, MA; <year>1995</year>. pp. <fpage>215</fpage>–<lpage>223</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006695.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kojima</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Iwamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yoshida</surname> <given-names>K</given-names></name>. <article-title>Memory of Learning Facilitates Saccadic Adaptation in the Monkey</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>: <fpage>7531</fpage>–<lpage>7539</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1741-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1741-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15329400</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herzfeld</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Vaswani</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Marko</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>A memory of errors in sensorimotor learning</article-title>. <source>Science</source>. <year>2014</year>;<volume>345</volume>: <fpage>1349</fpage>–<lpage>1353</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1253138" xlink:type="simple">10.1126/science.1253138</ext-link></comment> <object-id pub-id-type="pmid">25123484</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>The dynamics of memory as a consequence of optimal adaptation to a changing body</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>779</fpage>–<lpage>786</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1901" xlink:type="simple">10.1038/nn1901</ext-link></comment> <object-id pub-id-type="pmid">17496891</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Soetedjo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Noto</surname> <given-names>C</given-names></name>. <article-title>Distinct Short-Term and Long-Term Adaptation to Reduce Saccade Size in Monkey</article-title>. <source>Journal of Neurophysiology</source>. <year>2006</year>;<volume>96</volume>: <fpage>1030</fpage>–<lpage>1041</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01151.2005" xlink:type="simple">10.1152/jn.01151.2005</ext-link></comment> <object-id pub-id-type="pmid">16672299</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Havermann</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Lappe</surname> <given-names>M</given-names></name>. <article-title>The influence of the consistency of postsaccadic visual errors on saccadic adaptation</article-title>. <source>Journal of Neurophysiology</source>. <year>2010</year>;<volume>103</volume>: <fpage>3302</fpage>–<lpage>3310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00970.2009" xlink:type="simple">10.1152/jn.00970.2009</ext-link></comment> <object-id pub-id-type="pmid">20393067</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gonzalez Castro</surname> <given-names>LN</given-names></name>, <name name-style="western"><surname>Hadjiosif</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Hemphill</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>. <article-title>Environmental consistency determines the rate of motor adaptation</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>: <fpage>1050</fpage>–<lpage>1061</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.03.049" xlink:type="simple">10.1016/j.cub.2014.03.049</ext-link></comment> <object-id pub-id-type="pmid">24794296</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Shelhamer</surname> <given-names>M</given-names></name>. <article-title>Saccade adaptation improves in response to a gradually introduced stimulus perturbation</article-title>. <source>Neurosci Lett</source>. <year>2011</year>;<volume>500</volume>: <fpage>207</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neulet.2011.06.039" xlink:type="simple">10.1016/j.neulet.2011.06.039</ext-link></comment> <object-id pub-id-type="pmid">21741440</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huberdeau</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>AM</given-names></name>. <article-title>Dual-process decomposition in human sensorimotor adaptation</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2015</year>;<volume>33</volume>: <fpage>71</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2015.03.003" xlink:type="simple">10.1016/j.conb.2015.03.003</ext-link></comment> <object-id pub-id-type="pmid">25827272</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Haith</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Rethinking Motor Learning and Savings in Adaptation Paradigms: Model-Free Memory for Successful Actions Combines with Internal Models</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>70</volume>: <fpage>787</fpage>–<lpage>801</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.04.012" xlink:type="simple">10.1016/j.neuron.2011.04.012</ext-link></comment> <object-id pub-id-type="pmid">21609832</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>An implicit plan overrides an explicit strategy during visuomotor adaptation</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>3642</fpage>–<lpage>3645</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5317-05.2006" xlink:type="simple">10.1523/JNEUROSCI.5317-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16597717</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haith</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Krakauer</surname> <given-names>JW</given-names></name>. <article-title>Model-Based and Model-Free Mechanisms of Human Motor Learning</article-title>. <source>Advances in Experimental Medicine and Biology</source>. <year>2013</year>. pp. <fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-1-4614-5465-6_1" xlink:type="simple">10.1007/978-1-4614-5465-6_1</ext-link></comment> <object-id pub-id-type="pmid">23296478</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chaisanguanthum</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Sabes</surname> <given-names>PN</given-names></name>. <article-title>Motor variability arises from a slow random walk in neural state</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>12071</fpage>–<lpage>12080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3001-13.2014" xlink:type="simple">10.1523/JNEUROSCI.3001-13.2014</ext-link></comment> <object-id pub-id-type="pmid">25186752</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kojima</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Soetedjo</surname> <given-names>R</given-names></name>. <article-title>Elimination of the error signal in the superior colliculus impairs saccade motor learning</article-title>. <source>Proc Natl Acad Sci USA. National Academy of Sciences</source>; <year>2018</year>;<volume>115</volume>: <fpage>E8987</fpage>–<lpage>E8995</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1806215115" xlink:type="simple">10.1073/pnas.1806215115</ext-link></comment> <object-id pub-id-type="pmid">30185563</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu-Wilson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chen-Harris</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zee</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Shadmehr</surname> <given-names>R</given-names></name>. <article-title>Cerebellar Contributions to Adaptive Control of Saccades in Humans</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>: <fpage>12930</fpage>–<lpage>12939</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3115-09.2009" xlink:type="simple">10.1523/JNEUROSCI.3115-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19828807</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ritz</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>. <article-title>A Control Theoretic Model of Adaptive Learning in Dynamic Environments</article-title>. <source>J Cogn Neurosci</source>. <year>2018</year>;: <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01289" xlink:type="simple">10.1162/jocn_a_01289</ext-link></comment> <object-id pub-id-type="pmid">29877769</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dash</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thier</surname> <given-names>P</given-names></name>. <article-title>Cerebellum-Dependent Motor Learning: Lessons from Adaptation of Eye Movements in Primates</article-title>. <source>Prog Brain Res. Elsevier</source>; <year>2014</year>;<volume>210</volume>: <fpage>121</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/B978-0-444-63356-9.00006-6" xlink:type="simple">10.1016/B978-0-444-63356-9.00006-6</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006695.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Guigon</surname> <given-names>E</given-names></name>. <article-title>A Model of Reward- and Effort-Based Optimal Decision Making and Motor Control</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002716" xlink:type="simple">10.1371/journal.pcbi.1002716</ext-link></comment> <object-id pub-id-type="pmid">23055916</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ritz</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>. <article-title>A Control Theoretic Model of Adaptive Learning in Dynamic Environments</article-title>. <source>J Cogn Neurosci</source>. <year>2018</year>;: <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01289" xlink:type="simple">10.1162/jocn_a_01289</ext-link></comment> <object-id pub-id-type="pmid">29877769</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thakkar</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Diwadkar</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>. <article-title>Oculomotor Prediction: A Window into the Psychotic Mind</article-title>. <source>Trends Cogn Sci (Regul Ed)</source>. <year>2017</year>;<volume>21</volume>: <fpage>344</fpage>–<lpage>356</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2017.02.001" xlink:type="simple">10.1016/j.tics.2017.02.001</ext-link></comment> <object-id pub-id-type="pmid">28292639</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rösler</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Rolfs</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>van der Stigchel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Neggers</surname> <given-names>SFW</given-names></name>, <name name-style="western"><surname>Cahn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kahn</surname> <given-names>RS</given-names></name>, <etal>et al</etal>. <article-title>Failure to use corollary discharge to remap visual target locations is associated with psychotic symptom severity in schizophrenia</article-title>. <source>Journal of Neurophysiology. American Physiological Society</source>; <year>2015</year>;<volume>114</volume>: <fpage>1129</fpage>–<lpage>1136</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00155.2015" xlink:type="simple">10.1152/jn.00155.2015</ext-link></comment> <object-id pub-id-type="pmid">26108951</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muhammed</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Manohar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ben Yehuda</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>TTJ</given-names></name>, <name name-style="western"><surname>Tofaris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lennox</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Reward sensitivity deficits modulated by dopamine are associated with apathy in Parkinson’s disease</article-title>. <source>Brain</source>. <year>2016</year>;<volume>139</volume>: <fpage>2706</fpage>–<lpage>2721</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/aww188" xlink:type="simple">10.1093/brain/aww188</ext-link></comment> <object-id pub-id-type="pmid">27452600</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manohar</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>TTJ</given-names></name>, <name name-style="western"><surname>Apps</surname> <given-names>MAJ</given-names></name>, <name name-style="western"><surname>Batla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stamelou</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jarman</surname> <given-names>PR</given-names></name>, <etal>et al</etal>. <article-title>Reward Pays the Cost of Noise Reduction in Motor and Cognitive Control</article-title>. <source>Curr Biol</source>. <year>2015</year>;<volume>25</volume>: <fpage>1707</fpage>–<lpage>1716</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.05.038" xlink:type="simple">10.1016/j.cub.2015.05.038</ext-link></comment> <object-id pub-id-type="pmid">26096975</object-id></mixed-citation></ref>
<ref id="pcbi.1006695.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heinzle</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Aponte</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>. <article-title>Computational models of eye movements and their application to schizophrenia</article-title>. <source>Current Opinion in Behavioral Sciences</source>. <year>2016</year>;<volume>11</volume>: <fpage>21</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cobeha.2016.03.008" xlink:type="simple">10.1016/j.cobeha.2016.03.008</ext-link></comment></mixed-citation></ref>
</ref-list>
</back>
</article>