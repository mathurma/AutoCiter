<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006557</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01532</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>Old World monkeys</subject><subj-group><subject>Macaque</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Representations of regular and irregular shapes by deep Convolutional Neural Networks, monkey inferotemporal neurons and human judgments</article-title>
<alt-title alt-title-type="running-head">Deep neural network models of neural shape responses</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9957-1502</contrib-id>
<name name-style="western">
<surname>Kalfas</surname>
<given-names>Ioannis</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7038-9638</contrib-id>
<name name-style="western">
<surname>Vinken</surname>
<given-names>Kasper</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8778-835X</contrib-id>
<name name-style="western">
<surname>Vogels</surname>
<given-names>Rufin</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Laboratorium voor Neuro- en Psychofysiologie, Department of Neurosciences, KU Leuven, Leuven, Belgium</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Leuven Brain Institute, Leuven, Belgium</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kriegeskorte</surname>
<given-names>Nikolaus</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Medical Research Council, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">rufin.vogels@kuleuven.be</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>26</day>
<month>10</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>10</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>10</issue>
<elocation-id>e1006557</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>9</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Kalfas et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006557"/>
<abstract>
<p>Recent studies suggest that deep Convolutional Neural Network (CNN) models show higher representational similarity, compared to any other existing object recognition models, with macaque inferior temporal (IT) cortical responses, human ventral stream fMRI activations and human object recognition. These studies employed natural images of objects. A long research tradition employed abstract shapes to probe the selectivity of IT neurons. If CNN models provide a realistic model of IT responses, then they should capture the IT selectivity for such shapes. Here, we compare the activations of CNN units to a stimulus set of 2D regular and irregular shapes with the response selectivity of macaque IT neurons and with human similarity judgements. The shape set consisted of regular shapes that differed in nonaccidental properties, and irregular, asymmetrical shapes with curved or straight boundaries. We found that deep CNNs (Alexnet, VGG-16 and VGG-19) that were trained to classify natural images show response modulations to these shapes that were similar to those of IT neurons. Untrained CNNs with the same architecture than trained CNNs, but with random weights, demonstrated a poorer similarity than CNNs trained in classification. The difference between the trained and untrained CNNs emerged at the deep convolutional layers, where the similarity between the shape-related response modulations of IT neurons and the trained CNNs was high. Unlike IT neurons, human similarity judgements of the same shapes correlated best with the last layers of the trained CNNs. In particular, these deepest layers showed an enhanced sensitivity for straight versus curved irregular shapes, similar to that shown in human shape judgments. In conclusion, the representations of abstract shape similarity are highly comparable between macaque IT neurons and deep convolutional layers of CNNs that were trained to classify natural images, while human shape similarity judgments correlate better with the deepest layers.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The primate inferior temporal (IT) cortex is considered to be the final stage of visual processing that allows for object recognition, identification and categorization of objects. Electrophysiology studies suggest that an object’s shape is a strong determinant of the neuronal response patterns in IT. Here we examine whether deep Convolutional Neural Networks (CNNs), that were trained to classify natural images of objects, show response modulations for abstract shapes similar to those of macaque IT neurons. For trained and untrained versions of three state-of-the-art CNNs, we assessed the response modulations for a set of 2D shapes at each of their stages and compared these to those of a population of macaque IT neurons and human shape similarity judgements. We show that an IT-like representation of similarity amongst 2D abstract shapes develops in the deep convolutional CNN layers when these are trained to classify natural images. Our results reveal a high correspondence between the representation of shape similarity of deep trained CNN stages and macaque IT neurons and an analogous correspondence of the last trained CNN stages with shape similarity as judged by humans.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Fonds voor wetenschappelijk onderzoek Vlaanderen</institution>
</funding-source>
<award-id>G.0007.12-Odysseus</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8778-835X</contrib-id>
<name name-style="western">
<surname>Vogels</surname>
<given-names>Rufin</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Fonds voor wetenschappelijk onderzoek Vlaanderen</institution>
</funding-source>
<award-id>G.0932.14N</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8778-835X</contrib-id>
<name name-style="western">
<surname>Vogels</surname>
<given-names>Rufin</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Federaal Wetenschapsbeleid (BELSPO)</institution>
</funding-source>
<award-id>IUAP VII/11</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8778-835X</contrib-id>
<name name-style="western">
<surname>Vogels</surname>
<given-names>Rufin</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>The work was supported by the Fonds voor Wetenschappelijk Onderzoek (FWO) Vlaanderen (<ext-link ext-link-type="uri" xlink:href="http://www.fwo.be" xlink:type="simple">www.fwo.be</ext-link>; grant numbers: G.00007.12-Odysseus; G.0932.14N; RV) and Federaal Wetenschapsbeleid (BELSPO;<ext-link ext-link-type="uri" xlink:href="http://www.belspo.be" xlink:type="simple">www.belspo.be</ext-link>; grant number: IUAP VII/11; RV). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-11-12</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper. Software and neural response matrix is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kalfasyan/Regular_Irregular_ShapeSelectivity" xlink:type="simple">https://github.com/kalfasyan/Regular_Irregular_ShapeSelectivity</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Recently, several studies compared the representations of visual images in deep Convolutional Neural Networks (CNN) with those of biological systems, such as the primate ventral visual stream [<xref ref-type="bibr" rid="pcbi.1006557.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref004">4</xref>]. These studies showed that the representation of visual objects in macaque inferior temporal (IT) cortex corresponds better with the representations of these images in deep CNN layers than with representations of older computational models such as HMAX [<xref ref-type="bibr" rid="pcbi.1006557.ref005">5</xref>]. Similar findings were obtained with human fMRI data [<xref ref-type="bibr" rid="pcbi.1006557.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref010">10</xref>]. The images used in these studies were those of real objects in cluttered scenes, which are the same class of images as those employed to train the deep CNNs for classification. Other single unit studies of IT neurons employed two-dimensional (2D) shapes and observed highly selective responses to such stimuli (for review see [<xref ref-type="bibr" rid="pcbi.1006557.ref011">11</xref>]). If deep CNNs provide a realistic model of IT responses, then the CNNs should capture also the selectivity observed for such two-dimensional shapes in IT. To our knowledge, thus far there has been no comparison between the 2D-shape representation of IT neurons, measured with such reduced stimuli, and that of deep CNN models. It is impossible to predict from existing studies that compared deep CNN activations and neurophysiology whether the deep CNNs, which are trained with natural images, can faithfully model the selectivity of IT neurons for two-dimensional abstract shapes. Nonetheless, such correspondence between CNN models and single unit selectivity for abstract shapes is critical for assessing the generalizability of CNN models to stimuli that differ markedly from those of the trained task but have been shown to drive selectively IT neurons.</p>
<p>Previously, we showed that a linear combination of units of deep convolutional layers of CNNs trained with natural images could predict reasonably well the shape selectivity of single neurons recorded from an fMRI-defined body patch [<xref ref-type="bibr" rid="pcbi.1006557.ref004">4</xref>]. However, in that study, we adapted for each single unit the shapes to the shape preference of that neuron, precluding a comparison between the shape representation of the population of IT neurons and deep CNNs. To perform such a comparison, one should measure the responses of IT neurons to the same set of shapes. Furthermore, the shape set should include variations in shape properties IT neurons were shown to be sensitive to. Also, the IT response selectivities for such shapes should not trivially be explainable by physical image similarities, such as pixel-based differences in graylevels.</p>
<p>Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] measured the responses of single IT neurons to a set of shapes that varied in regularity and the presence of curved versus straight boundaries (<xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>). The first group of stimuli of [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] was composed of regular geometric shapes (shown in the first two rows of <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> and denoted as Regular (R)) that all have at least one axis of symmetry. These shapes are simple, i.e., have low medial axis complexity [<xref ref-type="bibr" rid="pcbi.1006557.ref013">13</xref>]. The stimulus pairs in each column of these two rows (denoted by <italic>a</italic> and <italic>b</italic>) differed in a non-accidental property (NAP). NAPs are stimulus properties that are relatively invariant with orientation in depth, such as whether a contour is straight or curved or whether a pair of edges is parallel or not. These properties can allow efficient object recognition at different orientations in depth not previously experienced [<xref ref-type="bibr" rid="pcbi.1006557.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref016">16</xref>]. NAPs can be contrasted with metric properties (MPs), which vary with orientation in depth, such as aspect ratio or the degree of curvature. The three other groups are all ‘Irregular’. They differed from the Regular shapes in that they do not have a single axis of symmetry. The two shapes in each row of the three Irregular groups differed in the configuration of their concavities and convexities or corners. The shapes in the Irregular Simple Curved (ISC) set all had curved contours. The Irregular Simple Straight (ISS) shapes were derived from the ISC shapes by replacing the curved contours with straight lines. Thus, the corresponding stimuli in the ISS and ISC shapes differed in a NAP. Last, the Irregular Complex (IC) group was more complex in that the shapes in that group had a greater number of contours.</p>
<fig id="pcbi.1006557.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Shape set.</title>
<p>One group of Regular (R) and three groups of Irregular shapes: Irregular Complex (IC), Irregular Simple Curved (ISC) and Irregular Simple Straight (ISS). A group of 16 shapes corresponds to two consecutive rows (labeled a,b) and the group names are depicted in the tree graph on the right. The 8 pairs (a,b) of each group are defined by the numbers on top of the figure (1,2, …8).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g001" xlink:type="simple"/>
</fig>
<p>Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] found that anterior IT neurons distinguished the four groups of shapes. Importantly, the differences in IT responses amongst the shapes could not be explained by pixel-based gray level differences, nor by HMAX C2 unit differences. In fact, none of the tested quantitative models of object processing could explain the IT response modulations. Furthermore, the IT response modulations were greater for the Regular shapes and when comparing the curved and straight Irregular Simple shapes than within the 3 Irregular shape groups, suggesting a greater sensitivity for NAPs than for MPs (see also [<xref ref-type="bibr" rid="pcbi.1006557.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref018">18</xref>]). We reasoned that this shape set and corresponding IT responses was useful to examine to what degree different layers of deep CNNs and IT neurons represent abstract shapes similarly. We employed deep CNNs that were pretrained to classify ImageNet data [<xref ref-type="bibr" rid="pcbi.1006557.ref019">19</xref>], consisting of images of natural objects in scenes. Hence, the CNNs were not exposed during training to silhouette shapes shown to the IT neurons. Deep CNNs have a particular architecture with early units having small receptive fields, nonlinear pooling of units of the previous layer, etc. Such a serial, hierarchical network architecture with increasing receptive field size across layers may result in itself, i.e. without training, in changes in the representational similarity across layers. To assess whether potential correlations between IT and CNN layer response modulations resulted from classification training or from the CNN architecture per se, we also compared the activations of untrained CNNs with the IT response modulations.</p>
<p>Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] had also human subjects sort the same shapes based on similarity and found that human subjects had a pronounced higher sensitivity to the difference between the curved and straight simple irregular shapes (relative to the regular shapes) than the IT neurons. We examined whether a similar difference in response pattern between macaque IT neurons and human similarity judgements would emerge in the deep CNNs. We expected that deeper layers would resemble the human response patterns while the IT response pattern would peak at less deep layers.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Kayaert et al [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] recorded the responses of 119 IT neurons to the 64 shapes shown in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>. The 64 shapes are divided in four groups based on their regularity, complexity and whether they differed in NAPs. We presented the same shapes to 3 deep CNNs: Alexnet [<xref ref-type="bibr" rid="pcbi.1006557.ref020">20</xref>], VGG-16, VGG-19 [<xref ref-type="bibr" rid="pcbi.1006557.ref021">21</xref>] and measured the activations of the units in each layer of the deep nets. These deep nets differ in their number of layers, the number of units in each layer and the presence of a normalization stage, but each have rectifying non-linearity (RELU) and max pooling stages (<xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref>). We employed deep nets that were pre-trained in classification of a database of natural images, which were very different in nature from the abstract shape stimuli that we employ here to test the models and neurons. The aim was to compare the representations of the shapes between IT neurons and each layer of the deep nets. To do this, we employed representational similarity analyses [<xref ref-type="bibr" rid="pcbi.1006557.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref023">23</xref>], following the logic of second order isomorphism [<xref ref-type="bibr" rid="pcbi.1006557.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref025">25</xref>], and examined the correlation between the neural IT-based similarities and CNN-based similarities in responses to shapes. We are not trying to reconstruct the shapes based on IT neuron or CNN unit outputs but we are examining whether shapes that are represented close to each other in the neural IT space are also represented close to each other in the CNN layer space.</p>
<fig id="pcbi.1006557.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Model architectures.</title>
<p>Deep Convolutional Neural Network (CNN) architectures for the 3 different networks that we employed: Alexnet, VGG-16 and VGG-19. Early computational blocks consist of consecutive operations such as: convolution (conv), RELU activation function, normalization (norm; only for Alexnet) and max pooling (pool). The later stages of each CNN incorporate three fully connected (fc) layers, where the first two are followed by a RELU activation function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g002" xlink:type="simple"/>
</fig>
<p>In a first analysis, we computed the pairwise dissimilarity between all 64 stimuli using the responses of the IT neurons and the activations in each of the CNN layers. We employed two dissimilarity metrics: Euclidean distance and 1 –Spearman rank correlation ρ. The dissimilarity matrices computed with the Euclidean distance metric for the IT neurons and for 5 layers of the trained CNNs are illustrated in <xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3B and 3C</xref>, respectively. In this and the next figures, we will show only the data for Alexnet and VGG-19, since VGG-16 and VGG19 produced similar results. In addition, <xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3A</xref> shows the pixel-based dissimilarities for all image pairs. Visual inspection of the dissimilarity matrices suggests that (1) the pattern of dissimilarities changes from the superficial to deep layers in a relatively similar way in the CNNs, (2) the dissimilarity matrix of the first layer (e.g. conv1.1) resembles the pixel-based similarities (<xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3A</xref>) and (3) the deeper layers resemble more the IT neural data (<xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3B</xref>).</p>
<fig id="pcbi.1006557.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Dissimilarity matrices.</title>
<p>Matrices of Euclidean distances for pixel gray-levels (A), the IT neurons (B), and 5 layers of the trained and untrained versions of 2 deep CNNs (C). Note that the dissimilarity matrices are by definition symmetric about the diagonal of zeros, which is plotted in white color. The stimulus groups are indicated in (A) as in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> and the CNN layers in (C) have the same terminology as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref>. The matrices have been separately normalized and are plotted in percentile units, following [<xref ref-type="bibr" rid="pcbi.1006557.ref023">23</xref>]. Dissimilarities increase from blue to yellow.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g003" xlink:type="simple"/>
</fig>
<p>We quantified the similarity between the IT shape representation and that of each layer by computing the Spearman Rank correlation between the corresponding pairwise dissimilarities of IT and each layer. Thus, we could assess to what degree stimuli that produce a very different (similar) pattern of responses in IT also show a different (similar) pattern of activations in a CNN layer. We found that for both dissimilarity metrics the similarity between IT neuronal responses and trained CNN layer activations increased significantly with the depth of the layer. This is shown using the Euclidean distance metric for Alexnet and VGG-19 in <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref> (see <xref ref-type="supplementary-material" rid="pcbi.1006557.s001">S1 Fig</xref> for the data of both distance metrics and the 3 networks). In the VGG nets, the similarity peaked at the deepest convolutional layers (<xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>) and then decreased for the deepest layers. In fact, the Spearman correlations for the last two fully connected layers did not differ significantly from that of the first convolutional layer in each CNN (<xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>). The decrease in similarity for the deepest layers was weaker in Alexnet. The peak similarity was similar amongst the 3 nets, with ρ hovering around 0.60, and were larger for the correlation (mean peak ρ = 0.64) compared with Euclidean distance metric (mean peak ρ = 0.58). To assess the degree to which the models explained the neural data, we computed the reliability of the neural-based distances giving the finite sampling of the IT neuron population. This noise ceiling was computed by randomly splitting the neurons into two groups, computing the dissimilarities for each group, followed by computation of the Spearman rank correlation between the dissimilarities of the two groups. This split-half reliability computation was performed for 10000 random splits. <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref> shows the 2.5, 50 (median) and 97.5 percentiles of the Spearman-Brown corrected correlations between the two groups. The correlations between (some) CNN layers and neural responses were close but still below the estimated noise of the neural dissimilarities.</p>
<fig id="pcbi.1006557.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Representational similarity analysis of deep CNN layers and IT neurons for the whole shape set.</title>
<p>Spearman rank correlation coefficients between IT and model layer similarities are shown for each layer of Alexnet (A) and VGG19 (B) using the Euclidean distance metric. Error bars depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuron pool (n = 119 neurons). Stars indicate layers for which the Spearman rank correlations for the trained version differed significantly from its untrained version (paired bootstrap test (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>); False Discovery Rate corrected q&lt;0.05). Crosses indicate trained layers which differed significantly from the first convolutional layer of the network (paired bootstrap test (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>); False Discovery Rate corrected q&lt;0.05). Layers are indicated by the same nomenclature as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref>. The horizontal line and gray band indicate the median and 95% interval, respectively, of the Spearman-Brown corrected split-half correlations (n = 10000 splits) of the neuronal distances, as an estimate of the noise ceiling.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g004" xlink:type="simple"/>
</fig>
<p>In order to assess to what degree the similarity between neural data and the CNN layers reflects the architecture of the CNNs versus image classification training, we computed also the similarity for untrained networks with random weights. <xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3C</xref> illustrates dissimilarity matrices computed using Euclidean distances for 5 untrained layers of two CNNs. Visual inspection suggests little change in the dissimilarity matrices of the different layers of the CNNs, except for fc8. Furthermore, the pattern of dissimilarities resembled the pixel-based dissimilarities shown in <xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3A</xref>. Both observations were confirmed by the quantitative analysis. The Spearman correlations of the neural data and untrained CNNs increased only weakly with depth, except for a marked decrease in correlation for the last two fully connected layers. Except for the deep convolutional and the last two layers, the trained and untrained networks showed similar Spearman correlations of the neural and CNN distances (<xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>). This suggests that overall the similarity between the IT data and the shallow CNN layers are unrelated to classification training but reflect merely the CNN architecture. Significant differences between trained and untrained CNNs were observed for the deeper convolutional layers (<xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>), suggesting that the similarity between IT and the deep convolutional layers depends on classification training. The similarities for the first fully connected layer (fc6 and relu6 in <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>) did not differ significantly between the trained and untrained layers (except for the correlation metric in AlexNet (<xref ref-type="supplementary-material" rid="pcbi.1006557.s001">S1 Fig</xref>). The deepest two (fully connected) layers showed again a significantly greater similarity for the trained compared with the untrained networks. However, this can be the result of the sharp drop in correlations for these layers in the untrained network. Overall, these data suggest that the shape representations of the trained deep convolutional layers, but not of the deepest layers, shows the highest similarity with shape representations in macaque IT.</p>
<p>Receptive field (RF) size increases along the layers of the CNNs, allowing deeper layer units to integrate information from larger spatial regions. The difference in IT-CNN similarity between untrained and trained layers shows that the increase in RF size cannot by itself explain the increased IT-CNN similarity in deeper layers, since untrained CNN also increase their RFs along the layer hierarchy. Also, the decrease in similarity between IT responses and the fully connected layers argues against RF size being the mere factor. Nonetheless, although not the only contributing factor, RF size is expected to matter since arguably small RFs cannot capture overall shape when the shape is relatively large. Hence, it is possible that the degree of IT-CNN similarity for different layers depends on shape size, with smaller shapes showing a greater IT-CNN similarity at earlier layers. We tested this by computing the activations to shapes that were reduced in size by a factor of two in all layers of each of the 3 trained CNNs. <xref ref-type="fig" rid="pcbi.1006557.g005">Fig 5</xref> compares the correlations between dissimilarities of the trained Alexnet and VGG-19 networks and IT dissimilarities for the original and reduced sizes, with dissimilarities computed using Euclidean distances. The stars indicate significant differences between the similarities for the two sizes (tested with a FDR corrected randomization test; same procedure as in <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref> when comparing trained and untrained correlations). In each of the CNNs (<xref ref-type="supplementary-material" rid="pcbi.1006557.s002">S2 Fig</xref>), the IT-CNN similarity increased at more superficial layers for the smaller shape. The overall peak IT-CNN similarity was highly similar for the two sizes in the VGG networks and occurred at the deep convolutional layers. For Alexnet, the overall similarity was significantly higher for the smaller shapes in the deep layers. This analysis indicates that shape size is a contributing factor that determines at which layer the IT-CNN similarity increases, but that for the VGG networks, peak similarity in the deep layers does not depend on size (at least not for the twofold variation in size employed here). Note that also for the smaller size the IT-CNN similarity drops markedly for the fully connected layers in the VGG networks. Thus, the overall trends are independent of a twofold change in shape size.</p>
<fig id="pcbi.1006557.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Representational similarity analysis of deep CNN layers and IT neurons for the whole shape set with two different sizes.</title>
<p>Spearman rank correlation coefficients between IT and model layer similarities are shown for each layer of Alexnet (A) and VGG19 (B) for the original and twofold smaller sizes (“reduced size”). The dissimilarities were Euclidean distances. Error bars depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuron pool (n = 119 neurons). Stars indicate layers for which the Spearman rank correlations for the trained version differed significantly from its untrained version (paired bootstrap test; False Discovery Rate corrected q&lt;0.05). The horizontal line and gray band indicate the median and 95% interval, respectively, of the Spearman-Brown corrected split-half correlations (n = 10000 splits) of the neuronal distances, as an estimate of the noise ceiling.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g005" xlink:type="simple"/>
</fig>
<p>In the preceding analyses, we included all units of each CNN layer. To examine whether the similarity between the CNN layers and the IT responses depends on a relatively small number of CNN units or is distributed amongst many units, we reran the representational similarity analysis of deep CNN layers and IT neurons for the whole shape set for smaller samples of CNN units. We took for each network the layer showing the peak IT-CNN similarity and for that layer sampled 10000 times at random a fixed percentage of units. We restricted the population of units to those that showed a differential activation (standard deviation of activation across stimuli greater than 0) since only those can contribute to the Euclidean distance. <xref ref-type="fig" rid="pcbi.1006557.g006">Fig 6A</xref> plots the median and 95% range of Spearman rank correlation coefficients between IT and CNN layer dissimilarities for the whole shape set as a function of the percent of sampled units for two CNNs. We found that the IT-CNN similarity was quite robust to the number of sampled units. For instance, for Alexnet, the IT-CNN similarity for the original and the 95% range of the 10% samples overlap, indicating that 315 Alexnet units can produce the same IT-CNN similarity as the full population of units. Note also that the lower bound of the 95% range is still above the IT-CNN similarities observed for the untrained network (median Spearman rho about 0.40; see <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>). This indicates that the IT-CNN similarity does not depend on a small subset of units, since otherwise the range of similarities (Spearman rho correlations) for the 10% samples would be much greater. The same holds for the other CNNs (<xref ref-type="supplementary-material" rid="pcbi.1006557.s003">S3 Fig</xref>), except that these tolerated even smaller percent sample size (for VGG19 even 0.1%, which corresponds to 100 units).</p>
<fig id="pcbi.1006557.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Similarities between IT and CNN peak layer shape dissimilarities as a function of percent of units.</title>
<p>(A) Spearman rank correlation coefficients between IT and peak CNN layer similarities are shown for each of two CNN models as a function of sample size, expressed as percentage of the total number of units that were activated differentially by the 64 shapes. (B) Pearson correlation coefficients between the mean neural distances and the mean distances of the peak CNN layer (n = 6 mean distances; <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10</xref>) as a function of percentage of the total number of units. The total number of units (100%) for each CNN layer is listed in the legend. Note that 0.1% corresponds to only 3 Alexnet units, explaining the large range of correlations for that sample size. The dissimilarities were Euclidean distances. Error bars depict 95% confidence intervals, determined by 10,000 random samples from the population of differentially activated CNN units of that layer.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g006" xlink:type="simple"/>
</fig>
<p>The above analysis appears to suggest that the activations of the CNN units to the shapes are highly correlated with each other. To address this directly, we performed Principal Component Analysis (PCA) of unit activations of the same peak CNN layers as in <xref ref-type="fig" rid="pcbi.1006557.g006">Fig 6</xref> and computed Euclidean distance based dissimilarities between all stimulus pairs for the first, first two, etc. principal components (PCs), followed by correlation with the neural dissimilarities as done before for the distances computed across all units of a CNN layer. For both the Alexnet and VGG-19 layer, the first 10 PCs explained about 70% of the variance in CNN unit activations to the 64 stimuli (<xref ref-type="fig" rid="pcbi.1006557.g007">Fig 7B</xref>). Only the first 3 (Alexnet) or 5 (VGG-19) PCs were required to obtain a similar correlation between the model and neural distances as observed when using all model units of the layer (<xref ref-type="fig" rid="pcbi.1006557.g007">Fig 7A</xref>; about 7 PCs were required for VGG-16; see <xref ref-type="supplementary-material" rid="pcbi.1006557.s004">S4 Fig</xref>). This analysis shows that the neural distances between the abstract shapes relate to a relatively low dimensional shape representation in the CNN layer, with a high redundancy between the CNN units.</p>
<fig id="pcbi.1006557.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Similarities between IT and CNN peak layer shape dissimilarities as a function of retained principal components.</title>
<p>(A) Spearman rank correlation coefficients between IT and peak CNN layer similarities are shown for each of two CNN models as a function of retained principal components of the CNN layer activations. The dissimilarities were Euclidean distances. Error bands depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuronal pool. (B) The cumulative proportion of explained variance as a function of principal component number for the Alexnet (black line) and VGG-19 layer (gray line). (C) Pearson correlation coefficients between the mean neural distances and the mean distances of the peak CNN layer (n = 6 mean distances; see <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10</xref>) as a function of retained principal components. In A and C, the bands represent 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuronal pool.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g007" xlink:type="simple"/>
</fig>
<p>In the above analyses, we compared the overall similarity of the shape representations in IT and CNN layers. However, a more stringent comparison between the shape representations in IT and the CNNs involves response modulations for the shape pairs for which Kayaert et al [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] observed striking differences between predictions of pixel-based models or computational models like HMAX and the neural responses. The average response modulations (quantified by pairwise Euclidean distances) for the different group pairs comparisons are shown in <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8</xref> for the IT neural data, the HMAX C2 layer and the pixel differences. Kayaert et al [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] showed that the mean response modulation in IT (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8A</xref>)was significantly greater for the regular shape pairs (1–8 in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>) than for the 3 irregular shape group pairs, despite the pixel differences between members of a pair being, on average, lower or similar for the regular group than for the 3 irregular groups (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8D</xref>). In addition, the response modulation to ISC vs. ISS was significantly greater than the modulations within IC, ISC and ISS, although the average pixel-difference within the ISC vs. ISS-pairs was much lower than the pixel-differences within the other pairs. This differential neural response modulation to ISC vs ISS was present for both members of the ISC and ISS pairs (a and b members: “ISCa vs ISSa” and “ISCb vs ISSb”) and thus was highly reliable. Note that the difference between ISC vs. ISS and the IC and ISS shape groups that are present in the neural data is not present for the HMAX C2 distances (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8C</xref>). Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] reported also a relatively higher sensitivity to the straight vs. curved contrast of the ISC vs. ISS comparison compared with the regular shapes in human similarity ratings (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8B</xref>), compared with the IT neural data. In other words, human subjects appear to be more sensitive to the curved versus straight NAP difference than macaque IT neurons.</p>
<fig id="pcbi.1006557.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Response modulations for the shape groups: IT neurons, human judgements, HMAX and pixel-based dissimilarities.</title>
<p>(A) Mean response modulations of IT neurons for the shape groups R, IC, ISC, ISS, “ISCa vs ISSa” and “ISCb vs ISSb”. See <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> for the nomenclature of the different shape groups. (B) Dissimilarities for the shape groups (R, IC, ISC, ISS, ISC vs ISS) based on human judgements. (C) Dissimilarities for the same shape groups based on the HMAX C2 layers’ output. (D) Pixel-based dissimilarities. Error bars indicate standard errors of the mean. B, C and D are taken from Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g008" xlink:type="simple"/>
</fig>
<p>In a second analysis, we determined whether the marked differences in IT response modulations and human judgements shown in <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8</xref> are present in the dissimilarities for the different layers of the deep CNNs. <xref ref-type="fig" rid="pcbi.1006557.g009">Fig 9</xref> illustrates the results for 8 layers of VGG-19. The left column of the figure plots the distances for the trained network. The dissimilarities for the first convolutional layer fits the pixel-based distances amongst the shape pairs (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8D</xref>; Pearson correlation between pixel-based distances and first layer distances = 0.966), but differ from those observed in IT and for human judgements. Similar trends are present until the very deep convolutional layers where the dissimilarities became strikingly similar to those observed in macaque IT (e.g. compare trained conv5.4 or pool5 of <xref ref-type="fig" rid="pcbi.1006557.g009">Fig 9</xref> with <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8A</xref>). The dissimilarities for the last two layers (e.g. trained relu7 and fc8 in <xref ref-type="fig" rid="pcbi.1006557.g009">Fig 9</xref>) are strikingly similar to those observed for the human judgements (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8B</xref>), and differ from the pattern seen in macaque IT neurons. Indeed, as noted above, the human judgements differ from the IT responses in their sensitivity for the ISC vs ISS comparison relative to that for the regular shape pairs: for the human judgement distances, the ISC vs ISS distances are greater than for the regular shape distances while for the neural distances both are statistically indistinguishable (Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>]). Therefore, we tested statistically for which CNN layer the ISC vs ISS distances were significantly greater than the regular shape distances (Wilcoxon test), thus mimicking the human distances. We found a significant difference for the very deep VGG19 layer fc8 (p = 0.039) and VGG16 layers fc7 (p = 0.039), relu7 (p = 0.023), and fc8 (p = 0.023). Although the deepest Alexnet (fully connected) layers showed the same trend, this failed to reach significance. These tests showed that only the very deep CNN layers mimicked the human judgements. None of the untrained CNN layers showed a dissimilarity profile similar to that observed in monkey IT or in human judgements (<xref ref-type="fig" rid="pcbi.1006557.g009">Fig 9</xref>, right column). In fact, the untrained data resembled more the pixel-based distances (see <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8D</xref>). Indeed, the Pearson correlation between the pixel-based distances and the conv1.1 distances was 0.999 for the untrained VGG-19.</p>
<fig id="pcbi.1006557.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Response dissimilarities for the shape groups: Deep CNN layers.</title>
<p>Dissimilarities for groups R, IC, ISC, ISS, “ISCa vs ISSa”, “ISCb vs ISSb” of selected trained (left column) and untrained (right column) versions of VGG-19 layers (same nomenclature as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref>). Same conventions as in <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8</xref>. The specific selection of layers is motivated by the fact that there were no critical differences in the layers that follow the selected ones.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g009" xlink:type="simple"/>
</fig>
<p>We quantified the correspondence between the neural response dissimilarities of <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8A</xref> and the CNN layer dissimilarities (as in <xref ref-type="fig" rid="pcbi.1006557.g009">Fig 9</xref>) by computing the Pearson correlation coefficient between the dissimilarity profiles (n = 6 dissimilarity pairs). The same quantification was performed for the human judgements (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8B</xref>) and the CNN dissimilarities (n = 5 pairs). These correlations are plotted in <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10A and 10B</xref> as a function of layer for two CNNs, trained and untrained. For the neural data, the correlations are negative for the shallow layers and highly similar for the trained and untrained CNNs. The negative correlations are a result of the nearly inverse relationship between neural and low-level (pixel) differences between the shapes (<xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8D</xref>). This was not accidental, but by design: when creating the stimuli, Kayaert et al [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] ensured that the NAP differences (e.g. between ISC and ISS) were smaller than MP differences. For both VGG networks (<xref ref-type="supplementary-material" rid="pcbi.1006557.s005">S5 Fig</xref>; <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10B</xref>), there was a sharp increase in correlations at the trained deep 5.1 convolutional layer, followed by a decrease of the correlations for the fully connected layers. This trend was similar, although more abrupt, to that observed for the global dissimilarities of <xref ref-type="fig" rid="pcbi.1006557.g004">Fig 4</xref>. For Alexnet, the increase of the correlations with increasing depth of the trained convolutional layers was more gradual, but like the VGG networks, high correlations were observed for the deeper trained convolutional layers. For the human judgement data, the correlations were already higher for the trained compared with the untrained CNNs at the shallow layers, although still negative. Like the neural data, there was a marked increase in correlation at the very deep trained layers. Contrary to the neural data, the correlations for the human judgements continued to increase along the trained fully connected layers, approaching a correlation of 1 at the last layer. These data show that the average response modulations of IT neurons for the shape groups of <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> correspond nearly perfectly with those of the deeper layers of CNNs, while the differences in human similarity judgements between the groups are captured by the later fully connected layers of the CNNs. This holds for Alexnet and VGG nets. Note that the deep CNN layers performed better at predicting the neural IT and human perceptual dissimilarities than the HMAX C2 layer output (<xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10C</xref>).</p>
<fig id="pcbi.1006557.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006557.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Correspondence between model dissimilarities and biological dissimilarities (IT responses and human judgement-based dissimilarities) for the shape groups.</title>
<p>(A, B). Gray curves show the Pearson correlation coefficients between the mean neural distances and the mean distances of the CNN layers (n = 6 mean distances per layer). Blue curves show the Pearson correlation coefficients of the CNN layer distances and the distances based on human judgements. Data for trained and untrained CNNs are plotted with full and dashed lines, respectively. Nomenclature of CNN layers as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref>. Results for all two models (Alexnet and VGG-19) are displayed in the subplots (A, B). (C) Neural: Pearson correlation coefficient between the mean IT distances and the mean distances of the peak Alexnet layer, peak VGG-19 layer, the mean HMAX C2 layer distances, and mean pixel-based distances, across shape groups. Human: Pearson correlation coefficient between the distances based on the human judgements and the peak Alexnet layer, peak VGG-19 layer, HMAX C2 layer and pixel-based distances.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.g010" xlink:type="simple"/>
</fig>
<p>As for the representational similarity analysis for all shapes (<xref ref-type="fig" rid="pcbi.1006557.g006">Fig 6A</xref>), we computed also the Pearson correlation coefficients between the dissimilarity profiles (n = 6 dissimilarity pairs) of the same peak CNN layers and the IT distances for the 6 shape groups (as in <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10</xref>) for smaller samples of units. As shown in <xref ref-type="fig" rid="pcbi.1006557.g006">Fig 6B</xref>, we observed similar IT-CNN correlations for the within-group distances up to the 1% and 0.1% samples compared with the full population of units for Alexnet and VGG, respectively. Again, this suggests that IT-CNN similarity does not depend on a small number of outlier CNN units. The greater tolerance for percent sample size for the VGG units is because the VGG layers consisted of a larger number of units per se (total number of units are indicated in the legend of <xref ref-type="fig" rid="pcbi.1006557.g006">Fig 6</xref>). In addition, we computed the mean distances for the same layers and their correlation with the mean neural modulations as a function of retained PCs (<xref ref-type="fig" rid="pcbi.1006557.g007">Fig 7B</xref>). Up to 30 PCs were required to obtain a similar correlation between neural and CNN layer distances for the six groups of shapes as when including all units of the layer (<xref ref-type="fig" rid="pcbi.1006557.g007">Fig 7B</xref>). This suggests that the close to perfect modeling of the mean response modulations across the 6 shape groups required a relatively high dimensional representation of the shapes within the CNN layer.</p>
</sec>
<sec id="sec003" sec-type="conclusions">
<title>Discussion</title>
<p>The particular set of shapes that we employed in the present study was designed originally to test the idea that the shape selectivity of IT neurons reflects the computational challenges posed when differentiating objects at different orientations in depth [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref014">14</xref>]. Here, we show that deep CNNs that were trained to classify a large set of natural images show response modulations to these shapes that are similar to those observed in macaque IT neurons. We show that untrained CNNs with the same architecture than the trained CNNs, but with random weights, demonstrate a poorer IT-CNN similarity than the CNNs trained in classification. The difference between the trained and untrained CNNs emerged at the deep convolutional layers, where the similarity between the shape-related response modulations of IT neurons and the trained CNNs was high. Unlike macaque IT neurons, human similarity judgements of the same shapes correlated best with the deepest layers of the trained CNNs.</p>
<p>Early and many later studies of IT neurons employed shapes as stimuli (e.g. [<xref ref-type="bibr" rid="pcbi.1006557.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref037">37</xref>]), in keeping with shape being an essential object property for identification and categorization. Deep CNNs are trained with natural images of objects in cluttered scenes. If deep CNNs are useful models of biological object recognition [<xref ref-type="bibr" rid="pcbi.1006557.ref038">38</xref>], their shape representations should mimic those of the biological system, although the CNNs were not trained with such isolated shapes. We show here that indeed the representation of the response modulations by rather abstract, unnatural shapes is highly similar for deep CNN layers and macaque IT neurons. Note that the parameters of these CNN models are set via supervised machine learning methods to do a task (i.e. classify objects) rather than to replicate the properties of the neural responses, as done in classic computational modeling of neural selectivities. Thus, the same CNN model that fits neural responses to natural images [<xref ref-type="bibr" rid="pcbi.1006557.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1006557.ref004">4</xref>] also simulates the selectivity of IT neurons for abstract shapes, demonstrating that these models show generalization across highly different stimulus families. Of course, the high similarity between deep CNN layers and IT neurons activation patterns we show here may not generalize for (perhaps less fundamental) shape properties that we did not vary in our study.</p>
<p>Kubilius et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref039">39</xref>] showed that deep nets captured shape sensitivities of human observers. They showed that deep Nets, in particular their deeper layers, show a NAP advantage for objects (“geons”), as does human perception (and macaque IT [<xref ref-type="bibr" rid="pcbi.1006557.ref018">18</xref>]). Although we also manipulated NAPs, our shapes differed in addition in other properties such as regularity and complexity. Furthermore, our shapes are unlike real objects and more abstract than the shaded 3D objects employed by Kubilius et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref039">39</xref>] when manipulating NAPs.</p>
<p>In both the representational similarity analysis and the response modulations comparisons amongst shape groups, we found that the correspondence between IT and deep CNN layers peaked at the deep convolutional layers and then decreased for the deeper layers. Recently, we observed a similar pattern when using deep CNN activations of individual layers to model the shape selectivity of single neurons of the middle Superior Temporal Sulcus body patch [<xref ref-type="bibr" rid="pcbi.1006557.ref004">4</xref>], a fMRI-defined region of IT that is located posterior with respect to the present recordings. The increase with deeper layers of the fit between CNN activations and neural responses has also been observed when predicting with CNN layers macaque IT multi-unit selectivity [<xref ref-type="bibr" rid="pcbi.1006557.ref040">40</xref>], voxel activations in human LO [<xref ref-type="bibr" rid="pcbi.1006557.ref009">9</xref>] and the representational similarity of macaque and human (putative) IT [<xref ref-type="bibr" rid="pcbi.1006557.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref010">10</xref>] using natural images. However, the decrease in correlation between CNNs and neural data that we observed for the deepest layers was not found in fMRI studies that examined human putative IT [<xref ref-type="bibr" rid="pcbi.1006557.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref010">10</xref>], although such a trend was present in [<xref ref-type="bibr" rid="pcbi.1006557.ref006">6</xref>] when predicting CNN features from fMRI activations. The deepest layers are close to or at the categorization stage of the CNN and hence strongly dependent on the classifications the network was trained on. The relatively poor performance of the last layers is in line with previous findings that IT neurons show little invariance across exemplars of the same semantic category [<xref ref-type="bibr" rid="pcbi.1006557.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1006557.ref042">42</xref>], unlike the deepest CNN units [<xref ref-type="bibr" rid="pcbi.1006557.ref043">43</xref>].</p>
<p>The question of what the different layers in the various CNN models with different depths represent neurally remains basically unanswered. Shallow CNN layers can be related to early visual areas (e.g. V1; V4) and deeper layers to late areas (e.g. IT). However, different laminae within the same visual area (e.g. input and output layers) may also correspond to different layers of CNNs. Furthermore, units of a single CNN layer may not correspond to a single area, but the mapping might be more mixed with some units of different CNN layers being mapped to area 1, while other units of partially overlapping CNN layers to area 2, etc. Finally, different CNN layers may represent different temporal processing stages within an area, although this may map partially to the different laminae within an area. Further research in which recordings in different laminae of several areas will be obtained for the same stimulus sets, followed by mapping these to units of different layers in various CNNs, may start to answer this complex issue.</p>
<p>In contrast with IT neurons, human similarity judgements of our shapes matched to a greater extent the last rather than the less deep convolutional layers. In particular, the deepest layers showed a similar enhanced sensitivity for straight versus curved irregular shapes. The untrained CNNs did not show such straight versus irregular bias for the irregular shapes. Thus, it appears that a system, be it artificial like the CNNs or a biological system like humans, that is required to classify natural images of objects develops such bias for curved versus straight contours, indicating that this shape property must be highly informative for object categorization. Whether this relates to straight versus curved being a NAP [<xref ref-type="bibr" rid="pcbi.1006557.ref014">14</xref>] is unclear.</p>
<p>Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] employed a sorting task to rate shape similarity. In this task, subjects were required to sort the shapes into groups based on their similarity. Although this is not the same as labeling an object, the task for which the CNNs were trained, higher order classification can intrude the sorting task judgements. This may explain why the human sortings of Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] resembled that closely the activation pattern seen at the deepest CNN layers, which are strongly category label driven. Interestingly, even for the shallow convolutional layers, the correlations between the human judgements and the CNN activations were higher for the trained compared with the untrained CNNs. This contrasted with the equal correlations for trained and untrained shallow layers for the IT data. This suggests that the trained shallow CNN layers show already some, albeit weak, bias for higher order category-related information.</p>
<p>Previous studies that compared deep CNNs and neural responses rarely included untrained CNNs as control (e.g. [<xref ref-type="bibr" rid="pcbi.1006557.ref008">8</xref>], [<xref ref-type="bibr" rid="pcbi.1006557.ref040">40</xref>]). We found the untrained CNNs helpful in interpreting our data. The comparison with untrained CNNs can inform to what extent neural responses reflect features that can be picked up by untrained CNNs (because of CNN architectural properties such as tiling of local RFs in shallow layers and non-linear pooling). Indeed, we found that most layers of the untrained CNNs represented rather closely the pixel-based graylevel differences between the shape groups, which assisted to interpret the representational similarity of the trained CNNs at shallow layers. Thus, we advise that future studies use untrained CNNs as control or benchmark.</p>
<p>Currently, deep CNNs are the best models we have of primate object recognition, providing the best quantitative fits of ventral stream stimulus selectivities and primate recognition behavior [<xref ref-type="bibr" rid="pcbi.1006557.ref038">38</xref>]. However, recent studies show that CNNs have their limitations, especially when stimuli are noisy or partially occluded. For instance, the commonly used deep CNNs tolerate less image degradation than humans [<xref ref-type="bibr" rid="pcbi.1006557.ref044">44</xref>], can be fooled by unrecognizable images [<xref ref-type="bibr" rid="pcbi.1006557.ref045">45</xref>] or show a sensitivity to imperceptible stimulus perturbations (“adversarial examples”; [<xref ref-type="bibr" rid="pcbi.1006557.ref046">46</xref>]). Our data show that training CNNs in object categorization produces at least some shape selectivities (that are thought to reflect fundamental aspects of shape processing [<xref ref-type="bibr" rid="pcbi.1006557.ref014">14</xref>]) similar to those that are observed in neural IT data and human similarity judgements. This does not imply that CNNs can explain all shape or stimulus selectivity in IT and there is still considerable room for model improvement (e.g. recurrent connectivity etc.).</p>
<p>In conclusion, deep CNN layers that were trained to classify complex natural images represented differences among relatively simple abstract 2D-shapes similar to macaque IT neurons. Human sorting of the same shapes corresponded better with the deepest layers of the CNNs. The similarity between IT neurons and the deeper convolutional layers is greater for trained compared to untrained CNNs, suggesting a role of image classification in shaping the shape selectivity of macaque IT neurons. The latter likely occurs during ontogenetic development, but may not result from the same supervised learning algorithm as employed to train the CNNs. Indeed, independent of the particular training protocol (e.g. supervised versus unsupervised), any biological object classification system may have similar shape representation biases that are inherently useful for performing invariant object classification.</p>
</sec>
<sec id="sec004" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec005">
<title>Ethics statement</title>
<p>Two male rhesus monkeys served as subjects. The animals were housed individually with visual and auditory contact with conspecifics. During the recording weeks, they had controlled access to fluids but food was available at libitum. All procedures were in accordance with the Weatherall report on “The use of non-human primates in research” and were approved by the Animal Ethics committee of the KU Leuven (protocol number: P631/2002).</p>
</sec>
<sec id="sec006">
<title>Stimuli</title>
<p>The 64 shapes were identical to the first stimulus set employed by Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] and are shown in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>. The Regular shapes R were created with Studio MAX, release 2.5, while the Irregular shapes were made with Fourier Boundary Descriptors, using MATLAB, release 5. The Irregular Simple Straight (ISS) stimuli were made by replacing the curves of the Irregular Simple Curved (ISC) shapes by straight lines while preserving the overall shape. The increase in complexity of the Irregular Complex (IC) shapes compared to the simpler ISC shapes was produced by increasing the number and frequency of the Fourier Boundary Descriptors.</p>
<p>Each group contains 8 pairs of stimuli (one stimulus in row a and one in row b in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>). The columns of <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> comprise a set of 4 pairs (one for each group) that were matched in overall size and aspect ratio, both within and between groups. The averaged pixel-based graylevel differences between the members of the pairs were balanced across groups (see [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] for more details). The members of the pairs within the Regular shapes differ in a NAP, such as parallel vs. nonparallel sides, or straight vs. curved contours. The differences among the members of an irregular pair were created by varying the phase, frequency or amplitude of the Fourier Boundary Descriptors. For the single unit recordings and the human behavioral study, all stimuli were filled with the same random dot texture pattern. The number of black and white dots was required to be equal for 2*2 squares of pixels, so the texture patterns were highly uniform. Stimuli were presented on a gray background. In the single unit study, they extended approximately 7 degrees and were shown at the center of the screen. We employed the identical shapes for the CNN modeling, except that the noise pattern was replaced by a uniform white surface (see <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> for the actual stimuli presented to the CNNs).</p>
</sec>
<sec id="sec007">
<title>Electrophysiology data of monkey IT</title>
<p>The single unit data have been published before [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>] and the procedures have been described in detail in that paper. Therefore, we will summarize here only briefly the experimental procedures. The IT recordings were made while the two monkeys performed a passive fixation task. Eye movements were measured with the scleral search coil technique or with a noninvasive eye tracker (ISCAN). During the recordings, their head was fixed by means of an implanted head post. We employed the standard dorsal approach to IT and recording sites were verified with MRI and CT scans with the guiding tube in situ. We lowered a tungsten microelectrode through the guiding tube that was fixed in a Crist grid, which was positioned within the plastic recording chamber. The signals of the electrode were amplified and filtered using standard single-cell recording equipment. Single units were isolated on line and their timing was stored together with stimulus and behavioral events for later offline analysis.</p>
<p>The stimuli were presented during fixation for 200 ms in a randomly interleaved fashion. In the present study, the response of a neuron was defined as the average firing rate in spikes/s during a time interval of 250 ms, starting from 50 to 150 ms after stimulus onset. The starting point of this time interval was chosen for each neuron to best capture its response, by inspection of the peristimulus time histograms averaged across the stimuli. Responses were averaged across presentations per stimulus. The minimum number of presentations per stimulus was 5 (median = 10). The data set consisted of 119 anterior IT neurons (76 in monkey 1 and 43 in monkey 2) that showed significant response selectivity to the stimuli of the set (ANOVA, p&lt;0.05). The data were pooled across animals. The neurons were located in the lower bank of the Superior Temporal Sulcus and the lateral convexity of anterior IT (TEad).</p>
</sec>
<sec id="sec008">
<title>Sorting task in human subjects</title>
<p>As described by Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>], printed versions of the shapes were given to 23 naive adult human subjects who were asked to sort the stimuli in groups based on shape similarity. No further definition of similarity was given and they could freely choose the number of groups. This is a classical task to measure image similarities [<xref ref-type="bibr" rid="pcbi.1006557.ref047">47</xref>]. Dissimilarity values between pairs of stimuli were computed by counting the number of subjects that put the two members in different groups.</p>
</sec>
<sec id="sec009">
<title>Deep convolutional neural networks</title>
<p>In order to compare the shape representation of the IT neurons’ population with deep CNN layers, we extracted stimulus features for each processing stage (layer) of three deep models: Alexnet [<xref ref-type="bibr" rid="pcbi.1006557.ref020">20</xref>], VGG-16 and VGG-19 [<xref ref-type="bibr" rid="pcbi.1006557.ref021">21</xref>]. We used the pretrained networks, which are available through the MatConvNet toolbox [<xref ref-type="bibr" rid="pcbi.1006557.ref048">48</xref>] in MATLAB, and their untrained versions. The pretrained CNNs were trained on ~1.2 million natural images divided in 1,000 classes for the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012). The untrained versions of these networks have the same architecture, but did not undergo any training, thus no update of their weights took place after initialization. Their layer weights were initialized by sampling randomly from a normal distribution, using the opts.weightInitMethod = 'gaussian' setting in the cnn_imagenet.m function of the MatConvNet toolbox. The stimuli shown to the CNNs were black and white images with pixel values ranging from 0–255 (0 for black and 255 for white). Before feature extraction, the mean of the ILSVRC2012 training images was subtracted from each stimulus, since this was also part of the preprocessing stage of the networks’ training procedure. In addition, the stimuli were rescaled accordingly to match each network’s input requirements (227x227 pixels for Alexnet and 224x224 pixels for VGG-16 &amp; VGG-19).</p>
<sec id="sec010">
<title>Alexnet</title>
<p>This deep CNN by Krizhevsky et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref020">20</xref>] was the first successful deep CNN that outperformed existing object recognition algorithms and won first place at the ILSVRC2012. It incorporates multiple processing stages (or layers) in 8 weight layer groups, including convolutional, Rectified Linear Units (RELU), normalization, max pooling and fully connected layers. The MatConvNet pretrained version of Alexnet on the validation data of ILSVRC2012 had a 42.6% top-1 error rate performance and 19.6% top-5 error rate.</p>
</sec>
<sec id="sec011">
<title>VGG-16 &amp; VGG-19</title>
<p>Introduced in 2014 [<xref ref-type="bibr" rid="pcbi.1006557.ref021">21</xref>], they are deeper CNNs than Alexnet and consist of 16 and 19 weight layers, respectively, including convolutional, RELU, max pooling and fully connected layers. The pretrained versions of MatConvNet follow the same principles as Alexnet in terms of their training procedure and processing stages, but they don’t include a normalization layer due to lack of performance improvement on the ILSVRC dataset and increased computation time and memory consumption [<xref ref-type="bibr" rid="pcbi.1006557.ref021">21</xref>]. The MatConvNet versions of VGG-16 and VGG-19 had 28.7% and 28.5% top-1 error rate performance, respectively, and a 9.9% top-5 error rate performance for both on the validation data of ILSVRC2012.</p>
</sec>
</sec>
<sec id="sec012">
<title>Data analyses</title>
<p>In all analyses, we employed as distance metric the normalized Euclidean distance between the neuronal responses or deep CNN unit activations:
<disp-formula id="pcbi.1006557.e001">
<alternatives>
<graphic id="pcbi.1006557.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006557.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <inline-formula id="pcbi.1006557.e002"><alternatives><graphic id="pcbi.1006557.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006557.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the response of neuron or deep CNN unit <italic>i</italic>, to stimulus 1, and <italic>n</italic> is the number of neurons or the number of deep CNN units in a specific layer. For the representational similarity analyses, we computed also a second distance metric: 1- Spearman’s correlation coefficient. The Spearman rank correlation coefficient ρ was computed between the neural responses or CNN units’ activations for all stimulus pairs.</p>
<p>To compare neuronal data to CNN layers, we performed representational similarity analysis [<xref ref-type="bibr" rid="pcbi.1006557.ref049">49</xref>], using both distance metrics. We constructed representational dissimilarity matrices (RDMs) for the whole stimulus set (n = 64 stimuli) for both IT neurons and each deep CNN layer (trained and untrained separately; for examples see <xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3</xref>), by arranging all possible pairwise distances in 64x64 RDMs. We extracted all values above the diagonal (upper triangle of the RDM, excluding the diagonal) of the symmetrical RDMs, and computed for each layer the Spearman rank correlation coefficient between the distances of the corresponding pairs of the neural and CNN matrices.</p>
<p>We computed 95% confidence intervals of the Spearman correlation coefficient between neural and CNN distances by resampling with replacement 10,000 times 119 neurons out of our pool of IT neurons and correlating each time the resulting neural distance matrix with each deep CNN layer for the trained and untrained versions of the same network. The confidence intervals corresponded to the 2.5 and 97.5 percentiles of the bootstrapped correlation coefficient distributions. To assess whether the trained deep CNN layers significantly differed from the untrained, we computed for each CNN layer the distribution of the paired differences of trained minus untrained layer correlations across the 10,000 iterations (one difference per bootstrapped neuronal sample). For each layer, we computed the percentile in the corresponding distribution of the zero difference value and these defined the p values of the test. For each of the 3 CNNs, we corrected the p values for multiple comparisons (n = number of CNN layers) using the Benjamini and Hochberg [<xref ref-type="bibr" rid="pcbi.1006557.ref050">50</xref>] False Discovery Rate (FDR) procedure. A difference between the trained and untrained CNNs was judged to be significant when FDR q &lt; 0.05. The same procedure was used to assess the significance of the difference in IT-CNN correlations between the original and reduced shape size for each of the CNN layers (<xref ref-type="fig" rid="pcbi.1006557.g005">Fig 5</xref>). We employed a similar procedure to test the significance of the difference in Spearman rank correlation coefficients of the neural and CNN distances between the first layer and each subsequent layer. Thus, we computed the pairwise difference between the correlation for the first and a subsequent layer for each of the 10,000 bootstrapped neural samples and then obtained the percentile of the zero difference in that distribution of differences. The p values were corrected for multiple comparisons using the FDR procedure and significance was defined when q &lt; 0.05.</p>
<p>In the second analysis, using only the original, non-bootstrapped distances, we compared the pairwise Euclidean stimulus distances amongst the 4 stimulus groups R, IC, ISC, ISS. For each group, we included only the stimulus pairs numbered 1–8 in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>, i.e. for each group the members of the a and b rows of <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>. In addition, we selected the distances for the “ISCa vs. ISSa” and “ISCb vs. ISSb” pairs of <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref>, e.g. the column-wise distances between row a of ISC and row a of ISS in <xref ref-type="fig" rid="pcbi.1006557.g001">Fig 1</xref> (likewise for the b rows). This produced twice 8 distances for the ISC versus ISS comparison, which we analyzed separately, unlike in Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>]. For each of the 4 groups and the two ISC versus ISS comparisons, we computed the mean distance (and standard errors of the mean) across the 8 pairs per group or comparison. To quantify the relationship between the mean distances across groups for the neural data and each CNN layer, we computed the Pearson correlation coefficient between the mean neural distances and the mean distances of the CNN layers (n = 6 pairs of distances per layer). A similar analysis was performed comparing the CNN layer distances and the distances based on the human ratings. However, for this analysis, the available human rating data consisted of the distances that were computed by Kayaert et al [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>], having an ISC versus ISS comparison of 8 stimulus pairs (for selection of those pairs, see [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>]) instead of twice 8 pairs as above. We compared those distances with the average of the “ISCa vs. ISSa” and “ISCb vs. ISSb” pairs of the CNN layers. Note that our average neural distances for the “ISCa vs. ISSa” and “ISCb vs. ISSb” pairs were highly similar to those for the 8 “ISC vs. ISS” pairs selected by Kayaert et al. [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>], justifying this procedure.</p>
<p>We compared neural dissimilarities also with dissimilarities based on pixel graylevels and the HMAX model [<xref ref-type="bibr" rid="pcbi.1006557.ref005">5</xref>], employing the same procedures as in Kayaert et al.. We computed the Euclidean distance between the gray-level values of the pixels for all image pairs (<xref ref-type="fig" rid="pcbi.1006557.g003">Fig 3B</xref>). In addition, we computed the Euclidean distances between the outputs of C2-units of the HMAX model as described by Riesenhuber and Poggio [<xref ref-type="bibr" rid="pcbi.1006557.ref005">5</xref>] and presented in [<xref ref-type="bibr" rid="pcbi.1006557.ref012">12</xref>]. The HMAX C2 units were designed to extract moderately complex features from objects, irrespective of size, position and their relative geometry in the image. HMAX-based dissimilarities were computed as the Euclidean distance between the output of the 256 C2 units.</p>
</sec>
</sec>
<sec id="sec013">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006557.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Representational similarity analysis of deep CNN layers and IT neurons for the whole shape set.</title>
<p>Spearman rank correlation coefficients between IT and model layer similarities are shown for each layer of the three CNN models used. The same analysis was performed for two distance metrics: Euclidean distance (left column) and 1-Spearman rank correlation (right column). Error bars depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuron pool (n = 119 neurons). Stars indicate layers for which the Spearman rank correlations for the trained version differed significantly from its untrained version (paired bootstrap test (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>); False Discovery Rate corrected q&lt;0.05). Crosses indicate trained layers which differed significantly from the first convolutional layer of the network (paired bootstrap test (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>); False Discovery Rate corrected q&lt;0.05). Layers are indicated by the same nomenclature as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref> of the main text. The horizontal line and gray band indicate the median and 95% interval, respectively, of the Spearman-Brown corrected split-half correlations (n = 10000 splits) of the neuronal distances, as an estimate of the noise ceiling.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006557.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Representational similarity analysis of deep CNN layers and IT neurons for the whole shape set with two different sizes.</title>
<p>Spearman rank correlation coefficients between IT and model layer similarities are shown for each layer of the three CNN models for the original and twofold smaller sizes (“reduced size”). The dissimilarities were Euclidean distances. Error bars depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuron pool (n = 119 neurons). Stars indicate layers for which the Spearman rank correlations for the trained version differed significantly from its untrained version (paired bootstrap test; False Discovery Rate corrected q&lt;0.05). The horizontal line and gray band indicate the median and 95% interval, respectively, of the Spearman-Brown corrected split-half correlations (n = 10000 splits) of the neuronal distances, as an estimate of the noise ceiling.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006557.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Similarities between IT and CCN peak layer shape dissimilarities as a function of percent of units.</title>
<p>(A) Spearman rank correlation coefficients between IT and peak CNN layer similarities are shown for each of the three CNN models as a function of sample size, expressed as percentage of the total number of units that were activated differentially by the 64 shapes. (B) Pearson correlation coefficients between the mean neural distances and the mean distances of the peak CNN layer (n = 6 mean distances; <xref ref-type="fig" rid="pcbi.1006557.g008">Fig 8</xref> of the main text) as a function of percentage of the total number of units. The total number of units (100%) for each CNN layer is listed in the legend. Note that 0.1% corresponds to only 3 Alexnet units, explaining the large range of correlations for that sample size. The dissimilarities were Euclidean distances. Error bars depict 95% confidence intervals, determined by 10,000 random samples from the population of differentially activated CNN units of that layer.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006557.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Similarities between IT and CNN peak layer shape dissimilarities as a function of retained Principal Components.</title>
<p>Top left panels: Spearman rank correlation coefficients between IT and peak CNN layer similarities are shown for each of the three CNN models as a function of retained principal components of the CNN layer activations. The dissimilarities were Euclidean distances. Error bands depict 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuronal pool. Top right panel: The cumulative proportion of explained variance as a function of principal component number for the 3 CNN. Bottom panels: Pearson correlation coefficients between the mean neural distances and the mean distances of the peak CNN layer (n = 6 mean distances; see <xref ref-type="fig" rid="pcbi.1006557.g010">Fig 10</xref> of the main text) as a function of retained principal components. The error bands represent 95% confidence intervals, determined by 10,000 bootstrap samples of the IT neuronal pool.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006557.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006557.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Correspondence between model dissimilarities and biological dissimilarities (IT responses and human judgement-based dissimilarities) for the shape groups.</title>
<p>(A, B, C). Gray curves show the Pearson correlation coefficients between the mean neural distances and the mean distances of the CNN layers (n = 6 mean distances per layer). Blue curves show the Pearson correlation coefficients of the CNN layer distances and the distances based on human judgements. Data for trained and untrained CNNs are plotted with full and dashed lines, respectively. Nomenclature of CNN layers as in <xref ref-type="fig" rid="pcbi.1006557.g002">Fig 2</xref> of the main text. Results for all three models (Alexnet, VGG-16 and VGG-19) are displayed in the subplots (A, B and C). (D) Neural: Pearson correlation coefficient between the mean IT distances and the mean distances of the peak Alexnet layer, peak VGG-16 layer, VGG-19 layer, the mean HMAX C2 layer distances, and mean pixel-based distances, across shape groups. Human: Pearson correlation coefficient between the distances based on the human judgements and the peak Alexnet layer, peak VGG-16 layer, peak VGG-19 layer, HMAX C2 layer and pixel-based distances.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>M. De Paep, P. Kayenbergh, G. Meulemans, G. Vanparrijs and W. Depuydt provided technical help for the IT recordings. We thank G. Kayaert and I. Biederman for their contributions to the work described in Kayaert et al.. We would like to thank NVIDIA for providing us with a GeForce GTX TITAN X.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006557.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Majaj</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>: <fpage>613</fpage>–<lpage>622</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4247" xlink:type="simple">10.1038/nn.4247</ext-link></comment> <object-id pub-id-type="pmid">26900926</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ardila</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <etal>et al</etal>. <article-title>Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003963" xlink:type="simple">10.1371/journal.pcbi.1003963</ext-link></comment> <object-id pub-id-type="pmid">25521294</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>The Code for Facial Identity in the Primate Brain</article-title>. <source>Cell</source>. <year>2017</year>;<volume>169</volume>: <fpage>1013</fpage>–<lpage>1028. e14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2017.05.011" xlink:type="simple">10.1016/j.cell.2017.05.011</ext-link></comment> <object-id pub-id-type="pmid">28575666</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalfas</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Shape Selectivity of Middle Superior Temporal Sulcus Body Patch Neurons</article-title>. <source>Eneuro</source>. <year>2017</year>; ENEURO.0113-17.2017. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/ENEURO.0113-17.2017" xlink:type="simple">10.1523/ENEURO.0113-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28660250</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horikawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kamitani</surname> <given-names>Y</given-names></name>. <article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title>. <source>Nat Commun</source>. Nature Publishing Group; <year>2015</year>;<volume>8</volume>: <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms15037" xlink:type="simple">10.1038/ncomms15037</ext-link></comment> <object-id pub-id-type="pmid">28530228</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eickenberg</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gramfort</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Varoquaux</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Thirion</surname> <given-names>B</given-names></name>. <article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title>. <source>Neuroimage</source>. <year>2017</year>;<volume>152</volume>: <fpage>184</fpage>–<lpage>194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.10.001" xlink:type="simple">10.1016/j.neuroimage.2016.10.001</ext-link></comment> <object-id pub-id-type="pmid">27777172</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>: <fpage>27755</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep27755" xlink:type="simple">10.1038/srep27755</ext-link></comment> <object-id pub-id-type="pmid">27282108</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>M a. J</given-names></name>. <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>10005</fpage>–<lpage>10014</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Vogels</surname> <given-names>R.</given-names></name> <chapter-title>Neural mechanisms of object recognition in nonhuman primates</chapter-title>. In: <name name-style="western"><surname>Lazareva</surname> <given-names>Olga F.</given-names></name>, <name name-style="western"><surname>Toru</surname> <given-names>Shimizu</given-names></name>, and <name name-style="western"><surname>Edward</surname> <given-names>A</given-names></name>. <source>Wasserman, How Animals see the World: Comparative Behavior, Biology, and Evolution of Vision</source>. <publisher-name>Oxford Univerity Press</publisher-name>, <year>2012</year>. pp. <fpage>246</fpage>–<lpage>268</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006557.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Representation of regular and irregular shapes in macaque inferotemporal cortex</article-title>. <source>Cereb Cortex</source>. <year>2005</year>;<volume>15</volume>: <fpage>1308</fpage>–<lpage>1321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhi014" xlink:type="simple">10.1093/cercor/bhi014</ext-link></comment> <object-id pub-id-type="pmid">15616128</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kimia</surname> <given-names>BB</given-names></name>. <article-title>On the role of medial geometry in human vision</article-title>. <source>Journal of Physiology Paris</source>. <year>2003</year>; <volume>97</volume>:<fpage>155</fpage>–<lpage>190</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jphysparis.2003.09.003" xlink:type="simple">10.1016/j.jphysparis.2003.09.003</ext-link></comment> <object-id pub-id-type="pmid">14766140</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I.</given-names></name> <article-title>Recognition-by-components: a theory of human image understanding</article-title>. <source>Psychol Rev</source>. <year>1987</year>;<volume>94</volume>: <fpage>115</fpage>–<lpage>147</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.94.2.115" xlink:type="simple">10.1037/0033-295X.94.2.115</ext-link></comment> <object-id pub-id-type="pmid">3575582</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gerhardstein</surname> <given-names>PC</given-names></name>. <article-title>Recognizing depth-rotated objects: evidence and conditions for three-dimensional viewpoint invariance</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1993</year>;<volume>19</volume>: <fpage>1162</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0090355" xlink:type="simple">10.1037/h0090355</ext-link></comment> <object-id pub-id-type="pmid">8294886</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>. <article-title>One-shot viewpoint invariance in matching novel objects</article-title>. <source>Vision Res</source>. <year>1999</year>;<volume>39</volume>: <fpage>2885</fpage>–<lpage>2899</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0042-6989(98)00309-5" xlink:type="simple">10.1016/S0042-6989(98)00309-5</ext-link></comment> <object-id pub-id-type="pmid">10492817</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lorincz</surname> <given-names>a</given-names></name>. <article-title>Inferior temporal neurons show greater sensitivity to nonaccidental than to metric shape differences</article-title>. <source>J Cogn Neurosci</source>. <year>2001</year>;<volume>13</volume>: <fpage>444</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/08989290152001871" xlink:type="simple">10.1162/08989290152001871</ext-link></comment> <object-id pub-id-type="pmid">11388918</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Shape tuning in macaque inferior temporal cortex</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>: <fpage>3016</fpage>–<lpage>27</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/16029211" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/16029211</ext-link> <object-id pub-id-type="pmid">12684489</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dong</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Socher</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>L-J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>F-F</given-names></name>. <article-title>ImageNet: A large-scale hierarchical image database</article-title>. <source>CVPR</source>. <year>2009</year>. pp. <fpage>248</fpage>–<lpage>255</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPRW.2009.5206848" xlink:type="simple">10.1109/CVPRW.2009.5206848</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2012</year>; <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006557.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simonyan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zisserman</surname> <given-names>A</given-names></name>. <article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title>. <source>arXiv Prepr</source>. <year>2014</year>; <fpage>1</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.infsof.2008.09.005" xlink:type="simple">10.1016/j.infsof.2008.09.005</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Beeck</surname> <given-names>HO</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title>. <source>Nat Neurosci</source>. <year>2001</year>;<volume>4</volume>: <fpage>1244</fpage>–<lpage>1252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn767" xlink:type="simple">10.1038/nn767</ext-link></comment> <object-id pub-id-type="pmid">11713468</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational similarity analysis–connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>; <volume>2</volume>: <fpage>4</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepard</surname> <given-names>RN</given-names></name>, <name name-style="western"><surname>Chipman</surname> <given-names>S</given-names></name>. <article-title>Second-order isomorphism of internal representations: shapes of states</article-title>. <source>Cognitive Psychology</source>. <year>1970</year>;<volume>1</volume>:<fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006557.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Edelman</surname> <given-names>S.</given-names></name> <article-title>Representation is representation of similarities</article-title>. <source>Behavioral and Brain Sciences</source>. <year>1998</year>; <volume>21</volume>:<fpage>448</fpage>–<lpage>498</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X98001253" xlink:type="simple">10.1017/S0140525X98001253</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Rocha-Miranda</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Bender</surname> <given-names>DB</given-names></name>. <article-title>Visual properties of neurons in inferotemporal cortex of the Macaque</article-title>. <source>J Neurophysiol</source>. <year>1972</year>;<volume>35</volume>: <fpage>96</fpage>–<lpage>111</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/4621506" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/4621506</ext-link> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1972.35.1.96" xlink:type="simple">10.1152/jn.1972.35.1.96</ext-link></comment> <object-id pub-id-type="pmid">4621506</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Desimonet</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Albrightf</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Grosso</surname> <given-names>CG</given-names></name>. <article-title>Shape recognition and inferior temporal neurons</article-title>. <source>Neurobiology</source>. <year>1983</year>;<volume>80</volume>: <fpage>5776</fpage>–<lpage>5778</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.80.18.5776" xlink:type="simple">10.1073/pnas.80.18.5776</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Albright</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>C</given-names></name>. <article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title>. <source>J Neurosci</source>. <year>1984</year>;<volume>4</volume>: <fpage>2051</fpage>–<lpage>2062</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/6470767" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/6470767</ext-link> <object-id pub-id-type="pmid">6470767</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sáry</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Cue-invariant shape selectivity of macaque inferior temporal neurons</article-title>. <source>Science</source>. <year>1993</year>;<volume>260</volume>: <fpage>995</fpage>–<lpage>997</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.8493538" xlink:type="simple">10.1126/science.8493538</ext-link></comment> <object-id pub-id-type="pmid">8493538</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gochin</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Colombo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dorfman</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Gerstein</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>CG</given-names></name>. <article-title>Neural ensemble coding in inferior temporal cortex</article-title>. <source>J Neurophysiol</source>. <year>1994</year>;<volume>71</volume>: <fpage>2325</fpage>–<lpage>2337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1994.71.6.2325" xlink:type="simple">10.1152/jn.1994.71.6.2325</ext-link></comment> <object-id pub-id-type="pmid">7931520</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rollenhagen</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>CR</given-names></name>. <article-title>Mirror-Image Confusion in Single Neurons of the Macaque Inferotemporal Cortex</article-title>. <source>Science</source> <year>2000</year>;<volume>287</volume>: <fpage>1506</fpage>–<lpage>1509</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.287.5457.1506" xlink:type="simple">10.1126/science.287.5457.1506</ext-link></comment> <object-id pub-id-type="pmid">10688803</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brincat</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Connor</surname> <given-names>CE</given-names></name>. <article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>: <fpage>880</fpage>–<lpage>886</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1278" xlink:type="simple">10.1038/nn1278</ext-link></comment> <object-id pub-id-type="pmid">15235606</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Multiple Object Response Normalization in Monkey Inferotemporal Cortex</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>8150</fpage>–<lpage>8164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2058-05.2005" xlink:type="simple">10.1523/JNEUROSCI.2058-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16148223</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sripati</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>CR</given-names></name>. <article-title>Responses to compound objects in monkey inferotemporal cortex: the whole is equal to the sum of the discrete parts</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>7948</fpage>–<lpage>7960</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0016-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0016-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20534843</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Baene</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Premereur</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Properties of shape tuning of macaque inferior temporal neurons examined using rapid serial visual presentation</article-title>. <source>J Neurophysiol</source>. <year>2007</year>;<volume>97</volume>: <fpage>2900</fpage>–<lpage>2916</lpage>. 00741.2006 [pii] <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00741.2006" xlink:type="simple">10.1152/jn.00741.2006</ext-link></comment> <object-id pub-id-type="pmid">17251368</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vighneshvel</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Arun</surname> <given-names>SP</given-names></name>. <article-title>Coding of relative size in monkey inferotemporal cortex</article-title>. <source>J Neurophysiol</source>. <year>2015</year>;<volume>113</volume>: <fpage>2173</fpage>–<lpage>2179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00907.2014" xlink:type="simple">10.1152/jn.00907.2014</ext-link></comment> <object-id pub-id-type="pmid">25589595</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhivago</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Arun</surname> <given-names>SP</given-names></name>. <article-title>Selective IT neurons are selective along many dimensions</article-title>. <source>J Neurophysiol</source>. <year>2016</year>;<volume>115</volume>: <fpage>1512</fpage>–<lpage>1520</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01151.2015" xlink:type="simple">10.1152/jn.01151.2015</ext-link></comment> <object-id pub-id-type="pmid">26823517</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N.</given-names></name> <article-title>Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing</article-title>. <source>Annu Rev Vis Sci</source>. <year>2015</year>;<volume>1</volume>: <fpage>417</fpage>–<lpage>446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-vision-082114-035447" xlink:type="simple">10.1146/annurev-vision-082114-035447</ext-link></comment> <object-id pub-id-type="pmid">28532370</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bracci</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>. <article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004896" xlink:type="simple">10.1371/journal.pcbi.1004896</ext-link></comment> <object-id pub-id-type="pmid">27124699</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>: <fpage>8619</fpage>–<lpage>8624</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment> <object-id pub-id-type="pmid">24812127</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname> <given-names>R.</given-names></name> <article-title>Categorization of complex visual images by rhesus monkeys. Part 1: Behavioural study</article-title>. <source>Eur J Neurosci</source>. <year>1999</year>;<volume>11</volume>: <fpage>1223</fpage>–<lpage>1238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1046/j.1460-9568.1999.00530.x" xlink:type="simple">10.1046/j.1460-9568.1999.00530.x</ext-link></comment> <object-id pub-id-type="pmid">10103118</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Popivanov</surname> <given-names>ID</given-names></name>, <name name-style="western"><surname>Jastorff</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Heterogeneous single-unit selectivity in an fMRI-defined body-selective patch</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>95</fpage>–<lpage>111</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2748-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2748-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24381271</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yosinski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Clune</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nguyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fuchs</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lipson</surname> <given-names>H</given-names></name>. <article-title>Understanding Neural Networks Through Deep Visualization</article-title>. <source>Int Conf Mach Learn—Deep Learn Work</source> <year>2015</year>. 2015; <volume>12</volume>. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1506.06579" xlink:type="simple">http://arxiv.org/abs/1506.06579</ext-link></mixed-citation></ref>
<ref id="pcbi.1006557.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geirhos</surname> <given-names>R.</given-names></name>, <name name-style="western"><surname>Janssen</surname> <given-names>D. H.</given-names></name>, <name name-style="western"><surname>Schütt</surname> <given-names>H. H.</given-names></name>, <name name-style="western"><surname>Rauber</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name>. <article-title>Comparing deep neural networks against humans: object recognition when the signal gets weaker</article-title>. <source>arXiv Prepr</source>. <year>2017</year>; 1706.06969, 2017. 5, 2017</mixed-citation></ref>
<ref id="pcbi.1006557.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nguyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yosinski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Clune</surname> <given-names>J</given-names></name>. <article-title>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</article-title>. <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source>. <year>2015</year>. pp. <fpage>427</fpage>–<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2015.7298640" xlink:type="simple">10.1109/CVPR.2015.7298640</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>IJ</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Szegedy</surname> <given-names>C</given-names></name>. <article-title>Explaining and Harnessing Adversarial Examples</article-title>. <source>arXiv Prepr</source>. <year>2014</year>; arXiv:1412.6572v3</mixed-citation></ref>
<ref id="pcbi.1006557.ref047"><label>47</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hahn</surname> <given-names>Ulrike</given-names></name> and <name name-style="western"><surname>Ramscar</surname> <given-names>M</given-names></name>. <source>Similarity and categorization</source>. <publisher-name>Oxford University Press</publisher-name>. <year>2001</year>. pp. <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/acprof" xlink:type="simple">10.1093/acprof</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vedaldi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lenc</surname> <given-names>K</given-names></name>. <article-title>MatConvNet—Convolutional Neural Networks for MATLAB</article-title>. <source>Arxiv</source>. <year>2014</year>; <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/2733373.2807412" xlink:type="simple">10.1145/2733373.2807412</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006557.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation></ref>
<ref id="pcbi.1006557.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name>. <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title> <source>Journal of the Royal Statistical Society B</source>. <year>1995</year>; <volume>57</volume>:<fpage>289</fpage>–<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2346101" xlink:type="simple">10.2307/2346101</ext-link></comment></mixed-citation></ref>
</ref-list>
</back>
</article>