<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.3000210</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-18-00335</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Audio signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Visual signals</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject><subj-group><subject>Event-related potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research assessment</subject><subj-group><subject>Research validity</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>To integrate or not to integrate: Temporal dynamics of hierarchical Bayesian causal inference</article-title>
<alt-title alt-title-type="running-head">Temporal dynamics of Bayesian causal inference</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7096-5180</contrib-id>
<name name-style="western">
<surname>Aller</surname>
<given-names>Máté</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Noppeney</surname>
<given-names>Uta</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Computational Neuroscience and Cognitive Robotics Centre, University of Birmingham, Birmingham, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Petkov</surname>
<given-names>Christopher</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Newcastle University Medical School, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">M.Aller@bham.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>2</day>
<month>4</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2019</year>
</pub-date>
<volume>17</volume>
<issue>4</issue>
<elocation-id>e3000210</elocation-id>
<history>
<date date-type="received">
<day>31</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>3</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Aller, Noppeney</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.3000210"/>
<abstract>
<p>To form a percept of the environment, the brain needs to solve the binding problem—inferring whether signals come from a common cause and are integrated or come from independent causes and are segregated. Behaviourally, humans solve this problem near-optimally as predicted by Bayesian causal inference; but the neural mechanisms remain unclear. Combining Bayesian modelling, electroencephalography (EEG), and multivariate decoding in an audiovisual spatial localisation task, we show that the brain accomplishes Bayesian causal inference by dynamically encoding multiple spatial estimates. Initially, auditory and visual signal locations are estimated independently; next, an estimate is formed that combines information from vision and audition. Yet, it is only from 200 ms onwards that the brain integrates audiovisual signals weighted by their bottom-up sensory reliabilities and top-down task relevance into spatial priority maps that guide behavioural responses. As predicted by Bayesian causal inference, these spatial priority maps take into account the brain’s uncertainty about the world’s causal structure and flexibly arbitrate between sensory integration and segregation. The dynamic evolution of perceptual estimates thus reflects the hierarchical nature of Bayesian causal inference, a statistical computation, which is crucial for effective interactions with the environment.</p>
</abstract>
<abstract abstract-type="toc">
<p>This electroencephalography-based study temporally resolves how the human brain uses Bayesian Causal Inference in perception, unravelling how the brain arbitrates between information integration and segregation by dynamically encoding multiple perceptual estimates.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The ability to tell whether various sensory signals come from the same or different sources is essential for forming a coherent percept of the environment. For example, when crossing a busy road at dusk, seeing and hearing an approaching car helps us estimate its location better, but only if its visual image is associated—correctly—with its sound and not with the sound of a different car far away. This is the so-called binding problem, and numerous studies have demonstrated that humans solve this near-optimally as predicted by Bayesian causal inference; however, the underlying neural mechanisms remain unclear. We combined Bayesian modelling, electroencephalography (EEG), and multivariate decoding in an audiovisual spatial localisation task to show that the brain dynamically encodes multiple spatial estimates while accomplishing Bayesian causal inference. First, auditory and visual signal locations are estimated independently; next, information from vision and audition is combined. Finally, from 200 ms onwards, the brain weights audiovisual signals by their sensory reliabilities and task relevance to guide behavioural responses as predicted by Bayesian causal inference.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>ERC-2012-StG_20111109 multsens</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Nopppeney</surname>
<given-names>Uta</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was funded by the European Research Council (<ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu/" xlink:type="simple">https://erc.europa.eu/</ext-link>; grant number: ERC-2012-StG_20111109 multsens). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="2"/>
<page-count count="31"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-04-12</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files (<xref ref-type="supplementary-material" rid="pbio.3000210.s001">S1 Data</xref>). Source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/allermat/Temporal-Dynamics-of-BCI" xlink:type="simple">https://github.com/allermat/Temporal-Dynamics-of-BCI</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In our natural environment, our senses are exposed to a barrage of sensory signals: the sight of a rapidly approaching truck, its looming motor noise, the smell of traffic fumes. How the brain effortlessly merges these signals into a seamless percept of the environment remains unclear. The brain faces two fundamental computational challenges: First, we need to solve the ‘binding’ or ‘causal inference’ problem—deciding whether signals come from a common cause and thus should be integrated or instead be treated independently [<xref ref-type="bibr" rid="pbio.3000210.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>]. Second, when there is a common cause, the brain should integrate signals taking into account their uncertainties [<xref ref-type="bibr" rid="pbio.3000210.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref004">4</xref>].</p>
<p>Hierarchical Bayesian causal inference provides a rational strategy to arbitrate between sensory integration and segregation in perception [<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>]. Bayesian causal inference explicitly models the potential causal structures that could have generated the sensory signals—i.e., whether signals come from common or independent sources. In line with Helmholtz’s notion of ‘unconscious inference’, the brain is then thought to invert this generative model during perception [<xref ref-type="bibr" rid="pbio.3000210.ref005">5</xref>]. In case of a common signal source, signals are integrated weighted in proportion to their relative sensory reliabilities (i.e., forced fusion [<xref ref-type="bibr" rid="pbio.3000210.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref006">6</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref010">10</xref>]). In case of independent sources, they are processed independently (i.e., full segregation [<xref ref-type="bibr" rid="pbio.3000210.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref012">12</xref>]). Iin a particular instance, the brain does not know the world’s causal structure that gave rise to the sensory signals. To account for this causal uncertainty, a final estimate (e.g., object’s location) is obtained by averaging the estimates under the two causal structures (i.e., common versus independent source models) weighted by each causal structure’s posterior probability—a strategy referred to as model averaging (for other decisional strategies, see [<xref ref-type="bibr" rid="pbio.3000210.ref013">13</xref>]).</p>
<p>A large body of psychophysics research has demonstrated that human observers combine sensory signals near-optimally as predicted by Bayesian causal inference [<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref013">13</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref016">16</xref>]. Most prominently, when locating events in the environment, observers gracefully transition between sensory integration and segregation as a function of audiovisual spatial disparity [<xref ref-type="bibr" rid="pbio.3000210.ref012">12</xref>]. For small spatial disparities, they integrate signals weighted by their reliabilities, leading to cross-modal spatial biases [<xref ref-type="bibr" rid="pbio.3000210.ref017">17</xref>]; for larger spatial disparities, audiovisual interactions are attenuated. A recent functional MRI (fMRI) study showed how Bayesian causal inference is accomplished within the cortical hierarchy [<xref ref-type="bibr" rid="pbio.3000210.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref016">16</xref>]: While early auditory and visual areas represented the signals on the basis that they were generated by independent sources (i.e., full segregation), the posterior parietal cortex integrated sensory signals into one unified percept (i.e., forced fusion). Only at the top of the cortical hierarchy, in anterior parietal cortex, the uncertainty about the world’s causal structure was taken into account and signals were integrated into a spatial estimate consistent with Bayesian causal inference.</p>
<p>The organisation of Bayesian causal inference across the cortical hierarchy raises the critical question of how these neural computations unfold dynamically over time within a trial. How does the brain merge spatial information that is initially coded in different reference frames and representational formats? Whereas the brain is likely to recurrently update all spatial estimates by passing messages forwards and backwards across the cortical hierarchy [<xref ref-type="bibr" rid="pbio.3000210.ref018">18</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref020">20</xref>], the unisensory estimates may to some extent precede the computation of the Bayesian causal inference estimate.</p>
<p>To characterise the neural dynamics of Bayesian causal inference, we presented human observers with auditory, visual, and audiovisual signals that varied in their spatial disparity in an auditory and visual spatial localisation task while recording their neural activity with electroencephalography (EEG). First, we employed cross-sensory decoding and temporal generalisation matrices [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>] of the unisensory auditory and visual signal trials to characterise the emergence and the temporal stability of spatial representations across the senses. Second, combining psychophysics, EEG, and Bayesian modelling, we temporally resolved the evolution of unisensory segregation, forced fusion, and Bayesian causal inference in multisensory perception.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>To determine the computational principles that govern multisensory perception we presented 13 participants with synchronous audiovisual spatial signals (i.e., white noise burst and Gaussian cloud of dots) that varied in their audiovisual spatial disparity and visual reliability (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1A and 1B</xref>). On each trial, participants reported their perceived location of either the auditory or the visual signal. In addition, we included unisensory auditory and visual signal trials under auditory or visual report, respectively.</p>
<fig id="pbio.3000210.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Experimental design, example trial, and behavioural and predicted AV weights (w<sub>AV</sub>).</title>
<p>(A) Experimental design. In a 4 × 4 × 2 × 2 factorial design, the experiment manipulated (1) the location of the visual (‘V’) signal (−10°, −3.3°, 3.3°, and 10°), (2) the location of the auditory (‘A’) signal (−10°, −3.3°, 3.3°, and 10°), (3) the reliability of the visual signal (VR+ versus low VR−, as defined by the spread of the visual cloud), and (4) task relevance (auditory versus visual report). In addition, we included unisensory auditory and visual VR+ and VR− trials. The greyscale codes the spatial disparity between the auditory and visual locations for each AV condition (i.e., darker greyscale = larger spatial disparity). (B) Time course of an example trial. (C) Behavioural AV weight index w<sub>AV</sub> computed from behavioural responses (left) and from the predictions of the Bayesian causal inference model (right; across-participants circular mean ± 68% CI and individual w<sub>AV</sub> represented by filled/empty circles, <italic>n</italic> = 13). The AV weight index w<sub>AV</sub> is shown as a function of (1) visual reliability: high [VR+] versus low [VR−]; (2) task relevance: auditory versus visual report; and (3) AV spatial disparity: small (≦6.6; D−) versus large (&gt;6.6; D+). The data used to make this figure are available in file <xref ref-type="supplementary-material" rid="pbio.3000210.s001">S1 Data</xref>. AV, audiovisual; D+, high disparity; D−, low disparity; VR+, high visual reliability; VR−, low visual reliability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.g001" xlink:type="simple"/>
</fig>
<p>Combining psychophysics, EEG, and computational modelling, we addressed two questions: First, we investigated when and how human observers form spatial representations from unisensory visual or auditory inputs, which generalise across the two sensory modalities. Second, we studied the computational principles and neural dynamics that mediate the integration of audiovisual signals into spatial representations that take into account the observer’s uncertainty about the world’s causal structure consistent with Bayesian causal inference.</p>
<sec id="sec003">
<title>Shared and distinct neural representations of space across vision and audition—Unisensory auditory and visual conditions</title>
<sec id="sec004">
<title>Behavioural results</title>
<p>Participants were able to locate unisensory auditory and visual signals reliably as indicated by a significant Pearson correlation between participants’ location responses and the true signal source location for both unisensory auditory (across subjects mean ± SEM: 0.88 ± 0.05), visual high reliability (VR+; across subjects mean ± SEM: 0.998 ± 0.19), and visual low reliability (VR−; across subjects mean ± SEM: 0.91 ± 0.05) conditions. As expected, observers were significantly less accurate when locating the sound than when locating the visual stimuli for both levels of visual reliability (VR+ versus A: t[12] 8.83, p &lt; 0.0001; VR− versus A: t[12] = 1.47, p = 0.005; see <xref ref-type="supplementary-material" rid="pbio.3000210.s003">S1 Fig</xref> for response distributions across all conditions).</p>
</sec>
<sec id="sec005">
<title>EEG results</title>
<p>Multivariate decoding of EEG activity patterns revealed how the brain dynamically encodes the location of unisensory auditory or visual signals. The decoding accuracy was expressed as the Pearson correlation coefficient between the true and the decoded stimulus locations and entered into so-called temporal generalisation matrices that illustrate the stability of EEG activity patterns encoding spatial location across time [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>]. If a support vector regression (SVR) model trained on EEG activity patterns at time t can correctly decode the stimulus location not only at time t but also at other time points, then the stimulus location is encoded in EEG activity patterns that are relatively stable across time (for further details about this temporal generalisation approach, see [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>]). If an SVR model cannot successfully generalise to EEG activity patterns at other time points, spatial locations are encoded in transient EEG activity patterns that differ across time.</p>
<p>For visual stimuli, spatial locations were successfully (i.e., significantly better than chance) decoded from EEG activity patterns from 60 ms onwards for visual stimuli (<xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>, upper right quadrant and <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2A Fig</xref>). Moreover, the temporal generalisation matrices suggest that the visual spatial representations were initially transient (i.e., 60–150 ms, significant decoding accuracy only near the diagonal), reflecting the early visual evoked EEG responses (e.g., N1, see <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2 Fig</xref>). Later (i.e., from about 250 ms), the location of the visual stimulus was encoded in a more sustained activity pattern (see <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2 Fig</xref>), leading to successful cross-temporal generalisation from 300 ms to 700 ms post stimulus (i.e., significantly better than chance decoding accuracy is present far off the diagonal).</p>
<fig id="pbio.3000210.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Temporal generalisation matrices within and across auditory and visual senses.</title>
<p>Each temporal generalisation matrix shows the decoding accuracy for each training (y-axis) and testing (x-axis) time point. We factorially manipulated the training data (auditory versus visual stimulation) and testing data (auditory versus visual stimulation). Decoding accuracy is quantified by the Pearson correlation between the true and the decoded locations of the auditory (or visual) stimulus. The grey line along the diagonal indicates where the training time is equal to the testing time (i.e., the time-resolved decoding accuracies). Horizontal and vertical grey lines indicate the stimulus onset. The thin grey lines encircle clusters with decoding accuracies that were significantly better than chance at <italic>p</italic> &lt; 0.05 corrected for multiple comparisons. The thick grey lines encircle the clusters with decoding accuracies that were significantly better than chance jointly for both (1) auditory-to-visual and (2) visual-to-auditory cross-temporal generalisation at <italic>p</italic> &lt; 0.05 corrected for multiple comparisons. The data used to make this figure are available in file <xref ref-type="supplementary-material" rid="pbio.3000210.s001">S1 Data</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.g002" xlink:type="simple"/>
</fig>
<p>By contrast, auditory spatial representations could be decoded significantly better than chance from about 95 ms onwards (see <xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>, lower left quadrant), which corresponds to the auditory N1 component (<xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3 Fig</xref>). Particularly from 200 ms onwards, the SVR decoder trained on EEG activity patterns can decode auditory spatial location significantly better than chance also from EEG activity patterns across other time points, even as late as 700 ms post stimulus (significant cluster encircled by thin grey lines in <xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>). This temporal generalisation profile indicates that auditory spatial locations were encoded in EEG activity patterns that were relatively stable across time later from 200 ms onwards. Visual inspection of the EEG topographies shows that auditory spatial location is encoded at these later processing stages in sustained activity patterns that correspond to the long latency auditory P2 component (see <xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3 Fig</xref>) [<xref ref-type="bibr" rid="pbio.3000210.ref022">22</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref024">24</xref>].</p>
<p>In addition to temporal generalisation within each sensory modality, we also investigated the extent to which the SVR decoding model generalised across sensory modalities throughout poststimulus time. Whereas earlier neural representations were more specific to each particular sensory modality, the SVR model was able to generalise significantly better than chance from audition to vision and vice versa from 160 to 360 ms (<xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>, upper left and lower right quadrant, areas encircled by thick grey line indicate significant generalisation across sensory modalities). This cross-sensory generalisation across visual- and auditory-evoked EEG activity patterns suggests that at those stages (i.e., 160 ms to 360 ms), the brain forms spatial representations that are relatively stable and rely on neural generators that may be partly shared across sensory modalities. By contrast, the spatial representations encoded in very early (&lt;160 ms) EEG activity patterns did not enable successful cross-sensory generalisation, suggesting that they are modality-specific. These statistically significant cross-sensory generalisation results are also illustrated by the EEG topographies evoked by unisensory auditory and visual signals (see <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2B</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3B</xref> Figs). From 200 ms to 400 ms, poststimulus auditory and visual stimuli elicit centro-posterior dominant topographies that depend on the stimulus location to some extent similarly in vision and audition. Although these results may point towards partly overlapping neural generators and representations potentially in parietal cortices that encode location both in audition and vision, it is important to emphasise that different configurations of neural generators can in principle elicit similar EEG scalp topographies.</p>
</sec>
</sec>
<sec id="sec006">
<title>Computational principles of audiovisual integration: GLM-based w<sub>AV</sub> and Bayesian modelling analysis—Audiovisual conditions</title>
<p>Combining psychophysics, multivariate EEG pattern decoding, and computational modelling, we next investigated the computational principles and neural dynamics underlying audiovisual integration of spatial representations using a general linear model (GLM)-based w<sub>AV</sub> and a Bayesian modelling analysis. As shown in <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3</xref>, both analyses were applied to the spatial estimates that were either reported by participants (i.e., behaviour, <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3B</xref> left) or decoded from EEG activity patterns independently for each poststimulus time point (i.e., neural, <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3B</xref> right, for further details, see the <xref ref-type="sec" rid="sec010">Methods</xref> section and the <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3</xref> legend).</p>
<fig id="pbio.3000210.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.g003</object-id>
<label>Fig 3</label>
<caption>
<title>GLM-based w<sub>AV</sub> and Bayesian modelling analysis overview.</title>
<p>(A) The GLM-based w<sub>AV</sub> and Bayesian modelling analysis were performed on auditory (‘A’) and visual (‘V’) spatial estimates that were indicated by participants as behavioural localisation responses (left, ‘Behaviour’) or decoded from participants’ EEG activity patterns (right, ‘Neural’). The neural spatial estimates were obtained by training an SVR model on ERP activity patterns at each time point of the AV congruent trials to learn the mapping from EEG pattern to external spatial locations (black diagonal line). This learnt mapping was then used to decode the spatial location from the ERP activity patterns of the spatially congruent and incongruent AV conditions (coloured arrows). (B) Distributions of spatial localisation responses (left, Behaviour: S<sub>Resp</sub>) and decoded spatial estimates (right, Neural: S<sub>Dec</sub>) were computed for each of the 64 conditions of the 4 (visual stimulus location) × 4 (auditory stimulus location) × 2 (visual reliability) × 2 (task relevance) factorial design. (C) Left: In the GLM-based w<sub>AV</sub> analysis, the perceived (or decoded at each time point) spatial estimates were predicted by the true visual and auditory spatial locations (S<sub>V1..8</sub>, S<sub>A1..8</sub>) for each of the eight conditions in the 2 (visual reliability: high versus low) × 2 (task relevance: auditory versus visual report) × 2 (spatial disparity: ≤6.6° versus &gt;6.6°) factorial design. As a summary index, we defined the relative audiovisual weight (w<sub>AV</sub>) as the four-quadrant inverse tangent of the visual (ß<sub>V1..8</sub>) and auditory (ß<sub>A1..8</sub>) parameter estimates for each of the eight conditions in each regression model. Right: In the Bayesian modelling analysis, we fitted the following models to observers’ behavioural and neural spatial estimates: SegA (green, for EEG only), SegV (red, for EEG only), SegV,A (light blue), ‘forced fusion’ (‘Fusion’, yellow), and BCI model (with model averaging, dark blue). We performed Bayesian model selection at the group level and computed the protected exceedance probability that one model is better than any of the other candidate models above and beyond chance [<xref ref-type="bibr" rid="pbio.3000210.ref025">25</xref>]. (D) Left: Based on previous studies [<xref ref-type="bibr" rid="pbio.3000210.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref016">16</xref>], we hypothesised that the w<sub>AV</sub> profile with an interaction between task relevance (i.e., visual versus auditory report) and spatial disparity that is characteristic for BCI would emerge relatively late. Right: Likewise, we expected the different models to dominate the EEG activity patterns to some extent sequentially: first the unisensory segregation model (SegV, SegA), followed by the forced-fusion model (‘Fusion’), and finally the BCI estimate. The fading of colours indicates that we did not have specific hypotheses for those times. AV, audiovisual; BCI, Bayesian causal inference; D+, high disparity; D−, low disparity; EEG, electroencephalography; ERP, event-related potential; GLM, general linear model; S<sub>Dec</sub>, Spatial estimate decoded; SegA, unisensory auditory segregation; SegV, unisensory visual segregation; SegV,A, audiovisual full-segregation; S<sub>Resp</sub>, spatial estimate responded; stim, stimulus; SVR, support vector regression; VR+, high visual reliability; VR−, low visual reliability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.g003" xlink:type="simple"/>
</fig>
<p>The GLM-based w<sub>AV</sub> analysis quantifies the influence of the true auditory and true visual location on (1) the reported or (2) EEG decoded auditory and visual spatial estimates in terms of an audiovisual weight index w<sub>AV</sub>.</p>
<p>The Bayesian modelling analysis formally assessed the extent to which (2) the full-segregation model(s) (<xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, encircled in light blue, red or green), (2) the forced-fusion model (<xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, yellow), and (3) the Bayesian causal inference model (i.e., using model averaging as decision function, encircled in dark blue; see supporting material <xref ref-type="supplementary-material" rid="pbio.3000210.s008">S1 Table</xref> for other decision functions) can account for the spatial estimates reported by observers (i.e., behaviour) or decoded from EEG activity pattern (i.e., neural).</p>
<sec id="sec007">
<title>Behavioural results</title>
<p>In a GLM-based w<sub>AV</sub> analysis, the behavioural audiovisual weight index w<sub>AV</sub> shows that observers integrated audiovisual signals weighted by their sensory reliabilities and task relevance (see <xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s003">S1 Fig</xref> for histograms of reported signal locations across all conditions).</p>
<p>The audiovisual weight index w<sub>AV</sub> was close to 90° (i.e., pure visual influence) when the visual signal needed to be reported (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref>, dark lines). But it shifted towards 0° when the auditory signal was task relevant (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref>, grey lines). In other words, we observed a significant main effect of task relevance on behavioural w<sub>AV</sub> (<italic>p</italic> = 0.0002). Observers flexibly adjusted the weights they assigned to auditory and visual signals in the integration process as a function of task relevance, giving more emphasis to the sensory modality that needed to be reported. The main effect of task relevance on w<sub>AV</sub> is inconsistent with classical forced-fusion models, in which audiovisual signals are integrated into one single unified percept irrespective of task relevance of the sensory modalities. In other words, even in the case of audiovisual spatial disparity, the observer would perceive the auditory and visual signals at the same location. Instead, it indicates that observers maintain separate auditory and visual spatial estimates for an audiovisual spatially disparate stimulus.</p>
<p>Consistent with Bayesian causal inference, the difference in w<sub>AV</sub> between auditory and visual report significantly increased for large (&gt;6.6°) relative to small (≤6.6°) spatial disparities (i.e., significant interaction between task relevance and spatial disparity: <italic>p</italic> = 0.0002). In other words, audiovisual integration and cross-modal spatial biasing broke down when auditory and visual signals were far apart and likely to be caused by independent sources. This attenuation of audiovisual interactions for large relative to small spatial disparities (i.e., interaction between task relevance and disparity) is the characteristic profile of Bayesian causal inference (see model predictions for w<sub>AV</sub> in <xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref> right).</p>
<p>Moreover, we observed significant two-way interactions between visual reliability and spatial disparity (<italic>p</italic> = 0.0014) and between visual reliability and task relevance (<italic>p</italic> = 0.0002). The effect of high versus low visual reliability was stronger when the two signals were close in space and the auditory (i.e., less reliable) signal needed to be reported. For auditory report conditions, the influence of the visual signal on the audiovisual spatial representation is stronger for high visual reliability and small disparity trials (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref>, difference between dashed and solid grey line for the small spatial disparity condition). Again, this interaction is expected for Bayesian causal inference, because the spatial estimate furnished by the forced-fusion model receives a stronger weight in Bayesian causal inference for low-spatial-disparity trials, when it is likely that the two signals come from a common source.</p>
<p>Consistent with the profile of the audiovisual weight index w<sub>AV</sub>, formal Bayesian model comparison showed that the Bayesian causal inference model outperformed the full-segregation and forced-fusion models (85.6% ± 0.3% variance explained, protected exceedance probability &gt; 0.99; <xref ref-type="table" rid="pbio.3000210.t001">Table 1</xref>). <xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref> (right) shows the profile of the audiovisual weight index w<sub>AV</sub> that is predicted by the Bayesian causal inference model fitted to the observer’s behavioural localisation responses. It illustrates that Bayesian causal inference inherently accounts for effects of task relevance (or reported modality) and the interaction between task relevance and spatial disparity by combining the forced-fusion estimate with the task-relevant full-segregation estimate weighted by the posterior probability of common and independent sources. Conversely, the interaction between reliability and spatial disparity arises because the forced-fusion model component, which integrates signals weighted by their reliabilities, is more dominant for small spatial disparities.</p>
<table-wrap id="pbio.3000210.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.t001</object-id>
<label>Table 1</label> <caption><title>Model parameters (across-subjects’ mean ± SEM) of the computational models fit to observers’ behavioural localisation reports.</title></caption>
<alternatives>
<graphic id="pbio.3000210.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Model</th>
<th align="justify">p<sub>c</sub></th>
<th align="justify">σ<sub>p</sub></th>
<th align="justify">σ<sub>A</sub></th>
<th align="justify">σ<sub>V1</sub></th>
<th align="justify">σ<sub>V2</sub></th>
<th align="justify">R<sup>2</sup></th>
<th align="justify">relBIC<sub>group</sub></th>
<th align="justify">PEP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">BCI</td>
<td align="left">0.15 ± 0.04</td>
<td align="left">36.4 ± 11.0</td>
<td align="left">4.4 ± 0.2</td>
<td align="left">0.3 ± 0.15</td>
<td align="left">3.5 ± 0.24</td>
<td align="left">0.86 ± 0.003</td>
<td align="left">0</td>
<td align="left">0.9992</td>
</tr>
<tr>
<td align="left">Fus</td>
<td align="left">-</td>
<td align="left">71.5 ± 7.4</td>
<td align="left">9.7 ± 0.3</td>
<td align="left">7.9 ± 0.3</td>
<td align="left">9.8 ± 0.35</td>
<td align="left">0.46 ± 5 × 10<sup>−4</sup></td>
<td align="left">−3.73 × 10<sup>−4</sup></td>
<td align="left">2.9 × 10<sup>−4</sup></td>
</tr>
<tr>
<td align="left">SegV,A</td>
<td align="left">-</td>
<td align="left">42.7 ± 15.2</td>
<td align="left">4.5 ± 0.3</td>
<td align="left">0.4 ± 0.16</td>
<td align="left">3.5 ± 0.23</td>
<td align="left">0.85 ± 0.004</td>
<td align="left">−865.1</td>
<td align="left">5.3 × 10<sup>−4</sup></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Abbreviations: BCI, Bayesian causal inference model; Fus, fusion model; PEP, protected exceedance probability [<xref ref-type="bibr" rid="pbio.3000210.ref025">25</xref>]; R<sup>2</sup>, coefficient of determination; relBIC<sub>group</sub>, group-level relative Bayesian information criterion; SegV,A, audiovisual full-segregation.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>In summary, our audiovisual weight index w<sub>AV</sub> and Bayesian modelling analysis of observers’ perceived/reported locations provided convergent evidence that human observers integrate audiovisual spatial signals weighted by their relative reliabilities at small spatial disparities. Yet, they mostly segregate audiovisual signals at large spatial disparities, when it is unlikely that signals come from a common source.</p>
</sec>
<sec id="sec008">
<title>EEG results—Temporal dynamics of audiovisual integration</title>
<p>To characterise the neural dynamics underlying integration of audiovisual signals into spatial representations, we applied the GLM-based w<sub>AV</sub> and the Bayesian modelling analysis to the ‘spatial estimates’ that were decoded from EEG activity patterns at each time point (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3B</xref> right). Because both the GLM-based w<sub>AV</sub> and the Bayesian modelling analysis require reliable spatial estimates, we report and interpret results limited to the time window from 55 ms to 700 ms post stimulus (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4</xref>, <xref ref-type="supplementary-material" rid="pbio.3000210.s006">S4 Fig</xref>), during which the location of congruent audiovisual stimuli could be decoded better than chance from EEG activity patterns (<italic>p</italic> &lt; 0.001).</p>
<fig id="pbio.3000210.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.g004</object-id>
<label>Fig 4</label>
<caption>
<title>EEG results for GLM-based w<sub>AV</sub> and Bayesian modelling analysis.</title>
<p>The neural audiovisual weight index w<sub>AV</sub> (across-participants’ circular mean ± 68% CI; <italic>n</italic> = 13). Neural w<sub>AV</sub> as a function of time is shown for (A) visual reliability: VR+ versus VR−; (B) task relevance: auditory (‘A’) versus visual (‘V’) report; (C) audiovisual spatial disparity: small (≦6.6; D−) versus large (&gt;6.6; D+); (D) the interaction between task relevance and disparity. Shaded grey areas indicate the time windows during which the main effect of (A) visual reliability, (B) task relevance, (C) audiovisual spatial disparity, or (D) the interaction between task relevance and disparity on w<sub>AV</sub> was statistically significant at <italic>p</italic> &lt; 0.05 corrected for multiple comparisons across time. (E) Time course of the circular–circular correlation (across-participants’ mean after Fisher z-transformation ± 68% CI; <italic>n</italic> = 13) between the neural and the behavioural audiovisual weight index w<sub>AV</sub>. Shaded grey areas indicate significant correlation at <italic>p</italic> &lt; 0.05 corrected for multiple comparisons across time. (F) Time course of the protected exceedance probabilities [<xref ref-type="bibr" rid="pbio.3000210.ref025">25</xref>] of the five models of the Bayesian modelling analysis: SegA (green), SegV (red), SegV,A (light blue), ‘forced fusion’ (‘Fusion’, yellow), and BCI model (with model averaging, dark blue). The early time window until 55 ms (delimited by black vertical line on all plots) is shaded in white, because the decoding accuracy was not greater than chance for audiovisual congruent trials; hence, the neural weight index w<sub>AV</sub> and Bayesian model fits are not interpretable in this window. The data used to make this figure are available in file <xref ref-type="supplementary-material" rid="pbio.3000210.s001">S1 Data</xref>. BCI, Bayesian causal inference; D+, high disparity; D−, low disparity; EEG, electroencephalography; GLM, general linear model; SegA, unisensory auditory segregation; SegV, unisensory visual segregation; SegV,A, audiovisual full-segregation; VR+, high visual reliability; VR−, low visual reliability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.g004" xlink:type="simple"/>
</fig>
<p>The GLM-based analysis of audiovisual weight index w<sub>AV</sub> investigated the effects of visual reliability, task relevance, and spatial disparity on the audiovisual neural weight index w<sub>AV</sub> that quantifies the influence of auditory and visual signals on the spatial representations decoded from EEG activity patterns. Our results show that sensory reliability significantly influenced the neural w<sub>AV</sub> from 65 to 510 ms. As expected, the spatial representations were more strongly influenced by the true visual signal location when the visual signal was more reliable (i.e., significant main effect of visual reliability, <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4A</xref>, <xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>). Moreover, consistent with our behavioural findings, we also observed a significant main effect of task relevance between 190 and 700 ms (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4B</xref>, <xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>). As expected, the decoded location was more strongly influenced by the visual signal when the visual modality was task relevant. We also observed a significant interaction between task relevance and spatial disparity from 310 to 440 ms and 510 to 590 ms. As discussed in the context of the behavioural results, this interaction is the profile that is characteristic for Bayesian causal inference: the brain integrates sensory signals at low spatial disparity (i.e., small difference for auditory versus visual report) but computes different spatial estimates for auditory and visual signals at large spatial disparities (see <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4D</xref>, <xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>).</p>
<table-wrap id="pbio.3000210.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.3000210.t002</object-id>
<label>Table 2</label> <caption><title>Statistical significance of main, interaction, and simple main effects for the behavioural and neural audiovisual weight indices (w<sub>AV</sub>) (‘model-free’ approach).</title></caption>
<alternatives>
<graphic id="pbio.3000210.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Effect</th>
<th align="center">Behavioural</th>
<th align="center">Neural</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VR</td>
<td align="left"><italic>p</italic> = 0.61</td>
<td align="left">65–510 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">TR</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">190–700 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">D</td>
<td align="left"><italic>p</italic> = 0.77</td>
<td align="left">55–130 ms: <italic>p</italic> = 0.0062<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">VR × TR</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">n.s.</td>
</tr>
<tr>
<td align="left">VR × D</td>
<td align="left"><italic>p</italic> = 0.0014<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">55–135 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref><break/>170–235 ms: <italic>p</italic> = 0.016<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">TR × D</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">310–440 ms: <italic>p</italic> = 0.004<xref ref-type="table-fn" rid="t002fn001">*</xref><break/>510–590 ms: <italic>p</italic> = 0.021<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">VR × TR × D</td>
<td align="left"><italic>p</italic> = 0.79</td>
<td align="left">n.s.</td>
</tr>
<tr>
<td align="left">VR in A</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">not tested</td>
</tr>
<tr>
<td align="left">VR in V</td>
<td align="left"><italic>p</italic> = 0.43</td>
<td align="left">not tested</td>
</tr>
<tr>
<td align="left">VR in D−</td>
<td align="left"><italic>p</italic> = 0.47</td>
<td align="left">105–165 ms: <italic>p</italic> = 0.008<xref ref-type="table-fn" rid="t002fn001">*</xref><break/>230–515 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">VR in D+</td>
<td align="left"><italic>p</italic> = 0.92</td>
<td align="left">55–380 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">TR in D−</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">230–535 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref><break/>570–630 ms: <italic>p</italic> = 0.024<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
<tr>
<td align="left">TR in D+</td>
<td align="left"><italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
<td align="left">235–670 ms: <italic>p</italic> = 0.0002<xref ref-type="table-fn" rid="t002fn001">*</xref></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001"><p>*Significant results (<italic>p</italic> &lt; 0.05, corrected at the cluster level for multiple comparisons).</p></fn>
<fn id="t002fn002"><p>Abbreviations: A, auditory report; D+, large disparity; D−, small disparity; n.s., not significant (p ≧ 0.05); TR, task relevance (visual, V, or auditory, A, report), V, visual report; VR, visual reliability (high or low).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>In addition to these key findings, we also observed a brief but pronounced significant main effect of spatial disparity on w<sub>AV</sub> at about 55–130 ms. Whereas a sound attracted the decoded spatial location at small spatial disparity (i.e., w<sub>AV</sub> is shifted below 90°, <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4C</xref> solid line), the decoded location is shifted away from the sound location (i.e., a repulsive effect) at large spatial disparity (i.e., w<sub>AV</sub> values above 90°, <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4C</xref>, dashed line). Moreover, in this early time window, which coincides with the visual-evoked N100 response, the decoded spatial estimate was overall dominated by the visual stimulus location (i.e, w<sub>AV</sub> was close to 90° for both small and large disparity). The effect of disparity may indicate that early multisensory processing is already influenced by a spatial window of integration (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4C</xref>, <xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>). Auditory stimuli affected the decoded spatial representations mainly when they were close in space with the visual signal. However, because spatial disparity was inherently correlated with the eccentricity of the audiovisual signals by virtue of our factorial and spatially balanced design, these two effects cannot be fully dissociated. While signals were presented parafoveally or peripherally for small-disparity trials, they were presented always in the periphery for large-disparity trials.</p>
<p>For completeness, we also observed a significant interaction between spatial disparity and visual reliability between 55 and 135 ms and between 170 and 235 ms (<xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>). This interaction results from a larger spatial window of integration for stimuli with low versus high visual reliability. In other words, it is easier to determine whether two signals come from different sources when the visual input is reliable, leading to a smaller window of integration.</p>
<p>Finally, we asked whether the neural audiovisual weights were related to the audiovisual weights that observers applied at the behavioural level. Hence, we computed the correlation between the values of the behavioural and neural weight indices w<sub>AV</sub> separately for each time point. The Fisher z-transformed correlation coefficient fluctuated around chance level until about 100 ms. From 100 ms onwards, it progressively increased over time until it peaked and reached a plateau at about 350 ms (R = 0.72). As expected, this coincides with the time window during which we observed a significant interaction between task relevance and spatial disparity—i.e., the profile characteristic for Bayesian causal inference. After 500 ms, it then slowly decreased towards the end of the trial. Cluster permutation test confirmed that the correlation between neural and behavioural weight indices w<sub>AV</sub> was significantly better than chance, revealing two significant clusters between 175 and 550 ms (<italic>p</italic> = 0.0012) and 575 and 665 ms (<italic>p</italic> = 0.013). These results indicate that the neural representations expressed in EEG activity patterns are critical for guiding observers’ responses.</p>
<p>In the EEG Bayesian modelling analysis, we fitted five models to the spatial estimates decoded from EEG activity patterns separately for each time point: (1) ‘full-segregation audiovisual’, (2) ‘forced-fusion’, (3) the ‘Bayesian causal inference’, (4) the ‘segregation auditory’, and (5) the ‘segregation visual’ models (<xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>). The segregation visual and segregation auditory models incorporate the hypothesis that neural generators may represent only the visual (or only the auditory) location irrespective of whether the visual (or auditory) location needs to be reported. In other words, they model a purely unisensory visual (or auditory) source. By contrast, the full-segregation audiovisual model embodies the hypothesis that a neural source represents the task-relevant location—i.e., the auditory location for auditory report and the visual location for visual report.</p>
<p>At the random-effects group level, Bayesian model comparison revealed a sequential pattern of protected exceedance probabilities across time (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4F</xref>): initially, the ‘segregation visual’ model dominated until about 100 ms post stimulus. This converges with our w<sub>AV</sub> analysis showing that spatial representations decoded from early EEG activity patterns are dominated by the location of the visual signal (i.e., w<sub>AV</sub> is close to 90°). From 100 to about 200 ms, the forced-fusion model outperformed the other models, indicating that spatial estimates are now influenced jointly by the locations of auditory and visual signals irrespective of their spatial disparity or task relevance. Again, this mirrors our w<sub>AV</sub> results in which we observed a significant effect of reliability on w<sub>AV</sub> early (i.e., as expected for forced fusion), whereas the effect of task relevance arose later and became prominent from 250 ms onwards.</p>
<p>Hence, both w<sub>AV</sub> and Bayesian modelling analyses suggest that in this early time window, audiovisual signals are predominantly integrated weighted by their reliability into a unified spatial representation irrespective of task relevance, as predicted by forced-fusion models. From about 200 ms onwards, the protected exceedance probability of the Bayesian causal inference model progressively increased, peaking with an exceedance probability of &gt;0.85 at about 350 ms followed by a plateau until 500 ms. Thus, consistent with the w<sub>AV</sub> results, audiovisual interactions consistent with Bayesian causal inference emerge relatively late at about 350 ms post stimulus.</p>
</sec>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>Integrating information from vision and audition into a coherent representation of the space around us is critical for effective interactions with the environment. This EEG study temporally resolved the neural dynamics that enable the brain to flexibly integrate auditory and visual signals into spatial representations in line with the predictions of Bayesian causal inference.</p>
<p>Auditory and visual senses code spatial location in different reference frames and representational formats [<xref ref-type="bibr" rid="pbio.3000210.ref026">26</xref>]. Vision provides spatial information in eye-centred and audition in head-centred reference frames [<xref ref-type="bibr" rid="pbio.3000210.ref027">27</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref028">28</xref>]. Furthermore, spatial location is directly coded in the retinotopic organisation in primary visual cortex [<xref ref-type="bibr" rid="pbio.3000210.ref029">29</xref>], whereas spatial location in audition is computed from sound latency and amplitude differences between the ears, starting in the brainstem [<xref ref-type="bibr" rid="pbio.3000210.ref027">27</xref>]. In auditory cortices of primates, spatial location is thought to be represented by neuronal populations with broad tuning functions [<xref ref-type="bibr" rid="pbio.3000210.ref030">30</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref031">31</xref>]. In order to merge spatial information from vision and audition, the brain thus needs to establish coordinate mappings and/or transform spatial information into partially shared ‘hybrid’ reference frames, as previously suggested by neurophysiological recordings in nonhuman primates [<xref ref-type="bibr" rid="pbio.3000210.ref030">30</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref032">32</xref>]. In the first step, we therefore investigated the neural dynamics of spatial representations encoded in EEG activity patterns separately for unisensory auditory and visual signals using the method of temporal generalisation matrices [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>]. In vision, spatial location was encoded initially at 60 ms in transient neural activity associated with the early P1 and N1 components and then turned into temporally more stable representations from 200 ms and particularly from 350 ms (<xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>, upper right quadrant, <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2 Fig</xref>). In audition, spatial location was encoded by relatively stable EEG activity from 95 ms and particularly from 250 ms, which is associated with the auditory long latency P2 component [<xref ref-type="bibr" rid="pbio.3000210.ref022">22</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref024">24</xref>] (<xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3 Fig</xref>).</p>
<p>Activity patterns encoding spatial location generalised not only across time but also across sensory modalities between 160 and 360 ms. As indicated in <xref ref-type="fig" rid="pbio.3000210.g002">Fig 2</xref>, SVR models trained on visual-evoked responses generalised to auditory-evoked responses and vice versa (upper left and lower right quadrant, significant cross-sensory generalisation encircled by thick grey line). These results suggest that unisensory auditory and visual spatial locations are initially represented by transient and modality-specific activity patterns. Later, at about 200 ms, they are transformed into temporally more stable representations that may rely on neural sources in frontoparietal cortices that are at least to some extent shared between auditory and visual modalities [<xref ref-type="bibr" rid="pbio.3000210.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref033">33</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref034">34</xref>].</p>
<p>Next, we asked when and how the human brain combines spatial information from vision and audition into a coherent representation of space. The brain should integrate sensory signals only when they come from a common event but should segregate signals from independent events [<xref ref-type="bibr" rid="pbio.3000210.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref012">12</xref>]. To investigate how the brain arbitrates between sensory integration and segregation, we presented observers with synchronous audiovisual signals that varied in their spatial disparity across trials. On each trial, observers reported either the auditory or the visual location. Our results show that a concurrent yet spatially disparate visual signal biased observers’ perceived sound location towards the visual location—a phenomenon coined spatial ventriloquist illusion [<xref ref-type="bibr" rid="pbio.3000210.ref017">17</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref035">35</xref>]. Consistent with reliability-weighted integration, this audiovisual spatial bias was significantly stronger when the visual signal was more reliable (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref> left, grey solid versus dashed lines). Furthermore, observers reported different locations for auditory and visual signals, and this difference was even greater for large- relative to small-spatial-disparity trials. This significant interaction between spatial disparity and task relevance indicates that human observers arbitrate between sensory integration and segregation depending on the probabilities of different causal structures of the world that can be inferred from audiovisual spatial disparity.</p>
<p>Using EEG, we then investigated how the brain forms neural spatial representations dynamically post stimulus. Our analysis of the neural audiovisual weight index w<sub>AV</sub> shows that the spatial estimates decoded from EEG activity patterns are initially dominated by visual inputs (i.e., w<sub>AV</sub> close to 90°). This visual dominance is most likely explained by the retinotopic representation of visual space that facilitates EEG decoding of space leading to visual predominance (for further discussion, see the <xref ref-type="sec" rid="sec010">Methods</xref> section). From about 65 ms onwards, visual reliability significantly influenced w<sub>AV</sub> (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4A</xref>): as expected, the location of the visual signal exerted a stronger influence on the spatial estimate decoded from EEG activity patterns when the visual signal was more reliable than unreliable. By contrast, the signal’s task relevance influenced the audiovisual weight index only later, from about 190 ms (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4B</xref>). Thus, visual reliability as a bottom-up stimulus-bound factor impacted the sensory weighting in audiovisual integration prior to top-down effects of task relevance. We observed a significant interaction between task relevance and spatial disparity as the characteristic profile for Bayesian causal inference from about 310 ms: the difference in w<sub>AV</sub> between auditory and visual report was significantly greater for large- than for small-disparity trials (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4D</xref>, <xref ref-type="table" rid="pbio.3000210.t002">Table 2</xref>). Thus, spatial disparity determined the influence of task-irrelevant signals on the spatial representations encoded in EEG activity from about 310 ms onwards. A task-irrelevant signal influenced the spatial representations mainly when auditory and visual signals were close in space and hence likely to come from a common event, but it had minimal influence when they were far apart in space. Collectively, our statistical analysis of the audiovisual weight index revealed a sequential emergence of visual dominance, reliability weighting (from about 100 ms), effects of task relevance (from about 200 ms), and finally the interaction between task relevance and spatial disparity (from about 310 ms, <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4A–4D</xref>).</p>
<p>This multistage process was also mirrored in the time course of exceedance probabilities furnished by our formal Bayesian model comparison: The unisensory visual segregation (SegV) model was the winning model for the first 100 ms, thereby modelling the early visual dominance. The audiovisual forced-fusion model embodying reliability-weighted integration dominated the time interval of 100–250 ms. Finally, the Bayesian causal inference model that enables the arbitration between sensory integration and segregation depending on spatial disparity outperformed all other models from 350 ms onwards. Hence, both our Bayesian modelling analysis and our w<sub>AV</sub> analysis showed that the hierarchical structure of Bayesian causal inference is reflected in the neural dynamics of spatial representations decoded from EEG. The Bayesian causal inference model also outperformed the audiovisual full-segregation (SegV,A) model that enables the representation of the location of the task-relevant stimulus unaffected by the location of the task-irrelevant stimulus. Instead, our Bayesian modelling analysis confirmed that from 350 ms onwards, the brain integrates audiovisual signals weighted by their bottom-up reliability and top-down task relevance into spatial priority maps [<xref ref-type="bibr" rid="pbio.3000210.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref037">37</xref>] that take into account the probabilities of the different causal structures consistent with Bayesian causal inference. The spatial priority maps were behaviourally relevant for guiding spatial orienting and actions, as indicated by the correlation between the neural and behavioural audiovisual weight indices, which progressively increased from 100 ms and culminated at about 300–400 ms. Two recent studies have also demonstrated such a temporal evolution of Bayesian causal inference in an audiovisual temporal numerosity judgement task [<xref ref-type="bibr" rid="pbio.3000210.ref038">38</xref>] and an audiovisual rate categorisation task [<xref ref-type="bibr" rid="pbio.3000210.ref039">39</xref>].</p>
<p>The timing and the parietal-dominant topographies of the AV potentials (see <xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3</xref> Figs) that form the basis for our spatial decoding (and hence for w<sub>AV</sub> and Bayesian modelling analyses) closely match the P3b component (i.e., a subcomponent of the classical P300). Although it is thought that the P3b relies on neural generators located mainly in parietal cortices [<xref ref-type="bibr" rid="pbio.3000210.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref041">41</xref>], its specific functional role remains controversial [<xref ref-type="bibr" rid="pbio.3000210.ref042">42</xref>]. Given its sensitivity to stimulus probability [<xref ref-type="bibr" rid="pbio.3000210.ref043">43</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref045">45</xref>] and discriminability [<xref ref-type="bibr" rid="pbio.3000210.ref046">46</xref>] as well as task context [<xref ref-type="bibr" rid="pbio.3000210.ref042">42</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref047">47</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref048">48</xref>], it was proposed to reflect neural processes involved in transforming sensory evidence into decisions and actions [<xref ref-type="bibr" rid="pbio.3000210.ref049">49</xref>]. Most recent research has suggested that the P3b may sustain processes of evidence accumulation [<xref ref-type="bibr" rid="pbio.3000210.ref050">50</xref>] that are influenced by observers’ prior [<xref ref-type="bibr" rid="pbio.3000210.ref051">51</xref>], incoming evidence (i.e., likelihood [<xref ref-type="bibr" rid="pbio.3000210.ref052">52</xref>]), and observers’ belief updating [<xref ref-type="bibr" rid="pbio.3000210.ref053">53</xref>]. Likewise, our supplementary time-frequency analyses revealed that alpha/beta power, which has previously been associated with the generation of the P3b component [<xref ref-type="bibr" rid="pbio.3000210.ref054">54</xref>], depended on bottom-up visual reliability between 200 and 400 ms and top-down task relevance between 350 and 550 ms post stimulus (see <xref ref-type="supplementary-material" rid="pbio.3000210.s007">S5 Fig</xref>, <xref ref-type="supplementary-material" rid="pbio.3000210.s009">S2 Table</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s002">S1 Text</xref>), thereby mimicking the temporal evolution of bottom-up and top-down influences observed in our main w<sub>AV</sub> and Bayesian modelling analysis.</p>
<p>Yet, our main analysis took a different approach. Rather than focusing on the effects of visual reliability, task relevance/attention, and spatial disparity directly on event-related potentials (ERPs) or time-frequency power, the w<sub>AV</sub> analysis investigated how these manipulations affect the spatial representations encoded in EEG activity patterns, and the Bayesian modelling analysis accommodated those effects directly in the computations of Bayesian causal inference. Along similar lines, two recent fMRI studies characterised the computations involved in integrating audiovisual spatial inputs across the cortical hierarchy [<xref ref-type="bibr" rid="pbio.3000210.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref016">16</xref>]: whereas low level auditory and visual areas predominantly encoded the unisensory auditory or visual locations (i.e., full-segregation model) [<xref ref-type="bibr" rid="pbio.3000210.ref055">55</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref064">64</xref>], higher-order visual areas and posterior parietal cortices combined audiovisual signals weighted by their sensory reliabilities (i.e., forced-fusion model) [<xref ref-type="bibr" rid="pbio.3000210.ref065">65</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref068">68</xref>]. Only at the top of the hierarchy, in anterior parietal cortices, did the brain integrate sensory signals consistent with Bayesian causal inference. Thus, the temporal evolution of Bayesian causal inference observed in our current EEG study mirrored its organisation across the cortical hierarchy observed in fMRI.</p>
<p>Fusing the results from EEG and fMRI studies (see caveats in the Methods section) thus suggests that Bayesian causal inference in multisensory perception relies on dynamic encoding of multiple spatial estimates across the cortical hierarchy. During early processing, multisensory perception is dominated by full-segregation models associated with activity in low-level sensory areas. Later audiovisual interactions that are governed by forced-fusion principles rely on posterior parietal areas. Finally, Bayesian causal inference estimates are formed in anterior parietal areas. Yet, although our results suggest that full segregation, forced fusion, and Bayesian causal inference dominate EEG activity patterns at different latencies, they do not imply a strictly feed-forward architecture. Instead, we propose that the brain concurrently accumulates evidence about the different spatial estimates and the underlying causal structure (i.e., common versus independent sources) most likely via multiple feedback loops across the cortical hierarchy [<xref ref-type="bibr" rid="pbio.3000210.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref019">19</xref>]. Only after 350 ms is a final perceptual estimate formed in anterior parietal cortices that takes into account the uncertainty about the world’s causal structure and combines audiovisual signals into spatial priority maps as predicted by Bayesian causal inference.</p>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec011">
<title>Participants</title>
<p>Sixteen right-handed participants participated in the experiment; three of those participants did not complete the entire experiment: two participants were excluded based on eye tracking results from the first day (the inclusion criterion was less than 10% of trials rejected because of eye blinks or saccades; see the Eye movement recording and analysis section for details), and one participant withdrew from the experiment. The remaining 13 participants (7 females, mean age = 22.1 years; SD = 3.0) completed the 3-day experiment and are thus included in the analysis. All participants had no history of neurological or psychiatric illnesses, had normal or corrected-to-normal vision, and had normal hearing.</p>
</sec>
<sec id="sec012">
<title>Ethics statement</title>
<p>All participants gave informed written consent to participate in the experiment. The study was approved by the research ethics committee of the University of Birmingham (approval number: ERN_11_0470AP4) and was conducted in accordance with the principles outlined in the Declaration of Helsinki.</p>
</sec>
<sec id="sec013">
<title>Stimuli</title>
<p>The visual (‘V’) stimulus was a cloud of 20 white dots (diameter = 0.43° visual angle, stimulus duration: 50 ms) sampled from a bivariate Gaussian distribution with vertical standard deviation of 2° and horizontal standard deviation of 2° or 12° visual angle presented on a dark grey background (67% contrast). Participants were told that the 20 dots were generated by one underlying source in the centre of the cloud. The visual cloud of dots was presented at one of four possible locations along the azimuth (i.e., −10°, −3.3°, 3.3°, or 10°).</p>
<p>The auditory (‘A’) stimulus was a 50-ms-long burst of white noise with a 5-ms on/off ramp. Each auditory stimulus was delivered at a 75-dB sound pressure level through one of four pairs of two vertically aligned loudspeakers placed above and below the monitor at four positions along the azimuth (i.e., −10°, −3.3°, 3.3°, or 10°). The volumes of the 2 × 4 speakers were carefully calibrated across and within each pair to ensure that participants perceived the sounds as emanating from the horizontal midline of the monitor.</p>
</sec>
<sec id="sec014">
<title>Experimental design and procedure</title>
<p>In a spatial ventriloquist paradigm, participants were presented with synchronous, spatially congruent or disparate visual and auditory signals (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1A and 1B</xref>). On each trial, visual and auditory locations were independently sampled from four possible locations along the azimuth (i.e., −10°, −3.3°, 3.3°, or 10°), leading to four levels of spatial disparity (i.e., 0°, 6.6°, 13.3°, or 20°; i.e., as indicated by the greyscale in <xref ref-type="fig" rid="pbio.3000210.g001">Fig 1A</xref>). In addition, we manipulated the reliability of the visual signal by setting the horizontal standard deviation of the Gaussian cloud to a 2° (high reliability) or 14° (low reliability) visual angle. In an intersensory selective-attention paradigm, participants reported either their auditory or visual perceived signal location and ignored signals in the other modality. For the visual modality, they were asked to determine the location of the centre of the visual cloud of dots. Hence, the 4 × 4 × 2 × 2 factorial design manipulated (1) the location of the visual stimulus (−10°, −3.3°, 3.3°, 10°; i.e., the mean of the Gaussian), (2) the location of the auditory stimulus (−10°, −3.3°, 3.3°, 10°), (3) the reliability of the visual signal (2°, 14°; SD of the Gaussian), and (4) task relevance (auditory-/visual-selective report), resulting in 64 conditions (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1A</xref>). To characterise the computational principles of multisensory integration, we reorganised these conditions into a 2 (visual reliability: high versus low) × 2 (task relevance: auditory versus visual report) × 2 (spatial disparity: ≤6.6° versus &gt;6.6°) factorial design for the statistical analysis of the behavioural and EEG data. In addition, we included 4 (locations: −10°, −3.3°, 3.3°, or 10°) × 2 (visual reliability: high, low) unisensory visual conditions and 4 (locations: −10°, −3.3°, 3.3°, or 10°) unisensory auditory conditions. We did not manipulate auditory reliability, because the reliability of auditory spatial information is anyhow limited. Furthermore, the manipulation of visual reliability is sufficient to determine reliability-weighted integration as a computational principle and arbitrate between the different multisensory integration models (see Bayesian modelling analysis section).</p>
<p>On each trial, synchronous audiovisual, unisensory visual, or unisensory auditory signals were presented for 50 ms, followed by a response cue 1,000 ms after stimulus onset (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1B</xref>). The response was cued by a central pure tone (1,000 Hz) and a blue colour change of the fixation cross presented in synchrony for 100 ms. Participants were instructed to withhold their response and avoid blinking until the presentation of the cue. They fixated on a central cross throughout the entire experiment. The next stimulus was presented after a variable response interval of 2.6–3.1 s.</p>
<p>Stimuli and conditions were presented in a pseudo-randomised fashion. The stimulus type (bisensory versus unisensory) and task relevance (auditory versus visual) was held constant within a run of 128 trials. This yielded four run types: audiovisual with auditory report, audiovisual with visual report, auditory with auditory report, and visual with visual report. The task relevance of the sensory modality in a given run was displayed to the participant at the beginning of the run. Furthermore, across runs we counterbalanced the response hand (i.e., left versus right hand) to partly dissociate spatial processing from motor responses. The order of the runs was counterbalanced across participants. All conditions within a run were presented an equal number of times. Each participant completed 60 runs, leading to 7,680 trials in total (3,840 auditory and 3,840 visual localisation tasks—i.e., 96 trials for each of the 76 conditions were included in total; apart from the four unisensory auditory conditions that included 192 trials). The runs were performed across 3 days with 20 runs per day. Each day was started with a brief practice run.</p>
</sec>
<sec id="sec015">
<title>Experimental setup</title>
<p>Stimuli were presented using Psychtoolbox version 3.0.11 [<xref ref-type="bibr" rid="pbio.3000210.ref069">69</xref>] (<ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/" xlink:type="simple">http://psychtoolbox.org/</ext-link>) under MATLAB R2014a (MathWorks) on a desktop PC running Windows 7. Visual stimuli were presented via a gamma-corrected 30” LCD monitor with a resolution of 2,560 × 1,600 pixels at a frame rate of 60 Hz. Auditory stimuli were presented at a sampling rate of 44.1 kHz via eight external speakers (Multimedia) and an ASUS Xonar DSX sound card. Exact audiovisual onset timing was confirmed by recording visual and auditory signals concurrently with a photodiode and a microphone. Participants rested their head on a chin rest at a distance of 475 mm from the monitor and at a height that matched participants’ ears to the horizontal midline of the monitor. Participants responded by pressing one of four response buttons on a USB keypad with their index, middle, ring, and little finger, respectively.</p>
</sec>
<sec id="sec016">
<title>Eye movement recording and analysis</title>
<p>To address potential concerns that results were confounded by eye movements, we recorded participants’ eye movements. Eye recordings were calibrated in the recommended field of view (32° horizontally and 24° vertically) for the EyeLink 1000 Plus system with the desktop mount at a sampling rate of 2,000 Hz. Eye position data were on-line parsed into events (saccade, fixation, eye blink) using the EyeLink 1000 Plus software. The ‘cognitive configuration’ was used for saccade detection (velocity threshold = 30°/sec, acceleration threshold = 8,000°/sec<sup>2</sup>, motion threshold = 0.15°) with an additional criterion of radial amplitude larger than 1°. Individual trials were rejected if saccades or eye blinks were detected from −100 to 700 ms post stimulus.</p>
</sec>
<sec id="sec017">
<title>Behavioural data analysis</title>
<p>Participants’ stimulus localisation accuracy was assessed as the Pearson correlation between their location responses and the true signal source location separately for unisensory auditory, visual high reliability, and visual low reliability conditions. To confirm whether localisation accuracy in vision exceeded performance in audition in both visual reliabilities, we performed Monte-Carlo permutation tests. Specifically, we entered the subject-specific Fisher z-transformed Pearson correlation differences between vision and audition (i.e., visual–auditory) separately for the two visual reliability levels into a Monte-Carlo permutation test at the group level based on the one-sample <italic>t</italic> statistic with 5,000 permutations [<xref ref-type="bibr" rid="pbio.3000210.ref070">70</xref>].</p>
</sec>
<sec id="sec018">
<title>EEG data acquisition</title>
<p>Continuous EEG signals were recorded from 64 channels using Ag/AgCl active electrodes arranged in a 10–20 layout (ActiCap, Brain Products GmbH, Gilching, Germany) at a sampling rate of 1,000 Hz, referenced at FCz. Channel impedances were kept below 10 kΩ.</p>
</sec>
<sec id="sec019">
<title>EEG preprocessing</title>
<p>Preprocessing was performed with the FieldTrip toolbox [<xref ref-type="bibr" rid="pbio.3000210.ref071">71</xref>] (<ext-link ext-link-type="uri" xlink:href="http://www.fieldtriptoolbox.org/" xlink:type="simple">http://www.fieldtriptoolbox.org/</ext-link>). For the decoding analysis, raw data were high-pass filtered at 0.1 Hz, re-referenced to average reference, and low-pass filtered at 120 Hz. Trials were extracted with a 100-ms prestimulus and 700-ms poststimulus period and baseline corrected by subtracting the average value of the interval between −100 and 0 ms from the time course. Trials were then temporally smoothed with a 20-ms moving window and downsampled to 200 Hz (note that a 20-ms moving average is comparable to a finite impulse response [FIR] filter with a cutoff frequency of 50 Hz). Trials containing artefacts were rejected based on visual inspection. Furthermore, trials were rejected if (1) they included eye blinks, (2) they included saccades, (3) the distance between eye fixation and the central fixation cross exceeded 2°, (4) participants responded prior to the response cue, or (5) there was no response. For ERPs (<xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3</xref> Figs), the preprocessing was identical to the decoding analysis, except that a 45-Hz low-pass filter was applied without additional temporal smoothing with a temporal moving window. Grand average ERPs were computed by averaging all trials for each condition first within each participant and then across participants.</p>
</sec>
<sec id="sec020">
<title>EEG multivariate pattern analysis</title>
<p>For the multivariate pattern analyses, we computed ERPs by averaging over sets of eight randomly assigned individual trials from the same condition. To characterise the temporal dynamics of the spatial representations, we trained linear SVR models (LIBSVM [<xref ref-type="bibr" rid="pbio.3000210.ref072">72</xref>], <ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" xlink:type="simple">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>) to learn the mapping from ERP activity patterns of the (1) unisensory auditory (for auditory decoding), (2) unisensory visual (for visual decoding), or (3) audiovisual congruent conditions (for audiovisual decoding) to external spatial locations separately for each time point (every 5 ms) over the course of the trial (<xref ref-type="supplementary-material" rid="pbio.3000210.s004">S2</xref>, <xref ref-type="supplementary-material" rid="pbio.3000210.s005">S3</xref> and <xref ref-type="supplementary-material" rid="pbio.3000210.s006">S4</xref> Figs). All SVR models were trained and evaluated in a 12-fold-stratified cross-validation (12 ERPs/fold) procedure with default hyperparameters (C = 1, ε = 0.001). The specific training and generalisation procedures were adjusted to the scientific questions (see the Shared and distinct neural representations of space across vision and audition section and the GLM analysis of audiovisual weight index w<sub>AV</sub> section for details).</p>
</sec>
<sec id="sec021">
<title>Overview of behavioural and EEG analysis</title>
<p>Combining psychophysics, computational modelling, and EEG, we addressed two questions: First, focusing selectively on the unisensory auditory and unisensory visual conditions, we investigated when spatial representations are formed that generalise across auditory and visual modalities. Second, focusing on the audiovisual conditions, we investigated when and how human observers integrate audiovisual signals into spatial representations that take into account the observer’s uncertainty about the world’s causal structure consistent with Bayesian causal inference. In the following sections, we will describe the analysis approaches to address these two questions in turn.</p>
</sec>
<sec id="sec022">
<title>Shared and distinct neural representations of space across vision and audition</title>
<p>First, we investigated how the brain forms spatial representations in either audition or vision using the so-called temporal generalisation method [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>]. Here, the SVR model is trained at time point t to learn the mapping from, e.g., unisensory visual (or auditory) ERP pattern to external stimulus location. This learnt mapping is then used to predict spatial locations from unisensory visual (or auditory) ERP activity patterns across all other time points. Training and generalisation were applied separately to unisensory auditory and visual ERPs. To match the number of trials for auditory and visual conditions, we applied this analysis to the visual ERPs pooled over the two levels of visual reliability. The decoding accuracy as quantified by the Pearson correlation coefficient between the true and decoded stimulus locations is entered into a training time × generalisation time matrix. The generalisation ability across time illustrates the similarity of EEG activity patterns relevant for encoding features (i.e., here: spatial location) and has been proposed to assess the stability of neural representations [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>]. In other words, if stimulus location is encoded in EEG activity patterns that are stable (or shared) across time, then an SVR model trained at time point t will be able to correctly decode stimulus location from EEG activity patterns at other time points. By contrast, if stimulus location is represented by transient or distinct EEG activity patterns across time, then an SVR model trained at time point t will not be able to decode stimulus location from EEG activity patterns at other time points. Hence, entering Pearson correlation coefficients as a measure for decoding accuracy for all combinations of training and test time into a temporal generalisation matrix has been argued to provide insights into the stability of neural representations whereby the spread of significant decoding accuracy to off-diagonal elements of the matrix indicates temporal generalisability or stability [<xref ref-type="bibr" rid="pbio.3000210.ref021">21</xref>].</p>
<p>Second, to examine whether and when neural representations are formed that are shared across vision and audition, we generalised to ERP activity patterns across time not only from the same sensory modality but also from the other sensory modality (i.e., from vision to audition and vice versa). This cross-sensory generalisation reveals neural representations that are shared across sensory modalities.</p>
<p>To assess whether decoding accuracies were better than chance, we entered the subject-specific matrices of the Fisher z-transformed Pearson correlation coefficients into a between-subjects Monte-Carlo permutation test using the one-sample <italic>t</italic> statistic with 5,000 permutations ([<xref ref-type="bibr" rid="pbio.3000210.ref070">70</xref>], as implemented in the FieldTrip toolbox). To correct for multiple comparisons within the two-dimensional (i.e., time × time) data, cluster-level inference was used based on the maximum of the summed <italic>t</italic> values within each cluster (‘maxsum’) with a cluster-defining threshold of <italic>p</italic> &lt; 0.05, and a two-tailed <italic>p</italic>-value was computed.</p>
</sec>
<sec id="sec023">
<title>Computational principles of audiovisual integration: GLM-based analysis of audiovisual weight index w<sub>AV</sub> and Bayesian modelling analysis</title>
<p>To characterise how human observers integrate auditory and visual signals into spatial representations at the behavioural and neural levels, we developed a GLM-based analysis of an audiovisual weight index w<sub>AV</sub> and a Bayesian modelling analysis that we applied to both (1) the reported auditory and visual spatial estimates (i.e., participants’ behavioural localisation responses) and (2) the neural spatial estimates decoded from EEG activity pattern evoked by audiovisual stimuli (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3</xref> and [<xref ref-type="bibr" rid="pbio.3000210.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref016">16</xref>]).</p>
</sec>
<sec id="sec024">
<title>GLM analysis of audiovisual weight index w<sub>AV</sub></title>
<sec id="sec025">
<title>SVR to decode spatial estimates from audiovisual EEG activity pattern</title>
<p>The neural spatial estimates were obtained by training a SVR model on the audiovisual congruent trials to learn the mapping from audiovisual ERP activity pattern at time t to external stimulus locations. This learnt mapping at time t was then used to decode the stimulus location from the ERP activity patterns of the spatially congruent and incongruent audiovisual conditions at time t (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3A and 3B</xref> right). These training and generalisation steps were repeated across all times t to obtain distributions of neural (i.e., decoded) spatial estimates for all 64 conditions for every time point t.</p>
</sec>
<sec id="sec026">
<title>Regression model and computation of behavioural and neural audiovisual weight index w<sub>AV</sub></title>
<p>In the ‘GLM-based’ analysis approach, we quantified the influence of the location of the auditory and visual stimuli on the reported (behavioural) or decoded (neural) spatial estimates using a linear regression model (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref> left). In this regression model, the reported (or decoded) spatial locations were predicted by the true auditory and visual stimulus locations for each of the eight conditions in the 2 (visual reliability: high versus low) × 2 (task relevance: auditory versus visual report) × 2 (spatial disparity: ≤6.6° versus &gt;6.6°) factorial design (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1A</xref>).</p>
<disp-formula id="pbio.3000210.e001">
<alternatives>
<graphic id="pbio.3000210.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ε</mml:mi>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<p>Hence, the regression model included 16 regressors in total—i.e., 8 (conditions) × 2 (true auditory or visual spatial locations). We computed one behavioural regression model for participants’ reported locations. Further, we computed 161 neural regression models for the spatial locations decoded from EEG activity pattern across time—i.e., one neural regression model for every 5-ms interval, leading to time courses of auditory (ß<sub>A</sub>) and visual (ß<sub>V</sub>) parameter estimates.</p>
<p>In each regression model, the auditory (ß<sub>A</sub>) and visual (ß<sub>V</sub>) parameter estimates quantified the influence of auditory and visual stimulus locations on the reported (or decoded) stimulus location for a particular condition. A positive ß<sub>V</sub> (or ß<sub>A</sub>) indicates that the true visual (or auditory) location has a positive weight and hence an attractive effect on the reported or decoded location (e.g., it is shifted towards the true visual location; see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref> left for an example). A negative ß<sub>V</sub> (or ß<sub>A</sub>) indicates that the true visual (or auditory) location has a negative weight and hence a repulsive effect on the reported or decoded location (e.g., it is shifted away from the true visual location). The auditory and visual parameter estimates need to be interpreted together. To obtain a summary index, we computed the relative audiovisual weight (w<sub>AV</sub>) as the four-quadrant inverse tangent of the visual (ß<sub>V</sub>) and auditory (ß<sub>A</sub>) parameter estimates for each of the eight conditions in each regression model (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref> left). The angles in radians are then converted to degrees:
<disp-formula id="pbio.3000210.e002">
<alternatives>
<graphic id="pbio.3000210.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:mn>180</mml:mn><mml:mspace width="0.25em"/><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>The four-quadrant inverse tangent was used to map each combination of (positive or negative) visual (ß<sub>V</sub>) and auditory (ß<sub>A</sub>) parameters uniquely to a value in the closed interval [−π, π], which was then transformed into degrees. If the reported/decoded estimate is dominated purely and positively by the visual signal (i.e., ß<sub>A</sub> = 0, ß<sub>V</sub> &gt; 0), then w<sub>AV</sub> is 90°. For pure (and positive) auditory dominance, it is 0° (i.e., ß<sub>A</sub> &gt; 0, ß<sub>V</sub> = 0). Furthermore, if the visual signal has an attractive influence (i.e., it attracts the perceived location towards the visual location) but the auditory signal has a repulsive influence (i.e., it shifts the perceived location away from the auditory location) on perceived/decoded location (i.e., ß<sub>A</sub> &lt; 0, ß<sub>V</sub> &gt; 0), then w<sub>AV</sub> is &gt;90° (e.g., <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4C</xref>, high-disparity condition).</p>
<p>We obtained one w<sub>AV</sub> for each of the eight conditions at the behavioural level and one w<sub>AV</sub> for each of the eight conditions and time point (every 5 ms) at the neural level. The neural w<sub>AV</sub> time courses were temporally smoothed using a 20-ms moving average filter.</p>
</sec>
<sec id="sec027">
<title>Statistical analysis of circular indices w<sub>AV</sub> for behavioural and neural data</title>
<p>We performed the statistics on the behavioural and neural audiovisual weight indices using a 2 (auditory versus visual report) × 2 (high versus low visual reliability) × 2 (large versus small spatial disparity) factorial design based on the likelihood ratio statistics for circular measures (LRTS) [<xref ref-type="bibr" rid="pbio.3000210.ref073">73</xref>]. Similar to an analysis of variance for linear data, LRTS computes the difference in log-likelihood functions for the full model that allows differences in the mean locations of circular measures between conditions (i.e., main and interaction effects) and the reduced null model that does not model any mean differences between conditions. LRTS were computed separately for the main effects (i.e., reliability, task relevance, spatial disparity) and interactions.</p>
<p>To refrain from making any parametric assumptions, we evaluated the main effects of visual reliability, task relevance, spatial disparity, and their interactions in the factorial design using randomisation tests (5,000 randomisations). To account for the within-subject repeated-measures design at the second random-effects level, randomisations were performed within each participant. For the main effects of visual reliability, task relevance, and spatial disparity, w<sub>AV</sub> values were permuted within the levels of the nontested factors. For tests of the two-way interactions (e.g., spatial disparity × task relevance), we permuted the simple main effects of the two factors of interest within the levels of the third factor [<xref ref-type="bibr" rid="pbio.3000210.ref074">74</xref>]. For tests of the three-way interaction, values were freely permuted across all conditions [<xref ref-type="bibr" rid="pbio.3000210.ref075">75</xref>]. These statistical tests were performed once for behavioural w<sub>AV</sub> and independently for each time point between 55 and 700 ms (i.e., 130 tests) for neural w<sub>AV</sub> (see below for multiple comparison correction across time points).</p>
<p>To assess the similarity between behavioural and neural audiovisual weight (w<sub>AV</sub>) indices, we computed the circular correlation coefficient (as implemented in the CircStat toolbox [<xref ref-type="bibr" rid="pbio.3000210.ref076">76</xref>]) between the eight behavioural (i.e., constant across time) and eight neural (i.e., variable across time) w<sub>AV</sub> from our 2 (high versus low visual reliability) × 2 (auditory versus visual report) × 2 (large versus small spatial disparity) factorial design separately for each time point.</p>
<p>Unless otherwise stated, results are reported at <italic>p</italic> &lt; 0.05. To correct for multiple comparisons across time, cluster-level inference was used based on the maximum of the summed LRTS values within each cluster (‘maxsum’) with an uncorrected cluster-defining threshold of <italic>p</italic> &lt; 0.05 (as implemented in the FieldTrip toolbox).</p>
<p>For plotting circular means of w<sub>AV</sub> (<xref ref-type="fig" rid="pbio.3000210.g001">Fig 1C</xref> for behavioural w<sub>AV</sub>, <xref ref-type="fig" rid="pbio.3000210.g004">Fig 4A–4D</xref> for neural w<sub>AV</sub>), we computed the means’ confidence intervals (as implemented in the CircStat toolbox [<xref ref-type="bibr" rid="pbio.3000210.ref076">76</xref>]).</p>
</sec>
</sec>
<sec id="sec028">
<title>Bayesian modelling analysis</title>
<sec id="sec029">
<title>Description of Bayesian models and decision strategies</title>
<p>Next, we fitted the full-segregation model(s), the forced-fusion model, and the Bayesian causal inference model to the spatial estimates that were reported by observers (i.e., behavioural response distribution, <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3B</xref> left) or decoded from ERP activity patterns at time t (i.e., neural spatial estimate distribution, <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3B</xref> right). Using Bayesian model comparison, we then assessed which of these models is the best explanation for the behavioural or neural spatial estimates.</p>
<p>In the following, we will first describe the Bayesian causal inference model from which we will then derive the forced-fusion and full-segregation models as special cases (details can be found in [<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref013">13</xref>–<xref ref-type="bibr" rid="pbio.3000210.ref015">15</xref>]).</p>
<p>Briefly, the generative model of Bayesian causal inference (see <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref> right) assumes that common (<italic>C</italic> = 1) or independent (<italic>C</italic> = 2) causes are sampled from a binomial distribution defined by the common cause prior <italic>P</italic><sub><italic>common</italic></sub>. For a common source, the ‘true’ location S<sub>AV</sub> is drawn from the spatial prior distribution N(μ<sub>AV</sub>, σ<sub>P</sub>). For two independent causes, the ‘true’ auditory (S<sub>A</sub>) and visual (S<sub>V</sub>) locations are drawn independently from this spatial prior distribution. For the spatial prior distribution, we assumed a central bias (i.e., μ = 0). We introduced sensory noise by drawing x<sub>A</sub> and x<sub>V</sub> independently from normal distributions centred on the true auditory (or visual) locations with parameters σ<sub>A</sub><sup>2</sup> (or σ<sub>V</sub><sup>2</sup>). Thus, the generative model included the following free parameters: the common source prior p<sub>common</sub>, the spatial prior variance σ<sub>P</sub><sup>2</sup>, the auditory variance σ<sub>A</sub><sup>2</sup>, and the two visual variances σ<sub>V</sub><sup>2</sup> corresponding to the two visual reliability levels.</p>
<p>The posterior probability of the underlying causal structure can be inferred by combining the common-source prior with the sensory evidence according to Bayes rule:
<disp-formula id="pbio.3000210.e003">
<alternatives>
<graphic id="pbio.3000210.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">common</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>In the case of a common source (C = 1), the optimal estimate of the audiovisual location is a reliability-weighted average of the auditory and visual percepts and the spatial prior (i.e., referred to as forced-fusion spatial estimate).</p>
<disp-formula id="pbio.3000210.e004">
<alternatives>
<graphic id="pbio.3000210.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">AV</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
<p>In the case of independent sources (C = 2), the auditory and visual stimulus locations (for the auditory and visual location report, respectively) are estimated independently (i.e., referred to as unisensory auditory or visual segregation estimates):
<disp-formula id="pbio.3000210.e005">
<alternatives>
<graphic id="pbio.3000210.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>To provide a final estimate of the auditory and visual locations, the brain can combine the estimates from the two causal structures using various decision functions such as ‘model averaging’, ‘model selection’, and ‘probability matching’ [<xref ref-type="bibr" rid="pbio.3000210.ref013">13</xref>].</p>
<p>According to the ‘model averaging’ strategy, the brain combines the integrated forced-fusion spatial estimate with the segregated, task-relevant unisensory (i.e., either auditory or visual) spatial estimates weighted in proportion to the posterior probability of the underlying causal structures.</p>
<disp-formula id="pbio.3000210.e006">
<alternatives>
<graphic id="pbio.3000210.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">AV</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
<disp-formula id="pbio.3000210.e007">
<alternatives>
<graphic id="pbio.3000210.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">AV</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
<p>According to the ‘model selection’ strategy, the brain reports the spatial estimate selectively from the more likely causal structure (<xref ref-type="disp-formula" rid="pbio.3000210.e009">Eq 8</xref> only shown for <inline-formula id="pbio.3000210.e008"><alternatives><graphic id="pbio.3000210.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub></mml:math></alternatives></inline-formula>):
<disp-formula id="pbio.3000210.e009">
<alternatives>
<graphic id="pbio.3000210.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>According to ‘probability matching’, the brain reports the spatial estimate of one causal structure stochastically selected in proportion to the posterior probability of this causal structure (<xref ref-type="disp-formula" rid="pbio.3000210.e011">Eq 9</xref> only shown for <inline-formula id="pbio.3000210.e010"><alternatives><graphic id="pbio.3000210.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub></mml:math></alternatives></inline-formula>):
<disp-formula id="pbio.3000210.e011">
<alternatives>
<graphic id="pbio.3000210.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Thus, Bayesian causal inference formally requires three spatial estimates (<inline-formula id="pbio.3000210.e012"><alternatives><graphic id="pbio.3000210.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">AV</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>), which are combined into a final Bayesian causal inference estimate (<inline-formula id="pbio.3000210.e013"><alternatives><graphic id="pbio.3000210.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub></mml:math></alternatives></inline-formula> or <inline-formula id="pbio.3000210.e014"><alternatives><graphic id="pbio.3000210.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">V</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, depending on which sensory modality is task relevant) according to one of the three decision functions.</p>
<p>In the main paper, we present behavioural results using ‘model averaging’ as the decision function, which was associated with the highest model evidence and exceedance probability at the group level. <xref ref-type="supplementary-material" rid="pbio.3000210.s008">S1 Table</xref> shows the model evidence, exceedance probabilities, and parameters for Bayesian causal inference across the three decision strategies for the behavioural data.</p>
<p>At the behavioural level, we evaluated whether and how participants integrate auditory and visual stimuli by comparing (1) the Bayesian causal inference model (i.e., with model averaging; <xref ref-type="table" rid="pbio.3000210.t001">Table 1</xref>), (<xref ref-type="table" rid="pbio.3000210.t002">2</xref>) the forced-fusion model that integrates auditory and visual signals in a mandatory fashion (i.e., formally, the BCI model with a fixed p<sub>common</sub> = 1, <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, encircled in yellow), and (3) the full-segregation model that estimates stimulus location independently for vision and audition (i.e., formally, the BCI model with a fixed p<sub>common</sub> = 0; i.e., <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, SegV,A encircled in light blue). This SegV,A model assumes that observers report <inline-formula id="pbio.3000210.e015"><alternatives><graphic id="pbio.3000210.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> when they are asked to report the auditory location and <inline-formula id="pbio.3000210.e016"><alternatives><graphic id="pbio.3000210.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> when they are asked to report the visual location. In short, the SegV,A model reads out the spatial estimate from the task-relevant unisensory segregation model.</p>
<p>At the neural level, we may also conceive a neural source (or brain region) that represents <inline-formula id="pbio.3000210.e017"><alternatives><graphic id="pbio.3000210.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, irrespective of which sensory modality needs to be reported (i.e., <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, SegV model, encircled in red). For instance, primary visual cortices may be considered predominantly unisensory with selective representations of the visual location even if the observer needs to report the auditory stimulus location. Likewise, we included a model that selectively represents the auditory location (i.e., <xref ref-type="fig" rid="pbio.3000210.g003">Fig 3C</xref>, unisensory auditory segregation [SegA] model, encircled in green). By contrast, the full-segregation audiovisual model (i.e., SegV,A, encircled in light blue) can be thought of as a neural source (or brain area) that encodes the task-relevant estimate computed in a full-segregation model. It differs from the Bayesian causal inference model by not allowing for any audiovisual interactions or biases irrespective of the probabilities of the world’s causal structure (i.e., operationally manipulated by spatial disparity in the current experiment).</p>
<p>At the behavioural level, the unisensory SegV and SegA models are not useful, because we would expect observers to follow instructions and report their auditory estimate for the auditory report conditions and their visual estimate for the visual report conditions. In other words, it does not seem reasonable to fit the unisensory SegV and SegA models jointly to visual and auditory localisation responses at the behavioural level. By contrast, at the neural level, spatial estimates decoded from EEG activity patterns may potentially reflect neural representations that are formed by ‘predominantly unisensory’ neural generators (e.g., primary visual cortex), particularly in early processing phases. Hence, we estimated and compared three models for the behavioural localisation reports and five models for the spatial estimates decoded from EEG activity patterns.</p>
</sec>
<sec id="sec030">
<title>Model fitting to behavioural and neural spatial estimates and Bayesian model comparison</title>
<p>We fitted each model individually to participants’ behavioural localisation responses (or spatial estimates decoded from EEG activity pattern at time t) based on the predicted distributions of the spatial estimates (i.e., <inline-formula id="pbio.3000210.e018"><alternatives><graphic id="pbio.3000210.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>; we use <inline-formula id="pbio.3000210.e019"><alternatives><graphic id="pbio.3000210.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> as a variable to refer generically to any spatial estimate) for each combination of auditory (S<sub>A</sub>) and visual (S<sub>V</sub>) source location. These predicted distributions marginalise over the internal sensory inputs (i.e., x<sub>A</sub>, x<sub>V</sub>) that are unknown to the experimenter (see [<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>] for further explanation). More specifically, we fit (1) the Bayesian causal inference model based on <inline-formula id="pbio.3000210.e020"><alternatives><graphic id="pbio.3000210.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> for auditory report conditions and <inline-formula id="pbio.3000210.e021"><alternatives><graphic id="pbio.3000210.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> for visual report conditions, (2) the forced-fusion model based on <inline-formula id="pbio.3000210.e022"><alternatives><graphic id="pbio.3000210.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">AV</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, and (3) the SegV,A model based on <inline-formula id="pbio.3000210.e023"><alternatives><graphic id="pbio.3000210.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> for auditory report conditions and <inline-formula id="pbio.3000210.e024"><alternatives><graphic id="pbio.3000210.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> for visual report conditions. At the neural level, we also fit the SegV model based on <inline-formula id="pbio.3000210.e025"><alternatives><graphic id="pbio.3000210.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and the SegA model based on <inline-formula id="pbio.3000210.e026"><alternatives><graphic id="pbio.3000210.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> to the spatial estimates decoded from EEG activity pattern across both visual and auditory report conditions.</p>
<p>To marginalise over the internal variables x<sub>A</sub> and x<sub>V</sub> that are not accessible to the experimenter, the predicted distributions were generated by simulating x<sub>A</sub> and x<sub>V</sub> 10,000 times for each of the 64 conditions and inferring the different sorts of spatial estimate <inline-formula id="pbio.3000210.e027"><alternatives><graphic id="pbio.3000210.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="pbio.3000210.e003">Eq 3</xref>–<xref ref-type="disp-formula" rid="pbio.3000210.e011">9</xref>. To link any of those <inline-formula id="pbio.3000210.e028"><alternatives><graphic id="pbio.3000210.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> to participants’ auditory and visual discrete localisation responses at the behavioural level, we assumed that participants selected the button that is closest to <inline-formula id="pbio.3000210.e029"><alternatives><graphic id="pbio.3000210.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and binned the <inline-formula id="pbio.3000210.e030"><alternatives><graphic id="pbio.3000210.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> accordingly into a histogram (with four bins corresponding to the four buttons). Thus, we obtained a histogram of predicted localisation responses for each of those five models separately for each condition and individually for each participant. Based on these histograms, we computed the probability of a participant’s counts of localisation responses using the multinomial distribution (see [<xref ref-type="bibr" rid="pbio.3000210.ref002">2</xref>]). This gives the likelihood of the model given participants’ response data. Assuming independence of conditions, we summed the log likelihoods across conditions.</p>
<p>At the neural level, we first binned the spatial estimates decoded from each ERP activity pattern at each time point based on their distance from the four true locations (i.e., −10°, −3.3°, 3.3°, or 10°) into four spatial bins before fitting the models to those discretised spatial estimates.</p>
<p>To obtain maximum-likelihood estimates for the parameters of the models (p<sub>common</sub>, σ<sub>P</sub>, σ<sub>A</sub>, σ<sub>V1</sub> − σ<sub>V2</sub> for the two levels of visual reliability; formally, the forced-fusion and segregation models assume p<sub>common</sub> = 1 or = 0, respectively), we used a nonlinear simplex optimisation algorithm as implemented in MATLAB’s fminsearch function (MATLAB R2016a). This optimisation algorithm was initialised with a parameter setting that obtained the highest log likelihood in a prior grid search.</p>
<p>The model fit for behavioural and neural data (i.e., at each time point) was assessed by the coefficient of determination R<sup>2</sup> [<xref ref-type="bibr" rid="pbio.3000210.ref077">77</xref>], defined as
<disp-formula id="pbio.3000210.e031">
<alternatives>
<graphic id="pbio.3000210.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:msup><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">ß</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <inline-formula id="pbio.3000210.e032"><alternatives><graphic id="pbio.3000210.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mi mathvariant="normal">l</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>ß</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> and l(0) denote the log likelihoods of the fitted and the null model, respectively, and <italic>n</italic> is the number of data points. For the null model, we assumed that an observer randomly chooses one of the four response options; i.e., we assumed a discrete uniform distribution with a probability of 0.25. As in our case, the Bayesian causal inference model’s responses were discretised to relate them to the four discrete response options, and the coefficient of determination was scaled (i.e., divided) by the maximum coefficient (see [<xref ref-type="bibr" rid="pbio.3000210.ref077">77</xref>]) defined as
<disp-formula id="pbio.3000210.e033">
<alternatives>
<graphic id="pbio.3000210.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e033" xlink:type="simple"/>
<mml:math display="block" id="M33">
<mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>To identify the optimal model for explaining participants’ data (i.e., localisation responses at the behavioural level or spatial estimates decoded from EEG activity pattern at the neural level), we compared the candidate models using the Bayesian information criterion (BIC) as an approximation to the model evidence [<xref ref-type="bibr" rid="pbio.3000210.ref078">78</xref>].
<disp-formula id="pbio.3000210.e034">
<alternatives>
<graphic id="pbio.3000210.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e034" xlink:type="simple"/>
<mml:math display="block" id="M34">
<mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>*</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where <inline-formula id="pbio.3000210.e035"><alternatives><graphic id="pbio.3000210.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.3000210.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mover accent="true"><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> denotes the likelihood, <italic>n</italic> the number of data points (i.e., EEG activity patterns summed over conditions at a time point t), and k the number of parameters. The BIC depends on both model complexity and model fit. We performed Bayesian model selection [<xref ref-type="bibr" rid="pbio.3000210.ref025">25</xref>] at the group (i.e., random-effects) level as implemented in SPM8 [<xref ref-type="bibr" rid="pbio.3000210.ref079">79</xref>] to obtain the protected exceedance probability that one model is better than any of the other candidate models above and beyond chance.</p>
</sec>
<sec id="sec031">
<title>Assumptions and caveats of EEG decoding analyses</title>
<p>The EEG activity patterns measured across 64 scalp electrodes represent a superposition of activity generated by potentially multiple neural sources located, for instance, in auditory, visual, and higher-order association areas. The extent to which auditory or visual information can be decoded from EEG activity patterns depends therefore inherently on how information is neurally encoded by the ‘neural generators’ in source space and on how these neural activities are expressed and superposed in sensor space (i.e., as measured by scalp electrodes). For example, visual space is retinotopically encoded, whereas auditory space is represented by broadly tuned neuronal populations (i.e., opponent channel coding model [<xref ref-type="bibr" rid="pbio.3000210.ref031">31</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref080">80</xref>]), rate-based code [<xref ref-type="bibr" rid="pbio.3000210.ref030">30</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref081">81</xref>], or spike latency and pattern [<xref ref-type="bibr" rid="pbio.3000210.ref082">82</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref083">83</xref>]. These differences in encoding of auditory and visual space may contribute to the visual bias we observed for the audiovisual weight index w<sub>AV</sub> in early processing (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4A–4D</xref>) and the dominance of the SegV model in the time course of exceedance probabilities (<xref ref-type="fig" rid="pbio.3000210.g004">Fig 4F</xref>). Furthermore, particularly at later stages, scalp EEG patterns likely rely on superposition of activity of multiple neural generators so that ‘decodability’ will also depend on how source activities combine and project to the scalp (e.g., source orientation etc.). Given the inverse problem involved in inferring sources from EEG topographies, recent studies suggested combining information from fMRI and EEG activity pattern via representational similarity analyses [<xref ref-type="bibr" rid="pbio.3000210.ref084">84</xref>,<xref ref-type="bibr" rid="pbio.3000210.ref085">85</xref>]. Although we informally also pursue this approach in the Discussion section of the current paper, when merging information from a previous fMRI study that used the same ventriloquist paradigm and analyses with our current EEG results, we recognise the limitations of such an fMRI and EEG fusion approach. For instance, different features encoded in neural activity may be expressed in BOLD-response and EEG scalp topographies [<xref ref-type="bibr" rid="pbio.3000210.ref086">86</xref>].</p>
<p>Finally, we trained the SVR model on the audiovisual congruent conditions pooled over task relevance and visual reliability to ensure that the decoder was based on activity patterns generated by sources related to auditory, visual, and audiovisual integration processes and that the effects of task relevance or reliability on the audiovisual weight index w<sub>AV</sub> cannot be attributed to differences in the decoding model (see [<xref ref-type="bibr" rid="pbio.3000210.ref065">65</xref>] for a related discussion).</p>
</sec>
</sec>
</sec>
<sec id="sec032">
<title>Supporting information</title>
<supplementary-material id="pbio.3000210.s001" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s001" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Zip file containing datasets underlying Figs <xref ref-type="fig" rid="pbio.3000210.g001">1C</xref>, <xref ref-type="fig" rid="pbio.3000210.g002">2</xref>, and <xref ref-type="fig" rid="pbio.3000210.g004">4</xref>.</title>
<p>The data are stored in MATLAB structures.</p>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s002" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supporting information: Time-frequency analysis.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s003" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Distributions of spatial estimates.</title>
<p>The distribution (across participants’ mean) of spatial estimates given by observers’ behavioural localisation responses (solid lines) or predicted by the Bayesian causal inference model fitted to observers’ behavioural responses (dashed lines, for model averaging) are shown across all conditions in our 2 task relevance (auditory: red versus visual: blue) × visual reliability (high: row 1–4 versus low: row 5–8) × 4 auditory location (columns as indicated) × 4 visual location (rows as indicated) design.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s004" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Time-resolved decoding of visual location for unisensory visual stimuli.</title>
<p>(A) Time course of decoding accuracy (i.e., Pearson correlation between true and predicted visual stimulus locations pooled over both visual reliabilities, black line) and the EEG evoked potentials (across participants’ mean) for the unisensory visual (high reliability only) signals at −10°, −3.3°, 3.3°, and 10°, averaged over occipital channels. Shaded grey area indicates the time window at which the decoding accuracy is significantly better than chance. EEG signals were averaged across the electrodes shown in the inset. (B) EEG topographies (across participants’ mean) for the unisensory visual signals (high reliability only) at −10°, −3.3°, 3.3°, and 10° shown at the given time points.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s005" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Time-resolved decoding of auditory location for unisensory auditory stimuli.</title>
<p>(A) Time course of decoding accuracy (i.e., Pearson correlation between true and predicted stimulus locations, black line) and the EEG evoked potentials (across participants’ mean) for the unisensory auditory signals at −10°, −3.3°, 3.3°, and 10°, averaged over central channels. Shaded grey area indicates decoding accuracy significantly better than chance. EEG signals were averaged across the electrodes shown in the inset. (B) EEG topographies (across participants’ mean) for the unisensory auditory stimuli at −10°, −3.3°, 3.3°, and 10° shown at the given time points.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s006" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Temporal generalisation matrix for audiovisual congruent trials.</title>
<p>The temporal generalisation matrix shows the decoding accuracy for audiovisual congruent trials across each combination of training (y-axis) and testing (x-axis) time point. The grey line along the diagonal indicates where the training time is equal to the testing time. Horizontal and vertical grey lines indicate the stimulus onset. The thin black lines encircle the cluster with decoding accuracies that were significantly better than chance at <italic>p</italic> &lt; 0.05 corrected for multiple comparisons.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s007" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Time-frequency results for oscillatory power in the alpha/beta band.</title>
<p>(A) Time courses of total power averaged over the alpha/beta (8–30 Hz) frequency bands (baseline corrected using prestimulus window [−400 ms to −200 ms]) are shown for the main effects of visual reliability (row 1), task relevance (row 2), spatial disparity (row 3), and the visual reliability × task relevance interaction (row 4) at three selected electrodes (i.e., Fz = left; Pz = middle; Oz = right columns). For each effect, we show the power for the difference (or interaction) and the individual conditions coded in different colours as indicated for each row. Grey shaded areas indicate the time windows where at least one electrode was part of the significant cluster after correcting for multiple comparisons across time (i.e., −200 ms to 700 ms), frequency (i.e., 4–30 Hz), and topography. (B) Topographies of the <italic>t</italic> values averaged across the significant time windows of the corresponding effects. Electrodes marked with black stars were part of the significant cluster (corrected across topography × time × frequency).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s008" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s008" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Model parameters (across-subjects mean ± SEM) and fit indices of the BCI models with different decision functions.</title>
<p>Model averaging (BCI<sub>avg</sub>), model selection (BCI<sub>sel</sub>), and probability matching (BCI<sub>match</sub>). BCI, Bayesian causal inference; PEP, protected exceedance probability; R<sup>2</sup>, coefficient of determination; relBIC<sub>group</sub>, group-level relative Bayesian information criterion [<xref ref-type="bibr" rid="pbio.3000210.ref025">25</xref>].</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.3000210.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.3000210.s009" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Time-frequency results.</title>
<p>Significant effects are shown for overall relative to baseline, main effect of VR, and main effect of task relevance (‘Task’), and the interaction between VR and task relevance is shown across rows. Columns of the table indicate the approximate time windows that the significant cluster spanned. All <italic>p</italic>-values are reported at the cluster level, corrected for multiple comparisons over time × topography × frequency. VR, visual reliability.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Ulrik Beierholm and Tim Rohe for kindly providing source code and Ágoston Mihalik and Johanna Zumer for very helpful discussions.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>EEG</term>
<def><p>electroencephalography</p></def>
</def-item>
<def-item><term>ERP</term>
<def><p>event-related potential</p></def>
</def-item>
<def-item><term>fMRI</term>
<def><p>functional MRI</p></def>
</def-item>
<def-item><term>GLM</term>
<def><p>general linear model</p></def>
</def-item>
<def-item><term>n.s.</term>
<def><p>not significant</p></def>
</def-item>
<def-item><term>relBICgroup</term>
<def><p>group-level relative Bayesian information criterion</p></def>
</def-item>
<def-item><term>SegA</term>
<def><p>unisensory auditory segregation</p></def>
</def-item>
<def-item><term>SegV</term>
<def><p>unisensory visual segregation</p></def>
</def-item>
<def-item><term>SegVA</term>
<def><p>audiovisual full-segregation</p></def>
</def-item>
<def-item><term>SVR</term>
<def><p>support vector regression</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.3000210.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>. <article-title>Causal inference in perception</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>: <fpage>425</fpage>–<lpage>432</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2010.07.001" xlink:type="simple">10.1016/j.tics.2010.07.001</ext-link></comment> <object-id pub-id-type="pmid">20705502</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS ONE</source>. <year>2007</year>;<volume>2</volume>: <fpage>e943</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0000943" xlink:type="simple">10.1371/journal.pone.0000943</ext-link></comment> <object-id pub-id-type="pmid">17895984</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name>. <article-title>Ventriloquist Effect Results from Near-Optimal Bimodal Integration</article-title>. <source>Curr Biol</source>. <year>2004</year>;<volume>14</volume>: <fpage>257</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2004.01.029" xlink:type="simple">10.1016/j.cub.2004.01.029</ext-link></comment> <object-id pub-id-type="pmid">14761661</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>von Helmholtz</surname> <given-names>H.</given-names></name> <chapter-title>Handbuch der physiologischen Optik</chapter-title> [Internet]. <source>Monatshefte für Mathematik und Physik</source>. <publisher-loc>Leipzig, Germany</publisher-loc>: <publisher-name>Leopold Voss</publisher-name>; <year>1896</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF01708548" xlink:type="simple">10.1007/BF01708548</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>. <article-title>Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>: <fpage>15601</fpage>–<lpage>15612</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2574-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2574-09.2009</ext-link></comment> <object-id pub-id-type="pmid">20007484</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butler</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Campos</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>. <article-title>Bayesian integration of visual and vestibular signals for heading</article-title>. <source>J Vis</source>. <year>2010</year>;<volume>10</volume>: <fpage>23</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/10.11.23" xlink:type="simple">10.1167/10.11.23</ext-link></comment> <object-id pub-id-type="pmid">20884518</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Beers</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Haggard</surname> <given-names>P</given-names></name>. <article-title>When feeling is more important than seeing in sensorimotor adaptation</article-title>. <source>Curr Biol</source>. <year>2002</year>;<volume>12</volume>: <fpage>834</fpage>–<lpage>837</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0960-9822(02)00836-9" xlink:type="simple">10.1016/S0960-9822(02)00836-9</ext-link></comment> <object-id pub-id-type="pmid">12015120</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Reliability-Weighted Integration of Audiovisual Signals Can Be Modulated by Top-down Attention</article-title>. <source>eNeuro</source>. <year>2018</year>;<volume>5</volume>: <fpage>315</fpage>–<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/ENEURO.0315-17.2018" xlink:type="simple">10.1523/ENEURO.0315-17.2018</ext-link></comment> <object-id pub-id-type="pmid">29527567</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Helbig</surname> <given-names>HB</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Ricciardi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Pietrini</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Thielscher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mayer</surname> <given-names>KM</given-names></name>, <etal>et al</etal>. <article-title>The neural mechanisms of reliability weighted integration of shape information from vision and touch</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>60</volume>: <fpage>1063</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.09.072" xlink:type="simple">10.1016/j.neuroimage.2011.09.072</ext-link></comment> <object-id pub-id-type="pmid">22001262</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roach</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Heron</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McGraw</surname> <given-names>P V</given-names></name>. <article-title>Resolving multisensory conflict: A strategy for balancing the costs and benefits of audio-visual integration</article-title>. <source>Proc R Soc B Biol Sci</source>. <year>2006</year>;<volume>273</volume>: <fpage>2159</fpage>–<lpage>2168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2006.3578" xlink:type="simple">10.1098/rspb.2006.3578</ext-link></comment> <object-id pub-id-type="pmid">16901835</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallace</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Roberson</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Hairston</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Vaughan</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Schirillo</surname> <given-names>JA</given-names></name>. <article-title>Unifying multisensory signals across time and space</article-title>. <source>Exp Brain Res</source>. <year>2004</year>;<volume>158</volume>: <fpage>252</fpage>–<lpage>258</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-004-1899-9" xlink:type="simple">10.1007/s00221-004-1899-9</ext-link></comment> <object-id pub-id-type="pmid">15112119</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>. <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000871" xlink:type="simple">10.1371/journal.pcbi.1000871</ext-link></comment> <object-id pub-id-type="pmid">20700493</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Cortical Hierarchies Perform Bayesian Causal Inference in Multisensory Perception</article-title>. <source>PLoS Biol</source>. <year>2015</year>;<volume>13</volume>: <fpage>e1002073</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002073" xlink:type="simple">10.1371/journal.pbio.1002073</ext-link></comment> <object-id pub-id-type="pmid">25710328</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Sensory reliability shapes perceptual inference via two mechanisms</article-title>. <source>J Vis</source>. <year>2015</year>;<volume>15</volume>: <fpage>22</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.5.22" xlink:type="simple">10.1167/15.5.22</ext-link></comment> <object-id pub-id-type="pmid">26067540</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Distinct computational principles govern multisensory integration in primary sensory and association cortices</article-title>. <source>Curr Biol</source>. <year>2016</year>;<volume>26</volume>: <fpage>509</fpage>–<lpage>514</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.12.056" xlink:type="simple">10.1016/j.cub.2015.12.056</ext-link></comment> <object-id pub-id-type="pmid">26853368</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bonath</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mishra</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schwiecker</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>HJ</given-names></name>, <etal>et al</etal>. <article-title>Neural Basis of the Ventriloquist Illusion</article-title>. <source>Curr Biol</source>. <year>2007</year>;<volume>17</volume>: <fpage>1697</fpage>–<lpage>1703</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2007.08.050" xlink:type="simple">10.1016/j.cub.2007.08.050</ext-link></comment> <object-id pub-id-type="pmid">17884498</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K.</given-names></name> <article-title>A theory of cortical responses</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2005</year>;<volume>360</volume>: <fpage>815</fpage>–<lpage>836</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2005.1622" xlink:type="simple">10.1098/rstb.2005.1622</ext-link></comment> <object-id pub-id-type="pmid">15937014</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>79</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/4580" xlink:type="simple">10.1038/4580</ext-link></comment> <object-id pub-id-type="pmid">10195184</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Talsma</surname> <given-names>D.</given-names></name> <article-title>Predictive coding and multisensory integration: an attentional account of the multisensory mind</article-title>. <source>Front Integr Neurosci</source>. <year>2015</year>;<volume>09</volume>: <fpage>19</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnint.2015.00019" xlink:type="simple">10.3389/fnint.2015.00019</ext-link></comment> <object-id pub-id-type="pmid">25859192</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Characterizing the dynamics of mental representations: The temporal generalization method</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>: <fpage>203</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2014.01.002" xlink:type="simple">10.1016/j.tics.2014.01.002</ext-link></comment> <object-id pub-id-type="pmid">24593982</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref022"><label>22</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Picton</surname> <given-names>TW</given-names></name>. <source>Late auditory evoked potentials: changing the things which are. Human auditory evoked potentials</source>. <publisher-loc>San Diego</publisher-loc>: <publisher-name>Plural Publishing</publisher-name>; <year>2011</year>. pp. <fpage>335</fpage>–<lpage>399</lpage>.</mixed-citation></ref>
<ref id="pbio.3000210.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CW</given-names></name>, <name name-style="western"><surname>London</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>. <article-title>Neural time course of visually enhanced echo suppression</article-title>. <source>J Neurophysiol</source>. <year>2012</year>;<volume>108</volume>: <fpage>1869</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00175.2012" xlink:type="simple">10.1152/jn.00175.2012</ext-link></comment> <object-id pub-id-type="pmid">22786953</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shrem</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Deouell</surname> <given-names>LY</given-names></name>. <article-title>Auditory-visual integration modulates location-specific repetition suppression of auditory responses</article-title>. <source>Psychophysiology</source>. <year>2017</year>;<volume>54</volume>: <fpage>1663</fpage>–<lpage>1675</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/psyp.12955" xlink:type="simple">10.1111/psyp.12955</ext-link></comment> <object-id pub-id-type="pmid">28752567</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>. <article-title>Bayesian model selection for group studies—Revisited</article-title>. <source>Neuroimage</source>. <year>2014</year>;<volume>84</volume>: <fpage>971</fpage>–<lpage>985</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2013.08.065" xlink:type="simple">10.1016/j.neuroimage.2013.08.065</ext-link></comment> <object-id pub-id-type="pmid">24018303</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maier</surname> <given-names>JX</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>Multisensory guidance of orienting behavior</article-title>. <source>Hear Res</source>. <year>2009</year>;<volume>258</volume>: <fpage>106</fpage>–<lpage>112</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heares.2009.05.008" xlink:type="simple">10.1016/j.heares.2009.05.008</ext-link></comment> <object-id pub-id-type="pmid">19520151</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grothe</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pecka</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McAlpine</surname> <given-names>D</given-names></name>. <article-title>Mechanisms of Sound Localization in Mammals</article-title>. <source>Physiol Rev</source>. <year>2010</year>;<volume>90</volume>: <fpage>983</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/physrev.00026.2009" xlink:type="simple">10.1152/physrev.00026.2009</ext-link></comment> <object-id pub-id-type="pmid">20664077</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Merriam</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Maps of Visual Space in Human Occipital Cortex Are Retinotopic, Not Spatiotopic</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>3988</fpage>–<lpage>3999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5476-07.2008" xlink:type="simple">10.1523/JNEUROSCI.5476-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18400898</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wandell</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Dumoulin</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Brewer</surname> <given-names>AA</given-names></name>. <article-title>Visual field maps in human cortex</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>(<issue>2</issue>): <fpage>366</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.10.012" xlink:type="simple">10.1016/j.neuron.2007.10.012</ext-link></comment> <object-id pub-id-type="pmid">17964252</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werner-Reiss</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>A Rate Code for Sound Azimuth in Monkey Auditory Cortex: Implications for Human Neuroimaging Studies</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>3747</fpage>–<lpage>3758</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5044-07.2008" xlink:type="simple">10.1523/JNEUROSCI.5044-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18385333</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ortiz-Rios</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Azevedo</surname> <given-names>FAC</given-names></name>, <name name-style="western"><surname>Kuśmierek</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Balla</surname> <given-names>DZ</given-names></name>, <name name-style="western"><surname>Munk</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Keliris</surname> <given-names>GA</given-names></name>, <etal>et al</etal>. <article-title>Widespread and Opponent fMRI Signals Represent Sound Location in Macaque Auditory Cortex</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>93</volume>: <fpage>971</fpage>–<lpage>983.e4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.01.013" xlink:type="simple">10.1016/j.neuron.2017.01.013</ext-link></comment> <object-id pub-id-type="pmid">28190642</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mullette-Gillman</surname> <given-names>OA</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>YE</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>Eye-Centered, Head-Centered, and Complex Coding of Visual and Auditory Targets in the Intraparietal Sulcus</article-title>. <source>J Neurophysiol</source>. <year>2005</year>;<volume>94</volume>: <fpage>2331</fpage>–<lpage>2352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00021.2005" xlink:type="simple">10.1152/jn.00021.2005</ext-link></comment> <object-id pub-id-type="pmid">15843485</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Godey</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Graaf</surname> <given-names>J</given-names></name>., <name name-style="western"><surname>Chauvel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Liégeois-Chauvel</surname> <given-names>C</given-names></name>. <article-title>Neuromagnetic source localization of auditory evoked fields and intracerebral evoked potentials: a comparison of data in the same patients</article-title>. <source>Clin Neurophysiol</source>. <year>2001</year>;<volume>112</volume>: <fpage>1850</fpage>–<lpage>1859</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1388-2457(01)00636-8" xlink:type="simple">10.1016/S1388-2457(01)00636-8</ext-link></comment> <object-id pub-id-type="pmid">11595143</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yvert</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bertrand</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pernier</surname> <given-names>J</given-names></name>. <article-title>Localization of human supratemporal auditory areas from intracerebral auditory evoked potentials using distributed source models</article-title>. <source>Neuroimage</source>. <year>2005</year>;<volume>28</volume>: <fpage>140</fpage>–<lpage>153</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2005.05.056" xlink:type="simple">10.1016/j.neuroimage.2005.05.056</ext-link></comment> <object-id pub-id-type="pmid">16039144</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Radeau</surname> <given-names>M</given-names></name>. <article-title>Cross-modal bias and perceptual fusion with auditory-visual spatial discordance</article-title>. <source>Percept Psychophys</source>. <year>1981</year>;<volume>29</volume>: <fpage>578</fpage>–<lpage>584</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03207374" xlink:type="simple">10.3758/BF03207374</ext-link></comment> <object-id pub-id-type="pmid">7279586</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bisley</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>ME</given-names></name>. <article-title>Attention, Intention, and Priority in the Parietal Lobe</article-title>. <source>Annu Rev Neurosci</source>. <year>2010</year>;<volume>33</volume>: <fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-060909-152823" xlink:type="simple">10.1146/annurev-neuro-060909-152823</ext-link></comment> <object-id pub-id-type="pmid">20192813</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gottlieb</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Snyder</surname> <given-names>LH</given-names></name>. <article-title>Spatial and non-spatial functions of the parietal cortex</article-title>. <source>Curr Opin Neurobiol</source>. <year>2010</year>;<volume>20</volume>: <fpage>731</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2010.09.015" xlink:type="simple">10.1016/j.conb.2010.09.015</ext-link></comment> <object-id pub-id-type="pmid">21050743</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ehlis</surname> <given-names>A-C</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>The neural dynamics of hierarchical Bayesian inference in multisensory perception</article-title>. <source>Nat Commun</source>. <year>2019</year>. In Press.</mixed-citation></ref>
<ref id="pbio.3000210.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Giordano</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>. <article-title>Causal inference in the multisensory brain</article-title>. <source>bioRxiv 500413</source> [Preprint]. <year>2018</year> [cited 2019 Feb 28]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/500413v1" xlink:type="simple">https://www.biorxiv.org/content/10.1101/500413v1</ext-link></mixed-citation></ref>
<ref id="pbio.3000210.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bledowski</surname> <given-names>C.</given-names></name> <article-title>Localizing P300 Generators in Visual Target and Distractor Processing: A Combined Event-Related Potential and Functional Magnetic Resonance Imaging Study</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>: <fpage>9353</fpage>–<lpage>9360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1897-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1897-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15496671</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linden</surname> <given-names>DEJ</given-names></name>. <article-title>The P300: Where in the Brain Is It Produced and What Does It Tell Us?</article-title> <source>Neurosci</source>. <year>2005</year>;<volume>11</volume>: <fpage>563</fpage>–<lpage>576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1073858405280524" xlink:type="simple">10.1177/1073858405280524</ext-link></comment> <object-id pub-id-type="pmid">16282597</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Polich</surname> <given-names>J.</given-names></name> <article-title>Updating P300: An integrative theory of P3a and P3b</article-title>. <source>Clin Neurophysiol</source>. <year>2007</year>;<volume>118</volume>: <fpage>2128</fpage>–<lpage>2148</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.clinph.2007.04.019" xlink:type="simple">10.1016/j.clinph.2007.04.019</ext-link></comment> <object-id pub-id-type="pmid">17573239</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duncan-Johnson</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>. <article-title>On quantifying surprise: the variation of event-related potentials with subjective probability</article-title>. <source>Psychophysiology</source>. <year>1977</year>;<volume>14</volume>: <fpage>456</fpage>–<lpage>467</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/905483" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/905483</ext-link> <object-id pub-id-type="pmid">905483</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duncan-Johnson</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>. <article-title>The P300 component of the event-related brain potential as an index of information processing</article-title>. <source>Biol Psychol</source>. <year>1982</year>;<volume>14</volume>: <fpage>1</fpage>–<lpage>52</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/6809064" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/6809064</ext-link> <object-id pub-id-type="pmid">6809064</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tueting</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zubin</surname> <given-names>J</given-names></name>. <article-title>Quantitative evoked potential correlates of the probability of events</article-title>. <source>Psychophysiology</source>. <year>1970</year>;<volume>7</volume>: <fpage>385</fpage>–<lpage>394</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/5510812" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/5510812</ext-link> <object-id pub-id-type="pmid">5510812</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tueting</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zubin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>John</surname> <given-names>ER</given-names></name>. <article-title>Information Delivery and the Sensory Evoked Potential</article-title>. <source>Science</source> (80-). <year>1967</year>;<volume>155</volume>: <fpage>1436</fpage>–<lpage>1439</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.155.3768.1436" xlink:type="simple">10.1126/science.155.3768.1436</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Coles</surname> <given-names>MGH</given-names></name>. <article-title>Is the P300 component a manifestation of context updating?</article-title> <source>Behav Brain Sci</source>. <year>1988</year>;<volume>11</volume>: <fpage>357</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X00058027" xlink:type="simple">10.1017/S0140525X00058027</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hillyard</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Hink</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Schwent</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Picton</surname> <given-names>TW</given-names></name>. <article-title>Electrical Signs of Selective Attention in the Human Brain</article-title>. <source>Science</source> (80-). <year>1973</year>;<volume>182</volume>: <fpage>177</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.182.4108.177" xlink:type="simple">10.1126/science.182.4108.177</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>R.</given-names></name> <article-title>A triarchic model of P300 amplitude</article-title>. <source>Psychophysiology</source>. <year>1986</year>;<volume>23</volume>: <fpage>367</fpage>–<lpage>84</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/3774922" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/3774922</ext-link> <object-id pub-id-type="pmid">3774922</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Connell</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Dockree</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Kelly</surname> <given-names>SP</given-names></name>. <article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>1729</fpage>–<lpage>1735</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3248" xlink:type="simple">10.1038/nn.3248</ext-link></comment> <object-id pub-id-type="pmid">23103963</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kopp</surname> <given-names>B.</given-names></name> <chapter-title>The P300 Component of the Event-Related Brain Potential And Bayes’ Theorem</chapter-title>. In: <name name-style="western"><surname>Sun</surname> <given-names>M-K</given-names></name>, editor. <source>Cognitive Sciences at the Leading Edge</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Nova Science</publisher-name>; <year>2007</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.13140/2.1.4049.4402" xlink:type="simple">10.13140/2.1.4049.4402</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kopp</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Seer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lange</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kluytmans</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kolossa</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fingscheidt</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>P300 amplitude variations, prior probabilities, and likelihoods: A Bayesian ERP study</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2016</year>;<volume>16</volume>: <fpage>911</fpage>–<lpage>928</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13415-016-0442-3" xlink:type="simple">10.3758/s13415-016-0442-3</ext-link></comment> <object-id pub-id-type="pmid">27406085</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mars</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Debener</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gladwin</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Haggard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>JC</given-names></name>, <etal>et al</etal>. <article-title>Trial-by-trial fluctuations in the event-related electroencephalogram reflect dynamic changes in the degree of surprise</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>12539</fpage>–<lpage>12545</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2925-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2925-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19020046</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yordanova</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kolev</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Polich</surname> <given-names>J</given-names></name>. <article-title>P300 and alpha event-related desynchronization (ERD)</article-title>. <source>Psychophysiology</source>. <year>2001</year>;<volume>38</volume>: <fpage>143</fpage>–<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1469-8986.3810143" xlink:type="simple">10.1111/1469-8986.3810143</ext-link></comment> <object-id pub-id-type="pmid">11321615</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>O’Connell</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Mills</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>. <article-title>Neuronal Oscillations and Multisensory Interaction in Primary Auditory Cortex</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>53</volume>: <fpage>279</fpage>–<lpage>292</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2006.12.011" xlink:type="simple">10.1016/j.neuron.2006.12.011</ext-link></comment> <object-id pub-id-type="pmid">17224408</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Petkov</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Augath</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>. <article-title>Functional Imaging Reveals Visual Modulation of Specific Fields in Auditory Cortex</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>1824</fpage>–<lpage>1835</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4737-06.2007" xlink:type="simple">10.1523/JNEUROSCI.4737-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17314280</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Molholm</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ritter</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Javitt</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Foxe</surname> <given-names>JJ</given-names></name>. <article-title>Multisensory auditory-visual interactions during early sensory processing in humans: A high-density electrical mapping study</article-title>. <source>Cogn Brain Res</source>. <year>2002</year>;<volume>14</volume>: <fpage>115</fpage>–<lpage>128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0926-6410(02)00066-6" xlink:type="simple">10.1016/S0926-6410(02)00066-6</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rieger</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Schoenfeld</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kanowski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hinrichs</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>H-J</given-names></name>, <etal>et al</etal>. <article-title>Audiovisual Temporal Correspondence Modulates Human Multisensory Superior Temporal Sulcus Plus Primary Sensory Cortices</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>11431</fpage>–<lpage>11441</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2252-07.2007" xlink:type="simple">10.1523/JNEUROSCI.2252-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17942738</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Audiovisual Synchrony Improves Motion Discrimination via Enhanced Connectivity between Early Visual and Auditory Areas</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>12329</fpage>–<lpage>12339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5745-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5745-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20844129</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werner</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Distinct Functional Contributions of Primary Sensory and Association Areas to Audiovisual Integration in Object Categorization</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>2662</fpage>–<lpage>2675</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5091-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5091-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20164350</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name>. <article-title>Temporal prediction errors in visual and auditory cortices</article-title>. <source>Curr Biol. Elsevier</source>; <year>2014</year>;<volume>24</volume>: <fpage>R309</fpage>–<lpage>R310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.02.007" xlink:type="simple">10.1016/j.cub.2014.02.007</ext-link></comment> <object-id pub-id-type="pmid">24735850</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Foxe</surname> <given-names>J</given-names></name>. <article-title>Multisensory contributions to low-level, “unisensory” processing</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2005</year>. pp. <fpage>454</fpage>–<lpage>458</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2005.06.008" xlink:type="simple">10.1016/j.conb.2005.06.008</ext-link></comment> <object-id pub-id-type="pmid">16019202</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atilgan</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Town</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>GP</given-names></name>, <name name-style="western"><surname>Maddox</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AKC</given-names></name>, <etal>et al</etal>. <article-title>Integration of Visual Information in Auditory Cortex Promotes Auditory Scene Analysis through Multisensory Binding</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>97</volume>: <fpage>640</fpage>–<lpage>655.e4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.12.034" xlink:type="simple">10.1016/j.neuron.2017.12.034</ext-link></comment> <object-id pub-id-type="pmid">29395914</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butler</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Foxe</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Fiebelkorn</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Mercier</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Molholm</surname> <given-names>S</given-names></name>. <article-title>Multisensory Representation of Frequency across Audition and Touch: High Density Electrical Mapping Reveals Early Sensory-Perceptual Coupling</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>15338</fpage>–<lpage>15344</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1796-12.2012" xlink:type="simple">10.1523/JNEUROSCI.1796-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23115172</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Deangelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>. <article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>146</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2983" xlink:type="simple">10.1038/nn.2983</ext-link></comment> <object-id pub-id-type="pmid">22101645</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Deangelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>. <article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title>. <source>Nat Rev Neurosci</source>. Nature Publishing Group; <year>2013</year>;<volume>14</volume>: <fpage>429</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3503" xlink:type="simple">10.1038/nrn3503</ext-link></comment> <object-id pub-id-type="pmid">23686172</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morgan</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>. <article-title>Multisensory Integration in Macaque Visual Cortex Depends on Cue Reliability</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>59</volume>: <fpage>662</fpage>–<lpage>673</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.06.024" xlink:type="simple">10.1016/j.neuron.2008.06.024</ext-link></comment> <object-id pub-id-type="pmid">18760701</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nardo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Santangelo</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name>. <article-title>Spatial orienting in complex audiovisual environments</article-title>. <source>Hum Brain Mapp</source>. <year>2014</year>;<volume>35</volume>: <fpage>1597</fpage>–<lpage>1614</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.22276" xlink:type="simple">10.1002/hbm.22276</ext-link></comment> <object-id pub-id-type="pmid">23616340</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/156856897X00357" xlink:type="simple">10.1163/156856897X00357</ext-link></comment> <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nichols</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>A</given-names></name>. <article-title>Nonparametric Permutation Tests for Functional Neuroimaging</article-title>. <source>Human Brain Function: Second Edition</source>. <year>2003</year>. pp. <fpage>887</fpage>–<lpage>910</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/B978-012264841-0/50048-2" xlink:type="simple">10.1016/B978-012264841-0/50048-2</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oostenveld</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>JM</given-names></name>. <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Comput Intell Neurosci</source>. <year>2011</year>;<volume>2011</volume>: <fpage>156869</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2011/156869" xlink:type="simple">10.1155/2011/156869</ext-link></comment> <object-id pub-id-type="pmid">21253357</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>C-J</given-names></name>. <article-title>Libsvm</article-title>. <source>ACM Trans Intell Syst Technol</source>. <year>2011</year>;<volume>2</volume>: <fpage>1</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1961189.1961199" xlink:type="simple">10.1145/1961189.1961199</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>CFJ</given-names></name>. <article-title>Measuring Location Effects from Factorial Experiments with a Directional Response</article-title>. <source>Int Stat Rev</source>. <year>1995</year>;<volume>63</volume>: <fpage>345</fpage>–<lpage>363</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.jstor.org/stable/1403484?casa_token=Av2iApF4mDsAAAAA:hhhrn0uyvFu8InecNUk5jdSAiEtbZt6R18wNShodIpNV3nM9oIurNKU5l-e92rZI_wXvcZKK-ywcWV5TrevOQzj3zGeI94nCICN3gDa5yMLqdtgMDEw&amp;seq=1#metadata_info_tab_contents" xlink:type="simple">https://www.jstor.org/stable/1403484?casa_token=Av2iApF4mDsAAAAA:hhhrn0uyvFu8InecNUk5jdSAiEtbZt6R18wNShodIpNV3nM9oIurNKU5l-e92rZI_wXvcZKK-ywcWV5TrevOQzj3zGeI94nCICN3gDa5yMLqdtgMDEw&amp;seq=1#metadata_info_tab_contents</ext-link></mixed-citation></ref>
<ref id="pbio.3000210.ref074"><label>74</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Edgington</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>Onghena</surname> <given-names>P</given-names></name>. <chapter-title>Randomization Tests</chapter-title> [Internet]. <edition>4th ed</edition>. <source>SpringerReference</source>. <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>Chapman &amp; Hall/CRC</publisher-name>; <year>2007</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-642-04898-2" xlink:type="simple">10.1007/978-3-642-04898-2</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gonzalez</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Manly</surname> <given-names>BFJ</given-names></name>. <article-title>Analysis of variance by randomization with small data sets</article-title>. <source>Environmetrics</source>. <year>1998</year>;<volume>9</volume>: <fpage>53</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/(SICI)1099-095X(199801/02)9:1&lt;53::AID-ENV285&gt;3.0.CO;2-#" xlink:type="simple">10.1002/(SICI)1099-095X(199801/02)9:1&lt;53::AID-ENV285&gt;3.0.CO;2-#</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berens</surname> <given-names>P.</given-names></name> <article-title>CircStat: A MATLAB Toolbox for Circular Statistics</article-title>. <source>J Stat Softw</source>. <year>2009</year>;<volume>31</volume>: <fpage>361</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18637/jss.v031.i10" xlink:type="simple">10.18637/jss.v031.i10</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagelkerke</surname> <given-names>NJD</given-names></name>. <article-title>A note on a general definition of the coefficient of determination</article-title>. <source>Biometrika</source>. <year>1991</year>;<volume>78</volume>: <fpage>691</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/78.3.691" xlink:type="simple">10.1093/biomet/78.3.691</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name>. <article-title>Bayes factors</article-title>. <source>J Am Stat Assoc</source>. <year>1995</year>;<volume>90</volume>: <fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1995.10476572" xlink:type="simple">10.1080/01621459.1995.10476572</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Poline J ‐</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Frackowiak</surname> <given-names>RSJ</given-names></name>. <article-title>Statistical parametric maps in functional imaging: A general linear approach</article-title>. <source>Hum Brain Mapp</source>. <year>1994</year>;<volume>2</volume>: <fpage>189</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.460020402" xlink:type="simple">10.1002/hbm.460020402</ext-link></comment></mixed-citation></ref>
<ref id="pbio.3000210.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harper</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>McAlpine</surname> <given-names>D</given-names></name>. <article-title>Optimal neural population coding of an auditory spatial cue</article-title>. <source>Nature</source>. <year>2004</year>;<volume>430</volume>: <fpage>682</fpage>–<lpage>686</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02768" xlink:type="simple">10.1038/nature02768</ext-link></comment> <object-id pub-id-type="pmid">15295602</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salminen</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>PJC</given-names></name>, <name name-style="western"><surname>Alku</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tiitinen</surname> <given-names>H</given-names></name>. <article-title>A population rate code of auditory space in the human cortex</article-title>. <source>PLoS ONE</source>. <year>2009</year>;<volume>4</volume>: <fpage>e7600</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0007600" xlink:type="simple">10.1371/journal.pone.0007600</ext-link></comment> <object-id pub-id-type="pmid">19855836</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Clock</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>. <article-title>A panoramic code for sound location by cortical neurons</article-title>. <source>Science</source> (80-). <year>1994</year>;<volume>264</volume>: <fpage>842</fpage>–<lpage>4</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/8171339" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/8171339</ext-link></mixed-citation></ref>
<ref id="pbio.3000210.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brugge</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Reale</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Hind</surname> <given-names>JE</given-names></name>. <article-title>Spatial receptive fields of primary auditory cortical neurons in quiet and in the presence of continuous background noise</article-title>. <source>J Neurophysiol</source>. <year>1998</year>;<volume>80</volume>: <fpage>2417</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1998.80.5.2417" xlink:type="simple">10.1152/jn.1998.80.5.2417</ext-link></comment> <object-id pub-id-type="pmid">9819253</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Teng</surname> <given-names>S</given-names></name>. <article-title>Resolving the neural dynamics of visual and auditory scene processing in the human brain: a methodological approach</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2017</year>;<volume>372</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2016.0108" xlink:type="simple">10.1098/rstb.2016.0108</ext-link></comment> <object-id pub-id-type="pmid">28044019</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N.</given-names></name> <article-title>Representational similarity analysis–connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>;<volume>2</volume>: <fpage>4</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation></ref>
<ref id="pbio.3000210.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosa</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>EEG-fMRI integration: a critical review of biophysical modeling and data analysis approaches</article-title>. <source>J Integr Neurosci</source>. <year>2010</year>;<volume>9</volume>: <fpage>453</fpage>–<lpage>476</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/21213414" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/21213414</ext-link> <object-id pub-id-type="pmid">21213414</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>