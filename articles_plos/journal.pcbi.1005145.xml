<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005145</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00761</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Motivation</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Motivation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Motivation</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Motivation</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Nucleus accumbens</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Nucleus accumbens</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Forgetting in Reinforcement Learning Links Sustained Dopamine Signals to Motivation</article-title>
<alt-title alt-title-type="running-head">Dynamic Equilibrium in Reinforcement Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6306-6600</contrib-id>
<name name-style="western">
<surname>Kato</surname>
<given-names>Ayaka</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Morita</surname>
<given-names>Kenji</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Biological Sciences, Graduate School of Science, The University of Tokyo, Tokyo, Japan</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Physical and Health Education, Graduate School of Education, The University of Tokyo, Tokyo, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gutkin</surname>
<given-names>Boris S.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>École Normale Supérieure, College de France, CNRS, FRANCE</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item><p><bold>Conceptualization:</bold> AK KM.</p></list-item> <list-item><p><bold>Formal analysis:</bold> AK KM.</p></list-item> <list-item><p><bold>Funding acquisition:</bold> KM.</p></list-item> <list-item><p><bold>Investigation:</bold> AK KM.</p></list-item> <list-item><p><bold>Methodology:</bold> AK KM.</p></list-item> <list-item><p><bold>Software:</bold> AK KM.</p></list-item> <list-item><p><bold>Supervision:</bold> KM.</p></list-item> <list-item><p><bold>Visualization:</bold> AK KM.</p></list-item> <list-item><p><bold>Writing – original draft:</bold> AK KM.</p></list-item> <list-item><p><bold>Writing – review &amp; editing:</bold> AK KM.</p></list-item></list></p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">morita@p.u-tokyo.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>13</day>
<month>10</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>10</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>10</issue>
<elocation-id>e1005145</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>5</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>9</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Kato, Morita</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005145"/>
<abstract>
<p>It has been suggested that dopamine (DA) represents reward-prediction-error (RPE) defined in reinforcement learning and therefore DA responds to unpredicted but not predicted reward. However, recent studies have found DA response sustained towards predictable reward in tasks involving self-paced behavior, and suggested that this response represents a motivational signal. We have previously shown that RPE can sustain if there is decay/forgetting of learned-values, which can be implemented as decay of synaptic strengths storing learned-values. This account, however, did not explain the suggested link between tonic/sustained DA and motivation. In the present work, we explored the motivational effects of the value-decay in self-paced approach behavior, modeled as a series of ‘Go’ or ‘No-Go’ selections towards a goal. Through simulations, we found that the value-decay can enhance motivation, specifically, facilitate fast goal-reaching, albeit counterintuitively. Mathematical analyses revealed that underlying potential mechanisms are twofold: (1) decay-induced sustained RPE creates a gradient of ‘Go’ values towards a goal, and (2) value-contrasts between ‘Go’ and ‘No-Go’ are generated because while chosen values are continually updated, unchosen values simply decay. Our model provides potential explanations for the key experimental findings that suggest DA's roles in motivation: (i) slowdown of behavior by post-training blockade of DA signaling, (ii) observations that DA blockade severely impairs effortful actions to obtain rewards while largely sparing seeking of easily obtainable rewards, and (iii) relationships between the reward amount, the level of motivation reflected in the speed of behavior, and the average level of DA. These results indicate that reinforcement learning with value-decay, or forgetting, provides a parsimonious mechanistic account for the DA's roles in value-learning and motivation. Our results also suggest that when biological systems for value-learning are active even though learning has apparently converged, the systems might be in a state of dynamic equilibrium, where learning and forgetting are balanced.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Dopamine (DA) has been suggested to have two reward-related roles: (1) representing reward-prediction-error (RPE), and (2) providing motivational drive. Role(1) is based on the physiological results that DA responds to unpredicted but not predicted reward, whereas role(2) is supported by the pharmacological results that blockade of DA signaling causes motivational impairments such as slowdown of self-paced behavior. So far, these two roles are considered to be played by two different temporal patterns of DA signals: role(1) by phasic signals and role(2) by tonic/sustained signals. However, recent studies have found sustained DA signals with features indicative of both roles (1) and (2), complicating this picture. Meanwhile, whereas synaptic/circuit mechanisms for role(1), i.e., how RPE is calculated in the upstream of DA neurons and how RPE-dependent update of learned-values occurs through DA-dependent synaptic plasticity, have now become clarified, mechanisms for role(2) remain unclear. In this work, we modeled self-paced behavior by a series of ‘Go’ or ‘No-Go’ selections in the framework of reinforcement-learning assuming DA's role(1), and demonstrated that incorporation of decay/forgetting of learned-values, which is presumably implemented as decay of synaptic strengths storing learned-values, provides a potential unified mechanistic account for the DA's two roles, together with its various temporal patterns.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>The Ministry of Education, Culture, Sports, Science and Technology in Japan</institution>
</funding-source>
<award-id>Grant-in-Aid for Scientific Research (No. 15H05876, No. 26120710)</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Morita</surname>
<given-names>Kenji</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Japan Agency for Medical Research and Development</institution>
</funding-source>
<award-id>Strategic Japanese - German Cooperative Programme on “Computational Neuroscience” (project title: neural circuit mechanisms of reinforcement learning)</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Morita</surname>
<given-names>Kenji</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Grant-in-Aid for Scientific Research (No. 15H05876, No. 26120710) of the Ministry of Education, Culture, Sports, Science and Technology in Japan (<ext-link ext-link-type="uri" xlink:href="http://www.mext.go.jp/en/" xlink:type="simple">http://www.mext.go.jp/en/</ext-link>) and Strategic Japanese - German Cooperative Programme on “Computational Neuroscience” (project title: neural circuit mechanisms of reinforcement learning) of the Japan Agency for Medical Research and Development (<ext-link ext-link-type="uri" xlink:href="http://www.amed.go.jp/en/" xlink:type="simple">http://www.amed.go.jp/en/</ext-link>) to KM. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="14"/>
<table-count count="0"/>
<page-count count="41"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All program files are publicly available on ModelDB (<ext-link ext-link-type="uri" xlink:href="https://senselab.med.yale.edu/ModelDB/showModel.cshtml?model=195890" xlink:type="simple">https://senselab.med.yale.edu/ModelDB/showModel.cshtml?model=195890</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Electrophysiological [<xref ref-type="bibr" rid="pcbi.1005145.ref001">1</xref>] and fast-scan cyclic voltammetry (FSCV) [<xref ref-type="bibr" rid="pcbi.1005145.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref003">3</xref>] studies have conventionally shown that dopamine (DA) neuronal activity and transmitter release respond to unpredicted but not predicted reward, consistent with the suggestion that DA represents reward-prediction-error (RPE) [<xref ref-type="bibr" rid="pcbi.1005145.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref004">4</xref>]. On the other hand, recent FSCV studies [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] have found DA response sustained towards presumably predictable reward, arguing that it may represent sustained motivational drive. DA's roles in motivation processes have long been suggested [<xref ref-type="bibr" rid="pcbi.1005145.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref013">13</xref>] primarily from pharmacological results. A key finding is that post-training blockade of DA signaling causes motivational impairments such as slowdown of behavior (e.g., [<xref ref-type="bibr" rid="pcbi.1005145.ref014">14</xref>]), and this is difficult to explain with respect to the known role of DA in RPE representation because post-training RPE should be negligible so that blockade of RPE should have little impact.</p>
<p>Therefore it has been considered that DA has two distinct reward-related roles, (1) representing RPE and (2) providing motivational drive, and these are played by phasic and tonic/sustained DA, respectively. Normative theories have been proposed for both the role as RPE [<xref ref-type="bibr" rid="pcbi.1005145.ref004">4</xref>] and the role as motivational drive [<xref ref-type="bibr" rid="pcbi.1005145.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>] in the framework of reinforcement learning (RL). On the other hand, as for the underlying synaptic/circuit mechanisms, much progress has been made for the role as RPE but not for the role as motivational drive. Specifically, how RPE is calculated in the upstream of DA neurons and how released DA implements RPE-dependent update of state/action values through synaptic plasticity have now become clarified [<xref ref-type="bibr" rid="pcbi.1005145.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref020">20</xref>]. In contrast, both the upstream and downstream mechanisms for DA's motivational role remain more elusive.</p>
<p>In fact, FSCV studies that found sustained DA signals [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] have shown that those DA signals exhibited features indicative of RPE. Moreover, sustained response towards presumably predictable reward has also been found in the activity of DA neurons [<xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref022">22</xref>], and these studies have also argued that the DA activity represents RPE. Consistent with these views, we have recently shown [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>] that RPE can actually sustain after training if decay/forgetting of learned values, which can presumably be implemented as decay of plastic changes of synaptic strengths, is assumed in RL. It was further indicated that whether RPE/DA sustains or not can be coherently understood as reflecting differences in how fast learned values decay in time: faster decay causes more sustained RPE/DA. However, this account did not explain the suggested link between sustained DA and motivation. Even on the contrary, decay of learned values is apparently wasteful and could be perceived as a loss of motivational drive.</p>
<p>In several recent studies reporting sustained DA signals [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>], a common feature is that self-paced actions are required, as argued in [<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>]. We conjectured that this feature could be critical for the putative motivational functions of sustained DA signals. However, in our previous study [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>], such a feature was not incorporated: our previous model was extremely simple and assumed that the subject automatically moved to the next state at every time step. In the present work, we constructed a new model, which incorporated the requirement of self-paced approach towards a goal, represented as a series of ‘Go’ or ‘No-Go’ (or ‘Stay’) selections, into RL with decay of learned values. Using this new model, we investigated: (1) if the model (as well as the previous non-self-paced model) generates both phasic and sustained RPE/DA signals so that their mechanisms can be coherently understood, (2) if the model demonstrates any association between sustained DA signals and motivation, and (3) if the model can mechanistically account for the key experimental findings that suggest DA's roles in motivation, specifically, the (i) slowdown of self-paced behavior by post-training blockade of DA signaling [<xref ref-type="bibr" rid="pcbi.1005145.ref014">14</xref>], (ii) severe impairment of effortful actions to obtain rewards, but not of seeking of easily obtainable rewards, by DA blockade [<xref ref-type="bibr" rid="pcbi.1005145.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>], and (iii) relationships between the reward amount, the level of motivation reflected in the speed of behavior, and the average level of DA [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>]. Through simulations and mathematical (bifurcation) analyses, we have successfully answered these questions.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>The value-decay facilitates fast goal-reaching, and reproduces the slowdown caused by DA blockade</title>
<p>We modeled a behavioral task requiring self-paced voluntary approach (whether spatially or not) towards a goal as a series of ‘Go’ or ‘Stay’ (‘No-Go’) selections as illustrated in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>. We then simulated subject's behavior by a temporal-difference (TD) RL model incorporating the decay of learned values (referred to as the ‘value-decay’ below). Specifically, we assumed that at every time step the subject selects ‘Go’ or ‘Stay’ depending on their learned values, which are updated according to RPE (TD error) when the corresponding action is taken. In addition, we also assumed that the learned values of all the actions (whether selected or not) decay in time at a constant rate (see the <xref ref-type="sec" rid="sec016">Materials and Methods</xref> for details). RPE at each time step was assumed to be represented by the level of DA at the time step, and the value decay was assumed to be implemented as a decay of plastic changes of synaptic strengths storing learned values.</p>
<fig id="pcbi.1005145.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Modeling the behavior of subject performing a task that requires self-paced voluntary approach (whether spatially or not) towards a goal.</title>
<p>We posited that self-paced voluntary approach can be represented as a series of ‘Go’ or ‘Stay’ selections, as illustrated here. Subject starts from <italic>S</italic><sub>1</sub>, and chooses ‘Go’ or ‘Stay’ according to their learned values in a soft-max manner in each state until reaching the goal (<italic>S</italic><sub>7</sub>), where reward is obtained. The values of actions (‘Go’ and ‘Stay’) are learned through temporal-difference (TD) reinforcement learning (RL) incorporating the decay of learned values (referred to as the ‘value-decay’): learned value of arbitrary action (‘Go’ or ‘Stay’) is multiplied, at every time step, by (1 –<italic>φ</italic>), where <italic>φ</italic> (0 ≤ <italic>φ</italic> ≤ 1) represents the decay rate: <italic>φ</italic> = 0 corresponds to the case without value-decay.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g001" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2A</xref> shows the number of time-steps needed for goal-reaching (i.e., from the start to the goal in a single trial; referred to as the ‘time needed for goal-reaching’ below) averaged over 500 trials, with the rate of the value-decay (referred to as the ‘decay rate’ below) varied. As shown in the figure, the time needed for goal-reaching is minimized in the case with a certain degree of value-decay. In other words, introduction of the value-decay can facilitate fast goal-reaching. <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2B</xref> shows the trial-by-trial change of the time needed for goal-reaching. Without the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2B</xref>, left), the subject initially learns to reach the goal quickly, but subsequently a significant slowdown occurs. In contrast, with the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2B</xref>, middle and right), the time needed for goal-reaching is kept small, never showing slowdown. The observed facilitation of fast goal-reaching by introduction of the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2A</xref>) is thus accompanied with such a qualitative change in the long-term dynamics.</p>
<fig id="pcbi.1005145.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g002</object-id>
<label>Fig 2</label>
<caption>
<title>RL model with the value-decay achieves fast goal-reaching, and reproduces the slowdown caused by post-training blockade of DA signaling.</title>
<p><bold>(A)</bold> Number of time steps needed for goal-reaching averaged over 500 trials (vertical axis) in the cases with various rates of value-decay (horizontal axis). The rate of the value-decay is referred to as the decay rate and represented by the parameter <italic>φ</italic>: "decay rate <italic>φ =</italic> 0" corresponds to the case without value-decay. The error bar indicates the mean ± standard error (SE) of 20 simulations. The bottom dashed line indicates the theoretical minimum number of time steps needed for goal-reaching (including the steps at the start and the goal) and the top dashed line indicates the chance level: these are also applied to (B) and (C). <bold>(B)</bold> The thick black lines indicate trial-by-trial changes of the number of time steps needed for goal-reaching averaged over every 5 trials (vertical axis) along with the progression of trials (horizontal axis). The gray lines indicate the mean ± SE of 20 simulations. The left, middle, and right panels show the cases with <italic>φ =</italic> 0 (without the value-decay), <italic>φ =</italic> 0.01, and <italic>φ =</italic> 0.02, respectively: this is also applied to (C). <bold>(C)</bold> Effects of post-training blockade of DA signaling on the number of time steps needed for goal-reaching. During simulations similar to (B), the size of TD-reward-prediction-error(RPE)-dependent increment of action values was reduced to zero (top panels) or to a quarter of the original size (bottom panels) after 250 trials were completed (indicated by the vertical dotted lines). The other configurations are the same as those in (B).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g002" xlink:type="simple"/>
</fig>
<p>In the same simulated task using the same model, we examined how post-training blockade of DA signaling affects the subject's speed (i.e., the time needed for goal-reaching), again varying the decay rate. Specifically, with the assumption that DA represents RPE, we simulated the post-training DA blockade by reducing the size of RPE-dependent increment of action values to zero (complete blockade) or to a quarter of the original size (partial blockade) after 250 trials were completed. <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref> shows the results. As shown in the left panels of <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref>, without the value-decay, DA blockade causes little effect on the subject's speed. In contrast, in the case with the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref>, middle and right panels), the same DA blockade rapidly causes pronounced slowdown (i.e., increase in the time needed for goal-reaching).</p>
</sec>
<sec id="sec004">
<title>The value-decay leads to sustained positive RPE and a gradient of ‘Go’ values</title>
<p>In order to explore mechanisms underlying the fast goal-reaching achieved with the value-decay and its impairment by DA blockade, we examined the action values of ‘Go’ and ‘Stay’ at each state. The black and gray lines in <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A</xref> respectively show the action values of ‘Go’ and ‘Stay’ at the end of the 500th trial, and <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3B</xref> shows their trial-by-trial evolutions. Without the value-decay (left panels of <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A and 3B</xref>), all the action values are eventually almost saturated to the reward amount (= 1), so that there remains little difference between the action values of ‘Stay’ and ‘Go’ at any states. As a result, subject should choose ‘Stay’ as frequently as ‘Go’. This explains the observed slowdown in the case without the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2B</xref>, left panel). In contrast, with the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A and 3B</xref>, middle and right panels), the action values of ‘Go’ shape a sustained gradient from the start to the goal, while the actions values of ‘Stay’ remain relatively small.</p>
<fig id="pcbi.1005145.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The value-decay leads to sustained RPE, which generates a gradient of ‘Go’ values towards the goal.</title>
<p><bold>(A)</bold> Action values of ‘Go’ (black lines/crosses) and ‘Stay’ (gray lines/crosses) at the end of the 500th trial. The horizontal axis indicates the indices of the actions (illustrated in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>), where the odd numbers (shown in gray) indicate ‘Stay’ whereas the even numbers (black) indicate ‘Go’. The error bars show the mean ± SE of 20 simulations. The left, middle, and right panels show the cases with <italic>φ =</italic> 0 (without the value-decay), <italic>φ =</italic> 0.01, and <italic>φ =</italic> 0.02, respectively: this is also applied to (B) and (C). <bold>(B)</bold> Trial-by-trial changes of action values. The color indicates the action value averaged over 20 simulations, in reference to the rightmost color scale bar. The vertical axis indicates the trials (from the top to the bottom) and the horizontal axis indicates the indices of the actions (odd/gray: ‘Stay’, even/black: ‘Go’: <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>). <bold>(C)</bold> Examples of RPE generated in successive trials of the task. The black solid lines indicate RPE and the vertical thin dotted lines delimit individual trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g003" xlink:type="simple"/>
</fig>
<p>Why does the value-decay create such a gradient of ‘Go’ values? <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref> shows examples of RPE generated during the task. In the case without the value-decay (left panel), positive RPE is generated at the beginning of each trial, but RPE is mostly nearly zero in other epochs. This is what we usually expect from TD RL models after learning [<xref ref-type="bibr" rid="pcbi.1005145.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref025">25</xref>]. On the contrary, in the case with the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>, middle and right panels), RPE remains to be positive in most of the time, indicating that decrement of action values due to the value-decay is balanced with RPE-dependent increment. Such sustained positive RPE is then considered to create the start-to-goal gradient of ‘Go’ values. This is because RPE generated when taking ‘Go’ at state <italic>S</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, …, 6) is calculated (see the <xref ref-type="sec" rid="sec016">Materials and Methods</xref>) as
<disp-formula id="pcbi.1005145.e001">
<alternatives>
<graphic id="pcbi.1005145.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mtext>RPE</mml:mtext><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Stay</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
(<italic>γ</italic>: time discount factor, satisfying 0 ≤ <italic>γ</italic> ≤ 1)
which is not greater than <italic>Q</italic>(‘Go’ at <italic>S</italic><sub><italic>i</italic> + 1</sub>) − <italic>Q</italic>(‘Go’ at <italic>S</italic><sub><italic>i</italic></sub>) provided <italic>Q</italic>(‘Stay’) ≤ <italic>Q</italic>(‘Go’) (this would naturally be expected), and then "0 &lt; RPE" ensures
<disp-formula id="pcbi.1005145.e002">
<alternatives>
<graphic id="pcbi.1005145.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>⇔</mml:mo><mml:mrow/></mml:munder><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‘</mml:mo><mml:mtext>Go</mml:mtext><mml:mo>’</mml:mo><mml:mtext> at </mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
which indicates a gradient towards the goal.</p>
<p>Looking at the pattern of RPE (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>), in the case with a relatively larger value-decay, RPE exhibits a ramp towards the goal (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>, right; notably, this decay rate does not achieve the fastest goal-reaching, but still realizes a faster goal-reaching than the case without value-decay: cf. <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2A</xref>). This resembles the experimentally observed ramp-like patterns of DA neuronal activity [<xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref022">22</xref>] or striatal DA concentration [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] as we have previously suggested using the non-self-paced model [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>]. But with a milder value-decay, RPE peaks both at the start and towards the goal, with the former more prominent (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>, middle). In this way, our model generates various patterns of RPE, from phasic to ramping, depending on the decay rate, or indeed the relative strength of the value-decay to the number of states. This could potentially be in line with the fact that the studies reporting DA ramping [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref022">22</xref>] used operant or navigation tasks in which several different states within a trial seem likely to be defined whereas the studies reporting clearly phasic DA response [<xref ref-type="bibr" rid="pcbi.1005145.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref003">3</xref>] used a simple classical conditioning task where a smaller number of states might be defined.</p>
<p>It has been also found in other studies [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] that elevations in DA levels occurred earlier in later task sessions. According to our simulation results (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>), such a change could potentially be explained in our model if the decay rate gradually decreases (i.e., from the right panel of <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref> to the middle panel). In our simulations, such a decrease in the decay rate is in the direction towards an optimal decay rate in terms of the time needed for goal-reaching averaged over 500 trials (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2A</xref>). This suggests that the experimentally observed changes in the DA response pattern across sessions [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] might be an indicative of meta-learning processes to adjust the decay rate to an optimal level. Despite these potentially successful explanations of the various DA response patterns, however, not all the patterns can be explained by our model. In particular, it has been shown that the DA concentration decreases during the reward delivery (sucrose infusion for 6 sec) [<xref ref-type="bibr" rid="pcbi.1005145.ref002">2</xref>]. Our model does not explain such a decrease of DA: to explain this, it would be necessary to extend the model to describe the actual process of reward delivery/consumption.</p>
</sec>
<sec id="sec005">
<title>Mechanistic explanations of the motivational impairments caused by DA blockade</title>
<p>The reason why the blockade of DA signaling causes slowdown in the cases with the value-decay but not in the cases without the value-decay in our model (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref>) can also be understood by looking at RPE. Specifically, in the cases with the value-decay, positive RPE is continued to be generated at every state (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>, middle and right), and each ‘Go’ value is kept around a certain value (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3B</xref>, middle and right) because increment according to RPE and decrement due to the value-decay are balanced. Then, if DA signaling is blocked and the size of RPE-dependent increment is reduced, such a balance is perturbed and thereby ‘Go’ values decrease, resulting in the slowdown. In contrast, in the cases without the value-decay, sustained positive RPE is generated only at the beginning of each trial (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>, left), and it does not increase the value of ‘Go’ taken later in the trial. Thus, after learning has settled down, ‘Go’ values are almost frozen, and therefore blockade of DA signaling has little impact on subject behavior.</p>
<p><xref ref-type="fig" rid="pcbi.1005145.g004">Fig 4</xref> shows the trial-by-trial changes of the action values (the top panels of <xref ref-type="fig" rid="pcbi.1005145.g004">Fig 4A and 4B</xref>) and the action values at the end of the 500th trial (the bottom panels) in the simulations where the size of RPE-dependent increment of action values was reduced to zero (A) or to a quarter of the original size (B) after 250 trials were completed. As shown in these figures, the abovementioned conjectures about the effects of DA blockade on the action values were confirmed. Given that the action values are represented in the striatal neural activity, the parallel reduction in the action values and the speed for goal-reaching by DA blockade in our model can be broadly in line with a recent finding of the parallel impairment of the striatal neural representation of actions and the action vigor in DA-depleted mice [<xref ref-type="bibr" rid="pcbi.1005145.ref026">26</xref>].</p>
<fig id="pcbi.1005145.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Changes in the action values caused by post-training blockade of DA signaling.</title>
<p>The left, middle, and right panels show the cases with <italic>φ =</italic> 0 (without the value-decay), <italic>φ =</italic> 0.01, and <italic>φ =</italic> 0.02, respectively. The top and bottom panels of (A,B) show the trial-by-trial changes of the action values and the action values at the end of the 500th trial, respectively, in the simulations where the size of RPE-dependent increment of action values was reduced to zero (A) or to a quarter of the original size (B) after 250 trials were completed (indicated by the horizontal dotted lines). The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3B</xref> (top panels of (A,B)) or <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A</xref> (bottom panels of (A,B)).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g004" xlink:type="simple"/>
</fig>
<p>Also, intriguingly, in the cases with the value-decay, after DA signaling is reduced to a quarter of the original (<xref ref-type="fig" rid="pcbi.1005145.g004">Fig 4B</xref>, middle and right panels), whereas the values of ‘Go’ actions distant from the goal degrade quite prominently, the values of ‘Go’ actions near the goal (i.e., <italic>A</italic><sub>12</sub> and <italic>A</italic><sub>10</sub>) remain relatively large, although they are also significantly decreased from the original values. This could potentially be in line with the experimental observations that DA blockade severely impairs costly or effortful actions to obtain rewards but seeking of easily obtainable rewards are largely spared [<xref ref-type="bibr" rid="pcbi.1005145.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>]. In order to more directly address this issue, we simulated an experiment examining the effects of DA depletion in the nucleus accumbens in a cost-benefit decision making task in a T-maze reported in [<xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>].</p>
<p>In one condition of the experiment, there was small reward in one of the two arms of the T-maze whereas there was large reward accompanied with a high cost (physical barrier) in the other arm. In the baseline period after training (exploration) of the maze, rats preferred the high-cost-high-return arm. However, DA depletion reversed the preference so that the rats switched to prefer the low-cost-low-return arm. DA depletion also increased the response latency (opening of the start door at the end of the start arm), although the latency subsequently recovered. In another condition of the experiment, the two arms contained small and large rewards as before, but neither was accompanied with a high cost. In this condition, rats preferred the large-reward arm, and DA depletion did not reverse the preference. Meanwhile, DA depletion still increased the response latency, though the latency subsequently recovered as before.</p>
<p>We simulated this experiment by representing a high cost as an extra state preceding the reward (State 5 in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A</xref>, right). <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5B and 5C</xref> show the ratio of choosing the large-reward arm (Arm 1 in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A</xref>) and the average time needed for reaching the T-junction (State 4 in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A</xref>, right), respectively, in the condition with a high cost in the large-reward arm (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A</xref>). <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5F and 5G</xref> show the results in the condition without a high cost (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5E</xref>). As shown in these figures, the model successfully reproduces the experimental observations that DA depletion induced a preference reversal only in the condition with a high cost (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5B and 5F</xref>) while increased the latency in both conditions (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5C and 5G</xref>), although the subsequent recovery of the latency is not reproduced. Looking at the action values in the case with a high-cost (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5D</xref>), the value of ‘Go’ to Arm 1 at the T-junction is fairly high before DA depletion. However, because this action is apart from reward, its value degrades quite prominently after DA depletion, becoming lower than the value of ‘Go’ to Arm 2, which is adjacent to reward (even though it is small reward). This explains the preference reversal (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5B</xref>). In contrast, in the case without a high-cost (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5H</xref>), the value of ‘Go’ to Arm 1 degrades only moderately after DA depletion, remaining higher than the value of ‘Go’ to Arm 2. In the meantime, in both conditions, initially there are value-contrasts between ‘Go’ and ‘Stay’ at States 1–3 but they degrade after DA depletion, explaining the increase in the latency (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5C and 5G</xref>).</p>
<fig id="pcbi.1005145.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Effects of DA depletion on a cost-benefit decision making task in a T-maze simulated by the model with the value-decay.</title>
<p><bold>(A)</bold> Schematic diagram of one condition of the simulated task, in which there was small reward in one of the two arms of the T-maze (Arm 2 in the figure) whereas there was large reward accompanied with high cost, represented as an extra state preceding the reward (explicitly shown in the right panel), in the other arm (Arm 1). <bold>(B)</bold> Ratio of choosing the large-reward arm (Arm 1) in the simulations of the task shown in (A). The thick black line indicates the ratio of choosing Arm 1 in every 10 trials averaged over 20 simulations, and the thin gray lines indicate the mean ± SE of the 20 simulations. Post-training DA depletion was simulated in such a way that the size of RPE-dependent increment of action values was reduced to a quarter of the original size after 500 trials were completed (indicated by the vertical dotted lines). <bold>(C)</bold> Average number of time-steps towards the T-junction (State 4 in (A)) in the simulations of the task shown in (A). The thick black line indicates the number of time-steps averaged over every 10 trials in each of 20 simulations, and the thin gray lines indicate the mean ± SE of the 20 simulations. The bottom dashed line indicates the theoretical minimum number of time steps to State 4 (including the steps at the start and State 4). <bold>(D)</bold> Average action values in the simulations of the task shown in (A). The color indicates the values of actions in the T-maze (arrows: ‘Go’, circles: ‘Stay’) averaged across 251–500 trials (left, before DA depletion) or 751–1000 trials (right, after DA depletion) and averaged over 20 simulations, in reference to the bottom color scale bar. <bold>(E)</bold> Schematic diagram of another condition of the simulated task, in which the two arms contained small and large rewards as before, but neither was accompanied with high cost. <bold>(F-H)</bold> The ratio of choosing the large-reward arm (Arm 1) (F), the average number of time-steps towards the T-junction (G), and the action values (H) in the simulations of the task condition shown in (E). The configurations are the same as those in (B-D).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>The value-decay creates contrasts between ‘Go’ and ‘Stay’ values</title>
<p>As we have shown above, the value-decay creates a gradient of ‘Go’ values towards the goal. It is known that temporal discounting of rewards also makes a gradient of values (c.f., [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>]). However, we assumed no temporal discounting (i.e., time discount factor <italic>γ</italic> = 1) in the above simulations and thus the value-gradient observed in the above was caused solely by the value-decay. In order to compare the effects of the value-decay and the effects of temporal discounting, we conducted simulations of the original unbranched self-paced task (<xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>) assuming no value-decay but instead temporal discounting (time discount factor <italic>γ</italic> = 0.8). <xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6</xref> shows the resulting action values (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6A and 6B</xref>), RPE (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6C</xref>), and the effect of DA blockade on the time needed for goal-reaching (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6D</xref>). As shown in <xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6A and 6B</xref>, a value-gradient is shaped, as expected. Contrary to the case with the value-decay, however, sustained positive RPE is generated only at the beginning of each trial (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6C</xref>), and because of this, post-training blockade of DA signaling causes little effect on the subject speed (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6D</xref>).</p>
<fig id="pcbi.1005145.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Simulation results without the value-decay but with temporal discounting of rewards (time discount factor <italic>γ</italic> = 0.8).</title>
<p><bold>(A)</bold> Action values at the end of the 500th trial. <bold>(B)</bold> Trial-by-trial changes of action values. <bold>(C)</bold> Examples of RPE. <bold>(D)</bold> Effects of post-training DA blockade on the number of time steps needed for goal-reaching. The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A–3C</xref> or <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g006" xlink:type="simple"/>
</fig>
<p>Comparing the value gradient caused by the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A and 3B</xref>, middle/right) and the gradient caused by temporal discounting (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6A and 6B</xref>), the differences of the action values between ‘Stay’ and ‘Go’ are much larger in the case with the value-decay. This is considered to be because, in the case with the value-decay, the values of unchosen actions just decay whereas those of chosen actions are kept updated according to RPE. In order to mathematically confirm this conjecture, especially, the long-term stability of such a large contrast between ‘Stay’ and ‘Go’ values, we considered a reduced dynamical system model of our original model, focusing on the last state preceding the goal (i.e., <italic>S</italic><sub>6</sub> in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>), and conducted bifurcation analysis. Specifically, we derived a two-dimensional dynamical system that approximately describes the dynamics of the action values of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’) at <italic>S</italic><sub>6</sub> (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7A</xref>; see the <xref ref-type="sec" rid="sec016">Materials and Methods</xref> for details), and examined how the system's behavior qualitatively changes along with the change in the degree of the value-decay. Temporal discounting was not assumed (i.e., <italic>γ</italic> was assumed to be 1) in this reduced model so as to isolate the effect of the value-decay.</p>
<fig id="pcbi.1005145.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g007</object-id>
<label>Fig 7</label>
<caption>
<title>The value-decay generates value-contrasts between ‘Go’ and ‘Stay’.</title>
<p><bold>(A)</bold> Schematic diagram of the selection of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’) at <italic>S</italic><sub>6</sub>. We considered a reduced continuous-time dynamical system model that describes the time evolution of <italic>q</italic>(<italic>A</italic><sub>11</sub>) and <italic>q</italic>(<italic>A</italic><sub>12</sub>), which are continuous-time variables approximately representing the action values of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’), respectively. <bold>(B)</bold> Bifurcation diagram of the reduced model, showing the equilibrium values of <italic>q</italic>(<italic>A</italic><sub>11</sub>(‘Stay’)) (red line) and <italic>q</italic>(<italic>A</italic><sub>12</sub>(‘Go’)) (blue line) (vertical axis) depending on the degree of the value-decay (horizontal axis; <italic>ψ</italic> = 0 corresponds to the case without the value-decay). Temporal discounting was not assumed. The thick parts of the lines indicate the stable equilibriums, whereas the thin part indicates the unstable equilibrium; the unstable equilibrium of <italic>q</italic>(<italic>A</italic><sub>12</sub>(‘Go’)) is overlapped by the stable equilibrium and is thus invisible. <bold>(C)</bold> Probability of selecting <italic>A</italic><sub>11</sub> (‘Stay’) (red) or <italic>A</italic><sub>12</sub>(‘Go’) (blue) at the equilibriums (vertical axis) depending on the degree of the value-decay (horizontal axis). The thick parts and thin parts correspond to the stable and unstable equilibriums, respectively. <bold>(D)</bold> A simulation result of the original model with the decay rate <italic>φ =</italic> 0.0045, in which there appears a phenomenon indicative of bistability: the value of <italic>A</italic><sub>11</sub> (‘Stay’) fluctuates between two levels in long time scales. <bold>(E)</bold> Phase diagrams in the cases with five different degrees of the value-decay. The red and blue lines indicate the nullclines on which the time derivative of <italic>q</italic>(<italic>A</italic><sub>11</sub>(‘Stay’)) or <italic>q</italic>(<italic>A</italic><sub>12</sub>(‘Go’)) is zero, respectively. The gray arrows indicate the direction of the time evolution of <italic>q</italic>(<italic>A</italic><sub>11</sub>(‘Stay’)) and <italic>q</italic>(<italic>A</italic><sub>12</sub>(‘Go’)) (indicating vectors (d<italic>q</italic>(<italic>A</italic><sub>11</sub>)/d<italic>t</italic>, d<italic>q</italic>(<italic>A</italic><sub>12</sub>)/d<italic>t</italic>)/2). Notably, the analysis of the reduced model was conducted under the assumption of <italic>q</italic>(<italic>A</italic><sub>11</sub>(‘Stay’)) ≤ <italic>q</italic>(<italic>A</italic><sub>12</sub>(‘Go’)), which corresponds to the upper left region of the black dashed line.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g007" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref> is the resulting bifurcation diagram showing the equilibrium action values of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’) at <italic>S</italic><sub>6</sub> (with approximations) with the degree of the value-decay varied, and <xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7C</xref> shows the probability of choosing <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’) at the equilibrium point. As shown in <xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref>, it was revealed that as the degree of the value-decay increases, qualitative changes occur twice (in technical terms, arrangements of the nullclines shown in <xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7E</xref> indicate that both of them are saddle-node bifurcations (c.f., [<xref ref-type="bibr" rid="pcbi.1005145.ref027">27</xref>])), and when the value-decay is larger than a critical degree (<italic>ψ</italic> ≈ 0.0559), there exists a unique stable equilibrium with a large contrast between the action values of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’). It is therefore mathematically confirmed that the value-decay causes a large contrast between the steady-state action values of ‘Stay’ (<italic>A</italic><sub>11</sub>) and ‘Go’ (<italic>A</italic><sub>12</sub>) as conjectured in the above. Similar mechanism is considered to underlie the observed contrasts between ‘Stay’ and ‘Go’ values at the other states (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A and 3B</xref>, middle/right).</p>
<p>Notably, the bifurcation diagram (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref>) suggests that there exists bistability when the degree of the value-decay is within a certain range. We conducted a simulation of the original model with the decay rate <italic>φ =</italic> 0.0045, and found that there indeed appears a phenomenon indicative of bistability. Specifically, the value of ‘Stay’ (<italic>A</italic><sub>11</sub>) was shown to fluctuate between two levels in long time scales (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7D</xref>). Such bistability can potentially cause a hysteresis, in a way that learned values depend on the initial condition or the learning history, although the range of the degree of the value-decay for bistability is not large. <xref ref-type="fig" rid="pcbi.1005145.g008">Fig 8</xref> shows the dependence of the bifurcation diagram on the RL parameters. As shown in the figure, the existence and the range of bistability critically depend on the inverse temperature (<italic>β</italic>) (representing the sharpness of soft-max selection) and the time discount factor (<italic>γ</italic>). The figure also indicates, however, that whether bistability exists or not, as the degree of the value-decay increases, there emerges a prominent contrast between ‘Stay’ and ‘Go’ values.</p>
<fig id="pcbi.1005145.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Dependence of the bifurcation diagram of the reduced model on the RL parameters.</title>
<p>The three panels with the gray background are the same as <xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref> (re-presented for comparison), showing the case with the standard RL parameter values: the learning rate <italic>α</italic> = 0.5, the inverse temperature <italic>β</italic> = 5, and the time discount factor <italic>γ</italic> = 1 (i.e., no temporal discounting). <bold>(A)</bold> The learning rate <italic>α</italic> was varied from the standard value 0.5 (middle panel). <bold>(B)</bold> The inverse temperature <italic>β</italic> was varied from the standard value 5 (middle panel). <bold>(C)</bold> The time discount factor <italic>γ</italic> was varied from the standard value 1 (right panel). The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g008" xlink:type="simple"/>
</fig>
<p>Importantly, it is considered that the facilitation of fast goal-reaching by the value-decay in the simulations shown so far is actually caused by the value-contrasts between ‘Stay’ and ‘Go’ rather than the gradient of ‘Go’ values explained before, because value-based choice is made between ‘Stay’ and ‘Go’ rather than between successive ‘Go’ actions. Nevertheless, the decay-induced value-gradient can indeed cause a facilitatory effect if selection of ‘Go’ or ‘Stay’ is based on the state values rather than the action values. Specifically, if our model is modified in the way that the probability of choosing ‘Go’ or ‘Stay’ depends on the value of the current and the next state (while action values are not defined: see the <xref ref-type="sec" rid="sec016">Materials and Methods</xref> for details), introduction of the decay of learned (state) values can still cause facilitation of goal-reaching (<xref ref-type="fig" rid="pcbi.1005145.g009">Fig 9A</xref>). Since the values of ‘Go’ and ‘Stay’ are not defined and thus the "value-contrast" appeared in the original model does not exist, this facilitation is considered to come from the gradient of state values (<xref ref-type="fig" rid="pcbi.1005145.g009">Fig 9B</xref>). Facilitation appears to be in similar levels as the decay rate changes from 0.01 to 0.02 (<xref ref-type="fig" rid="pcbi.1005145.g009">Fig 9A</xref>), and it is considered to be because, while the slope near the start becomes shallower, the slope near the goal becomes steeper (<xref ref-type="fig" rid="pcbi.1005145.g009">Fig 9B</xref>).</p>
<fig id="pcbi.1005145.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Effects of the value-decay in the cases in which action selection is based on the state values.</title>
<p><bold>(A)</bold> Number of time steps needed for goal-reaching averaged over 500 trials (vertical axis) in the cases with various decay rates (i.e., rates of decay of the state values) (horizontal axis). The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2A</xref>. <bold>(B)</bold> Trial-by-trial changes of the state values (top panels) and the state values at the end of the 500th trial (bottom panels) in the case with the decay rate <italic>φ =</italic> 0.01 (left) or 0.02 (right). The color indicates the state value averaged over 20 simulations, in reference to the rightmost color scale bar.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Dependence of the effect of the value-decay on the RL parameters and algorithms</title>
<p>We examined how the effect of the value-decay on fast goal-reaching depends on the RL parameters, specifically, the learning rate, the inverse temperature, and the time discount factor. <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10A</xref> shows the time needed for goal-reaching averaged over 500 trials in conditions varying one of the RL parameters and the decay rate. As shown in the figure panels, although a large inverse temperature (indicating an exploitative choice policy) realizes fast goal-reaching without the value-decay (middle panel of <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10A</xref>), facilitation of fast goal-reaching by introduction of the value-decay occurs within a wide range of RL parameters. Notably, the right panel of <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10A</xref> shows that the value-decay can realize faster goal-reaching than temporal discounting does, given that the other parameters are fixed to the values used here. This is considered to reflect that while both the value-decay and temporal discounting create a value-gradient from the start to the goal, only the value-decay additionally induces value-contrasts between ‘Stay’ and ‘Go’ as we have shown above.</p>
<fig id="pcbi.1005145.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Dependence of the effect of the value-decay on the RL parameters and algorithms.</title>
<p><bold>(A)</bold> Dependence on the RL parameters. The color indicates the number of time steps needed for goal-reaching averaged over 500 trials, further averaged over 20 simulations, in reference to the rightmost color scale bar. The horizontal axis indicates the decay rate (<italic>φ</italic> = 0~0.02), and the vertical axis indicates the RL parameter that was varied: the learning rate <italic>α</italic> (left panel), inverse temperature <italic>β</italic> (middle panel), and time discount factor <italic>γ</italic> (right panel). The asterisks at the right edge of each panel indicate the standard RL parameter values used in the simulations shown in the previous figures unless otherwise described. <bold>(B)</bold> Results of the case where RPE was assumed to be calculated according to the SARSA algorithm rather than the Q-learning algorithm, which was assumed in the simulations/analyses shown so far (note that Q-learning-type RPE calculation was again assumed in the simulations/analyses in Figs <xref ref-type="fig" rid="pcbi.1005145.g011">11</xref>–<xref ref-type="fig" rid="pcbi.1005145.g014">14</xref>). The configurations are the same as those in (A). <bold>(C)</bold> Action values of ‘Go’ (black lines/crosses) and ‘Stay’ (gray lines/crosses) at the end of the 500th trial in the case with SARSA-type RPE. The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3A</xref>. <bold>(D)</bold> Average RPE generated upon taking ‘Stay’ and ‘Go’ in the case assuming SARSA-type (left panel) and Q-learning-type (right panel) RPE.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g010" xlink:type="simple"/>
</fig>
<p>In the results presented so far, we assumed in the model that RPE is calculated according to a major RL algorithm called Q-learning [<xref ref-type="bibr" rid="pcbi.1005145.ref028">28</xref>] (<xref ref-type="disp-formula" rid="pcbi.1005145.e003">Eq (1)</xref> in the Materials and Methods), based on the empirical suggestions that DA neuronal activity in the rat ventral tegmental area (VTA) and DA concentration in the nucleus accumbens represent Q-learning-type RPE [<xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref029">29</xref>]. However, there is in fact also an empirical suggestion that DA neuronal activity represents RPE calculated according to another major RL algorithm called SARSA [<xref ref-type="bibr" rid="pcbi.1005145.ref030">30</xref>] (<xref ref-type="disp-formula" rid="pcbi.1005145.e005">Eq (2)</xref> in the Materials and Methods) rather than Q-learning in the monkey substantia nigra pars compacta (SNc) [<xref ref-type="bibr" rid="pcbi.1005145.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref032">32</xref>]. It remains elusive whether such a difference comes from the differences in the species, regions, task paradigms or other conditions. We examined how the model's behavior changes if SARSA-type RPE is assumed instead of Q-learning type RPE. <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10B</xref> shows the time needed for goal-reaching averaged over 500 trials, with the RL parameters varied as before, and <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10C</xref> shows the learned values of each action at the end of 500 trials. As shown in the figures, it turned out that the effects of the value-decay, as well as the underlying value-gradient and value-contrast, are very similar to the cases with Q-learning type RPE.</p>
<p>There is, however, a prominent difference between the cases of SARSA and Q-learning. Specifically, in the case of SARSA, RPE generated upon taking ‘Go’ was much larger than RPE generated upon taking ‘Stay’ (<xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10D</xref>, left), whereas there was no such difference in the case of Q-learning (<xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10D</xref>, right). The difference in RPE between ‘Go’ and ‘Stay’ in the SARSA case is considered to reflect the value-contrast between the learned values of ‘Go’ and ‘Stay’ (<xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10C</xref>). This is not the case with Q-learning because the Q-learning-type RPE calculation uses the value of the maximum-valued action candidates, which would be ‘Go’ in most cases, regardless of which action is actually selected. The SARSA-type RPE calculation, by contrast, uses the value of actually selected action (compare Eqs (<xref ref-type="disp-formula" rid="pcbi.1005145.e003">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005145.e005">2</xref>) in the Materials and Methods). The difference in RPE between ‘Go’ and ‘Stay’ in the SARSA case could potentially be related to a recent finding [<xref ref-type="bibr" rid="pcbi.1005145.ref033">33</xref>] that DA in the rat nucleus accumbens responded to a reward-predicting cue when movement was initiated but not when animal had to stay. However, our present model would be too simple to accurately represent the task used in that study and the neural circuits that are involved, and elaboration of the model is desired in the future.</p>
</sec>
<sec id="sec008">
<title>Reward-amount-dependences of the effect of the value-decay, subject's speed, and the average RPE</title>
<p>We examined how the facilitatory effect of the value-decay depends on the amount of the reward obtained at the goal, which was fixed at <italic>r</italic> = 1 in the simulations so far presented (we again consider Q-learning-type RPE in the following). <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11A, 11B, 11C and 11D</xref> show the time needed for goal-reaching averaged over 500 trials, with the RL parameters varied as before, in the cases with reward amount 0.5, 0.75, 1.25, and 1.5, respectively. As shown in the figures, the overall tendency of the effect of the value-decay does not largely change across this threefold range of reward amount.</p>
<fig id="pcbi.1005145.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Reward-amount-dependences of the effect of the value-decay, subject's speed, and the average RPE.</title>
<p><bold>(A-D)</bold> The number of time steps needed for goal-reaching averaged over 500 trials for the cases with reward amount 0.5 (A), 0.75 (B), 1.25 (C), and 1.5 (D). The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10A</xref>. <bold>(E,F)</bold> Relationship between the reward amount and the average number of time steps for goal-reaching (E) or the average RPE per time-step (F), in the case with the standard values of RL parameters (i.e., <italic>α</italic> = 0.5, <italic>β</italic> = 5, and <italic>γ</italic> = 1) and the decay rate of <italic>φ =</italic> 0.01 (black symbols) or <italic>φ =</italic> 0 (gray symbols). The lines show the average over 500 trials, and the error bars indicate the mean ± SE of 20 simulations. The circles and the crosses show the average over 1–100 trials and 401–500 trials, respectively. The two dashed lines in (E) indicate the theoretical minimum (bottom) and the chance level (top).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g011" xlink:type="simple"/>
</fig>
<p>Meanwhile, the figures indicate that as the reward amount increases, the time needed for goal-reaching generally decreases, or in other words, the subject's speed increases. The black line in <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E</xref> shows this relationship in the case with the standard RL parameters used so far and the decay rate of 0.01. As shown in this figure, there is a clear negative relationship between the reward amount and the time needed for goal-reaching. We also examined how the average RPE per time-step during 500 trials depends on the reward amount. As shown in the black line in <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11F</xref>, we found that there is a positive relationship between the reward amount and the average RPE. These negative and positive reward-amount-dependences of the time needed for goal-reaching and the average RPE, respectively, are in line with the experimental findings [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>] that the subject's latency and the minute-by-minute DA level in the nucleus accumbens were negatively and positively related with the reward rate, respectively, given that RPE in our model is represented by DA as we assumed.</p>
<p>The commonality of the effect of the value-decay across the range of reward amount (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11A–11D</xref>) and the positive reward-amount-dependence of the average RPE (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11F</xref>, black line) are considered to appear because our model is largely scalable to (i.e., variables are scaled in proportion to) the changes in the reward amount except for the effect of the inverse temperature. The negative reward-amount-dependence of the time needed for goal-reaching (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E</xref>, black line) is considered to appear because as the reward amount increases, the overall magnitudes of learned values, and thereby also the value-contrasts between ‘Stay’ and ‘Go’, increase.</p>
<p>The gray lines in <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E and 11F</xref> show the relationship between the reward amount and the time needed for goal-reaching (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E</xref>) or the RPE per time-step (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11F</xref>) in the case without the value-decay, averaged over 500 trials. The gray circles and crosses in these figures show the averages for 1–100 trials and 401–500 trials, respectively. As shown in these, in the case without the value-decay, there are negative and positive reward-amount-dependences of the time needed for goal-reaching and the RPE per time-step in the initial phase, but such dependences gradually degrade along with trials. This is considered to be because the values of ‘Stay’ actions gradually increase toward the saturation (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3B</xref>, left). In contrast, in the case with the value-decay (<italic>φ =</italic> 0.01), there are little differences in the time needed for goal-reaching and the RPE per time-step between 1–100 trials (black circles in <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E and 11F</xref>) and 401–500 trials (black crosses in <xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E and 11F</xref>). This is reasonable given that gradual saturation of ‘Stay’ values does not occur in the case with the value-decay (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3B</xref>, middle).</p>
</sec>
<sec id="sec009">
<title>Additional analyses (1): Dependence on the model architectures, and robustness to perturbations in reward environments</title>
<p>We further examined how the facilitatory effect of the value-decay depends on the architectures of the model, in particular, the number of states and the number of action candidates. Regarding the number of states, in the results so far shown, we assumed seven states, including the start and the goal, as shown in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>. <xref ref-type="fig" rid="pcbi.1005145.g012">Fig 12A and 12B</xref> show the time needed for goal-reaching averaged over 500 trials in the cases with four or ten states, respectively. As shown in the figures, although the optimal decay rate that realizes fastest goal-reaching varies depending on the number of states, facilitation of fast goal-reaching by introduction of the value-decay can occur in either case.</p>
<fig id="pcbi.1005145.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Dependence of the effect of the value-decay on the model architectures.</title>
<p><bold>(A,B)</bold> Results for the models with 4 (A) or 10 (B) states, including the start and the goal. <bold>(C)</bold> Results for the model (with 7 states) that incorporated ‘Back’ action, in addition to ‘Go’ and ‘Stay’, at each state except for the start and the goal. The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10A</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g012" xlink:type="simple"/>
</fig>
<p>Regarding the number of the action candidates, we have so far assumed that either of the two actions, ‘Go’ or ‘Stay’, can be taken at each state except for the goal (or the T-junction in the case of the T-maze). This can be a good model of certain types of self-paced tasks that are intrinsically unidirectional, such as pressing a lever for a fixed amount of times to get reward. However, there are also self-paced tasks that are more like bidirectional, for instance, movements in an elongated space with reward given at one of the ends. Such tasks might be better represented by adding ‘Back’ action to the action candidates at each state except for the start and the goal. <xref ref-type="fig" rid="pcbi.1005145.g012">Fig 12C</xref> shows the time needed for goal-reaching averaged over 500 trials in the case where the ‘Back’ action was added. As shown in this figure, while the time needed for goal-reaching is generally larger than the cases without the ‘Back’ action as naturally expected, the value-decay can facilitate fast goal-reaching in this case too.</p>
<p>It is also a question of how robust the effect of the value-decay is to perturbations in reward environments. In particular, given that the values of unchosen actions just decay, it is conceivable that, if small reward is given at a state between the start and the goal (e.g., <italic>S</italic><sub>4</sub>: <xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13A</xref>) whenever subject is located there (i.e., repeatedly at every time step if subject stays at <italic>S</italic><sub>4</sub>), subject might learn to stay there persistently rather than to reach the goal. Denoting the size of the small reward by <italic>x</italic> (&lt; 1, which is the amount of the reward given at the goal), if 7<italic>x</italic> &lt; <italic>x</italic> + 1 ⇔ <italic>x</italic> &lt; 0.166…, such a persistent stay is however inferior to the fastest repetition of goal-reaching in terms of the average reward obtained per time-step. We examined the behavior of modeled subject when small reward is given at <italic>S</italic><sub>4</sub> with its size <italic>x</italic> varied from 0 to 0.1, in the case with the value-decay (<italic>φ =</italic> 0.01). <xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13B</xref> shows the resulting percentage of simulation runs (out of total 20 runs for each condition) in which subject completed 500 trials within 35000 time steps (i.e., within 70 time steps per trial on average) without settling at <italic>S</italic><sub>4</sub>. As shown in the figure, the percentage for the completion of 500 trials is 100% when the size of the reward at <italic>S</italic><sub>4</sub> is ≤ 0.04, whereas the percentage then decreases as the size of the reward at <italic>S</italic><sub>4</sub> further increases. This indicates that a persistent stay at <italic>S</italic><sub>4</sub> actually occurs even if it is not advantageous: <xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13C and 13D</xref> show such an example. <xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13E</xref> shows the number of time steps needed for goal-reaching averaged over 500 trials, only for the simulation runs completing 500 trials in the cases where the completion rate is less than 100%. As shown in the figure, the speed of goal-reaching is kept fast, comparable to the case without reward at <italic>S</italic><sub>4</sub> (i.e., <italic>x</italic> = 0). These results indicate that the facilitatory effect of the value-decay on fast goal-reaching has a certain degree of tolerance to this kind of perturbation in reward environments, although it eventually fails as the perturbation becomes larger.</p>
<fig id="pcbi.1005145.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Robustness of the effect of the value-decay to perturbations in reward environments.</title>
<p><bold>(A)</bold> Simulated perturbation: small reward of size <italic>x</italic> (&lt; 1, which is the amount of the reward given at the goal) is given at <italic>S</italic><sub>4</sub> whenever subject is located there (i.e., repeatedly at every time step if subject stays at <italic>S</italic><sub>4</sub>). <bold>(B)</bold> Percentage of simulation runs (out of total 20 runs for each condition) in which subject completed 500 trials within 35000 time steps (i.e., within 70 time steps per trial on average) without settling at <italic>S</italic><sub>4</sub>. <bold>(C)</bold> Time evolution of the action values in a simulation run with <italic>x</italic> = 0.1 in which subject settled at <italic>S</italic><sub>4</sub> before completing 500 trials. The color indicates the action value in reference to the rightmost color scale bar: note that the color is saturated for values ≥ 1. The vertical axis indicates the time steps (from top to bottom) and the horizontal axis indicates the indices of the actions (odd/gray: ‘Stay’, even/black: ‘Go’: Fig 13A). At around time-step 650, the value of <italic>A</italic><sub>7</sub> (i.e., ‘Stay’ at <italic>S</italic><sub>4</sub>) became very large while the values of the other actions decayed out, indicating that subject settled at <italic>S</italic><sub>4</sub>. <bold>(D)</bold> The subject's state transitions in the simulation run shown in (C) around time-step 650, showing that the subject indeed settled at <italic>S</italic><sub>4</sub> around this time. <bold>(E)</bold> Number of time steps needed for goal-reaching averaged over 500 trials. The solid line with error bars indicates the mean ± SE for the simulation runs in which 500 trials were completed. The two dashed lines indicate the theoretical minimum (bottom) and the chance level (top). <bold>(F)</bold> Simulation results with <italic>x</italic> = 0.1 for the cases with both the value-decay (horizontal axis) and temporal discounting (vertical axis). The color indicates the number of time steps needed for goal-reaching averaged over 500 trials, further averaged over 20 simulations, in reference to the rightmost color scale bar: the gray zone at the top (<italic>γ</italic> = 1) indicates that, in these conditions (i.e., without temporal discounting), subject did not complete 500 trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g013" xlink:type="simple"/>
</fig>
<p>Nonetheless, when temporal discounting (<italic>γ</italic> = 0.9, 0.8, …) was also assumed in the model with the small reward <italic>x</italic> = 0.1 at <italic>S</italic><sub>4</sub>, persistent stay at <italic>S</italic><sub>4</sub> before completing 500 trials was not observed in 20 simulation runs for each of the tested decay rates, and the value-decay could have facilitatory effects (<xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13F</xref>). The absence of persistent stay at <italic>S</italic><sub>4</sub> is considered to be because the value of ‘Stay’ at <italic>S</italic><sub>4</sub> is bounded due to temporal discounting. For example, in the case with <italic>γ</italic> = 0.9 and no value-decay, if the subject keeps staying at <italic>S</italic><sub>4</sub>, the value of ‘Stay’ at <italic>S</italic><sub>4</sub> converges to 1 (solution of the equation of <italic>V</italic>: 0 = 0.1 + 0.9<italic>V</italic> − <italic>V</italic>). This is still larger than the convergence value of ‘Go’ at <italic>S</italic><sub>4</sub>, which is 0.9<sup>2</sup> = 0.81. However, since the growth of the ‘Stay’ value from the initial value 0 is likely to be slower than the growth of the ‘Go’ value, subject would rarely begin to settle at <italic>S</italic><sub>4</sub>. In contrast, in the case with no temporal discounting and no value-decay, if the subject keeps staying at <italic>S</italic><sub>4</sub>, the value of ‘Stay’ at <italic>S</italic><sub>4</sub> increases unboundedly, leading to a persistent stay. Actually, the value-decay also bounds the value of ‘Stay’ at <italic>S</italic><sub>4</sub>, but its effect is weak when the decay rate is small as we have so far assumed. For example, in the case with no temporal discounting, <italic>φ =</italic> 0.01, and the learning rate <italic>α</italic> = 0.5, if the subject keeps staying at <italic>S</italic><sub>4</sub>, the value of ‘Stay’ at <italic>S</italic><sub>4</sub> converges to 4.95 (solution of the equation of <italic>V</italic>: <italic>V</italic> = (1 − 0.01)(<italic>V</italic> + 0.5×0.1)), which is fairly large. In this way, temporal discounting effectively prevents the subject from settling at <italic>S</italic><sub>4</sub>. The value-decay can then facilitate fast goal-reaching by creating the value-contrast between ‘Go’ and ‘Stay’.</p>
</sec>
<sec id="sec010">
<title>Additional analyses (2): Elaboration of the model towards accurate reproduction of behavioral profiles</title>
<p>So far we have assumed that subject exists in one of the discrete set of states, and selects either ‘Go’ or ‘Stay’, moving to the next state or staying at the same state. Given this simple structure, our model can potentially represent a variety of self-paced behavior, from spatial movement to more abstract Go/No-Go decision sequences. At the same time, however, our model is likely to be too simple to accurately model any specific behavior. In particular, in the case of spatial movement, subject does not really exist only in one of a small number of locations, and would not abruptly stop or literally ‘stay’ at a particular location. Meanwhile, subject should stop or slow down in the face of a physical constraint (e.g., the start, the junction, or the end of a maze) or a salient event (e.g., reward) as observed in experiments [<xref ref-type="bibr" rid="pcbi.1005145.ref006">6</xref>]. An emerging question is whether our model can be extended to reproduce these observations while preserving its main features.</p>
<p>In order to examine this, we developed an elaborated model of self-paced spatial movement in the T-maze. In this model, the exact one-to-one correspondence between the subject's physical location and the internal state assumed in the original model was changed into a loose coupling, in which each state corresponds to a range of physical locations (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14A</xref>). Also, ‘Stay’ action in the original model was replaced with ‘Slow’ action unless there is a physical constraint (i.e., the start, the T-junction, or the end). By selecting ‘Slow’, subject moves straightforward for a time step with the "velocity" halved from the previous time step (or further decreased when there is a physical constraint). ‘Slow’ was introduced to eliminate the abrupt/complete stop appeared in the original model, and mechanistically, it can represent inertia in decision and/or motor processes [<xref ref-type="bibr" rid="pcbi.1005145.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref035">35</xref>]. With these modifications, state transitions can sometimes occur even when subject chooses ‘Slow’ rather than ‘Go’ (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14A</xref>, Case 2), different from the original model. At the T-junction, subject was assumed to take ‘Go’ to either of the two arms or ‘Stay’ in the same manner as in the original model. At the reward location, subject was assumed to take the consummatory action for a time step (indicated by the double-lined arrows in <xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14B and 14F</xref>), and proceed to the end state.</p>
<fig id="pcbi.1005145.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005145.g014</object-id>
<label>Fig 14</label>
<caption>
<title>Simulations of the cost-benefit decision making task in a T-maze by an elaborated model, aiming at reproducing the velocity profiles observed in a (different) T-maze task.</title>
<p><bold>(A)</bold> Schematic explanation of the elaborated model. The one-to-one correspondence between the subject's physical location and the internal state assumed in the original model was changed into a loose coupling, in which each state corresponds to a range of physical locations as illustrated. At each time step, subject at a given location chooses either ‘Go’ or ‘Slow’, except that the subject is at the start, the T-junction, or the reward location (in the ends of the T-maze). By selecting ‘Go’, subject moves straightforward for a time step with the "velocity" 1, meaning that the subject's physical location is displaced by 1, unless there is a physical constraint. By selecting ‘Slow’, subject moves straightforward for a time step with the "velocity" halved, meaning that the subject's physical location is displaced by the half of the displacement during the previous time interval, unless there is a physical constraint. At the start (State 1), subject was assumed to take ‘Go’ or ‘Stay’, and at the T-junction (State 4), subject was assumed to take ‘Go’ to either of the two arms or ‘Stay’, in the same manners as in the original model. (Case 1) shows the case where subject at the start point chooses ‘Go’ three times in succession, whereas (Case 2) shows the case where subject chooses ‘Go’, ‘Slow’, ‘Slow’, ‘Go’, and ‘Slow’. Notably, in Case 2, subject transitions from State 3 to State 4 by choosing ‘Slow’ rather than ‘Go’. <bold>(B)</bold> Schematic diagram of the task condition where there are high-cost-high-return and low-cost-low-return options. When reaching reward, subject is assumed to take the consummatory action (indicated by double-lined arrows). <bold>(C,D)</bold> Ratio of choosing the large-reward arm (Arm 1) (C) and the average number of time-steps towards the T-junction (State 4) (D). DA depletion (to the quarter of the original) after 500 trials was simulated as before. The configurations are the same as those in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5B and 5C</xref>. <bold>(E)</bold> Average "velocity", i.e., displacement from the previous time step, when subject reached or stayed in each state (horizontal axis). The black and red solid lines indicate the "velocity" averaged across all the cases in 251–500 trials (before DA depletion) and 751–1000 trials (after DA depletion), respectively. The gray and magenta lines indicate the "velocity" averaged across the cases where subject stayed in the state at the previous and current time steps: notably, because of the decoupling of the physical location and the internal state, subject can still move. The error bars indicate the mean ± SE of 20 simulations (black and red) or of simulations (out of the total 20) which had the corresponding data (gray and magenta). <bold>(F-I)</bold> Same as (B-E) for the different task condition where there are low-cost-high-return and low-cost-low-return options.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005145.g014" xlink:type="simple"/>
</fig>
<p>Using this elaborated model (see the <xref ref-type="sec" rid="sec016">Materials and Methods</xref> for details), we simulated the T-maze cost-benefit decision making task with DA depletion [<xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>] that was simulated by the original model before (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5</xref>). <xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14C and 14D</xref> show the simulation results about the ratio of choosing the large-reward arm (Arm 1) and the average time needed for reaching the T-junction in the task conditions with high cost in the large-reward arm (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14B</xref>), respectively. <xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14G and H</xref> show the results in the task conditions without high cost in the large-reward arm (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14F</xref>). As shown in the figures, the experimentally observed effects of DA depletion, i.e., the severe impairment of high-cost-high-return choice but not low-cost-high-return choice (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14C and 14G</xref>) and the slowdown in both conditions (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14D and 14H</xref>), can be reproduced by the elaborated model, as well as by the original model (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5</xref>). Simultaneously, the elaborated model can also reproduce the velocity profiles observed in a (different) T-maze task [<xref ref-type="bibr" rid="pcbi.1005145.ref006">6</xref>], specifically, the slowdown and stop at the T-junction and the end of the maze and the absence of complete stop in the other locations (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14E and 14I</xref>). This exemplifies the potential of our original model to be extended to accurately represent specific self-paced behavior.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>We have shown that the value-decay in RL can realize sustained fast goal-reaching in a situation requiring self-paced approach towards a goal, modeled as a series of ‘Go’ or ‘No-Go’ (or ‘Stay’) selections. The underlying potential mechanisms turned out to be twofold: (1) a value-gradient towards the goal is shaped by value-decay-induced sustained positive RPE, and (2) value-contrasts between ‘Go’ and ‘Stay’ are generated because chosen values are continually updated whereas unchosen values simply decay. We have then shown that our model with the value-decay can provide potential mechanistic explanations for the key experimental findings that suggest the DA's roles in motivation, under the parsimonious assumption that the representation of RPE is the sole reward-related role of DA. Specifically, our model explains the (i) slowdown of self-paced behavior by post-training blockade of DA signaling [<xref ref-type="bibr" rid="pcbi.1005145.ref014">14</xref>] (<xref ref-type="fig" rid="pcbi.1005145.g002">Fig 2C</xref>), (ii) severe impairment of effortful actions to obtain rewards, but not of seeking of easily obtainable rewards, by DA blockade [<xref ref-type="bibr" rid="pcbi.1005145.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>] (Figs <xref ref-type="fig" rid="pcbi.1005145.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1005145.g014">14</xref>), and (iii) relationships between the reward amount, the level of motivation reflected in the speed of behavior, and the average level of DA [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>] (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11E and 11F</xref>). Simultaneously, our model also explains the various temporal patterns of DA signals (<xref ref-type="fig" rid="pcbi.1005145.g003">Fig 3C</xref>), confirming and extending the suggestion previously made by the non-self-paced model [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>]. Moreover, the simulation results of the SARSA-version of our model could also potentially account for the recent finding [<xref ref-type="bibr" rid="pcbi.1005145.ref033">33</xref>] that DA ramping occurred when movement was initiated but not when animal had to stay (<xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10D</xref>).</p>
<sec id="sec012">
<title>Dopamine, RPE, and motivation</title>
<p>The notion that DA represents RPE has been supported by electrophysiological [<xref ref-type="bibr" rid="pcbi.1005145.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref004">4</xref>], FSCV [<xref ref-type="bibr" rid="pcbi.1005145.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref036">36</xref>] and neuroimaging [<xref ref-type="bibr" rid="pcbi.1005145.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref039">39</xref>] results. Recently, optogenetic manipulations of DA neurons causally demonstrated the DA's role in representing RPE [<xref ref-type="bibr" rid="pcbi.1005145.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref041">41</xref>]. On the other hand, pharmacological blockade of DA signaling has been shown to cause motivational impairments such as slowdown of behavior [<xref ref-type="bibr" rid="pcbi.1005145.ref014">14</xref>]. Crucially, such effects have been observed even when DA signaling was blocked after animals were well trained and RPE-based learning had presumably already been completed. These motivational effects have thus been difficult to explain by the notion that DA represents RPE, unless different function of DA was also assumed [<xref ref-type="bibr" rid="pcbi.1005145.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref043">43</xref>].</p>
<p>Given such situations, Niv and colleagues [<xref ref-type="bibr" rid="pcbi.1005145.ref015">15</xref>] proposed a hypothesis that while DA's phasic response encodes RPE, DA's tonic concentration represents the average reward rate per unit time. They argue that as the reward rate decreases, optimal action speed should also decrease because the opportunity cost for not acting becomes relatively smaller than the extra cost for quickly acting, explaining why DA blockade causes slowdown. Extending this hypothesis, Lloyd and Dayan [<xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>] proposed that quasi-tonic DA represents the expected amount of time discount of the value of next state caused by postponing action to get to the next state. This can explain the experimentally observed ramping DA signals [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>] as reflecting a gradient of state values created by temporal discounting (as in our <xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6A and 6B</xref>), also consistent with the arguments by [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>]. These normative hypotheses, at the Marr's levels of computation and algorithm [<xref ref-type="bibr" rid="pcbi.1005145.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref045">45</xref>], provide intriguing predictions that are desired to be experimentally tested. Meanwhile, it is also important to explore the Marr's level of implementation, namely, circuit/synaptic operations, which could potentially provide inspirations for the upper levels and <italic>vice versa</italic> [<xref ref-type="bibr" rid="pcbi.1005145.ref045">45</xref>]. The abovementioned normative hypotheses highlight essential issues at the circuit/synaptic level, including how the sustained DA signals are generated in the upstream and utilized in the downstream, how the selection of action timing is implemented, and how temporal discounting is implemented.</p>
<p>In our model, sustained DA signals are assumed to represent RPE, and thus the upstream and downstream mechanisms of sustained DA signaling should be nothing more than the mechanisms of how RPE is calculated in the upstream of DA neurons and how RPE-dependent value-update occurs through DA-dependent synaptic plasticity. Both of these mechanisms for RPE have been extensively explored (e.g., [<xref ref-type="bibr" rid="pcbi.1005145.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref047">47</xref>]) and have now become clarified [<xref ref-type="bibr" rid="pcbi.1005145.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref020">20</xref>]. Regarding the selection of action timing, we assumed that it consists of a series of selections of two actions, ‘Go’ and ‘Stay’. We could thus assume general mechanisms of action selection, for which implementation has been explored [<xref ref-type="bibr" rid="pcbi.1005145.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref052">52</xref>] with empirical supports [<xref ref-type="bibr" rid="pcbi.1005145.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref054">54</xref>], although this leaves an important issue regarding how time is represented. As for the implementation of temporal discounting, we will discuss it below, in relation to the value-decay that can be implemented as decay of the plastic changes of the synaptic strengths.</p>
<p>There exists a different model that has also tried to give a bottom-up unified explanation of both the learning and motivation roles of DA, referring to circuit architectures of the basal ganglia [<xref ref-type="bibr" rid="pcbi.1005145.ref055">55</xref>]. However, although this model captures a wide range of phenomena, there are several potential issues or limitations. Firstly, this model assumes that phasic DA represents a simple form of RPE, called the Rescorla-Wagner prediction error [<xref ref-type="bibr" rid="pcbi.1005145.ref056">56</xref>], which lacks the upcoming-value term. However, RL models of the DA system, including our present model, widely assume the more complex form of RPE called the temporal difference (TD) RPE or TD error [<xref ref-type="bibr" rid="pcbi.1005145.ref025">25</xref>] (see [<xref ref-type="bibr" rid="pcbi.1005145.ref057">57</xref>] for detailed explanation) because there is a wealth of empirical supports that DA signals represent TD-RPE [<xref ref-type="bibr" rid="pcbi.1005145.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref058">58</xref>]. Secondly, because this model assumes the Rescorla-Wagner, rather than TD-, RPE, this model cannot describe the learning of the values of a series of actions or states, nor the changes of RPE, within a trial. As a corollary to this, this model does not explain the experimentally observed sustained DA signals [<xref ref-type="bibr" rid="pcbi.1005145.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref022">22</xref>]. Lastly, this model assumes that the two major basal ganglia pathways, the direct and indirect pathways, are associated with positive and negative reinforcement, respectively. Although this assumption is based on several lines of empirical results, alternative possibilities [<xref ref-type="bibr" rid="pcbi.1005145.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref060">60</xref>] have also been proposed for the operations of these pathways.</p>
</sec>
<sec id="sec013">
<title>Decay/forgetting of learned values in reinforcement learning</title>
<p>Decay, or forgetting, is apparently wasteful. However, recent work [<xref ref-type="bibr" rid="pcbi.1005145.ref061">61</xref>] has suggested that decay/forgetting is in fact necessary to maximize future rewards in dynamic environments. Even in a static environment, potential benefit of decay/forgetting has been pointed out [<xref ref-type="bibr" rid="pcbi.1005145.ref062">62</xref>]. There is also a study [<xref ref-type="bibr" rid="pcbi.1005145.ref063">63</xref>] that considered decay to explain features of extinction. Forgetting for capturing extinction effects was also assumed in the model that we have discussed right above [<xref ref-type="bibr" rid="pcbi.1005145.ref055">55</xref>]. However, the authors clearly mentioned that they "assumed some forgetting" "to capture overall extinction effects" and "none of the results are qualitatively dependent on" the parameter for forgetting. Therefore, their work should not have anything to do with the effects of forgetting explored in our present work. Along with these theoretical/modeling works, it has been suggested that RL models with decay could fit the experimental data of human [<xref ref-type="bibr" rid="pcbi.1005145.ref064">64</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref066">66</xref>], monkey [<xref ref-type="bibr" rid="pcbi.1005145.ref067">67</xref>], and rat [<xref ref-type="bibr" rid="pcbi.1005145.ref068">68</xref>] choice behavior potentially better than models without decay. Moreover, existence and benefits of decay/forgetting have also been suggested in other types of learning [<xref ref-type="bibr" rid="pcbi.1005145.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref070">70</xref>].</p>
<p>Nonetheless, decay of learned values (value-decay) is not usually considered in RL model-based accounts of the functions of DA and cortico-basal ganglia circuits. RL models typically have the time discount factor and the inverse temperature (representing choice sharpness) as major parameters [<xref ref-type="bibr" rid="pcbi.1005145.ref025">25</xref>]. Temporal discounting generates a value-gradient (<xref ref-type="fig" rid="pcbi.1005145.g006">Fig 6A and 6B</xref>) [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>], and is suggested [<xref ref-type="bibr" rid="pcbi.1005145.ref071">71</xref>] to ensure that maximizing rewards simultaneously minimizes deviations from physiologically desirable states. Gradually increasing the inverse temperature, i.e., choice sharpness, is known to be good for global optimization [<xref ref-type="bibr" rid="pcbi.1005145.ref072">72</xref>]. Possible neural implementation of these parameters have been explored [<xref ref-type="bibr" rid="pcbi.1005145.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref073">73</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref075">75</xref>]. However, it is not sure whether these parameters are actually biologically implemented in their original forms. We have shown that the value-decay can generate a value-gradient, and also value-contrasts which lead to a sharp choice of ‘Go’. Choice-sharpening effect of decay is implied also in previous studies [<xref ref-type="bibr" rid="pcbi.1005145.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref066">66</xref>]. These indicate a possibility that the value-decay, or its presumed biological substrate, synaptic decay, might in effect partially implement the parameters for temporal discounting and inverse temperature. In this sense, the suggestions that sustained DA represents/reflects time-discounted state values [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>] and our value-decay-based account are not necessarily mutually exclusive. Apart from temporal discounting and the inverse temperature, there is an additional note. There have been suggestions [<xref ref-type="bibr" rid="pcbi.1005145.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref035">35</xref>] that animal's and human's decision making can be affected by the subject's own choice history, which is not included in standard RL models. The value-decay assumed in our model is expected to cause a dependency of decision making on choice history. Whether it can (partly) explain experimentally observed choice patterns would be an interesting issue to explore.</p>
</sec>
<sec id="sec014">
<title>Limitations and testable predictions</title>
<p>If the rate of the value-decay is always constant, after subject interrupts performing the task for a long period, learned values eventually diminish almost completely. Therefore, in order for our model to be valid, some sort of context-dependence of the value-decay needs to be assumed. There are several empirical implications. At the synaptic level, conditional synaptic decay depending on NMDA receptor-channels [<xref ref-type="bibr" rid="pcbi.1005145.ref076">76</xref>] or DA (in drosophila) [<xref ref-type="bibr" rid="pcbi.1005145.ref077">77</xref>] has been found. Behaviorally, memory decay was found to be highly context-dependent in motor learning [<xref ref-type="bibr" rid="pcbi.1005145.ref078">78</xref>]. More generally, it is widely observed that reactivation of consolidated memories makes them transiently labile [<xref ref-type="bibr" rid="pcbi.1005145.ref079">79</xref>]. With these in mind, we assume that the value-decay occurs when and only when subject is actively engaged in the relevant task/behavior. However, this issue awaits future verification.</p>
<p>There is also an important limitation of our present model regarding the explanatory power for the experimental observations. Specifically, as mentioned before, our model explains the increase in the latency caused by DA depletion in the cost-benefit decision making task in a T-maze [<xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>], but does not explain the subsequent recovery of the latency. This recovery could possibly be explained if some slow compensatory mechanisms are additionally assumed in the model. It is important in future work to elaborate the model to account for this issue, as well as a diverse array of experimental observations on the DA's roles in motivation that are not dealt with in the present work.</p>
<p>There are also many open issues in the model, both the functional ones and the structural ones. The functional issues include how the states and the time are represented [<xref ref-type="bibr" rid="pcbi.1005145.ref080">80</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref081">81</xref>] and how ‘Go’ and ‘Stay’ (or ‘No-Go’ or ‘Slow’) are represented. As for the latter, while ‘Go’ and ‘Stay’ might be represented as two distinct actions, ‘Stay’ could instead be represented as disengagement of working-memory/attention as proposed in a recent work [<xref ref-type="bibr" rid="pcbi.1005145.ref082">82</xref>]. The structural issues include, among others, how different parts of the cortico-basal ganglia circuits and different subpopulations of DA neurons cooperate or divide labor [<xref ref-type="bibr" rid="pcbi.1005145.ref083">83</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref090">90</xref>]. Regarding this, a recent study [<xref ref-type="bibr" rid="pcbi.1005145.ref091">91</xref>] has shown that DA axons conveying motor signals are largely different from those conveying reward signals and that the motor and reward signals are dominant in the dorsal and ventral striatum, respectively. DA in our model is assumed to represent RPE, and it should thus be released from the axons conveying reward signals that are dense in the ventral striatum. Even with this specification, the structure of our model is still quite simple, and exploring whether and to what extent the present results can be extended to models with rich dynamics at the levels of circuits (in the cortex [<xref ref-type="bibr" rid="pcbi.1005145.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref092">92</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref096">96</xref>], the striatum [<xref ref-type="bibr" rid="pcbi.1005145.ref097">97</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref103">103</xref>], the DAergic nuclei [<xref ref-type="bibr" rid="pcbi.1005145.ref104">104</xref>], and the entire cortico-basal ganglia system [<xref ref-type="bibr" rid="pcbi.1005145.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref105">105</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref114">114</xref>]), neurons [<xref ref-type="bibr" rid="pcbi.1005145.ref115">115</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref116">116</xref>], and synapses [<xref ref-type="bibr" rid="pcbi.1005145.ref117">117</xref>–<xref ref-type="bibr" rid="pcbi.1005145.ref120">120</xref>] would be important future work.</p>
<p>Our model provides predictions that can be tested by various methods. First, if sustained DA signals indeed represent value-decay-induced sustained RPE, rather than being caused by other reasons [<xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref121">121</xref>], the rate of the value-decay estimated from fitting of measured DA signals by our model should match the decay-rate estimated behaviorally. Behavioral estimation of decay-rate would be possible by preparing two choice options that are initially indifferent, manipulating the frequencies of their presentations, and then examining whether, and to what degree, less-frequently-presented option will be chosen less frequently. On the other hand, if sustained DA signals represent time-discounted state values [<xref ref-type="bibr" rid="pcbi.1005145.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref016">16</xref>], time discount factor estimated from model-fitting of measured DA signals is expected to match behavioral estimation, e.g., from intertemporal choices. Note, however, that the value-decay and temporal discounting might not be completely distinct entities; the value-decay could be a partial implementation of temporal discounting (and the inverse temperature) as we discussed before.</p>
<p>Second, our model predicts that the strengths of cortico-striatal synapses are subject to decay in a context-dependent manner. This could be tested by measuring structural plasticity [<xref ref-type="bibr" rid="pcbi.1005145.ref018">18</xref>] during learning tasks (across several sessions and intervals). Our model further predicts that manipulations of synaptic decay affect DA dynamics and behavior in specific ways. It has been indicated that a protein kinase that is constitutively active, protein kinase M<italic>ζ</italic> (PKM<italic>ζ</italic>), is necessary for maintaining various kinds of memories, including drug reward memory in the nucleus accumbens [<xref ref-type="bibr" rid="pcbi.1005145.ref122">122</xref>]. Specifically, inhibition of PKM<italic>ζ</italic> in the nucleus accumbens core by injecting a selective peptide inhibitor has been shown to impair long-term drug reward memory [<xref ref-type="bibr" rid="pcbi.1005145.ref122">122</xref>]. It has also been shown that overexpression of PKM<italic>ζ</italic> in the neocortex enhances long-term memory [<xref ref-type="bibr" rid="pcbi.1005145.ref123">123</xref>]. We predict that overexpression of PKM<italic>ζ</italic> in the nucleus accumbens (ventral striatum) enhances reward memory, or in other words, reduces the value-decay, and thereby diminishes sustained DA signals and impairs goal-approach through the mechanisms described in the present work. Apart from PKM<italic>ζ</italic>, it has also been indicated that DA is required for transforming the early phase of long-term potentiation (LTP), which generally declines, into the late phase of LTP in the hippocampus [<xref ref-type="bibr" rid="pcbi.1005145.ref124">124</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref125">125</xref>]. Similar DAergic regulation of the stability of LTP could potentially exist in the striatum that is the target of the present work, and if so, the decay rate could be manipulated by DA receptor agonists or antagonists. In the striatal synapses, however, DA signaling would be required for the induction of potentiation before its maintenance, as we have actually assumed in our model. Therefore, it would be necessary to explore ways to specifically manipulate maintenance (decay rate) of potentiation.</p>
</sec>
<sec id="sec015">
<title>Concluding remarks</title>
<p>The results of the present study suggest that when biological systems for value-learning are active (i.e., when subject is actively engaged in the relevant task/behavior) even though learning has apparently converged, the systems might be in a state of dynamic, rather than static, equilibrium where decay and update are balanced. As we have shown, such dynamic operation can potentially facilitate self-paced goal-reaching behavior, and this effect could be seen as a simple biologically plausible, though partial, implementation of temporal discounting and simulated annealing. It is also tempting to speculate that value-decay-induced sustained RPE might be subjectively felt as sustained motivation, considering recently suggested relationship between RPE and subjective happiness [<xref ref-type="bibr" rid="pcbi.1005145.ref126">126</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref127">127</xref>]. This is in accordance with the suggestion that DA signals subjective reward value [<xref ref-type="bibr" rid="pcbi.1005145.ref128">128</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref129">129</xref>], or more precisely, "utility prediction error" [<xref ref-type="bibr" rid="pcbi.1005145.ref130">130</xref>]. Despite that dynamic operation has these potential advantages, however, there can also be disadvantages. Specifically, continual decay and update of values must be costly, especially given that DA signaling is highly energy-consuming [<xref ref-type="bibr" rid="pcbi.1005145.ref131">131</xref>]. This could potentially be related to neuropsychiatric and neurological disorders, in particular, Parkinson's disease [<xref ref-type="bibr" rid="pcbi.1005145.ref131">131</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref132">132</xref>], which is characterized by motor and motivational impairments that are suggested to be independently associated with DA [<xref ref-type="bibr" rid="pcbi.1005145.ref133">133</xref>]. Better understanding of the dynamic nature of biological value-learning systems will hopefully contribute to clinical strategies against these diseases.</p>
</sec>
</sec>
<sec id="sec016" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec017">
<title>Modeling self-paced operant task by reinforcement learning with value-decay/forgetting</title>
<p>We posited that behavioral task requiring self-paced voluntary approach (whether spatially or not) towards a goal can be represented as a series of ‘Go’ or ‘Stay’ (‘No-Go’) selections as illustrated in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>. Discrete states (<italic>S</italic><sub>1</sub> ~ <italic>S</italic><sub>7</sub>) and time steps were assumed. In each trial, subject starts from <italic>S</italic><sub>1</sub>. At each time step, subject can take one of two actions, specifically, ‘Go’: moving to the next state or ‘Stay’: staying at the same state. Subject was assumed to learn the value of each action (‘Go’ or ‘Stay’) by a temporal-difference (TD) reinforcement learning (RL) algorithm incorporating the decay of learned values (referred to as the ‘value-decay’ below) [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>], and select an action based on their learned values in a soft-max manner [<xref ref-type="bibr" rid="pcbi.1005145.ref134">134</xref>].</p>
<p>Specifically, at each time step (<italic>t</italic>), TD reward prediction error (RPE) <italic>δ</italic>(<italic>t</italic>) was assumed to be calculated according to the algorithm called Q-learning [<xref ref-type="bibr" rid="pcbi.1005145.ref028">28</xref>], which has been suggested to be implemented in the cortico-basal ganglia circuit [<xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref059">59</xref>], as follows:
<disp-formula id="pcbi.1005145.e003">
<alternatives>
<graphic id="pcbi.1005145.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>S</italic>(<italic>t</italic>) represents the state where subject exists at time step <italic>t</italic>. <italic>R</italic>(<italic>S</italic>(<italic>t</italic>)) represents reward obtained at <italic>S</italic>(<italic>t</italic>), which is <italic>r</italic> (&gt; 0) when <italic>S</italic>(<italic>t</italic>) = <italic>S</italic><sub>7</sub> (goal) and 0 at the other states, unless otherwise described. "<italic>Q</italic>(<italic>A</italic>)" generally represents the learned value of action <italic>A</italic>. <italic>A</italic><sub><italic>cand</italic></sub>(<italic>t</italic>) represents the candidate of action that can be taken at time step <italic>t</italic>: when <italic>S</italic>(<italic>t</italic>) = <italic>S</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, 2, …, 6), <italic>A</italic><sub><italic>cand</italic></sub>(<italic>t</italic>) = <italic>A</italic><sub>2<italic>i</italic>−1</sub>(‘Stay’) or <italic>A</italic><sub>2<italic>i</italic></sub>(‘Go’); when <italic>S</italic>(<italic>t</italic>) = <italic>S</italic><sub>7</sub> (goal), candidate of action was not defined and the term <inline-formula id="pcbi.1005145.e004"><alternatives><graphic id="pcbi.1005145.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>γ</mml:mi><mml:msub><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> was replaced with 0. <italic>A</italic>(<italic>t</italic> − 1) represents the action taken at time step <italic>t</italic> − 1; at the beginning of each trial, <italic>A</italic>(<italic>t</italic> − 1) was not defined and the term <italic>Q</italic>(<italic>A</italic>(<italic>t</italic> − 1)) was replaced with 0 so as to represent that the beginning of trial is not predictable. <italic>γ</italic> is the time discount factor (0 ≤ <italic>γ</italic> ≤ 1). In a separate set of simulations (<xref ref-type="fig" rid="pcbi.1005145.g010">Fig 10B, 10C and 10D</xref>, left), we also examined the case in which TD-RPE is calculated according to another RL algorithm called SARSA [<xref ref-type="bibr" rid="pcbi.1005145.ref030">30</xref>] as follows:
<disp-formula id="pcbi.1005145.e005">
<alternatives>
<graphic id="pcbi.1005145.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>A</italic>(<italic>t</italic>) represents the action taken at time step <italic>t</italic>.</p>
<p>At each time step other than the beginning of a trial, the learned value of <italic>A</italic>(<italic>t</italic> − 1) was assumed to be updated as follows:
<disp-formula id="pcbi.1005145.e006">
<alternatives>
<graphic id="pcbi.1005145.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>new</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>old</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>α</italic> is the learning rate (0 ≤ <italic>α</italic> ≤ 1). It was further assumed that the learned value of arbitrary action <italic>A</italic> decays at every time step as follows:
<disp-formula id="pcbi.1005145.e007">
<alternatives>
<graphic id="pcbi.1005145.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>new</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>old</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <italic>φ</italic> (0 ≤ <italic>φ</italic> ≤ 1) is a parameter referred to as the decay rate: <italic>φ</italic> = 0 corresponds to the case without value-decay. This sort of value-decay was introduced in [<xref ref-type="bibr" rid="pcbi.1005145.ref043">43</xref>] to account for the ramp-like activity of DA neurons reported in [<xref ref-type="bibr" rid="pcbi.1005145.ref021">21</xref>], and was analyzed in [<xref ref-type="bibr" rid="pcbi.1005145.ref023">23</xref>]. In the present study, the decay rate <italic>φ</italic> was varied from 0 to 0.02 by 0.002, unless otherwise described. Note that because (1 − <italic>φ</italic>) is multiplied at every time step, even if <italic>φ</italic> is very close to 0, significant decay can occur during a trial. For example, when the decay rate <italic>φ</italic> is 0.01, the action values decline to at least (1–0.01)<sup>7</sup> (≈ 0.932)-fold of the original values during a trial. It should also be noted that the value-decay defined as above is fundamentally different from the decay of eligibility trace, which is a popular notion in the RL theory [<xref ref-type="bibr" rid="pcbi.1005145.ref025">25</xref>]: in terms of the eligibility trace, we assumed that only the value of the immediately preceding action (Q(<italic>A</italic>(<italic>t</italic> − 1))) is eligible for RPE-dependent update (<xref ref-type="disp-formula" rid="pcbi.1005145.e006">Eq (3)</xref>), corresponding to the TD(0) algorithm.</p>
<p>At each time step other than when the goal was reached, action ‘Go’ or ‘Stay’ was assumed to be selected according to the following probabilities:
<disp-formula id="pcbi.1005145.e008">
<alternatives>
<graphic id="pcbi.1005145.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<disp-formula id="pcbi.1005145.e009">
<alternatives>
<graphic id="pcbi.1005145.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <italic>β</italic> is a parameter called the inverse temperature, which represents the sharpness of the soft-max selection [<xref ref-type="bibr" rid="pcbi.1005145.ref134">134</xref>].</p>
<p>A trial ended when subject reached the goal and got the reward. Subsequently the subject was assumed to be (automatically) returned to the start (<italic>S</italic><sub>1</sub>), and the next trial began. The learning rate <italic>α</italic>, the inverse temperature <italic>β</italic>, and the time discount factor <italic>γ</italic> were set to <italic>α</italic> = 0.5, <italic>β</italic> = 5, and <italic>γ</italic> = 1 unless otherwise described. Initial values of all the action values were set to 0. The amount of reward obtained at the goal, <italic>r</italic>, was set to 1 in most simulations and analyses, but we also examined the cases with <italic>r</italic> = 0.5, 0.75, 1.25, or 1.5 (<xref ref-type="fig" rid="pcbi.1005145.g011">Fig 11</xref>). The magnitude of rewards can in reality vary even more drastically. However, it has been shown [<xref ref-type="bibr" rid="pcbi.1005145.ref135">135</xref>] that the gain of DA neuron's response adaptively changes according to actual reward sizes. It could thus be possible to assume that <italic>r</italic> does not vary too drastically by virtue of such adaptive mechanisms. In a separate set of simulations (<xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13</xref>), in order to examine the robustness of the effect of the value-decay to perturbations in reward environments, we assumed that there is also small reward, with size <italic>x</italic>, at <italic>S</italic><sub>4</sub>, which is given whenever subject is located at <italic>S</italic><sub>4</sub> (i.e., repeatedly at every time step if subject stays at <italic>S</italic><sub>4</sub>).</p>
<p>In order to examine the dependence of the effect of the value-decay on the number of states from the start to the goal, we also conducted simulations for models that were modified to have 4 or 10 states, including the start and the goal, instead of 7 states in the original model (<xref ref-type="fig" rid="pcbi.1005145.g012">Fig 12A and 12B</xref>). We also examined the case where the subject is allowed to take not only ‘Go’ or ‘Stay’ but also ‘Back’ action at <italic>S</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 2, 3, …, 6) (for this, we again assumed 7 states), which causes a backward transition to <italic>S</italic><sub><italic>i</italic>−1</sub>. In this case (<xref ref-type="fig" rid="pcbi.1005145.g012">Fig 12C</xref>), selection of ‘Go’, ‘Stay’, and ‘Back’ at <italic>S</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 2, 3, …, 6) was assumed to be according to the probabilities: <italic>P</italic>(<italic>A</italic><sub>*</sub>) = exp(<italic>βQ</italic>(<italic>A</italic><sub>*</sub>))/<italic>Sum</italic>, where <italic>A</italic><sub>*</sub> was either <italic>A</italic><sub><italic>Go</italic></sub>, <italic>A</italic><sub><italic>Stay</italic></sub>, or <italic>A</italic><sub><italic>Back</italic></sub>, and <italic>Sum</italic> was exp(<italic>βQ</italic>(<italic>A</italic><sub><italic>Go</italic></sub>)) + exp(<italic>βQ</italic>(<italic>A</italic><sub><italic>Stay</italic></sub>)) + exp(<italic>βQ</italic>(<italic>A</italic><sub><italic>Back</italic></sub>)). Initial values of all the action values, including the values of ‘Back’ actions, were set to 0.</p>
<p>Further, in a separate set of simulations (<xref ref-type="fig" rid="pcbi.1005145.g009">Fig 9</xref>), we considered a different model in which selection of ‘Go’ or ‘Stay’ is based on the state values rather than the action values (‘Back’ was not considered in this model). Specifically, in this model, RPE is calculated as:
<disp-formula id="pcbi.1005145.e010">
<alternatives>
<graphic id="pcbi.1005145.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <italic>V</italic>(<italic>S</italic>(<italic>t</italic>)) represents the state value of <italic>S</italic>(<italic>t</italic>); if <italic>S</italic>(<italic>t</italic>) = <italic>S</italic><sub>7</sub>, <italic>V</italic>(<italic>S</italic>(<italic>t</italic> + 1)) is assumed to be 0. The state values are updated as follows:
<disp-formula id="pcbi.1005145.e011">
<alternatives>
<graphic id="pcbi.1005145.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>new</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>old</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
The learned value of arbitrary state <italic>S</italic> was assumed to decay at every time step as follows:
<disp-formula id="pcbi.1005145.e012">
<alternatives>
<graphic id="pcbi.1005145.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>new</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>old</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
‘Go’ is selected at <italic>S</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 2, 3, …, 6) with the probability exp(<italic>βV</italic>(<italic>S</italic><sub><italic>i+</italic>1</sub>))/{exp(<italic>βV</italic>(<italic>S</italic><sub><italic>i</italic></sub>)) + exp(<italic>βV</italic>(<italic>S</italic><sub><italic>i+</italic>1</sub>))}, and ‘Stay’ is selected otherwise. The parameters were set to <italic>α</italic> = 0.5, <italic>β</italic> = 5, <italic>γ</italic> = 1, and <italic>φ</italic> = 0.01, and initial values of all the state values were set to 0.</p>
<p>For each condition with different parameter values or model architectures, 20 simulations of 500 trials with different series of pseudorandom numbers were performed, unless otherwise described. The particular number 500 was chosen because it was considered to be largely in the range of the number of trials used in experiments: e.g., in [<xref ref-type="bibr" rid="pcbi.1005145.ref006">6</xref>], rats completed ~15 or more sessions with each session containing 40 trials. 20 simulations could be interpreted to represent 20 subjects. In the figures showing the number of time steps needed for goal-reaching, we presented the mean ± standard error (SE) of the 20 simulations except for <xref ref-type="fig" rid="pcbi.1005145.g013">Fig 13E</xref>, where the mean ± SE for the simulation runs completing 500 trials (which could be less than 20 for several conditions) were presented. We also presented the theoretical minimum (in the model with 7 states, it is 7, including the steps at the start and the goal) and the chance level, which is calculated (in the model with 7 states) as:
<disp-formula id="pcbi.1005145.e013">
<alternatives>
<graphic id="pcbi.1005145.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mrow><mml:mn>7</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>6</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>13</mml:mn><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <italic>h</italic>(6, <italic>k</italic>) represents the number of ways for a repeated (overlapping) combination of <italic>k</italic> out of 6 and is calculated as <italic>h</italic>(6, <italic>k</italic>) = (<italic>k</italic> + 5)!(k! · 5!). Simulations were performed using MATLAB (MathWorks Inc.). Program files to run simulations and make figures are available from ModelDB (<ext-link ext-link-type="uri" xlink:href="https://senselab.med.yale.edu/modeldb/showModel.cshtml?model=195890" xlink:type="simple">https://senselab.med.yale.edu/modeldb/showModel.cshtml?model=195890</ext-link>) after the publication of this article.</p>
</sec>
<sec id="sec018">
<title>Modeling blockade of DA signaling</title>
<p>To simulate post-training blockade of DA signaling, we replaced <italic>δ</italic>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="pcbi.1005145.e006">Eq (3)</xref> with 0 (complete blockade) or <italic>δ</italic>(<italic>t</italic>)/4 (partial blockade) after 250 trials (Figs <xref ref-type="fig" rid="pcbi.1005145.g002">2C</xref>, <xref ref-type="fig" rid="pcbi.1005145.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1005145.g006">6D</xref>) or 500 trials (Figs <xref ref-type="fig" rid="pcbi.1005145.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1005145.g014">14</xref>) were completed. <italic>δ</italic>(<italic>t</italic>) was non-negative in those simulations because of the structure of the simulated tasks and the assumed Q-leaning-type calculation of RPE, and so the replacement of <italic>δ</italic>(<italic>t</italic>) with 0 or <italic>δ</italic>(<italic>t</italic>)/4 corresponded to that the size of an increment of action values according to non-negative RPE was reduced to zero or to a quarter of the original size. Notably, at the cellular/synaptic level, DA is known to have two major functions: (i) induce/modulate plasticity of corticostriatal synapses, and (ii) modulate responsiveness of striatal neurons [<xref ref-type="bibr" rid="pcbi.1005145.ref136">136</xref>]. Function (i) has been suggested to implement RPE-dependent update of learned values (<xref ref-type="disp-formula" rid="pcbi.1005145.e006">Eq (3)</xref>) (e.g., [<xref ref-type="bibr" rid="pcbi.1005145.ref018">18</xref>]), and in the present work we incorporated the effect of DA blockade on this function into the model as described above, although function (ii) can also affect reaction time and valuation (e.g., [<xref ref-type="bibr" rid="pcbi.1005145.ref043">43</xref>]) and assuming both of (i) and (ii) might be necessary to account for a wider range of phenomena caused by DA manipulations, in particular, changes in the speed or response time of a single rapid movement (e.g., [<xref ref-type="bibr" rid="pcbi.1005145.ref137">137</xref>, <xref ref-type="bibr" rid="pcbi.1005145.ref138">138</xref>]) rather than (or in addition to) of a series of actions.</p>
</sec>
<sec id="sec019">
<title>Reduced dynamical system model of ‘Go’ or ‘Stay’ selection, and bifurcation analysis</title>
<p>In order to obtain qualitative understandings of how the value-decay affects the time evolution and steady-state of action values, beyond observations of simulation results, we reduced the original model (<xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>) to a simpler model through approximations, and conducted bifurcation analysis. Specifically, we considered a reduced continuous-time dynamical system model that approximately describes the time evolution of the values of ‘Stay’ and ‘Go’ at the state preceding the goal (i.e., <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’) at <italic>S</italic><sub>6</sub> in <xref ref-type="fig" rid="pcbi.1005145.g001">Fig 1</xref>). The reduced model is as follows:
<disp-formula id="pcbi.1005145.e014">
<alternatives>
<graphic id="pcbi.1005145.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mi>α</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
<disp-formula id="pcbi.1005145.e015">
<alternatives>
<graphic id="pcbi.1005145.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where <italic>q</italic>(<italic>A</italic><sub>11</sub>) and <italic>q</italic>(<italic>A</italic><sub>12</sub>) are the continuous-time variables that approximately represent the action values of <italic>A</italic><sub>11</sub> (‘Stay’) and <italic>A</italic><sub>12</sub> (‘Go’), respectively. <italic>y</italic> approximately represents the expected value of the number of repetitions of <italic>A</italic><sub>11</sub> (‘Stay’) choice (i.e., how many time steps subject chooses <italic>A</italic><sub>11</sub> (‘Stay’) at <italic>S</italic><sub>6</sub>) in a single trial, and it is calculated as:
<disp-formula id="pcbi.1005145.e016">
<alternatives>
<graphic id="pcbi.1005145.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
where <italic>p</italic>(<italic>A</italic><sub>11</sub>) represents the probability that <italic>A</italic><sub>11</sub> is chosen out of <italic>A</italic><sub>11</sub> and <italic>A</italic><sub>12</sub> according to <xref ref-type="disp-formula" rid="pcbi.1005145.e009">Eq (6)</xref> when the values of <italic>A</italic><sub>11</sub> and <italic>A</italic><sub>12</sub> are <italic>q</italic>(<italic>A</italic><sub>11</sub>) and <italic>q</italic>(<italic>A</italic><sub>12</sub>), respectively:
<disp-formula id="pcbi.1005145.e017">
<alternatives>
<graphic id="pcbi.1005145.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
and substituting <xref ref-type="disp-formula" rid="pcbi.1005145.e017">Eq (14)</xref> into <xref ref-type="disp-formula" rid="pcbi.1005145.e016">Eq (13)</xref> results in:
<disp-formula id="pcbi.1005145.e018">
<alternatives>
<graphic id="pcbi.1005145.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<inline-formula id="pcbi.1005145.e019"><alternatives><graphic id="pcbi.1005145.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005145.e020"><alternatives><graphic id="pcbi.1005145.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represent TD-RPE generated when <italic>A</italic><sub>11</sub> or <italic>A</italic><sub>12</sub> with the value <italic>q</italic>(<italic>A</italic><sub>11</sub>) or <italic>q</italic>(<italic>A</italic><sub>12</sub>) is chosen, respectively:
<disp-formula id="pcbi.1005145.e021">
<alternatives>
<graphic id="pcbi.1005145.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
<disp-formula id="pcbi.1005145.e022">
<alternatives>
<graphic id="pcbi.1005145.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
where <italic>r</italic> is the reward amount (= 1). <italic>ψ</italic> is a parameter representing the degree of the value-decay in a trial, which roughly corresponds to the decay rate <italic>φ</italic> in the original model multiplied by the number of time steps needed for goal-reaching. Notably, the reduced model is a continuous-time approximation of an algorithm in which update and decay of learned values occur once per every trial in a batch-wise manner whereas the original model is described as an online algorithm where update and value-decay occur at every time step; this difference is contained in our expression "approximate" referring to the reduced model. We analyzed the two-dimensional dynamics of <italic>q</italic>(<italic>A</italic><sub>11</sub>) and <italic>q</italic>(<italic>A</italic><sub>12</sub>) (Eqs (<xref ref-type="disp-formula" rid="pcbi.1005145.e014">11</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005145.e015">12</xref>)) under the assumption that <italic>q</italic>(<italic>A</italic><sub>11</sub>) ≤ <italic>q</italic>(<italic>A</italic><sub>12</sub>) (i.e., max{<italic>q</italic>(<italic>A</italic><sub>11</sub>), <italic>q</italic>(<italic>A</italic><sub>12</sub>)} = <italic>q</italic>(<italic>A</italic><sub>12</sub>) in <xref ref-type="disp-formula" rid="pcbi.1005145.e021">Eq (16)</xref>). More specifically, we numerically solved the equations <inline-formula id="pcbi.1005145.e023"><alternatives><graphic id="pcbi.1005145.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005145.e024"><alternatives><graphic id="pcbi.1005145.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005145.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> to draw the nullclines (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7E</xref>), and also numerically found the equilibriums and examined their stabilities to draw the bifurcation diagram (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref>) and calculate <italic>p</italic>(<italic>A</italic><sub>11</sub>) and <italic>p</italic>(<italic>A</italic><sub>12</sub>) (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7C</xref>) by using MATLAB. The result of the bifurcation analysis in the case with <italic>α</italic> = 0.5, <italic>β</italic> = 5, and <italic>γ</italic> = 1 (<xref ref-type="fig" rid="pcbi.1005145.g007">Fig 7B</xref>) was further confirmed by using XPP-Aut (<ext-link ext-link-type="uri" xlink:href="http://www.math.pitt.edu/~bard/xpp/xpp.html" xlink:type="simple">http://www.math.pitt.edu/~bard/xpp/xpp.html</ext-link>).</p>
</sec>
<sec id="sec020">
<title>Simulation of a cost-benefit decision making task in a T-maze</title>
<p>We simulated an experiment examining the effects of DA depletion in the nucleus accumbens in a T-maze task reported in [<xref ref-type="bibr" rid="pcbi.1005145.ref024">24</xref>]. There were two conditions in the task. In the first condition, there was small reward in one of the two arms of the T-maze whereas there was large reward accompanied with high cost (physical barrier preceding the reward) in the other arm. In the second condition, the two arms contained small and large rewards as before, but neither was accompanied with high cost. We simulated this experiment by representing the high cost as an extra state preceding the reward. Specifically, we assumed a state-action diagram as shown in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A and 5E</xref> (right panels). There were two action candidates, ‘Go’ and ‘Stay’, at every state, except for the state at the T-junction (State 4) and the state at the trial end, which was reached if ‘Go’ was chosen at State 7 or 8. In State 4, there were three action candidates, ‘Choose, and Go to, one of the arm (Arm 1)’, ‘Choose, and Go to, the other arm (Arm 2)’, and ‘Stay’. In the state at the trial end (State 9, which is not depicted in <xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A and 5E</xref>), there was no action candidate, and subject was assumed to be automatically moved to the start state (State 1) at the next time step. In the first condition of the simulated task (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5A</xref>), small reward (size 0.5) was given when subject reached State 6 for the first time (i.e., only once in a trial), whereas large reward (size 1) was given when subject reached State 7 for the first time. One extra state, i.e., State 5, preceding the state associated with large reward (State 7) was assumed to represent high cost accompanied with the large reward. In the second condition (<xref ref-type="fig" rid="pcbi.1005145.g005">Fig 5E</xref>), small (size 0.5) or large (size 1) reward was given when subject reached State 6 or State 5, respectively, for the first time, representing that neither reward was accompanied with high cost. Calculation of Q-learning-type RPE and RPE-dependent update of action values were assumed in the same manner as before, with the parameters <italic>α</italic> = 0.5, <italic>β</italic> = 5, and <italic>γ</italic> = 1. The value-decay was also assumed similarly, with the decay rate <italic>φ</italic> = 0.01. Initial values of all the action values were set to 0. 20 simulations of 1000 trials were conducted for each condition, and post-training DA depletion was simulated in such a way that the size of RPE-dependent increment of action values was reduced to a quarter of the original size after 500 trials were completed.</p>
</sec>
<sec id="sec021">
<title>Elaborated model aiming at reproducing velocity profiles in a T-maze</title>
<p>By modifying the original model described above, we developed an elaborated model of self-paced spatial movement, and simulated the cost-benefit decision making task in a T-maze mentioned above. In this elaborated model, the exact one-to-one correspondence between the subject's physical location and the internal state assumed in the original model was changed into a loose coupling, in which each state corresponds to a range of physical locations (<xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14A</xref>). Also, ‘Stay’ action in the original model was replaced with ‘Slow’ action unless there is a physical constraint (i.e., the start, the T-junction, or the end). Specifically, it was assumed that, at each time step <italic>t</italic>, subject at a given location chooses either ‘Go’ or ‘Slow’, except that the subject is at the start, T-junction, or the reward location (in the ends of the T-maze). By selecting ‘Go’, subject moves straightforward for a time step with the "velocity" 1, meaning that the subject's physical location is displaced by 1, or moves to the T-junction or the reward location when it is within 1 from the current location. By selecting ‘Slow’, subject moves straightforward for a time step with the "velocity" halved, meaning that the subject's physical location is displaced by the half of the displacement during the previous time interval (between <italic>t</italic> − 1 and <italic>t</italic>), or moves to the T-junction or the reward location when it is within the calculated displacement from the current location. In these ways, the "velocity" in this model was defined as the displacement in a time step. At the start (State 1), subject was assumed to take ‘Go’ or ‘Stay’ as in the original model (because at the start, the previous "velocity" was not defined). At the T-junction, subject was assumed to take ‘Choose, and Go to, one of the arm (Arm 1)’, ‘Choose, and Go to, the other arm (Arm 2)’, or ‘Stay’. By selecting ‘Choose, and Go to, Arm 1 or 2’, the subject's physical location is displaced by 1 on the selected arm. By selecting ‘Stay’, subject stays at the same place (T-junction). At the reward location, subject was assumed to take the consummatory action for a time step (indicated by the double-lined arrows in <xref ref-type="fig" rid="pcbi.1005145.g014">Fig 14B and 14F</xref>), and proceed to the end state. Calculation of Q-learning-type TD-RPE, update of action values, and the value-decay were assumed in the same manner as in the original model.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>The authors thank Dr. Jerome Foo for his comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005145.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roitman</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Stuber</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>. <article-title>Dopamine operates as a subsecond modulator of food seeking</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>(<issue>6</issue>):<fpage>1265</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3823-03.2004" xlink:type="simple">10.1523/JNEUROSCI.3823-03.2004</ext-link></comment> <object-id pub-id-type="pmid">14960596</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Day</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Roitman</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>. <article-title>Associative learning mediates dynamic shifts in dopamine signaling in the nucleus accumbens</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>(<issue>8</issue>):<fpage>1020</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1923" xlink:type="simple">10.1038/nn1923</ext-link></comment> <object-id pub-id-type="pmid">17603481</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>J Neurosci</source>. <year>1996</year>;<volume>16</volume>(<issue>5</issue>):<fpage>1936</fpage>–<lpage>47</lpage>. <object-id pub-id-type="pmid">8774460</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wassum</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Ostlund</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Maidment</surname> <given-names>NT</given-names></name>. <article-title>Phasic mesolimbic dopamine signaling precedes and predicts performance of a self-initiated action sequence task</article-title>. <source>Biol Psychiatry</source>. <year>2012</year>;<volume>71</volume>(<issue>10</issue>):<fpage>846</fpage>–<lpage>54</lpage>. PubMed Central PMCID: PMCPMC3471807. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.biopsych.2011.12.019" xlink:type="simple">10.1016/j.biopsych.2011.12.019</ext-link></comment> <object-id pub-id-type="pmid">22305286</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howe</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Tierney</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Sandberg</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name>. <article-title>Prolonged dopamine signalling in striatum signals proximity and value of distant rewards</article-title>. <source>Nature</source>. <year>2013</year>;<volume>500</volume>(<issue>7464</issue>):<fpage>575</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12475" xlink:type="simple">10.1038/nature12475</ext-link></comment> <object-id pub-id-type="pmid">23913271</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hamid</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Pettibone</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Mabrouk</surname> <given-names>OS</given-names></name>, <name name-style="western"><surname>Hetrick</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Vander Weele</surname> <given-names>CM</given-names></name>, <etal>et al</etal>. <article-title>Mesolimbic dopamine signals the value of work</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>1</issue>):<fpage>117</fpage>–<lpage>26</lpage>. PubMed Central PMCID: PMCPMC4696912. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4173" xlink:type="simple">10.1038/nn.4173</ext-link></comment> <object-id pub-id-type="pmid">26595651</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Greenfield</surname> <given-names>VY</given-names></name>, <name name-style="western"><surname>Bye</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Linker</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Wassum</surname> <given-names>KM</given-names></name>. <article-title>Dynamic mesolimbic dopamine signaling during action sequence learning and expectation violation</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>:<fpage>20231</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep20231" xlink:type="simple">10.1038/srep20231</ext-link></comment> <object-id pub-id-type="pmid">26869075</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>BJ</given-names></name>. <article-title>Neurobehavioural mechanisms of reward and motivation</article-title>. <source>Curr Opin Neurobiol</source>. <year>1996</year>;<volume>6</volume>(<issue>2</issue>):<fpage>228</fpage>–<lpage>36</lpage>. S0959-4388(96)80077-8 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(96)80077-8" xlink:type="simple">10.1016/S0959-4388(96)80077-8</ext-link></comment> <object-id pub-id-type="pmid">8725965</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berridge</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>TE</given-names></name>. <article-title>What is the role of dopamine in reward: hedonic impact, reward learning, or incentive salience?</article-title> <source>Brain Res Brain Res Rev</source>. <year>1998</year>;<volume>28</volume>(<issue>3</issue>):<fpage>309</fpage>–<lpage>69</lpage>. S0165017398000198 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0165-0173(98)00019-8" xlink:type="simple">10.1016/S0165-0173(98)00019-8</ext-link></comment> <object-id pub-id-type="pmid">9858756</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salamone</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Correa</surname> <given-names>M</given-names></name>. <article-title>Motivational views of reinforcement: implications for understanding the behavioral functions of nucleus accumbens dopamine</article-title>. <source>Behav Brain Res</source>. <year>2002</year>;<volume>137</volume>(<issue>1–2</issue>):<fpage>3</fpage>–<lpage>25</lpage>. S0166432802002826 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0166-4328(02)00282-6" xlink:type="simple">10.1016/S0166-4328(02)00282-6</ext-link></comment> <object-id pub-id-type="pmid">12445713</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Reward, motivation, and reinforcement learning</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>(<issue>2</issue>):<fpage>285</fpage>–<lpage>98</lpage>. S0896627302009637 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00963-7" xlink:type="simple">10.1016/S0896-6273(02)00963-7</ext-link></comment> <object-id pub-id-type="pmid">12383782</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Cost, benefit, tonic, phasic: what do response rates tell us about dopamine and motivation?</article-title> <source>Ann N Y Acad Sci</source>. <year>2007</year>;<volume>1104</volume>:<fpage>357</fpage>–<lpage>76</lpage>. annals.1390.018 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1196/annals.1390.018" xlink:type="simple">10.1196/annals.1390.018</ext-link></comment> <object-id pub-id-type="pmid">17416928</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ikemoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Panksepp</surname> <given-names>J</given-names></name>. <article-title>Dissociations between appetitive and consummatory responses by pharmacological manipulations of reward-relevant brain regions</article-title>. <source>Behav Neurosci</source>. <year>1996</year>;<volume>110</volume>(<issue>2</issue>):<fpage>331</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0735-7044.110.2.331" xlink:type="simple">10.1037/0735-7044.110.2.331</ext-link></comment> <object-id pub-id-type="pmid">8731060</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology (Berl)</source>. <year>2007</year>;<volume>191</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00213-006-0502-4" xlink:type="simple">10.1007/s00213-006-0502-4</ext-link></comment> <object-id pub-id-type="pmid">17031711</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lloyd</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tamping Ramping: Algorithmic, Implementational, and Computational Explanations of Phasic Dopamine Signals in the Accumbens</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e1004622</fpage>. PubMed Central PMCID: PMCPMC4689534. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004622" xlink:type="simple">10.1371/journal.pcbi.1004622</ext-link></comment> <object-id pub-id-type="pmid">26699940</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Hyland</surname> <given-names>BI</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>JR</given-names></name>. <article-title>A cellular mechanism of reward-related learning</article-title>. <source>Nature</source>. <year>2001</year>;<volume>413</volume>(<issue>6851</issue>):<fpage>67</fpage>–<lpage>70</lpage>. 35092560 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35092560" xlink:type="simple">10.1038/35092560</ext-link></comment> <object-id pub-id-type="pmid">11544526</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yagishita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Urakubo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kasai</surname> <given-names>H</given-names></name>. <article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title>. <source>Science</source>. <year>2014</year>;<volume>345</volume>(<issue>6204</issue>):<fpage>1616</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1255514" xlink:type="simple">10.1126/science.1255514</ext-link></comment> <object-id pub-id-type="pmid">25258080</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bukwich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Hemmelder</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>N</given-names></name>. <article-title>Arithmetic and local circuitry underlying dopamine prediction errors</article-title>. <source>Nature</source>. <year>2015</year>;<volume>525</volume>(<issue>7568</issue>):<fpage>243</fpage>–<lpage>6</lpage>. PubMed Central PMCID: PMCPMC4567485. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14855" xlink:type="simple">10.1038/nature14855</ext-link></comment> <object-id pub-id-type="pmid">26322583</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keiflin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Janak</surname> <given-names>PH</given-names></name>. <article-title>Dopamine Prediction Errors in Reward Learning and Addiction: From Theory to Neural Circuitry</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>88</volume>(<issue>2</issue>):<fpage>247</fpage>–<lpage>63</lpage>. PubMed Central PMCID: PMCPMC4760620. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.08.037" xlink:type="simple">10.1016/j.neuron.2015.08.037</ext-link></comment> <object-id pub-id-type="pmid">26494275</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roesch</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Calu</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>(<issue>12</issue>):<fpage>1615</fpage>–<lpage>24</lpage>. nn2013 [pii]; PubMed Central PMCID: PMCPMC2562672. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn2013" xlink:type="simple">10.1038/nn2013</ext-link></comment> <object-id pub-id-type="pmid">18026098</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname> <given-names>YK</given-names></name>, <name name-style="western"><surname>Roesch</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Toreson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>O'Donnell</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Expectancy-related changes in firing of dopamine neurons depend on orbitofrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>12</issue>):<fpage>1590</fpage>–<lpage>7</lpage>. PubMed Central PMCID: PMCPMC3225718. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2957" xlink:type="simple">10.1038/nn.2957</ext-link></comment> <object-id pub-id-type="pmid">22037501</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kato</surname> <given-names>A</given-names></name>. <article-title>Striatal dopamine ramping may indicate flexible reinforcement learning with forgetting in the cortico-basal ganglia circuits</article-title>. <source>Front Neural Circuits</source>. <year>2014</year>;<volume>8</volume>:<fpage>36</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncir.2014.00036" xlink:type="simple">10.3389/fncir.2014.00036</ext-link></comment> <object-id pub-id-type="pmid">24782717</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salamone</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Cousins</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Bucher</surname> <given-names>S</given-names></name>. <article-title>Anhedonia or anergia? Effects of haloperidol and nucleus accumbens dopamine depletion on instrumental response selection in a T-maze cost/benefit procedure</article-title>. <source>Behav Brain Res</source>. <year>1994</year>;<volume>65</volume>(<issue>2</issue>):<fpage>221</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0166-4328(94)90108-2" xlink:type="simple">10.1016/0166-4328(94)90108-2</ext-link></comment> <object-id pub-id-type="pmid">7718155</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>A</given-names></name>. <source>Reinforcement Learning</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-1-4899-7502-7_720-1" xlink:type="simple">10.1007/978-1-4899-7502-7_720-1</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005145.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Panigrahi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Graves</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Vollmer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Dopamine Is Required for the Neural Representation and Control of Movement Vigor</article-title>. <source>Cell</source>. <year>2015</year>;<volume>162</volume>(<issue>6</issue>):<fpage>1418</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cell.2015.08.014" xlink:type="simple">10.1016/j.cell.2015.08.014</ext-link></comment> <object-id pub-id-type="pmid">26359992</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Strogatz</surname> <given-names>SH</given-names></name>. <source>Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry, And Engineering</source>: <publisher-name>Westview Press</publisher-name>; <year>1994</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1063/1.4823332" xlink:type="simple">10.1063/1.4823332</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005145.ref028"><label>28</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Watkins</surname> <given-names>C</given-names></name>. <source>Learning from Delayed Rewards</source>: <publisher-name>University of Cambridge</publisher-name>; <year>1989</year>.</mixed-citation></ref>
<ref id="pcbi.1005145.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Day</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>. <article-title>Phasic nucleus accumbens dopamine release encodes effort- and delay-related costs</article-title>. <source>Biol Psychiatry</source>. <year>2010</year>;<volume>68</volume>(<issue>3</issue>):<fpage>306</fpage>–<lpage>9</lpage>. PubMed Central PMCID: PMCPMC2907444. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.biopsych.2010.03.026" xlink:type="simple">10.1016/j.biopsych.2010.03.026</ext-link></comment> <object-id pub-id-type="pmid">20452572</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Rummery GA, Niranjan M. On-line Q-learning using connectionist systems. Technical Report CUED/F-INFENG/TR 166: Cambridge University Engineering Department; 1994.</mixed-citation></ref>
<ref id="pcbi.1005145.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Nevet</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Arkadir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>. <article-title>Midbrain dopamine neurons encode decisions for future action</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1057</fpage>–<lpage>63</lpage>. nn1743 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1743" xlink:type="simple">10.1038/nn1743</ext-link></comment> <object-id pub-id-type="pmid">16862149</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Choice values</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>8</issue>):<fpage>987</fpage>–<lpage>8</lpage>. nn0806-987 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn0806-987" xlink:type="simple">10.1038/nn0806-987</ext-link></comment> <object-id pub-id-type="pmid">16871163</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Syed</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Grima</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Magill</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>. <article-title>Action initiation shapes mesolimbic dopamine encoding of future rewards</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>1</issue>):<fpage>34</fpage>–<lpage>6</lpage>. PubMed Central PMCID: PMCPMC4697363. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4187" xlink:type="simple">10.1038/nn.4187</ext-link></comment> <object-id pub-id-type="pmid">26642087</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys</article-title>. <source>J Exp Anal Behav</source>. <year>2005</year>;<volume>84</volume>(<issue>3</issue>):<fpage>555</fpage>–<lpage>79</lpage>. PubMed Central PMCID: PMCPMC1389781. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1901/jeab.2005.110-04" xlink:type="simple">10.1901/jeab.2005.110-04</ext-link></comment> <object-id pub-id-type="pmid">16596980</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akaishi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Umeda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nagase</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>. <article-title>Autonomous mechanism of internal choice estimate underlies decision inertia</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>1</issue>):<fpage>195</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.10.018" xlink:type="simple">10.1016/j.neuron.2013.10.018</ext-link></comment> <object-id pub-id-type="pmid">24333055</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hart</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Rutledge</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>PE</given-names></name>. <article-title>Phasic dopamine release in the rat nucleus accumbens symmetrically encodes a reward prediction error term</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>3</issue>):<fpage>698</fpage>–<lpage>704</lpage>. PubMed Central PMCID: PMCPMC3891951. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2489-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2489-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24431428</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Critchley</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Temporal difference models and reward-related learning in the human brain</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>38</volume>(<issue>2</issue>):<fpage>329</fpage>–<lpage>37</lpage>. S0896627303001697 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(03)00169-7" xlink:type="simple">10.1016/S0896-6273(03)00169-7</ext-link></comment> <object-id pub-id-type="pmid">12718865</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Berns</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>Temporal prediction errors in a passive learning task activate human striatum</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>38</volume>(<issue>2</issue>):<fpage>339</fpage>–<lpage>46</lpage>. S0896627303001545 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(03)00154-5" xlink:type="simple">10.1016/S0896-6273(03)00154-5</ext-link></comment> <object-id pub-id-type="pmid">12718866</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rutledge</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Caplin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Testing the reward prediction error hypothesis with an axiomatic model</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>(<issue>40</issue>):<fpage>13525</fpage>–<lpage>36</lpage>. PubMed Central PMCID: PMCPMC2957369. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1747-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1747-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20926678</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steinberg</surname> <given-names>EE</given-names></name>, <name name-style="western"><surname>Keiflin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Boivin</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Witten</surname> <given-names>IB</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Janak</surname> <given-names>PH</given-names></name>. <article-title>A causal link between prediction errors, dopamine neurons and learning</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>(<issue>7</issue>):<fpage>966</fpage>–<lpage>73</lpage>. PubMed Central PMCID: PMCPMC3705924. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3413" xlink:type="simple">10.1038/nn.3413</ext-link></comment> <object-id pub-id-type="pmid">23708143</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>CY</given-names></name>, <name name-style="western"><surname>Esber</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Marrero-Garcia</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yau</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Bonci</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Brief optogenetic inhibition of dopamine neurons mimics endogenous negative reward prediction errors</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>1</issue>):<fpage>111</fpage>–<lpage>6</lpage>. PubMed Central PMCID: PMCPMC4696902. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4191" xlink:type="simple">10.1038/nn.4191</ext-link></comment> <object-id pub-id-type="pmid">26642092</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A computational substrate for incentive salience</article-title>. <source>Trends Neurosci</source>. <year>2003</year>;<volume>26</volume>(<issue>8</issue>):<fpage>423</fpage>–<lpage>8</lpage>. S0166223603001772 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0166-2236(03)00177-2" xlink:type="simple">10.1016/S0166-2236(03)00177-2</ext-link></comment> <object-id pub-id-type="pmid">12900173</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Morishima</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kawaguchi</surname> <given-names>Y</given-names></name>. <article-title>Dopaminergic control of motivation and reinforcement learning: a closed-circuit account for reward-oriented behavior</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>20</issue>):<fpage>8866</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4614-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4614-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23678129</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>From understanding computation to understanding neural circuitry</article-title>. <source>Neurosci Res Program Bull</source>. <year>1977</year>;<volume>15</volume>:<fpage>470</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005145.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Langdon</surname> <given-names>A</given-names></name>. <source>Reinforcement learning with Marr</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cobeha.2016.04.005" xlink:type="simple">10.1016/j.cobeha.2016.04.005</ext-link></comment> <object-id pub-id-type="pmid">27408906</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Morishima</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kawaguchi</surname> <given-names>Y</given-names></name>. <article-title>Reinforcement learning: computing the temporal difference of values via distinct corticostriatal pathways</article-title>. <source>Trends Neurosci</source>. <year>2012</year>;<volume>35</volume>(<issue>8</issue>):<fpage>457</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2012.04.009" xlink:type="simple">10.1016/j.tins.2012.04.009</ext-link></comment> <object-id pub-id-type="pmid">22658226</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kawaguchi</surname> <given-names>Y</given-names></name>. <article-title>Computing reward-prediction error: an integrated account of cortical timing and basal-ganglia pathways for appetitive and aversive learning</article-title>. <source>Eur J Neurosci</source>. <year>2015</year>;<volume>42</volume>(<issue>4</issue>):<fpage>2003</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/ejn.12994" xlink:type="simple">10.1111/ejn.12994</ext-link></comment> <object-id pub-id-type="pmid">26095906</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name>. <article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>(<issue>4</issue>):<fpage>1314</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3733-05.2006" xlink:type="simple">10.1523/JNEUROSCI.3733-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16436619</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>. <article-title>Cortico-basal ganglia circuit mechanism for a decision threshold in reaction time tasks</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>(<issue>7</issue>):<fpage>956</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1722" xlink:type="simple">10.1038/nn1722</ext-link></comment> <object-id pub-id-type="pmid">16767089</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name>. <article-title>Neural circuit dynamics underlying accumulation of time-varying evidence during perceptual decision making</article-title>. <source>Front Comput Neurosci</source>. <year>2007</year>;<volume>1</volume>:<fpage>6</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.10.006.2007" xlink:type="simple">10.3389/neuro.10.006.2007</ext-link></comment> <object-id pub-id-type="pmid">18946528</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name>. <article-title>From biophysics to cognition: reward-dependent adaptive choice behavior</article-title>. <source>Curr Opin Neurobiol</source>. <year>2008</year>;<volume>18</volume>(<issue>2</issue>):<fpage>209</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2008.07.003" xlink:type="simple">10.1016/j.conb.2008.07.003</ext-link></comment> <object-id pub-id-type="pmid">18678255</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jitsevb</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name>. <article-title>Corticostriatal circuit mechanisms of value-based action selection: Implementation of reinforcement learning algorithms and beyond</article-title>. <source>Behav Brain Res</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bbr.2016.05.017" xlink:type="simple">10.1016/j.bbr.2016.05.017</ext-link></comment> <object-id pub-id-type="pmid">27173430</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hunt</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Kolling</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>Mechanisms underlying cortical activity during value-guided choice</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>3</issue>):<fpage>470</fpage>–<lpage>6</lpage>, <fpage>S1</fpage>–<lpage>3</lpage>. PubMed Central PMCID: PMCPMC3378494. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3017" xlink:type="simple">10.1038/nn.3017</ext-link></comment> <object-id pub-id-type="pmid">22231429</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jocham</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hunt</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Near</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>A mechanism for value-guided choice based on the excitation-inhibition balance in prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>7</issue>):<fpage>960</fpage>–<lpage>1</lpage>. PubMed Central PMCID: PMCPMC4050076. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3140" xlink:type="simple">10.1038/nn.3140</ext-link></comment> <object-id pub-id-type="pmid">22706268</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Opponent actor learning (OpAL): modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title>. <source>Psychol Rev</source>. <year>2014</year>;<volume>121</volume>(<issue>3</issue>):<fpage>337</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0037015" xlink:type="simple">10.1037/a0037015</ext-link></comment> <object-id pub-id-type="pmid">25090423</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref056"><label>56</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>. <chapter-title>A theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. In: <name name-style="western"><surname>Black</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Prokasy</surname> <given-names>WF</given-names></name>, editors. <source>Classical Conditioning II: Current Research and Theory</source>: <publisher-name>Appleton-Century-Crofts</publisher-name>; <year>1972</year>. p. <fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005145.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Dialogues on prediction errors</article-title>. <source>Trends Cogn Sci</source>. <year>2008</year>;<volume>12</volume>(<issue>7</issue>):<fpage>265</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2008.03.006" xlink:type="simple">10.1016/j.tics.2008.03.006</ext-link></comment> <object-id pub-id-type="pmid">18567531</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2011</year>;<volume>108</volume> <issue>Suppl 3</issue>:<fpage>15647</fpage>–<lpage>54</lpage>. 1014269108 [pii]; PubMed Central PMCID: PMCPMC3176615. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1014269108" xlink:type="simple">10.1073/pnas.1014269108</ext-link></comment> <object-id pub-id-type="pmid">21389268</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>. <article-title>Differential cortical activation of the striatal direct and indirect pathway cells: reconciling the anatomical and optogenetic results by using a computational method</article-title>. <source>J Neurophysiol</source>. <year>2014</year>;<volume>112</volume>(<issue>1</issue>):<fpage>120</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00625.2013" xlink:type="simple">10.1152/jn.00625.2013</ext-link></comment> <object-id pub-id-type="pmid">24598515</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keeler</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Pretsell</surname> <given-names>DO</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>. <article-title>Functional implications of dopamine D1 vs. D2 receptors: A 'prepare and select' model of the striatal direct vs. indirect pathways</article-title>. <source>Neuroscience</source>. <year>2014</year>;<volume>282C</volume>:<fpage>156</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroscience.2014.07.021" xlink:type="simple">10.1016/j.neuroscience.2014.07.021</ext-link></comment> <object-id pub-id-type="pmid">25062777</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brea</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>. <article-title>A normative theory of forgetting: lessons from the fruit fly</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003640</fpage>. PubMed Central PMCID: PMCPMC4046926. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003640" xlink:type="simple">10.1371/journal.pcbi.1003640</ext-link></comment> <object-id pub-id-type="pmid">24901935</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tamosiunaite</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ainge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kulvicius</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Porr</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dudchenko</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>. <article-title>Path-finding in real and simulated rats: assessing the influence of path characteristics on navigation learning</article-title>. <source>J Comput Neurosci</source>. <year>2008</year>;<volume>25</volume>(<issue>3</issue>):<fpage>562</fpage>–<lpage>82</lpage>. PubMed Central PMCID: PMCPMC3085791. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-008-0094-6" xlink:type="simple">10.1007/s10827-008-0094-6</ext-link></comment> <object-id pub-id-type="pmid">18446432</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pan</surname> <given-names>WX</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Hyland</surname> <given-names>BI</given-names></name>. <article-title>Tripartite mechanism of extinction suggested by dopamine neuron activity and temporal difference model</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>(<issue>39</issue>):<fpage>9619</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0255-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0255-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18815248</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erev</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Roth</surname> <given-names>AE</given-names></name>. <article-title>Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria</article-title>. <source>Am Econ Rev</source>. <year>1998</year>;<volume>88</volume>(<issue>4</issue>):<fpage>848</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005145.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dai</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kerestes</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Upton</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Stout</surname> <given-names>JC</given-names></name>. <article-title>An improved cognitive model of the Iowa and Soochow Gambling Tasks with regard to model fitting performance and tests of parameter consistency</article-title>. <source>Front Psychol</source>. <year>2015</year>;<volume>6</volume>:<fpage>229</fpage>. PubMed Central PMCID: PMCPMC4357250. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2015.00229" xlink:type="simple">10.3389/fpsyg.2015.00229</ext-link></comment> <object-id pub-id-type="pmid">25814963</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daniel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Geana</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Leong</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Radulescu</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Reinforcement learning in multidimensional environments relies on attention mechanisms</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>21</issue>):<fpage>8145</fpage>–<lpage>57</lpage>. PubMed Central PMCID: PMCPMC4444538. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2978-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2978-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26019331</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Quilodran</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Enel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Procyk</surname> <given-names>E</given-names></name>. <article-title>Behavioral Regulation and the Modulation of Information Coding in the Lateral Prefrontal and Cingulate Cortex</article-title>. <source>Cereb Cortex</source>. <year>2015</year>;<volume>25</volume>(<issue>9</issue>):<fpage>3197</fpage>–<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhu114" xlink:type="simple">10.1093/cercor/bhu114</ext-link></comment> <object-id pub-id-type="pmid">24904073</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>31</issue>):<fpage>9861</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6157-08.2009" xlink:type="simple">10.1523/JNEUROSCI.6157-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19657038</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hirashima</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nozaki</surname> <given-names>D</given-names></name>. <article-title>Learning with slight forgetting optimizes sensorimotor transformation in redundant motor systems</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>(<issue>6</issue>):<fpage>e1002590</fpage>. PubMed Central PMCID: PMCPMC3386159. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002590" xlink:type="simple">10.1371/journal.pcbi.1002590</ext-link></comment> <object-id pub-id-type="pmid">22761568</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hardt</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nader</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nadel</surname> <given-names>L</given-names></name>. <article-title>Decay happens: the role of active forgetting in memory</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year>;<volume>17</volume>(<issue>3</issue>):<fpage>111</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2013.01.001" xlink:type="simple">10.1016/j.tics.2013.01.001</ext-link></comment> <object-id pub-id-type="pmid">23369831</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gutkin</surname> <given-names>B</given-names></name>. <article-title>Homeostatic reinforcement learning for integrating reward collection and physiological stability</article-title>. <source>Elife</source>. <year>2014</year>;<volume>3</volume>. PubMed Central PMCID: PMCPMC4270100. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.04811" xlink:type="simple">10.7554/eLife.04811</ext-link></comment> <object-id pub-id-type="pmid">25457346</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirkpatrick</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gelatt</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Vecchi</surname> <given-names>MP</given-names></name>. <article-title>Optimization by simulated annealing</article-title>. <source>Science</source>. <year>1983</year>;<volume>220</volume>(<issue>4598</issue>):<fpage>671</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.220.4598.671" xlink:type="simple">10.1126/science.220.4598.671</ext-link></comment> <object-id pub-id-type="pmid">17813860</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Metalearning and neuromodulation</article-title>. <source>Neural Netw</source>. <year>2002</year>;<volume>15</volume>(<issue>4–6</issue>):<fpage>495</fpage>–<lpage>506</lpage>. S0893-6080(02)00044-8 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(02)00044-8" xlink:type="simple">10.1016/S0893-6080(02)00044-8</ext-link></comment> <object-id pub-id-type="pmid">12371507</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamawaki</surname> <given-names>S</given-names></name>. <article-title>Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>(<issue>8</issue>):<fpage>887</fpage>–<lpage>93</lpage>. nn1279 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1279" xlink:type="simple">10.1038/nn1279</ext-link></comment> <object-id pub-id-type="pmid">15235607</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beeler</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Frazier</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Zhuang</surname> <given-names>X</given-names></name>. <article-title>Tonic dopamine modulates exploitation of reward learning</article-title>. <source>Front Behav Neurosci</source>. <year>2010</year>;<volume>4</volume>:<fpage>170</fpage>. PubMed Central PMCID: PMCPMC2991243. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnbeh.2010.00170" xlink:type="simple">10.3389/fnbeh.2010.00170</ext-link></comment> <object-id pub-id-type="pmid">21120145</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xiao</surname> <given-names>MY</given-names></name>, <name name-style="western"><surname>Niu</surname> <given-names>YP</given-names></name>, <name name-style="western"><surname>Wigström</surname> <given-names>H</given-names></name>. <article-title>Activity-dependent decay of early LTP revealed by dual EPSP recording in hippocampal slices from young rats</article-title>. <source>Eur J Neurosci</source>. <year>1996</year>;<volume>8</volume>(<issue>9</issue>):<fpage>1916</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.1996.tb01335.x" xlink:type="simple">10.1111/j.1460-9568.1996.tb01335.x</ext-link></comment> <object-id pub-id-type="pmid">8921282</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berry</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Cervantes-Sandoval</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Nicholas</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>RL</given-names></name>. <article-title>Dopamine is required for learning and forgetting in Drosophila</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>3</issue>):<fpage>530</fpage>–<lpage>42</lpage>. PubMed Central PMCID: PMCPMC4083655. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.007" xlink:type="simple">10.1016/j.neuron.2012.04.007</ext-link></comment> <object-id pub-id-type="pmid">22578504</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ingram</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Flanagan</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Context-dependent decay of motor memories during skill acquisition</article-title>. <source>Curr Biol</source>. <year>2013</year>;<volume>23</volume>(<issue>12</issue>):<fpage>1107</fpage>–<lpage>12</lpage>. PubMed Central PMCID: PMCPMC3688072. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2013.04.079" xlink:type="simple">10.1016/j.cub.2013.04.079</ext-link></comment> <object-id pub-id-type="pmid">23727092</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nader</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hardt</surname> <given-names>O</given-names></name>. <article-title>A single standard for memory: the case for reconsolidation</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year>;<volume>10</volume>(<issue>3</issue>):<fpage>224</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2590" xlink:type="simple">10.1038/nrn2590</ext-link></comment> <object-id pub-id-type="pmid">19229241</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Takahashi</surname> <given-names>YK</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Orbitofrontal cortex as a cognitive map of task space</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>2</issue>):<fpage>267</fpage>–<lpage>79</lpage>. PubMed Central PMCID: PMCPMC4001869. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.11.005" xlink:type="simple">10.1016/j.neuron.2013.11.005</ext-link></comment> <object-id pub-id-type="pmid">24462094</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Ludvig</surname> <given-names>EA</given-names></name>. <article-title>Time representation in reinforcement learning models of the basal ganglia</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>7</volume>:<fpage>194</fpage>. PubMed Central PMCID: PMCPMC3885823. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2013.00194" xlink:type="simple">10.3389/fncom.2013.00194</ext-link></comment> <object-id pub-id-type="pmid">24409138</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Pavlovian-instrumental interaction in 'observing behavior'</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>(<issue>9</issue>). PubMed Central PMCID: PMCPMC2936515. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000903" xlink:type="simple">10.1371/journal.pcbi.1000903</ext-link></comment> <object-id pub-id-type="pmid">20838580</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AC</given-names></name>. <article-title>Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective</article-title>. <source>Cognition</source>. <year>2009</year>;<volume>113</volume>(<issue>3</issue>):<fpage>262</fpage>–<lpage>80</lpage>. PubMed Central PMCID: PMCPMC2783353. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2008.08.011" xlink:type="simple">10.1016/j.cognition.2008.08.011</ext-link></comment> <object-id pub-id-type="pmid">18926527</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornstein</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Multiplicity of control in the basal ganglia: computational roles of striatal subregions</article-title>. <source>Curr Opin Neurobiol</source>. <year>2011</year>;<volume>21</volume>(<issue>3</issue>):<fpage>374</fpage>–<lpage>80</lpage>. S0959-4388(11)00036-5 [pii]; PubMed Central PMCID: PMCPMC3269306. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2011.02.009" xlink:type="simple">10.1016/j.conb.2011.02.009</ext-link></comment> <object-id pub-id-type="pmid">21429734</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Badre</surname> <given-names>D</given-names></name>. <article-title>Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis</article-title>. <source>Cereb Cortex</source>. <year>2012</year>;<volume>22</volume>(<issue>3</issue>):<fpage>509</fpage>–<lpage>26</lpage>. PubMed Central PMCID: PMCPMC3278315. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhr114" xlink:type="simple">10.1093/cercor/bhr114</ext-link></comment> <object-id pub-id-type="pmid">21693490</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>. <article-title>Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies</article-title>. <source>Front Behav Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>79</fpage>. PubMed Central PMCID: PMCPMC3506961. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnbeh.2012.00079" xlink:type="simple">10.3389/fnbeh.2012.00079</ext-link></comment> <object-id pub-id-type="pmid">23205006</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saddoris</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Cacciapaglia</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>. <article-title>Differential Dopamine Release Dynamics in the Nucleus Accumbens Core and Shell Reveal Complementary Signals for Error Prediction and Incentive Motivation</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>33</issue>):<fpage>11572</fpage>–<lpage>82</lpage>. PubMed Central PMCID: PMCPMC4540796. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2344-15.2015" xlink:type="simple">10.1523/JNEUROSCI.2344-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26290234</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>. <article-title>Parallel basal ganglia circuits for voluntary and automatic behaviour to reach rewards</article-title>. <source>Brain</source>. <year>2015</year>;<volume>138</volume>(<issue>Pt 7</issue>):<fpage>1776</fpage>–<lpage>800</lpage>. PubMed Central PMCID: PMCPMC4492412. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/awv134" xlink:type="simple">10.1093/brain/awv134</ext-link></comment> <object-id pub-id-type="pmid">25981958</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ko</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wanat</surname> <given-names>MJ</given-names></name>. <article-title>Phasic Dopamine Transmission Reflects Initiation Vigor and Exerted Effort in an Action- and Region-Specific Manner</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>7</issue>):<fpage>2202</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1279-15.2016" xlink:type="simple">10.1523/JNEUROSCI.1279-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26888930</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>NF</given-names></name>, <name name-style="western"><surname>Cameron</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Taliaferro</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <etal>et al</etal>. <article-title>Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target</article-title>. <source>Nat Neurosci</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4287" xlink:type="simple">10.1038/nn.4287</ext-link></comment> <object-id pub-id-type="pmid">27110917</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howe</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Dombeck</surname> <given-names>DA</given-names></name>. <article-title>Rapid signalling in distinct dopaminergic axons during locomotion and reward</article-title>. <source>Nature</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature18942" xlink:type="simple">10.1038/nature18942</ext-link></comment> <object-id pub-id-type="pmid">27398617</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jirsa</surname> <given-names>VK</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>(<issue>8</issue>):<fpage>e1000092</fpage>. PubMed Central PMCID: PMCPMC2519166. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000092" xlink:type="simple">10.1371/journal.pcbi.1000092</ext-link></comment> <object-id pub-id-type="pmid">18769680</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>. <article-title>Computational significance of transient dynamics in cortical networks</article-title>. <source>Eur J Neurosci</source>. <year>2008</year>;<volume>27</volume>(<issue>1</issue>):<fpage>217</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2007.05976.x" xlink:type="simple">10.1111/j.1460-9568.2007.05976.x</ext-link></comment> <object-id pub-id-type="pmid">18093174</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niyogi</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Wong-Lin</surname> <given-names>K</given-names></name>. <article-title>Dynamic excitatory and inhibitory gain modulation can produce flexible, robust and optimal decision-making</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>6</issue>):<fpage>e1003099</fpage>. PubMed Central PMCID: PMCPMC3694816. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003099" xlink:type="simple">10.1371/journal.pcbi.1003099</ext-link></comment> <object-id pub-id-type="pmid">23825935</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klampfl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Emergence of dynamic memory traces in cortical microcircuit models through STDP</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>28</issue>):<fpage>11515</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5044-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5044-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23843522</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedrich</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Goal-Directed Decision Making with Spiking Neurons</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>5</issue>):<fpage>1529</fpage>–<lpage>46</lpage>. PubMed Central PMCID: PMCPMC4737768. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2854-15.2016" xlink:type="simple">10.1523/JNEUROSCI.2854-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26843636</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ponzi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>J</given-names></name>. <article-title>Sequentially switching cell assemblies in random inhibitory networks of spiking neurons in the striatum</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>(<issue>17</issue>):<fpage>5894</fpage>–<lpage>911</lpage>. 30/17/5894 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5540-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5540-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20427650</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ponzi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>JR</given-names></name>. <article-title>Optimal balance of the striatal medium spiny neuron network</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>4</issue>):<fpage>e1002954</fpage>. PubMed Central PMCID: PMCPMC3623749. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002954" xlink:type="simple">10.1371/journal.pcbi.1002954</ext-link></comment> <object-id pub-id-type="pmid">23592954</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toledo-Suárez</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Duarte</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name>. <article-title>Liquid computing on and off the edge of chaos with a striatal microcircuit</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>130</fpage>. PubMed Central PMCID: PMCPMC4240071. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00130" xlink:type="simple">10.3389/fncom.2014.00130</ext-link></comment> <object-id pub-id-type="pmid">25484864</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Damodaran</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cressman</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Jedrzejewski-Szmek</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Blackwell</surname> <given-names>KT</given-names></name>. <article-title>Desynchronization of fast-spiking interneurons reduces β-band oscillations and imbalance in firing in the dopamine-depleted striatum</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>3</issue>):<fpage>1149</fpage>–<lpage>59</lpage>. PubMed Central PMCID: PMCPMC4300321. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3490-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3490-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25609629</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bahuguna</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Aertsen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kumar</surname> <given-names>A</given-names></name>. <article-title>Existence and control of Go/No-Go decision transition threshold in the striatum</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>4</issue>):<fpage>e1004233</fpage>. PubMed Central PMCID: PMCPMC4409064. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004233" xlink:type="simple">10.1371/journal.pcbi.1004233</ext-link></comment> <object-id pub-id-type="pmid">25910230</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gouvêa</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Monteiro</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Motiwala</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Soares</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Paton</surname> <given-names>JJ</given-names></name>. <article-title>Striatal dynamics explain duration judgments</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>. PubMed Central PMCID: PMCPMC4721960. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.11386" xlink:type="simple">10.7554/eLife.11386</ext-link></comment> <object-id pub-id-type="pmid">26641377</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Angulo-Garcia</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Berke</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Torcini</surname> <given-names>A</given-names></name>. <article-title>Cell Assembly Dynamics of Sparsely-Connected Inhibitory Networks: A Simple Model for the Collective Activity of Striatal Projection Neurons</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>2</issue>):<fpage>e1004778</fpage>. PubMed Central PMCID: PMCPMC4767417. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004778" xlink:type="simple">10.1371/journal.pcbi.1004778</ext-link></comment> <object-id pub-id-type="pmid">26915024</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joshua</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Adler</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Prut</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>. <article-title>Synchronization of midbrain dopaminergic neurons is enhanced by rewarding events</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>5</issue>):<fpage>695</fpage>–<lpage>704</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.04.026" xlink:type="simple">10.1016/j.neuron.2009.04.026</ext-link></comment> <object-id pub-id-type="pmid">19524528</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bar-Gad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>. <article-title>Information processing, dimensionality reduction and reinforcement learning in the basal ganglia</article-title>. <source>Prog Neurobiol</source>. <year>2003</year>;<volume>71</volume>(<issue>6</issue>):<fpage>439</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2003.12.001" xlink:type="simple">10.1016/j.pneurobio.2003.12.001</ext-link></comment> <object-id pub-id-type="pmid">15013228</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>KN</given-names></name>. <article-title>A physiologically plausible model of action selection and oscillatory activity in the basal ganglia</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>(<issue>50</issue>):<fpage>12921</fpage>–<lpage>42</lpage>. 26/50/12921 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3486-06.2006" xlink:type="simple">10.1523/JNEUROSCI.3486-06.2006</ext-link></comment> <object-id pub-id-type="pmid">17167083</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Samanta</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SJ</given-names></name>. <article-title>Hold your horses: impulsivity, deep brain stimulation, and medication in parkinsonism</article-title>. <source>Science</source>. <year>2007</year>;<volume>318</volume>(<issue>5854</issue>):<fpage>1309</fpage>–<lpage>12</lpage>. 1146157 [pii]. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1146157" xlink:type="simple">10.1126/science.1146157</ext-link></comment> <object-id pub-id-type="pmid">17962524</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>. <article-title>Dopaminergic Control of the Exploration-Exploitation Trade-Off via the Basal Ganglia</article-title>. <source>Front Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>9</fpage>. PubMed Central PMCID: PMCPMC3272648. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnins.2012.00009" xlink:type="simple">10.3389/fnins.2012.00009</ext-link></comment> <object-id pub-id-type="pmid">22347155</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berthet</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hellgren-Kotaleski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Action selection performance of a reconfigurable basal ganglia inspired model with Hebbian-Bayesian Go-NoGo connectivity</article-title>. <source>Front Behav Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>65</fpage>. PubMed Central PMCID: PMCPMC3462417. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnbeh.2012.00065" xlink:type="simple">10.3389/fnbeh.2012.00065</ext-link></comment> <object-id pub-id-type="pmid">23060764</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hsiao</surname> <given-names>PY</given-names></name>, <name name-style="western"><surname>Lo</surname> <given-names>CC</given-names></name>. <article-title>A plastic corticostriatal circuit model of adaptation in perceptual decision making</article-title>. <source>Front Comput Neurosci</source>. <year>2013</year>;<volume>7</volume>:<fpage>178</fpage>. PubMed Central PMCID: PMCPMC3857537. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2013.00178" xlink:type="simple">10.3389/fncom.2013.00178</ext-link></comment> <object-id pub-id-type="pmid">24339814</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref111"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroll</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hamker</surname> <given-names>FH</given-names></name>. <article-title>Computational models of basal-ganglia pathway functions: focus on functional neuroanatomy</article-title>. <source>Front Syst Neurosci</source>. <year>2013</year>;<volume>7</volume>:<fpage>122</fpage>. PubMed Central PMCID: PMCPMC3874581. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2013.00122" xlink:type="simple">10.3389/fnsys.2013.00122</ext-link></comment> <object-id pub-id-type="pmid">24416002</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Bar-Gad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Korngreen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>. <article-title>Basal ganglia: physiological, behavioral, and computational studies</article-title>. <source>Front Syst Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>150</fpage>. PubMed Central PMCID: PMCPMC4139593. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2014.00150" xlink:type="simple">10.3389/fnsys.2014.00150</ext-link></comment> <object-id pub-id-type="pmid">25191233</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mandali</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rengaswamy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chakravarthy</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>. <article-title>A spiking Basal Ganglia model of synchrony, exploration and decision making</article-title>. <source>Front Neurosci</source>. <year>2015</year>;<volume>9</volume>:<fpage>191</fpage>. PubMed Central PMCID: PMCPMC4444758. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnins.2015.00191" xlink:type="simple">10.3389/fnins.2015.00191</ext-link></comment> <object-id pub-id-type="pmid">26074761</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pavlides</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hogan</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>. <article-title>Computational Models Describing Possible Mechanisms for Generation of Excessive Beta Oscillations in Parkinson's Disease</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e1004609</fpage>. PubMed Central PMCID: PMCPMC4684204. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004609" xlink:type="simple">10.1371/journal.pcbi.1004609</ext-link></comment> <object-id pub-id-type="pmid">26683341</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref115"><label>115</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lobb</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Troyer</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Paladini</surname> <given-names>CA</given-names></name>. <article-title>Disinhibition bursting of dopaminergic neurons</article-title>. <source>Front Syst Neurosci</source>. <year>2011</year>;<volume>5</volume>:<fpage>25</fpage>. PubMed Central PMCID: PMCPMC3095811. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsys.2011.00025" xlink:type="simple">10.3389/fnsys.2011.00025</ext-link></comment> <object-id pub-id-type="pmid">21617731</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref116"><label>116</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oster</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Faure</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gutkin</surname> <given-names>BS</given-names></name>. <article-title>Mechanisms for multiple activity modes of VTA dopamine neurons</article-title>. <source>Front Comput Neurosci</source>. <year>2015</year>;<volume>9</volume>:<fpage>95</fpage>. PubMed Central PMCID: PMCPMC4516885. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2015.00095" xlink:type="simple">10.3389/fncom.2015.00095</ext-link></comment> <object-id pub-id-type="pmid">26283955</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref117"><label>117</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindskog</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wikström</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Blackwell</surname> <given-names>KT</given-names></name>, <name name-style="western"><surname>Kotaleski</surname> <given-names>JH</given-names></name>. <article-title>Transient calcium and dopamine increase PKA activity and DARPP-32 phosphorylation</article-title>. <source>PLoS Comput Biol</source>. <year>2006</year>;<volume>2</volume>(<issue>9</issue>):<fpage>e119</fpage>. PubMed Central PMCID: PMCPMC1562452. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0020119" xlink:type="simple">10.1371/journal.pcbi.0020119</ext-link></comment> <object-id pub-id-type="pmid">16965177</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref118"><label>118</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakano</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Doi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Yoshimoto</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>A kinetic model of dopamine- and calcium-dependent striatal synaptic plasticity</article-title>. <source>PLoS Comput Biol</source>. <year>2010</year>;<volume>6</volume>(<issue>2</issue>):<fpage>e1000670</fpage>. PubMed Central PMCID: PMCPMC2820521. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000670" xlink:type="simple">10.1371/journal.pcbi.1000670</ext-link></comment> <object-id pub-id-type="pmid">20169176</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref119"><label>119</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetzlaff</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kolodziejski</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Markelic</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>. <article-title>Time scales of memory, learning, and plasticity</article-title>. <source>Biol Cybern</source>. <year>2012</year>;<volume>106</volume>(<issue>11–12</issue>):<fpage>715</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-012-0529-z" xlink:type="simple">10.1007/s00422-012-0529-z</ext-link></comment> <object-id pub-id-type="pmid">23160712</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref120"><label>120</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hawes</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Gillani</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wallace</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Blackwell</surname> <given-names>KT</given-names></name>. <article-title>Signaling pathways involved in striatal synaptic plasticity are sensitive to temporal pattern and exhibit spatial specificity</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>3</issue>):<fpage>e1002953</fpage>. PubMed Central PMCID: PMCPMC3597530. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002953" xlink:type="simple">10.1371/journal.pcbi.1002953</ext-link></comment> <object-id pub-id-type="pmid">23516346</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref121"><label>121</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>Dopamine ramps are a consequence of reward prediction errors</article-title>. <source>Neural Comput</source>. <year>2014</year>;<volume>26</volume>(<issue>3</issue>):<fpage>467</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00559" xlink:type="simple">10.1162/NECO_a_00559</ext-link></comment> <object-id pub-id-type="pmid">24320851</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref122"><label>122</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>YQ</given-names></name>, <name name-style="western"><surname>Xue</surname> <given-names>YX</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>YY</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>FQ</given-names></name>, <name name-style="western"><surname>Xue</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>CM</given-names></name>, <etal>et al</etal>. <article-title>Inhibition of PKMzeta in nucleus accumbens core abolishes long-term drug reward memory</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>14</issue>):<fpage>5436</fpage>–<lpage>46</lpage>. PubMed Central PMCID: PMCPMC3150199. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5884-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5884-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21471379</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref123"><label>123</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shema</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Haramati</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ron</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hazvi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sacktor</surname> <given-names>TC</given-names></name>, <etal>et al</etal>. <article-title>Enhancement of consolidated long-term memory by overexpression of protein kinase Mzeta in the neocortex</article-title>. <source>Science</source>. <year>2011</year>;<volume>331</volume>(<issue>6021</issue>):<fpage>1207</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1200215" xlink:type="simple">10.1126/science.1200215</ext-link></comment> <object-id pub-id-type="pmid">21385716</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref124"><label>124</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frey</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Matthies</surname> <given-names>H</given-names></name>. <article-title>Dopaminergic antagonists prevent long-term maintenance of posttetanic LTP in the CA1 region of rat hippocampal slices</article-title>. <source>Brain Res</source>. <year>1990</year>;<volume>522</volume>(<issue>1</issue>):<fpage>69</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0006-8993(90)91578-5" xlink:type="simple">10.1016/0006-8993(90)91578-5</ext-link></comment> <object-id pub-id-type="pmid">1977494</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref125"><label>125</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Grace</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Duzel</surname> <given-names>E</given-names></name>. <article-title>A neoHebbian framework for episodic memory; role of dopamine-dependent late LTP</article-title>. <source>Trends Neurosci</source>. <year>2011</year>;<volume>34</volume>(<issue>10</issue>):<fpage>536</fpage>–<lpage>47</lpage>. PubMed Central PMCID: PMCPMC3183413. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2011.07.006" xlink:type="simple">10.1016/j.tins.2011.07.006</ext-link></comment> <object-id pub-id-type="pmid">21851992</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref126"><label>126</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rutledge</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Skandali</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>A computational and neural model of momentary subjective well-being</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>(<issue>33</issue>):<fpage>12252</fpage>–<lpage>7</lpage>. PubMed Central PMCID: PMCPMC4143018. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1407535111" xlink:type="simple">10.1073/pnas.1407535111</ext-link></comment> <object-id pub-id-type="pmid">25092308</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref127"><label>127</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rutledge</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Skandali</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dopaminergic Modulation of Decision Making and Subjective Well-Being</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>27</issue>):<fpage>9811</fpage>–<lpage>22</lpage>. PubMed Central PMCID: PMCPMC4495239. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0702-15.2015" xlink:type="simple">10.1523/JNEUROSCI.0702-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26156984</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref128"><label>128</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stauffer</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Dopamine prediction error responses integrate subjective value from different reward dimensions</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>(<issue>6</issue>):<fpage>2343</fpage>–<lpage>8</lpage>. PubMed Central PMCID: PMCPMC3926061. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1321596111" xlink:type="simple">10.1073/pnas.1321596111</ext-link></comment> <object-id pub-id-type="pmid">24453218</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref129"><label>129</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stauffer</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Lak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Dopamine reward prediction error responses reflect marginal utility</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>(<issue>21</issue>):<fpage>2491</fpage>–<lpage>500</lpage>. PubMed Central PMCID: PMCPMC4228052. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2014.08.064" xlink:type="simple">10.1016/j.cub.2014.08.064</ext-link></comment> <object-id pub-id-type="pmid">25283778</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref130"><label>130</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>. <article-title>Phasic dopamine signals: from subjective reward value to formal economic utility</article-title>. <source>Curr Opin Behav Sci</source>. <year>2015</year>;<volume>5</volume>:<fpage>147</fpage>–<lpage>54</lpage>. PubMed Central PMCID: PMCPMC4692271. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cobeha.2015.09.006" xlink:type="simple">10.1016/j.cobeha.2015.09.006</ext-link></comment> <object-id pub-id-type="pmid">26719853</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref131"><label>131</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pissadaki</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Bolam</surname> <given-names>JP</given-names></name>. <article-title>The energy cost of action potential propagation in dopamine neurons: clues to susceptibility in Parkinson's disease</article-title>. <source>Front Comput Neurosci</source>. <year>2013</year>;<volume>7</volume>:<fpage>13</fpage>. PubMed Central PMCID: PMCPMC3600574. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2013.00013" xlink:type="simple">10.3389/fncom.2013.00013</ext-link></comment> <object-id pub-id-type="pmid">23515615</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref132"><label>132</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bolam</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Pissadaki</surname> <given-names>EK</given-names></name>. <article-title>Living on the edge with too many mouths to feed: why dopamine neurons die</article-title>. <source>Mov Disord</source>. <year>2012</year>;<volume>27</volume>(<issue>12</issue>):<fpage>1478</fpage>–<lpage>83</lpage>. PubMed Central PMCID: PMCPMC3504389. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/mds.25135" xlink:type="simple">10.1002/mds.25135</ext-link></comment> <object-id pub-id-type="pmid">23008164</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref133"><label>133</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Le Bouc</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Degos</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Welter</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Vidailhet</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Computational Dissection of Dopamine Motor and Motivational Functions in Humans</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>(<issue>25</issue>):<fpage>6623</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3078-15.2016" xlink:type="simple">10.1523/JNEUROSCI.3078-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27335396</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref134"><label>134</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Cortical substrates for exploratory decisions in humans</article-title>. <source>Nature</source>. <year>2006</year>;<volume>441</volume>(<issue>7095</issue>):<fpage>876</fpage>–<lpage>9</lpage>. PubMed Central PMCID: PMCPMC2635947. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04766" xlink:type="simple">10.1038/nature04766</ext-link></comment> <object-id pub-id-type="pmid">16778890</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref135"><label>135</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Fiorillo</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Adaptive coding of reward value by dopamine neurons</article-title>. <source>Science</source>. <year>2005</year>;<volume>307</volume>(<issue>5715</issue>):<fpage>1642</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1105370" xlink:type="simple">10.1126/science.1105370</ext-link></comment> <object-id pub-id-type="pmid">15761155</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref136"><label>136</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerfen</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Surmeier</surname> <given-names>DJ</given-names></name>. <article-title>Modulation of Striatal Projection Systems by Dopamine</article-title>. <source>Annu Rev Neurosci</source>. <year>2011</year>;<volume>34</volume>:<fpage>441</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-061010-113641" xlink:type="simple">10.1146/annurev-neuro-061010-113641</ext-link></comment> <object-id pub-id-type="pmid">21469956</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref137"><label>137</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Phillips</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Stuber</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Heien</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Wightman</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Carelli</surname> <given-names>RM</given-names></name>. <article-title>Subsecond dopamine release promotes cocaine seeking</article-title>. <source>Nature</source>. <year>2003</year>;<volume>422</volume>(<issue>6932</issue>):<fpage>614</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature01476" xlink:type="simple">10.1038/nature01476</ext-link></comment> <object-id pub-id-type="pmid">12687000</object-id></mixed-citation></ref>
<ref id="pcbi.1005145.ref138"><label>138</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yttri</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Dudman</surname> <given-names>JT</given-names></name>. <article-title>Opponent and bidirectional control of movement velocity in the basal ganglia</article-title>. <source>Nature</source>. <year>2016</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature17639" xlink:type="simple">10.1038/nature17639</ext-link></comment> <object-id pub-id-type="pmid">27135927</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>