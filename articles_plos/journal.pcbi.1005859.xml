<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005859</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00637</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Magnetoencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Invariant recognition drives neural representations of action sequences</article-title>
<alt-title alt-title-type="running-head">Invariant recognition drives neural representations of action sequences</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9311-9171</contrib-id>
<name name-style="western">
<surname>Tacchetti</surname>
<given-names>Andrea</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Isik</surname>
<given-names>Leyla</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Poggio</surname>
<given-names>Tomaso</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA, United States</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Berniker</surname>
<given-names>Max</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Northwestern University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">atacchet@mit.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>18</day>
<month>12</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>12</issue>
<elocation-id>e1005859</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>4</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Tacchetti et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005859"/>
<abstract>
<p>Recognizing the actions of others from visual stimuli is a crucial aspect of human perception that allows individuals to respond to social cues. Humans are able to discriminate between similar actions despite transformations, like changes in viewpoint or actor, that substantially alter the visual appearance of a scene. This ability to generalize across complex transformations is a hallmark of human visual intelligence. Advances in understanding action recognition at the neural level have not always translated into precise accounts of the computational principles underlying what representations of action sequences are constructed by human visual cortex. Here we test the hypothesis that invariant action discrimination might fill this gap. Recently, the study of artificial systems for static object perception has produced models, Convolutional Neural Networks (CNNs), that achieve human level performance in complex discriminative tasks. Within this class, architectures that better support invariant object recognition also produce image representations that better match those implied by human and primate neural data. However, whether these models produce representations of action sequences that support recognition across complex transformations and closely follow neural representations of actions remains unknown. Here we show that spatiotemporal CNNs accurately categorize video stimuli into action classes, and that deliberate model modifications that improve performance on an invariant action recognition task lead to data representations that better match human neural recordings. Our results support our hypothesis that performance on invariant discrimination dictates the neural representations of actions computed in the brain. These results broaden the scope of the invariant recognition framework for understanding visual intelligence from perception of inanimate objects and faces in static images to the study of human perception of action sequences.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Recognizing the actions of others from video sequences across changes in viewpoint, gait or illumination is a hallmark of human visual intelligence. A large number of studies have highlighted which areas in the human brain are involved in the processing of biological motion, while others have described how single neurons behave in response to videos of human actions. However, little is known about the computational necessities that shaped these neural mechanisms either through evolution or experience. In this paper, we test the hypothesis that this computational goal is the discrimination of action categories from complex video stimuli and across identity-preserving transformations. We show that, within the class of Spatiotemporal Convolutional Neural Networks (ST-CNN), deliberate model modifications leading to representations of videos that better support robust action discrimination, also produce representations that better match human neural data. Importantly, increasing model performance on invariant action recognition leads to a better match with human neural data, despite the model never being exposed to such data. These results suggest that, similarly to what is known for object recognition, supporting invariant discrimination within the constraints of hierarchical ST-CNN architectures drives the neural mechanisms underlying our ability to perceive the actions of others.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>CCF-1231216</award-id>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100007065</institution-id>
<institution>Nvidia</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9311-9171</contrib-id>
<name name-style="western">
<surname>Tacchetti</surname>
<given-names>Andrea</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Martinos Imaging Center at MIT</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9311-9171</contrib-id>
<name name-style="western">
<surname>Tacchetti</surname>
<given-names>Andrea</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. We are grateful to the Martinos Imaging Center at MIT, where the neural recordings used in this work were acquired and to the McGovern Institute for Brain Research at MIT for supporting this research. Additional support was provided by the Eugene McDermott Foundation (TP). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-01-02</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The video sequences used in Experiment 1, 2 and 3 are available for download from The Center for Brains, Minds and Machines: <ext-link ext-link-type="uri" xlink:href="http://cbmm.mit.edu" xlink:type="simple">cbmm.mit.edu</ext-link>. The Magnetoencephalography recordings used for our Comparison of model representations and neural recordings are available for download from The Center for Brains, Minds and Machines: cbmm.mit.edu.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Humans’ ability to recognize the actions of others is a crucial aspect of visual perception. Remarkably, the accuracy with which we can finely discern what others are doing is largely unaffected by transformations that substantially change the visual appearance of a given scene, but do not change the semantics of what we observe (e.g. a change in viewpoint). Recognizing actions, the middle ground between action primitives and activities [<xref ref-type="bibr" rid="pcbi.1005859.ref001">1</xref>], across these transformations is a hallmark of human visual intelligence, which has proven difficult to replicate in artificial systems. Because of this, invariance to transformations that are orthogonal to a learning task has been the subject of extensive theoretical and empirical investigation in both artificial and biological perception [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref003">3</xref>].</p>
<p>Over the past few decades, artificial systems for action processing have received considerable attention. These methods can be divided into global and local approaches. Some space-time global approaches rely on fitting the present scene to a joint-based model of human bodies, actions are then described as sequences of joint configurations over time [<xref ref-type="bibr" rid="pcbi.1005859.ref004">4</xref>]. Other global methods use descriptors that are computed using the entire input video at once [<xref ref-type="bibr" rid="pcbi.1005859.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref007">7</xref>]. Local approaches, on the other hand, extract information from video sequences in a bottom-up fashion, by detecting, in their input video, the presence of features that are local in space and time. These local descriptors are then combined, following a hierarchical architecture, to construct more complex representations [<xref ref-type="bibr" rid="pcbi.1005859.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>]. A specific class of bottom up, local architectures, spatial-temporal Convolutional Neural Networks (ST-CNNs), as well as their recursive extensions [<xref ref-type="bibr" rid="pcbi.1005859.ref011">11</xref>], are currently the best performing models on action recognition tasks.</p>
<p>Alongside these computational advances, recent studies have furthered our understanding of the neural basis of action perception. Broadly, the neural computations underlying action recognition in visual cortex are organized as a hierarchical succession of spatiotemporal feature detectors of increasing size and complexity [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref012">12</xref>]. In addition, other studies have highlighted of which specific brain areas are involved in the processing of biological motion and actions. In humans and other primates, the Superior Temporal Sulcus, and particularly its posterior portion, is believed to participate in the processing of biological motion and actions [<xref ref-type="bibr" rid="pcbi.1005859.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref020">20</xref>]. In addition to studying which brain regions engage during action processing, a number of studies have characterized the responses of individual neurons. The preferred stimuli of neurons in visual areas V1 and MT are well approximated by moving edge-detection filters and energy-based pooling mechanisms [<xref ref-type="bibr" rid="pcbi.1005859.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref022">22</xref>]. Neurons in the STS region of macaque monkeys respond selectively to actions, are invariant to changes in actors and viewpoint [<xref ref-type="bibr" rid="pcbi.1005859.ref023">23</xref>] and their tuning curves are well modeled by simple snippet-matching models [<xref ref-type="bibr" rid="pcbi.1005859.ref024">24</xref>]. Finally, mirror neurons, cells that exhibit strong responses when subjects are both observing and performing goal directed actions, have been carefully described in recent years [<xref ref-type="bibr" rid="pcbi.1005859.ref025">25</xref>].</p>
<p>Despite the characterization of the regional and single-unit responses that are involved in constructing neural representations of action sequences, little information is available on what computational tasks might be relevant to explaining and recapitulating how these representations are organized, and in particular which robustness properties are present. The idea of visual representations, internal encodings of incoming stimuli that are useful to the viewer, has a long history in the study of human perception and, since its inception, has provided a powerful tool to link neurophysiology and brain imaging data to more abstract computational concepts like recognition or detection [<xref ref-type="bibr" rid="pcbi.1005859.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref028">28</xref>]. Fueled by advances in computer vision methods for object and scene categorization, recent studies have made progress towards linking neural recordings to computational concepts through quantitatively accurate models of single neurons and entire brain regions. Interestingly, these studies have highlighted a correlation between performance optimization on discriminative object recognition tasks and the accuracy of neural predictions both at the single recording site and neural representation level [<xref ref-type="bibr" rid="pcbi.1005859.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref032">32</xref>]. However, these results have not been extended to action perception and dynamic stimuli.</p>
<p>Here we take advantage of recent advances in artificial systems for action processing to test the hypothesis that invariant recognition drives the representations of action sequences computed by visual cortex. We do so by comparing representations obtained with biologically plausible artificial systems and those measured in human subjects through Magnetoencephalography (MEG) recordings [<xref ref-type="bibr" rid="pcbi.1005859.ref033">33</xref>]. In this paper we show that, within the Spatiotemporal Convolutional Neural Networks model class [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref035">35</xref>], deliberate modifications that result in better performing models on invariant action recognition, also lead to empirical dissimilarity matrices that better match those obtained with human neural recordings. Our results suggest that discriminative tasks, and especially those that require generalization across complex transformations, alongside the constraints imposed by the hierarchical organization of visual processing in human cortex, determined which representations of action sequences are computed by visual cortex. Importantly, we quantify the degree of overlap between neural and artificial representations using Representational Similarity Analysis [<xref ref-type="bibr" rid="pcbi.1005859.ref032">32</xref>]. This measure of agreement between two encodings, does not rely on a one-to-one mapping between neural signal sources and their artificial counterpart, but rather, exploits similarity structures directly in the representation spaces to establish a measure of consensus. Moreover, by highlighting the role of robustness to nuisances that are orthogonal to the discrimination task, our results extend the scope of invariant recognition as a computational framework for understanding human visual intelligence to the study of action recognition from video sequences.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Action discrimination with Spatiotemporal Convolutional representations</title>
<p>We filmed a video dataset showing five actors, performing five actions (drink, eat, jump, run and walk) at five different viewpoints (<xref ref-type="fig" rid="pcbi.1005859.g001">Fig 1</xref>). We then developed four variants of feedforward hierarchical models of visual cortex and used them to extract feature representations of videos showing two different viewpoints, frontal and side. Subsequently, we trained a machine learning classifier to discriminate video sequences into different action classes based on each model’s output. We then evaluated the classifier’s accuracy in predicting the action content of new, unseen videos.</p>
<fig id="pcbi.1005859.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Action recognition stimulus set.</title>
<p>Sample frames from action recognition dataset consisting of 2s video clips depicting five actors performing five actions (top row: drink, eat, jump, run and walk). Actions were recorded at five different viewpoints (bottom row: 0-frontal, 45, 90-side, 135 and 180 degrees with respect to the normal to the focal plane), they were all performed on a treadmill and actors held a water bottle and an apple in their hand regardless of the action they performed in order to minimize low-level object/action confounds. Actors were centered in the frame and the background was held constant regardless of viewpoint. The authors who collected the videos identified themselves and the purpose of the videos to the people being video recorded. The individuals agreed to have their videos taken and potentially published.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g001" xlink:type="simple"/>
</fig>
<p>The four models we developed to extract representations of action sequences from videos were instances of Spatiotemporal Convolutional Neural Networks (ST-CNNs), currently the best performing artificial perception systems for action recognition [<xref ref-type="bibr" rid="pcbi.1005859.ref034">34</xref>] and were specifically designed to exhibit a varying degree of performance on invariant action recognition tasks. ST-CNN architectures are direct extensions of the Convolutional Neural Networks used to recognize objects or faces in static images [<xref ref-type="bibr" rid="pcbi.1005859.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref036">36</xref>], to input stimuli that extend both in space and time. ST-CNNs are hierarchical models that build selectivity to specific stimuli through template matching operations and robustness to transformations through pooling operations (<xref ref-type="fig" rid="pcbi.1005859.g002">Fig 2</xref>). Qualitatively, Spatiotemporal Convolutional Neural Networks detect the presence of a certain video segment (a template) in their input stimulus; detections for various templates are then aggregated, following a hierarchical architecture, to construct video representations. Nuisances that should not be reflected in the model’s output, like changes in position, are discarded through the pooling mechanism [<xref ref-type="bibr" rid="pcbi.1005859.ref026">26</xref>].</p>
<fig id="pcbi.1005859.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Spatiotemporal Convolutional Neural Networks.</title>
<p>Schematic overview of the class of models we used: Spatiotemporal Convolutional Neural Networks (ST-CNNs). ST-CNNs are hierarchical feature extraction architectures. Input videos go through layers of computation and the output of each layer serves as input to the next layer. The output of the last layer constitutes the video representation used in downstream tasks. The models we considered consisted of two convolutional-pooling layers’ pairs, denoted as Conv1, Pool1, Conv2 and Pool2. Convolutional layers performed template matching with a shared set of templates at all positions in space and time (spatiotemporal convolution), and pooling layers increased robustness through max-pooling operations. Convolutional layers’ templates can be either fixed a priori, sampled or learned. In this example, templates in the first layer Conv1 are fixed and depict moving Gabor-like receptive fields, while templates in the second simple layer Conv2 are sampled from a set of videos containing actions and filmed at different viewpoints. The authors who collected the videos identified themselves and the purpose of the videos to the people being video recorded. The individuals agreed to have their videos taken and potentially published.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g002" xlink:type="simple"/>
</fig>
<p>We considered a basic, purely convolutional model, and subsequently introduced modifications to its pooling mechanism and template learning rule to improve performance on invariant action recognition [<xref ref-type="bibr" rid="pcbi.1005859.ref036">36</xref>]. The first, purely convolutional model, consisted of convolutional layers with fixed templates, interleaved by pooling layers that computed max-operations across contiguous regions of space. In particular, templates in the first convolutional layer contained moving Gabor filters, while templates in the second convolutional layer were sampled from a set of action sequences collected at various viewpoints. The second, Unstructured Pooling model, allowed pooling units in the last layer to span random sets of templates as well as contiguous space regions (<xref ref-type="fig" rid="pcbi.1005859.g003">Fig 3B</xref>). The third, Structured Pooling model, allowed pooling over contiguous regions of space as well as across templates depicting the same action at various viewpoints. The 3D orientation of each template was discarded through this pooling mechanism, similarly to how position in space is discarded in traditional CNNs (<xref ref-type="fig" rid="pcbi.1005859.g003">Fig 3A</xref>) [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref037">37</xref>]. The fourth and final model employed backpropagation, a gradient based optimization method, to learn convolutional layers’ templates by iteratively maximizing performance on an action recognition task [<xref ref-type="bibr" rid="pcbi.1005859.ref036">36</xref>].</p>
<fig id="pcbi.1005859.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Structured and unstructured pooling.</title>
<p>We introduced modifications to the basic ST-CNN to increase robustness to changes in 3D-viewpoint. Qualitatively Spatiotemporal Convolutional Neural Networks detect the presence of a certain video segment (a template) in their input stimulus. The 3D orientation of this template is discarded by the pooling mechanism in our structured pooling model, analogous to how position in space is discarded in a traditional CNN. a) In models with Structured Pooling (model 3, in the main text), the template set for Conv2 layer cells was sampled from a set of videos containing four actors performing five actions at five different viewpoints (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>). All templates sampled from videos of a specific actor and performing a specific action were pooled together by one Pool2 layer unit. b) Models employing Unstructured Pooling (model 2, in the main text) allowed Pool2 cells to pool over the entire spatial extent of their input as well as across channels. These models used the exact same templates employed by models relying on Structured Pooling and matched these models in the number of templates wired to a pooling unit. However, the assignment of templates to pooling was randomized (uniform without replacement) and did not reflect any semantic structure. The authors who collected the videos identified themselves and the purpose of the videos to the people being video recorded. The individuals agreed to have their videos taken and potentially published.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g003" xlink:type="simple"/>
</fig>
<p>The basic, purely convolutional model we used as a starting point has been shown to be a reliable model of biological motion processing in human visual cortex [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref012">12</xref>]. The modifications we introduced aimed to improve its performance on a challenging invariant action recognition task. In particular, structured and unstructured template pooling mechanisms have been analyzed and theoretically motivated in recent years [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref003">3</xref>]. Moreover, these pooling mechanisms have successfully applied to robust face and object recognition [<xref ref-type="bibr" rid="pcbi.1005859.ref037">37</xref>]. Finally, backpropagation, the gradient based optimization method used to construct the last model, is widely used in computer vision systems [<xref ref-type="bibr" rid="pcbi.1005859.ref036">36</xref>], and recently it has been applied to vision science [<xref ref-type="bibr" rid="pcbi.1005859.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref031">31</xref>]. While prima facie this method might not be relevant to brain science (see <xref ref-type="sec" rid="sec007">Discussion</xref>), we found here, that the representations obtained with this technique better match human brain data.</p>
<p>We used these models to recognize actions in video sequences in a simple three-steps experimental procedure: first we constructed feedforward hierarchical architectures and used them to extract feature representations of a number of video sequences. We then trained a machine learning classifier to predict the action label of a sequence based on each feature representation. Finally, we quantified the performance of the classifier by measuring prediction accuracy on a set of new unseen videos. The procedure just outlined was performed using three separate subsets of the video dataset described above, one for each step. In particular, constructing spatiotemporal convolutional models requires access to video sequences to sample, or learn, convolutional layers’ templates. The subset of videos used for this particular purpose was called the <bold>embedding set</bold>. Likewise, training and testing a classifier requires access to model responses extracted from action sequences; the videos used in these two steps were organized in a <bold>training set</bold> and a <bold>test set</bold>. There was never any overlap between the <bold>test set</bold> and the union of <bold>training</bold> and <bold>embedding set</bold>.</p>
<p>Specifically, we sought to evaluate the four models based on how well they could support discrimination between the five actions in our video dataset both across and within changes in viewpoint. To this end, in Experiment 1, we trained and tested the classifier using model features extracted from videos captured at the same viewpoint while in Experiment 2, we trained and tested the classifier using model features computed from videos at mismatching viewpoints (e.g. if the classifier was trained using videos captured at the frontal viewpoint, then testing would be conducted using videos at the side viewpoint).</p>
<sec id="sec004">
<title>Experiment 1: Action discrimination–viewpoint match condition</title>
<p>In Experiment 1, we trained and tested the action classifier using feature representations of videos acquired at the same viewpoint, and therefore did not investigate robustness to changes in viewpoint. In this case, the <bold>embedding set</bold> contained videos showing all five actions performed at all five viewpoints by four of the five actors. The <bold>training set</bold> was a subset of the <bold>embedding set</bold> and contained all videos at either the frontal or the side viewpoint. Finally, the <bold>test set</bold> contained videos of all five actions performed by the fifth, left-out actor, and performed at the viewpoint matching that shown in the <bold>training set.</bold> All models produced representations that successfully classified videos based on the action they depicted (<xref ref-type="fig" rid="pcbi.1005859.g004">Fig 4</xref>). We observed a significant difference in performance between model 4, the end-to-end trainable model, and fixed template models 1, 2 and 3 (see <xref ref-type="sec" rid="sec008">Methods</xref> Section). However, the task considered in Experiment 1 was not sufficient to rank the four types of ST-CNN models.</p>
<fig id="pcbi.1005859.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Action recognition: Viewpoint match condition.</title>
<p>We trained a supervised machine learning classifier to discriminate videos based on their action content by using the feature representation computed by each of the Spatiotemporal Convolutional Neural Network models we considered. This figure shows the prediction accuracy of a machine learning classifier trained and tested using videos recorded at the same viewpoint. The classifier was trained on videos depicting four actors performing five actions at either the frontal or side view. The machine learning classifier accuracy was then assessed using new, unseen videos of a new, unseen actor performing those same five actions. No generalization across changes in 3D viewpoints was required of the feature extraction and classification system. Here we report the mean and standard error of the classification accuracy over the five possible choices of test actor. Models with learned templates outperform models with fixed templates significantly on this task. Chance is 1/5 and is indicated by a horizontal line. Horizontal lines at the top indicate significant difference between two conditions (p &lt; 0.05) based on group ANOVA or Bonferroni corrected paired t-test (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref> section).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Experiment 2: Action discrimination–viewpoint mismatch condition</title>
<p>The four ST-CNN models we developed were designed to have varying degrees of tolerance to changes in viewpoint. In Experiment 2, we investigated how well these model representations could support learning to discriminate video sequences based on their action content, across changes in viewpoint. The general experimental procedure was identical to the one outlined for Experiment 1 and used the exact same models. In this case however, we used features extracted from videos acquired at mismatching viewpoints for training and testing (e.g., a classifier trained using videos captured at the frontal viewpoint, would be tested on videos at the side viewpoint). We focused exclusively on to views: 0 and 90 degree with respect to frontal, to test the same extreme case of generalization across changes in viewpoint (training on a single view that is non-adjacent and non-mirror-symmetric to the test view) as used for the MEG experiments (see Experiment 3 and <xref ref-type="sec" rid="sec008">Materials and Methods</xref>). All the models we considered produced representations that were, at least to a minimal degree, useful to discriminate actions invariantly to changes in viewpoint (<xref ref-type="fig" rid="pcbi.1005859.g005">Fig 5</xref>). Unlike what we observed in Experiment 1, it was possible to rank the models we considered based on performance on this task. This was expected, since the various architectures were designed to exhibit various degrees of robustness to changes in viewpoint (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>). The end-to-end trainable models (model 4) performed better than models 1,2 and 3, which used fixed templates, on this task. Within the fixed templates models group, as expected, models that employed a Structured Channel Pooling mechanism to increase robustness performed best [<xref ref-type="bibr" rid="pcbi.1005859.ref038">38</xref>].</p>
<fig id="pcbi.1005859.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Action recognition: Viewpoint mismatch condition.</title>
<p>This figure shows the prediction accuracy of a machine learning classifier trained and tested using feature representations of videos at opposed viewpoints. Hierarchical models were constructed using convolutional templates sampled or learned from videos showing all five viewpoints. During the training and testing of the classifier however, mismatching viewpoints were used. When the classifier was trained using videos at, say, the frontal viewpoint, its accuracy in discriminating new, unseen videos would be established using videos recorded at the side viewpoint. Here we report the mean and standard error of the classification accuracy over the five possible choices of test actor. Models with learned templates resulted in significantly higher accuracy in this task. Among models with fixed templates, Spatiotemporal Convolutional Neural Networks employing Structured pooling outperformed both purely convolutional and Unstructured Pooling models. Chance is 1/5 indicated with horizontal line. Horizontal lines at the top indicate significant difference between two conditions (p &lt; 0.05) based on group ANOVA or Bonferroni corrected paired t-test (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g005" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec006">
<title>Comparison of model representations and neural recordings</title>
<p>We used Representational Similarity Analysis (RSA) to assess how well each model feature representation, as well as an ideal categorical oracle, matched human neural data. RSA produces a measure of agreement between artificial models and brain recordings based on the correlation between empirical dissimilarity matrices constructed using either the model representation of a set of stimuli, or recordings of the neural responses these stimuli elicit (<xref ref-type="fig" rid="pcbi.1005859.g006">Fig 6</xref>) [<xref ref-type="bibr" rid="pcbi.1005859.ref032">32</xref>]. We used video feature representations extracted by each model from a set of new, unseen stimuli to construct model dissimilarity matrices. We also constructed dissimilarity matrices using Magnetoencephalograpy (MEG) data from the average of eight subjects viewing the same action video clips. The MEG data consisted of magnetometer and gradiometer recordings from 306 sensors, averaged over a 100ms window centered at the time when action identity was best decoded from these data in a separate experiment [<xref ref-type="bibr" rid="pcbi.1005859.ref033">33</xref>] (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>). Finally, we constructed a dissimilarity matrix using an action categorical oracle, a simulated ideal observer able to perfectly classify video sequences based on their action content. In this case, the dissimilarity between videos of the same action was zero and the distance across actions was one.</p>
<fig id="pcbi.1005859.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Feature representation empirical dissimilarity matrices.</title>
<p>We used feature representations, extracted with the four Spatiotemporal Convolutional Neural Network models, from 50 videos depicting five actors performing five actions at two different viewpoints, frontal and side. Moreover, we obtained Magnetoencephalography (MEG) recordings of human subjects’ brain activity while they were watching these same videos, and used these recordings as a proxy for the neural representation of these videos. These videos were not used to construct or learn any of the models. For each of the six representations of each video (four artificial models, a categorical oracle and one neural recordings) we constructed an empirical dissimilarity matrix using linear correlation and normalized it between 0 and 1. Empirical dissimilarity matrices on the same set of stimuli constructed with video representations from a) Model 1: Purely Convolutional model, b) Model 2: Unstructured pooling model, c) Model 3: Structured pooling model d) Model 4: Learned templates model e) Categorical oracle and f) Magnetoencephalography brain recordings.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g006" xlink:type="simple"/>
</fig>
<p>We observed that end-to-end trainable models (model 4) produced dissimilarity structures that better agreed with those constructed from neural data than models with fixed templates (<xref ref-type="fig" rid="pcbi.1005859.g007">Fig 7</xref>). Within models with fixed templates, model 3, constructed using a Structured Pooling mechanism to build invariance to changes in viewpoint, produced representations that agree better with the neural data than models employing Unstructured Pooling (model 2) and purely convolutional models (model 1). The category oracle did not match the MEG data as well as the highest performing models (models 3 and 4), suggesting that improving performance on the action recognition task does not trivially improve matching with the neural data.</p>
<fig id="pcbi.1005859.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005859.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Representational Similarity Analysis between model representations and human neural data.</title>
<p>We computed the Spearman Correlation Coefficient (SCC) between the lower triangular portion of the dissimilarity matrix constructed with each of the artificial models we considered and the dissimilarity matrix constructed with neural data (shown and described in <xref ref-type="fig" rid="pcbi.1005859.g006">Fig 6</xref>). We assessed the uncertainty of this measure by resampling the rows and columns of the matrices we constructed. In order to give the SCC score a meaningful interpretation we reported here a normalized score: the SCC is normalized so that the noise ceiling is 1 and the noise floor is 0. The noise ceiling was assessed by computing the SCC between each individual human subjects’ dissimilarity matrix and the average dissimilarity matrix over the rest of the subjects. The noise floor was computed by assessing the SCC between the lower portion of the dissimilarity matrix constructed using each of the model representation and a scrambled version of the neural dissimilarity matrix. This normalization embeds the intuition that we cannot expect artificial representations to match human data better than an individual human subject’s data matches the mean of other humans and that we should only be concerned care with how much better the models we considered are, on this scale, than a random guess. Models with learned templates agree with the neural data significantly better than models with fixed templates. Among these, models with Structured Pooling outperform both purely Convolutional and Unstructured models. Horizontal lines at the top indicate significant difference between two conditions (p &lt; 0.05) based on group ANOVA or Bonferroni corrected paired t-test (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>We have shown that, within the Spatiotemporal Convolutional Neural Networks model class and across a deliberate set of model modifications, feature representations that are more useful to discriminate actions in video sequences in a manner that is robust to changes in viewpoint, produce empirical dissimilarity structures that are more similar to those constructed using human neural data. These results support our hypothesis that performance on invariant discriminative tasks drives the neural representations of actions that are computed by our visual cortex. Moreover, dissimilarity matrices constructed with ST-CNNs representations match those built with neural data better than a purely categorical dissimilarity matrix. This highlights the importance of both the computational task and the architectural constraints, described in previous accounts of the neural processing of action and motions, to build quantitatively accurate models of neural data representations [<xref ref-type="bibr" rid="pcbi.1005859.ref039">39</xref>]. Our findings are in agreement with what has been reported for the perception of objects from static images, both at the single recording site and at the whole brain level [<xref ref-type="bibr" rid="pcbi.1005859.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref031">31</xref>], and identify a computational task that explains and recapitulates the properties of the representations of human action in visual cortex.</p>
<p>We developed the four ST-CNN models using deliberate modifications to improve the models’ feature representations to invariant action recognition. In so doing, we verified that structured pooling architectures and memory based learning (model 3), as previously described and theoretically motivated [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref003">3</xref>], can be applied to build representations of video sequences that support recognition invariant to complex, non-affine transformations. However, empirically, we found that learning model templates using gradient based methods and a fully supervised action recognition task (model 4), led to better results, both in terms of classification accuracy and agreement with neural recordings [<xref ref-type="bibr" rid="pcbi.1005859.ref031">31</xref>].</p>
<p>The five actions in our dataset were selected to be highly familiar, include both goal-directed hand-arm movements and whole body movements, and span coarse (run vs. eat) as well as fine (drink vs. eat) action discriminations. While the five actions we considered are far from exhaustive, they allow us rank the performance of our four different models on invariant action recognition. Importantly, we show that our top-performing models capture non-trivial aspects of the neural representations of these actions, as shown by the fact that the ST-CNN models match MEG data better than a categorical oracle.</p>
<p>A limitation of the methods used here is that the extent of the match between a model representation and the neural data is appraised solely based on the correlation between the empirical dissimilarity structures constructed with neural recordings and model representations. This relatively abstract comparison provides no guidance in establishing a one-to-one mapping between model units and brain regions or sub-regions and therefore cannot exclude models on the basis of biological implausibility [<xref ref-type="bibr" rid="pcbi.1005859.ref030">30</xref>]. In this work, we mitigated this limitation by constraining the model class to reflect previous accounts of neural computational units and mechanisms that are involved in the perception of motion [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref040">40</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref041">41</xref>].</p>
<p>Furthermore, the class of models we developed in our experiments is purely feedforward, however, the neural recordings were maximally action discriminative 470ms after stimulus onset. This late in the visual processing, it is likely that feedback signals are among the energy sources captured by the recordings. These signals are not accounted for in our models. We provide evidence that adding a feedback mechanism, through recursion, does not improve recognition performance nor correlation with the neural data (<xref ref-type="supplementary-material" rid="pcbi.1005859.s001">S1 Fig</xref>). We cannot, however, exclude that this is due to the stimuli and discrimination task we designed, which only considered pre-segmented, relatively short action sequences.</p>
<p>Recognizing the actions of others from complex visual stimuli is a crucial aspect of human perception. We investigated the relevance of invariant action discrimination to improving model representations’ agreement with neural recordings and showed that it is one of the computational principles shaping the representation of human action sequences human visual cortex evolved, or learned to compute. Our deliberate approach to model design underlined the relevance of both supervised, gradient based, performance optimization methods and memory based, structured pooling methods to the modeling of neural data representations. While memory-based learning and structured pooling have been investigated extensively as a biologically plausible learning algorithms [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref042">42</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref043">43</xref>], if and how primate visual cortex could implement gradient based optimization or acquire the necessary supervision remains, despite recent efforts, an unsettled matter [<xref ref-type="bibr" rid="pcbi.1005859.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref046">46</xref>]. Irrespective of the precise biological mechanisms that could carry out performance optimization on invariant discriminative tasks, computational studies point to its relevance to understanding neural representations of visual scenes [<xref ref-type="bibr" rid="pcbi.1005859.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref031">31</xref>]. Recognizing the semantic category of visual stimuli across photometric, geometric or more complex changes, in very low sample regimes is a hallmark of human visual intelligence. By building data representations that support this kind of robust recognition, we have shown here, one obtains empirical dissimilarity structures that match those constructed using human neural data. In the wider context of the study of perception, our results strengthen the claim that the computational goal of human visual cortex is to support invariant recognition by broadening it to the study of action perception.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec009">
<title>Ethics statement</title>
<p>The MIT Committee on the Use of Humans as Experimental Subjects approved the experimental protocol. Subjects provided informed written consent before the experiment. Approval number: 0403000026.</p>
</sec>
<sec id="sec010">
<title>Action recognition dataset</title>
<p>We collected a dataset of five actors performing five actions (drink, eat, jump, run and walk) on a treadmill at five different viewpoints (0, 45, 90, 135 and 180 degrees between the line across the center of the treadmill and the line normal to the focal plane of the video-camera). We rotated the treadmill rather than the camera to keep the background constant across changes in viewpoint (<xref ref-type="fig" rid="pcbi.1005859.g001">Fig 1</xref>). The actors were instructed to hold an apple and a bottle in their hand regardless of the action they were performing, so that objects and background would not differ between actions. Each action/actor/view was filmed for at least 52s. Subsequently the original videos were cut into 26 clips, each 2s long resulting in a dataset of 3,250 video clips. Video clips started at random points in the action cycle (for example a jump might start mid-air or before the actor’s feet left the ground) and each 2s clip contained a full action cycle. The authors manually identified one single spatial bounding box that contained the entire body of each actor and cropped all videos according to this bounding box. The authors who collected the videos identified themselves and the purpose of the videos to the people being video recorded. The individuals agreed to have their videos taken and potentially published.</p>
</sec>
<sec id="sec011">
<title>Recognizing actions with spatiotemporal convolutional representations</title>
<sec id="sec012">
<title>General experimental procedure</title>
<p>Experiment 1 and Experiment 2 were designed to quantify the amount of action information extracted from video sequences by four computational models of primate visual cortex. In Experiment 1, we tested basic action recognition. In Experiment 2, in particular, we further quantified whether this action information could support action recognition robustly to changes in viewpoint. The motivating idea behind our design is that, if a machine learning classifier is able to discriminate unseen video sequences based on their action content, using the output of a computational model, then this model representation contains some action information. Moreover, if the classifier is able to discriminate videos based on action at new, unseen viewpoints, using model outputs then it must be that these model representations not only carry action information, but that changes in viewpoint are not reflected in the model output. This procedure is analogous to neural decoding techniques with the important difference that the output of an artificial model is used in lieu of brain recordings [<xref ref-type="bibr" rid="pcbi.1005859.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref048">48</xref>].</p>
<p>The general experimental procedure is as follows: we constructed feedforward hierarchical spatiotemporal convolutional models and used them to extract feature representations of a number of video sequences. We then trained a machine learning classifier to predict the action label of a video sequence based on its feature representation. Finally, we quantified the performance of the classifier, by measuring prediction accuracy on a set of new, unseen videos.</p>
<p>The procedure outlined above was performed using three separate subsets of the action recognition dataset described in the previous section. In particular, constructing spatiotemporal convolutional model requires access to video sequences depicting actions to sample or learn convolutional layers’ templates. The subset of video sequences used to learn or sample templates was called <bold>embedding set</bold>. Training and testing the classifier required extracting model responses from a number of video sequences; these sequences were organized in two subsets: <bold>training set</bold> and <bold>test set</bold>. There was never any overlap between the <bold>test set</bold> and the union of <bold>training set</bold> and <bold>embedding set</bold>.</p>
</sec>
<sec id="sec013">
<title>Experiment 1</title>
<p>The purpose of Experiment 1 was to assess how well the data representations produced by each of the four models, supported a non-invariant action recognition task. In particular, the <bold>embedding set</bold> used to sample or learn templates contained videos showing all five actions at all five viewpoints performed by four of the five actors. The <bold>training set</bold> was a subset of the embedding set, and contained videos at either the frontal viewpoint or the side viewpoint. Lastly the <bold>test set</bold> contained videos of all five actions, performed by the fifth left-out actor and performed at either the frontal or side viewpoint. We obtained five different splits by choosing each of the five actors exactly once for test. After the templates had either been learned or sampled we used each model to extract representations of the <bold>train</bold> and <bold>test sets</bold> videos. We averaged the classifier’s performance over the two possible choices of training viewpoint, frontal or side. We report the mean and standard error of the classification accuracy across the five possible choices of the test actor.</p>
</sec>
<sec id="sec014">
<title>Experiment 2</title>
<p>Experiment 2 was designed to assess the performance of each model in producing data representations that were useful to classify videos according to their action content, when a generalization across changes in viewpoint was required. The experiment is identical to Experiment 1, and used the exact same models. However, when the <bold>training set</bold> contained videos recorded at the frontal viewpoint, the <bold>test set</bold> would contain videos at side viewpoint and vice-versa. We report the mean and standard deviation over the choice of the test actor of the average accuracy over the choice of training viewpoint.</p>
</sec>
<sec id="sec015">
<title>Feedforward Spatiotemporal Convolutional Neural Networks</title>
<p>Feedforward Spatiotemporal Convolutional Neural Networks (ST-CNNs) are hierarchical models: input video sequences go through layers of computations and the output of each layer serves as input to the next layer (<xref ref-type="fig" rid="pcbi.1005859.g002">Fig 2</xref>). These models are direct generalizations of models of the neural mechanisms that support recognizing objects in static images [<xref ref-type="bibr" rid="pcbi.1005859.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref027">27</xref>], to stimuli that extend in both space and time (i.e. video stimuli). Within each layer, single computational units process a portion of the input video sequence that is compact both in space and time. The outputs of each layer’s units are then processed and aggregated by units in the subsequent layers to construct a final signature representation for the whole input video. The sequence of layers we adopted alternates layers of units which perform template matching (or convolutional layers), and layers of units which perform max pooling operations [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref034">34</xref>]. Units’ receptive field sizes increases as the signal propagates through the hierarchy of layers.</p>
<p>All convolutional units within a layer share the same set of templates (filter bank) and output the dot-product between each filter and their input. Qualitatively, these models work by detecting the presence of a certain video segment (a template) in the input stimulus. The exact position in space and time of the detection is discarded by the pooling mechanism. The specific models we present here consist of two convolutional-pooling layers’ pairs. The layers are denoted as Conv1, Pool1, Conv2 and Pool2 (<xref ref-type="fig" rid="pcbi.1005859.g002">Fig 2</xref>). Convolutional layers are completely characterized by the size, content and stride of their units’ receptive fields and pooling layers are completely characterized by the operation they perform (in the cases we considered, output the maximum value of their input) and their pooling regions (which can extend across space, time and filters).</p>
</sec>
<sec id="sec016">
<title>Model 1: Purely convolutional model with sampled templates</title>
<p>The purely convolutional models with fixed and sampled templates we considered were implemented using the Cortical Network Simulator package [<xref ref-type="bibr" rid="pcbi.1005859.ref049">49</xref>].</p>
<p>The input videos were (128x76 pixel) x 60 frames; the model received the original input videos alongside two scaled-down versions of it (scaling of factors ½ and ¼ in each spatial dimension respectively).</p>
<p>The first layer, Conv1, consisted of convolutional units with 72 templates of size (7x7 pixel) x 3 frames, (9x9 pixel) x 4 frames and (11x11 pixel) x 5 frames. Convolution was carried out with a stride of 1 pixel (no spatial subsampling). Conv1 filters were obtained by letting Gabor-like receptive fields shift in space over frames (as described in previous studies describing the receptive fields of V1 and MT cells [<xref ref-type="bibr" rid="pcbi.1005859.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref040">40</xref>]). The full expression for each filter was as follows:
<disp-formula id="pcbi.1005859.e001">
<alternatives>
<graphic id="pcbi.1005859.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005859.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Where <italic>x</italic>′(<italic>θ</italic>,<italic>ρ</italic>,<italic>t</italic>) and <italic>y</italic>′(<italic>θ</italic>,<italic>ρ</italic>,<italic>t</italic>), are transformed coordinates that take into account a rotation by <italic>θ</italic> and a shift by <italic>ρt</italic> in the direction orthogonal to <italic>θ</italic>. The Gabor filters we considered had a spatial aperture (in both spatial directions) of <italic>σ</italic> = 0.6 <italic>S</italic>, with <italic>S</italic> representing the spatial receptive field size and a wavelength <inline-formula id="pcbi.1005859.e002"><alternatives><graphic id="pcbi.1005859.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005859.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mspace width="0.5em"/><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1005859.ref050">50</xref>]. Each filter had a preferred orientation <italic>θ</italic> chosen among 8 possible orientations (0, 45, 90, 135, 180, 225, 270, 315 degrees with respect to vertical). Each template was obtained by letting the Gabor-like receptive field just described, shift in the orthogonal direction to its preferred orientation (e.g. a vertical edge would move sideways) with a speed <italic>ρ</italic> chosen from a linear grid of 3 points between 4/3 and 4 pixels per frame (the shift in the first frame of the template was chosen so that the mean of Gabor-like receptive field’s envelop would be centered in the middle frame). Lastly, Conv1 templates had time modulation <inline-formula id="pcbi.1005859.e003"><alternatives><graphic id="pcbi.1005859.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005859.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:math></alternatives></inline-formula> with <italic>n</italic> = 3 and <italic>t</italic> = 0,…,<italic>T</italic> with <italic>T</italic> the temporal receptive field size [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref022">22</xref>].</p>
<p>The second layer, Pool1, performed max pooling operations on its input by simply finding and outputting the maximum value of each pooling region. Responses to each channel in the Conv1 filter bank was pooled independently and units pooled across regions of space: (4x4 units in space) x 1 unit in time with a stride of 2 units in space, and 1 unit in time, and two scale channels. The functional form of the kernel was chosen based on established models of action processing in visual cortex [<xref ref-type="bibr" rid="pcbi.1005859.ref010">10</xref>].</p>
<p>A second simple layer Conv2, followed Pool1. Templates in this case were sampled randomly from the Pool1 responses to videos in the <bold>embedding set</bold>. We used a model with 512 Conv2 units with sizes (9x9 units in space) x 3 units in time, (17x17 units in space) x 7 units in time and (25x25 units in space) x 11 units in time, and stride of 1 in all directions.</p>
<p>Finally, the Pool2 layer units performed max pooling. Pooling regions extended over the entire spatial input, one temporal unit, all remaining scales, and a single Conv2 channel.</p>
</sec>
<sec id="sec017">
<title>Model 2 and 3: Structured and Unstructured Pooling models with sampled templates</title>
<p>Structured and Unstructured Pooling models (model 2 and 3, respectively) were constructed by modifying the Pool2 layer of the purely convolutional models. Specifically, in these models Pool2 units pooled over the entire spatial input, one temporal unit, all remaining scales, and 9 Conv2 channels, (512 Conv2 channels and 60 Pool2 units mean that some Pool2 units operated on 8 channels and others on 9).</p>
<p>In the models employing a Structured Pooling mechanism, all templates sampled from videos of a particular actor performing a particular action, regardless of viewpoint were pooled together (<xref ref-type="fig" rid="pcbi.1005859.g003">Fig 3B</xref>). Templates of different sizes and corresponding to different scale channels were pooled independently. This resulted in 6 Pool2 units per action/actor pair, one for each receptive-field-size/scale-channel pair. The intuition behind the Structured Pooling mechanism is that the resulting Pool2 units will respond strongly to the presence of a certain template (e.g. the torso of someone running) regardless of its 3D pose [<xref ref-type="bibr" rid="pcbi.1005859.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref038">38</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref043">43</xref>,<xref ref-type="bibr" rid="pcbi.1005859.ref051">51</xref>–<xref ref-type="bibr" rid="pcbi.1005859.ref054">54</xref>].</p>
<p>The models employing an Unstructured Pooling mechanism followed a similar pattern however, the wiring between simple and complex cells was random (<xref ref-type="fig" rid="pcbi.1005859.g003">Fig 3A</xref>). The fixed templates models (model 1,2 and 3) employed the exact same set of templates (we sampled the templates from the embedding sets only once and used them in all three models) and differed only in their pooling mechanisms.</p>
</sec>
<sec id="sec018">
<title>Model 4: Model with learned templates</title>
<p>Models with learned templates were implemented using Torch packages. These models’ templates were trained to recognize actions from videos in the embedding set using a Cross Entropy Loss function, full supervision and backpropagation [<xref ref-type="bibr" rid="pcbi.1005859.ref055">55</xref>]. The models’ general architecture was similar to the one we used for models with structured and unstructured pooling. Specifically, during template learning we used two stacked Convolution-BatchNorm-MaxPooling-BatchNorm modules [<xref ref-type="bibr" rid="pcbi.1005859.ref056">56</xref>] followed by two Linear-ReLU-BatchNorm modules (ReLU units are half-rectifiers) and a final Log-Soft-Max layer. During feature extraction, the Linear and LogSoftMax layers were discarded.</p>
<p>Input videos were resized to (128x76 pixel) x 60 frames, like in the fixed-template models. The first convolutional layer’s filter bank comprised 72 filters of size (9x9 pixel) x 9 frames and convolution was applied with stride of 2 in all directions. The first max-pooling layer used pooling regions of size (4x4 units in space) x 1 unit in time and were applied with stride of 2 units in both spatial directions and 1 unit in time. The second convolutional layer’s filter bank was made up of 60 templates of size (17x17 units in space) x 3 units in time, responses were computed with a stride of 2 units in time and 1 unit in all spatial directions. The second MaxPooling layer’s units pooled over the full extent of both spatial dimensions, 1 unit in time and 5 channels. Lastly, the Linear layers had 256 and 128 units respectively and free bias terms. Model training was carried out using Stochastic Gradient Descent [<xref ref-type="bibr" rid="pcbi.1005859.ref055">55</xref>] and mini-batches of 10 videos.</p>
</sec>
<sec id="sec019">
<title>Machine learning classifier</title>
<p>We used the GURLS package [<xref ref-type="bibr" rid="pcbi.1005859.ref057">57</xref>] to train and test a Regularized Least Squares Gaussian-Kernel classifier using features extracted from the training and test set respectively and the corresponding action labels. The aperture of the Gaussian Kernel as well as the l2 regularization parameter were chosen with a Leave-One-Out cross-validation procedure on the training set. Accuracy was evaluated separately for each class and then averaged over classes.</p>
</sec>
<sec id="sec020">
<title>Significance testing: Model accuracy</title>
<p>We used a group one-way ANOVA to assess the significance of the difference in performance between all the fixed-template methods and the models with learned templates. We then used a paired-sample t-test with Bonferroni correction to assess the significance level of the difference between the performance of individual models. Difference were deemed significant p &lt; 0.05.</p>
</sec>
</sec>
<sec id="sec021">
<title>Quantifying agreement between model representations and neural recordings</title>
<sec id="sec022">
<title>Neural recordings</title>
<p>The brain activity of 8 human participants with normal or corrected to normal vision was recorded with an Elekta Neuromag Triux Magnetoencephalography (MEG) scanner while they watched 50 videos (five actors, five actions, two viewpoints: front and side) acquired with the same procedure outlined above, but not included in the dataset used for model template sampling or training. The MEG recordings data was first presented in [<xref ref-type="bibr" rid="pcbi.1005859.ref033">33</xref>] (the reference also details all acquisition, preprocessing and decoding methods). The MIT Committee on the Use of Humans as Experimental Subjects approved the experimental protocol. Subjects provided informed written consent before the experiment.</p>
<p>In the original neural recording study MEG recordings were used to train a pattern classifier to discriminate video stimuli on the basis of the neural response they elicited. The performance of the pattern classifier was then assessed on a separate set of recordings from the same subjects. This train/test decoding procedure was repeated every 10ms and individually for each subject both in a non-invariant (train and test at the same viewpoint) and an invariant (train at one viewpoint and test at the different viewpoint) case. It was possible to discriminate videos according to their action content based on the neural response they elicited [<xref ref-type="bibr" rid="pcbi.1005859.ref033">33</xref>].</p>
<p>We used the filtered MEG recordings (all 306 sensors) elicited by each of the 50 videos mentioned above, averaged across subjects and averaged over a 100ms window centered around 470ms after stimulus onset as a proxy to the neural representation of the video (maximum accuracy for action decoding, as reported in the original study, RSA score with the entire time course is shown, for completeness in (<xref ref-type="supplementary-material" rid="pcbi.1005859.s002">S2 Fig</xref>).).</p>
</sec>
<sec id="sec023">
<title>Representational Similarity Analysis</title>
<p>We computed the pairwise correlation-based dissimilarity matrix for each of the model representations of the 50 videos that were shown to human subjects in the MEG. Likewise, we computed the empirical dissimilarity matrix computed using MEG neural recordings. We then performed 50 rounds of bootstrap, in each round we randomly sampled 30 videos out of the original 50 (corresponding to 30 rows and columns of the dissimilarity matrices). For each 30-videos sample, we assessed the level of agreement of the dissimilarity matrix induced by each model representation, with the one computed using neural data by calculating the Spearman Correlation Coefficient (SCC) between the lower triangular portions of the two matrices.</p>
<p>We computed an estimate for the noise ceiling in the neural data by repeating the bootstrap procedure outlined above to assess the level of agreement between an individual human subject and the average of the rest. We then selected the highest possible match score across subjects and across 100 rounds of bootstrap to serve as noise ceiling.</p>
<p>Similarly, we assessed a chance level for the Representational Similarity score by computing the match between each model and a scrambled version of the neural data matrix. We performed 100 rounds of bootstrap per model (reshuffling the neural dissimilarity matrix rows and columns each time) and selected the maximum score across rounds of bootstrap and models to serve as baseline score [<xref ref-type="bibr" rid="pcbi.1005859.ref032">32</xref>].</p>
<p>We normalized the SCC obtained by comparing each model representation to the neural recordings, by re-scaling them to fall between 0 (chance level) and 1 (noise ceiling). In this normalized scale, anything positive matches neural data better than chance with p &lt; 0.01.</p>
</sec>
<sec id="sec024">
<title>Significance testing: Matching neural data</title>
<p>We used a one-way group ANOVA to assess the difference between the Spearman Correlation Coefficient (SCC) obtained using models that employed fixed templates and models with learned templates. Subsequently, we assessed the significance of the difference between the SCC of each model by performing a paired t-test between the samples obtained through the bootstrap procedure. We deemed differences to be significant when p &lt; 0.05 (Bonferroni corrected).</p>
</sec>
</sec>
</sec>
<sec id="sec025">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005859.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title/>
<p>a) Classification accuracy, within and across changes in 3D viewpoint for a Recurrent Convolutional Neural Network. This architecture does not outperform a purely feedforward baseline. b) A Recurrent Convolutional Neural Network does not produce a dissimilarity structure that better agrees with the neural data than a purely feedforward baseline.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005859.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Spearman Correlation Coefficient between the dissimilarity structure constructed using the representation of 50 videos computed from the Spatiotemporal Convolutional Neural Network with learned templates and the neural data over all possible choices of the neural data time bin.</title>
<p>Neural data is most informative for action content of the stimulus at the time indicated by the vertical black line [<xref ref-type="bibr" rid="pcbi.1005859.ref033">33</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005859.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005859.s003" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Recurrent neural networks and RSA over time.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Georgios Evangelopoulos, Charles Frogner, Patrick Winston, Gabriel Kreiman, Martin Giese, Charles Jennings, Heuihan Jhuang, and Cheston Tan for their feedback on this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005859.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moeslund</surname> <given-names>TB</given-names></name>, <name name-style="western"><surname>Granum</surname> <given-names>E</given-names></name>. <article-title>A Survey of Computer Vision-Based Human Motion Capture</article-title>. <source>Comput Vis Image Underst</source>. <year>2001</year>;<volume>81</volume>: <fpage>231</fpage>–<lpage>268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/cviu.2000.0897" xlink:type="simple">10.1006/cviu.2000.0897</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anselmi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Rosasco</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Mutch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tacchetti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Unsupervised learning of invariant representations</article-title>. <source>Theor Comput Sci</source>. <year>2015</year>; <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tcs.2015.06.048" xlink:type="simple">http://dx.doi.org/10.1016/j.tcs.2015.06.048</ext-link></mixed-citation></ref>
<ref id="pcbi.1005859.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Poggio</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Anselmi</surname> <given-names>F</given-names></name>. <source>Visual Cortex and Deep Networks: Learning Invariant Representations</source>. <publisher-name>MIT Press</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pcbi.1005859.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johansson</surname> <given-names>G</given-names></name>. <article-title>Visual perception of biological motion and a model for its analysis</article-title>. <source>Percept Psychophys</source>. <year>1973</year>;<volume>14</volume>: <fpage>201</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03212378" xlink:type="simple">10.3758/BF03212378</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bobick</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>JW</given-names></name>. <article-title>The recognition of human movement using temporal templates</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2001</year>;<volume>23</volume>: <fpage>257</fpage>–<lpage>267</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/34.910878" xlink:type="simple">10.1109/34.910878</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weinland</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ronfard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Boyer</surname> <given-names>E</given-names></name>. <article-title>Free viewpoint action recognition using motion history volumes</article-title>. <source>Comput Vis Image Underst</source>. <year>2006</year>;<volume>104</volume>: <fpage>249</fpage>–<lpage>257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cviu.2006.07.013" xlink:type="simple">10.1016/j.cviu.2006.07.013</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gorelick</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Blank</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shechtman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Irani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Basri</surname> <given-names>R</given-names></name>. <article-title>Actions as space-time shapes</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2007</year>;<volume>29</volume>: <fpage>2247</fpage>–<lpage>2253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2007.70711" xlink:type="simple">10.1109/TPAMI.2007.70711</ext-link></comment> <object-id pub-id-type="pmid">17934233</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laptev</surname> <given-names>I</given-names></name>. <article-title>On space-time interest points</article-title>. <source>International Journal of Computer Vision</source>. <year>2005</year>. pp. <fpage>107</fpage>–<lpage>123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11263-005-1838-7" xlink:type="simple">10.1007/s11263-005-1838-7</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref009"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Dollár P, Rabaud V, Cottrell G, Belongie S. Behavior recognition via sparse spatio-temporal features. Proceedings - 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance, VS-PETS. 2005. pp. 65–72. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/VSPETS.2005.1570899" xlink:type="simple">10.1109/VSPETS.2005.1570899</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jhuang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A Biologically Inspired System for Action Recognition</article-title>. <source>IEEE International Conference on Computer Vision (ICCV)</source>. <year>2007</year>. pp. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005859.ref011"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Donahue J, Hendricks LA, Guadarrama S, Rohrbach M, Venugopalan S, Darrell T, et al. Long-term recurrent convolutional networks for visual recognition and description. IEEE Conf on Computer Vision and Pattern Recognition (CVPR). 2015. pp. 2625–2634. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2015.7298878" xlink:type="simple">10.1109/CVPR.2015.7298878</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giese</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Neural Mechanisms for the Recognition of Biological Movements</article-title>. <source>Nat Rev Neurosci</source>. <year>2003</year>;<volume>4</volume>: <fpage>179</fpage>–<lpage>192</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1057" xlink:type="simple">10.1038/nrn1057</ext-link></comment> <object-id pub-id-type="pmid">12612631</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrett</surname> <given-names>DI</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Mistlin</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Chitty</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Head</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Potter</surname> <given-names>DD</given-names></name>, <etal>et al</etal>. <article-title>Visual analysis of body movements by neurones in the temporal cortex of the macaque monkey: a preliminary report</article-title>. <source>Behav Brain Res</source>. <year>1985</year>;<volume>16</volume>: <fpage>153</fpage>–<lpage>170</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0166-4328(85)90089-0" xlink:type="simple">10.1016/0166-4328(85)90089-0</ext-link></comment> <object-id pub-id-type="pmid">4041214</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vangeneugden</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pollick</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Functional differentiation of macaque visual temporal cortical neurons using a parametric action space</article-title>. <source>Cereb Cortex</source>. <year>2009</year>;<volume>19</volume>: <fpage>593</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhn109" xlink:type="simple">10.1093/cercor/bhn109</ext-link></comment> <object-id pub-id-type="pmid">18632741</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Blake</surname> <given-names>R</given-names></name>. <article-title>Brain areas active during visual perception of biological motion</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>35</volume>: <fpage>1167</fpage>–<lpage>1175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(02)00897-8" xlink:type="simple">10.1016/S0896-6273(02)00897-8</ext-link></comment> <object-id pub-id-type="pmid">12354405</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vaina</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chowdhury</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sinha</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Belliveau</surname> <given-names>J</given-names></name>. <article-title>Functional neuroanatomy of biological motion perception in humans</article-title>. <source>Proc Natl Acad Sci</source>. <year>2001</year>;<volume>98</volume>: <fpage>11656</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.191374198" xlink:type="simple">10.1073/pnas.191374198</ext-link></comment> <object-id pub-id-type="pmid">11553776</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Haxby</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>A</given-names></name>. <article-title>FMRI responses to video and point-light displays of moving humans and manipulable objects</article-title>. <source>J Cogn Neurosci</source>. <year>2003</year>;<volume>15</volume>: <fpage>991</fpage>–<lpage>1001</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892903770007380" xlink:type="simple">10.1162/089892903770007380</ext-link></comment> <object-id pub-id-type="pmid">14614810</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Downing</surname> <given-names>P</given-names></name>. <article-title>Selectivity for the human body in the fusiform gyrus</article-title>. <source>J Neurophysiol</source>. <year>2005</year>;<volume>93</volume>: <fpage>603</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00513.2004" xlink:type="simple">10.1152/jn.00513.2004</ext-link></comment> <object-id pub-id-type="pmid">15295012</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jardine</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pyles</surname> <given-names>J</given-names></name>. <article-title>fMR-Adaptation Reveals Invariant Coding of Biological Motion on the Human STS</article-title>. <source>Front Hum Neurosci</source>. <year>2010</year>;<volume>4</volume>: <fpage>15</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.09.015.2010" xlink:type="simple">10.3389/neuro.09.015.2010</ext-link></comment> <object-id pub-id-type="pmid">20431723</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vangeneugden</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Peelen M</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tadin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Battelli</surname> <given-names>L</given-names></name>. <article-title>Distinct neural mechanisms for body form and body motion discriminations</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>574</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4032-13.2014" xlink:type="simple">10.1523/JNEUROSCI.4032-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24403156</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adelson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bergen</surname> <given-names>J</given-names></name>. <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>J Opt Soc Am</source>. <year>1985</year>;<volume>2</volume>: <fpage>284</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005859.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>D</given-names></name>. <article-title>A model of neuronal responses in visual area MT</article-title>. <source>Vision Res</source>. <year>1998</year>;<volume>38</volume>: <fpage>743</fpage>–<lpage>761</lpage>. <object-id pub-id-type="pmid">9604103</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sheinberg</surname> <given-names>D</given-names></name>. <article-title>Temporal cortex neurons encode articulated actions as slow sequences of integrated poses</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>3133</fpage>–<lpage>3145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3211-09.2010" xlink:type="simple">10.1523/JNEUROSCI.3211-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20181610</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sheinberg</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Neural representation of action sequences: how far can a simple snippet-matching model take us?</article-title> <source>Adv Neural Inf Process Syst</source>. <year>2013</year>; <fpage>593</fpage>–<lpage>601</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005859.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kilner</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Lemon</surname> <given-names>RN</given-names></name>. <article-title>What we know currently about mirror neurons</article-title>. <source>Curr Biol</source>. <year>2013</year>;<volume>23</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2013.10.051" xlink:type="simple">10.1016/j.cub.2013.10.051</ext-link></comment> <object-id pub-id-type="pmid">24309286</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architechture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci</source>. <year>2007</year>;<volume>104</volume>: <fpage>6424</fpage>–<lpage>6429</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0700622104" xlink:type="simple">10.1073/pnas.0700622104</ext-link></comment> <object-id pub-id-type="pmid">17404214</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukushima</surname> <given-names>K</given-names></name>. <article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biol Cybern</source>. <year>1980</year>;<volume>36</volume>: <fpage>193</fpage>–<lpage>202</lpage>. <object-id pub-id-type="pmid">7370364</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>J</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2014</year>;<volume>111</volume>: <fpage>8619</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment> <object-id pub-id-type="pmid">24812127</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>J</given-names></name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>: <fpage>356</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4244" xlink:type="simple">10.1038/nn.4244</ext-link></comment> <object-id pub-id-type="pmid">26906502</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isik</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Tacchetti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>TA</given-names></name>. <article-title>A fast, invariant representation for human action in the visual system</article-title>. <source>J Neurophysiol</source>. American Physiological Society; <year>2017</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00642.2017" xlink:type="simple">10.1152/jn.00642.2017</ext-link></comment> <object-id pub-id-type="pmid">29118198</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref034"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Karpathy A, Toderici G, Shetty S, Leung T, Sukthankar R, Li FF. Large-scale video classification with convolutional neural networks. IEEE Conf on Computer Vision and Pattern Recognition (CVPR). 2014. pp. 1725–1732. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2014.223" xlink:type="simple">10.1109/CVPR.2014.223</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ji</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>W</given-names></name>. <article-title>3D convolutional neural networks for human action recognition</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2013</year>;<volume>35</volume>: <fpage>221</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2012.59" xlink:type="simple">10.1109/TPAMI.2012.59</ext-link></comment> <object-id pub-id-type="pmid">22392705</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>: <fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liao</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Anselmi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>TA</given-names></name>. <article-title>The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex</article-title>. <source>PLoS Comput Biol</source>. Public Library of Science; <year>2015</year>;<volume>11</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004390" xlink:type="simple">10.1371/journal.pcbi.1004390</ext-link></comment> <object-id pub-id-type="pmid">26496457</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Leibo J, Mutch J, Poggio T. Learning to discount transformations as the computational goal of visual cortex? Present FGVC/CVPR 2011, Color Springs, CO. 2011;</mixed-citation></ref>
<ref id="pcbi.1005859.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">Jhuang H. A biologically inspired system for action recognition. 2008; 58.</mixed-citation></ref>
<ref id="pcbi.1005859.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>E</given-names></name>. <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>: <fpage>945</fpage>–<lpage>956</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.05.021" xlink:type="simple">10.1016/j.neuron.2005.05.021</ext-link></comment> <object-id pub-id-type="pmid">15953422</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>How MT cells analyze the motion of visual patterns</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>: <fpage>1421</fpage>–<lpage>1431</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1786" xlink:type="simple">10.1038/nn1786</ext-link></comment> <object-id pub-id-type="pmid">17041595</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liao</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Anselmi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>View-Tolerant Face Recognition and Hebbian Learning Imply Mirror-Symmetric Neural Tuning to Head Orientation</article-title>. <source>Curr Biol</source>. <year>2016</year>;<volume>27</volume>: <fpage>1</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2016.10.044" xlink:type="simple">10.1016/j.cub.2016.10.044</ext-link></comment> <object-id pub-id-type="pmid">27916526</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiskott</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>T</given-names></name>. <article-title>Slow feature analysis: Unsupervised learning of invariances</article-title>. <source>Neural Comput</source>. <year>2002</year>;<volume>14</volume>: <fpage>715</fpage>–<lpage>770</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602317318938" xlink:type="simple">10.1162/089976602317318938</ext-link></comment> <object-id pub-id-type="pmid">11936959</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Bengio Y, Lee D-H, Bornschein J, Lin Z. Towards Biologically Plausible Deep Learning. arxiv:15020415. 2015; 1–18. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s13398-014-0173-7.2" xlink:type="simple">10.1007/s13398-014-0173-7.2</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref045"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">Liao Q, Leibo J, Poggio T. How Important is Weight Symmetry in Backpropagation? arXiv:151005067. 2015;</mixed-citation></ref>
<ref id="pcbi.1005859.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazzoni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>M</given-names></name>. <article-title>A more biologically plausible learning rule for neural networks</article-title>. <source>Proc Natl Acad Sci</source>. <year>1991</year>;<volume>88</volume>: <fpage>4433</fpage>–<lpage>4437</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.88.10.4433" xlink:type="simple">10.1073/pnas.88.10.4433</ext-link></comment> <object-id pub-id-type="pmid">1903542</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jacobs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fridman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Alam</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Prusky</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Ruling out and ruling in neural codes</article-title>. <source>Proc Natl Acad Sci</source>. National Acad Sciences; <year>2009</year>;<volume>106</volume>: <fpage>5936</fpage>–<lpage>5941</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0900573106" xlink:type="simple">10.1073/pnas.0900573106</ext-link></comment> <object-id pub-id-type="pmid">19297621</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isik</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Meyers</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>The dynamics of invariant object recognition in the human visual system</article-title>. <source>J Neurophysiol</source>. <year>2014</year>;<volume>111</volume>: <fpage>91</fpage>–<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00394.2013" xlink:type="simple">10.1152/jn.00394.2013</ext-link></comment> <object-id pub-id-type="pmid">24089402</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref049"><label>49</label><mixed-citation publication-type="other" xlink:type="simple">Mutch J, Knoblich U, Poggio T. CNS: a GPU-based framework for simulating cortically-organized networks. MIT-CSAIL-TR. 2010;2010–13.</mixed-citation></ref>
<ref id="pcbi.1005859.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Mutch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Anselmi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tacchetti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rosasco</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <source>Invariant Recognition Predicts Tuning of Neurons in Sensory Cortex. Computational and Cognitive Neuroscience of Vision</source>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>Springer Singapore</publisher-name>; <year>2017</year>. pp. <fpage>85</fpage>–<lpage>104</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005859.ref051"><label>51</label><mixed-citation publication-type="other" xlink:type="simple">Liao Q, Leibo J, Poggio T. Unsupervised learning of clutter-resistant visual representations from natural videos. arXiv:14093879. 2014;</mixed-citation></ref>
<ref id="pcbi.1005859.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stringer</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Perry</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Proske</surname> <given-names>JH</given-names></name>. <article-title>Learning invariant object recognition in the visual system with continuous transformations</article-title>. <source>Biol Cybern</source>. <year>2006</year>;<volume>94</volume>: <fpage>128</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-005-0030-z" xlink:type="simple">10.1007/s00422-005-0030-z</ext-link></comment> <object-id pub-id-type="pmid">16369795</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>H</given-names></name>. <article-title>Effects of temporal association on recognition memory</article-title>. <source>Proc Natl Acad Sci</source>. <year>2001</year>;<volume>98</volume>: <fpage>4800</fpage>–<lpage>4804</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.071028598" xlink:type="simple">10.1073/pnas.071028598</ext-link></comment> <object-id pub-id-type="pmid">11287633</object-id></mixed-citation></ref>
<ref id="pcbi.1005859.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Földiák</surname> <given-names>P</given-names></name>. <article-title>Learning Invariance from Transformation Sequences</article-title>. <source>Neural Comput</source>. <year>1991</year>;<volume>3</volume>: <fpage>194</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1991.3.2.194" xlink:type="simple">10.1162/neco.1991.3.2.194</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Kingma D, Ba JL. Adam: a Method for Stochastic Optimization. Int Conf Learn Represent. 2015; 1–13.</mixed-citation></ref>
<ref id="pcbi.1005859.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Ioffe S, Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:150203167. 2015; 1–11. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s13398-014-0173-7.2" xlink:type="simple">10.1007/s13398-014-0173-7.2</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005859.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tacchetti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mallapragada</surname> <given-names>PK</given-names></name>, <name name-style="western"><surname>Rosasco</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Santoro</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rosasco</surname> <given-names>L</given-names></name>. <article-title>GURLS: A Least Squares Library for Supervised Learning</article-title>. <source>J Mach Learn Res</source>. <year>2013</year>;<volume>14</volume>: <fpage>3201</fpage>–<lpage>3205</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>