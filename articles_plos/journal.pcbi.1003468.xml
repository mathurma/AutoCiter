<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00025</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003468</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Slowness and Sparseness Have Diverging Effects on Complex Cell Learning</article-title>
<alt-title alt-title-type="running-head">Slowness and Sparseness in Complex Cell Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lies</surname><given-names>Jörn-Philipp</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Häfner</surname><given-names>Ralf M.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bethge</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Werner Reichardt Centre for Integrative Neuroscience, University of Tübingen, Tübingen, Germany</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Swartz Center for Theoretical Neurobiology, Brandeis University, Waltham, Massachusetts, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Bernstein Center for Computational Neuroscience, Tübingen, Germany</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Lucke</surname><given-names>Jorg</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Technische Universität Berlin, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">matthias@bethgelab.org</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JPL RMH MB. Performed the experiments: JPL. Analyzed the data: JPL RMH MB. Wrote the paper: JPL MB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>3</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>6</day><month>3</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>3</issue>
<elocation-id>e1003468</elocation-id>
<history>
<date date-type="received"><day>2</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>19</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Lies et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Following earlier studies which showed that a sparse coding principle may explain the receptive field properties of complex cells in primary visual cortex, it has been concluded that the same properties may be equally derived from a slowness principle. In contrast to this claim, we here show that slowness and sparsity drive the representations towards substantially different receptive field properties. To do so, we present complete sets of basis functions learned with <italic>slow subspace analysis</italic> (SSA) in case of natural movies as well as translations, rotations, and scalings of natural images. SSA directly parallels independent subspace analysis (ISA) with the only difference that SSA maximizes slowness instead of sparsity. We find a large discrepancy between the filter shapes learned with SSA and ISA. We argue that SSA can be understood as a generalization of the Fourier transform where the power spectrum corresponds to the maximally slow subspace energies in SSA. Finally, we investigate the trade-off between slowness and sparseness when combined in one objective function.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>A key question in visual neuroscience is how neural representations achieve invariance against appearance changes of objects. In particular, the invariance of complex cell responses in primary visual cortex against small translations is commonly interpreted as a signature of an invariant coding strategy possibly originating from an unsupervised learning principle. Various models have been proposed to explain the response properties of complex cells using a sparsity or a slowness criterion and it has been concluded that physiologically plausible receptive field properties can be derived from either criterion. Here, we show that the effect of the two objectives on the resulting receptive field properties is in fact very different. We conclude that slowness alone cannot explain the filter shapes of complex cells and discuss what kind of experimental measurements could help us to better asses the role of slowness and sparsity for complex cell representations.</p>
</abstract>
<funding-group><funding-statement>This work was supported by the Max Planck Society and the German Ministry of Education, Science, Research and Technology through the Bernstein award to MB (BMBF; FKZ: 01GQ0601), the Bernstein Center for Computational Neuroscience, Tuebingen (BMBF; FKZ: 01GQ1002), and the German Excellency Initiative through the Centre for Integrative Neuroscience Tübingen (EXC307). RMH acknowledges funding from the Swartz Foundation. We acknowledge support by Deutsche Forschungsgemeinschaft and Open Access Publishing Fund of Tuebingen University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="11"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The appearance of objects in an image can change dramatically depending on their pose, distance, and illumination. Learning representations that are invariant against such appearance changes can be viewed as an important preprocessing step which removes distracting variance from a data set in order to improve performance of downstream classifiers or regression estimators <xref ref-type="bibr" rid="pcbi.1003468-Burges1">[1]</xref>. Clearly, it is an inherent part of training a classifier to make its response invariant against all within-class variations. Rather than learning these invariances for each object class individually, however, we observe that many transformations such as translation, rotation and scaling apply to any object independent of its specific shape. This suggests that signatures of such transformations exist in the spatio-temporal statistics of natural images which allow one to learn invariant representations in an unsupervised way.</p>
<p>Complex cells in primary visual cortex are commonly seen as building blocks for such invariant image representations (e.g. <xref ref-type="bibr" rid="pcbi.1003468-Riesenhuber1">[2]</xref>). While complex cells, like simple cells, respond to edges of particular orientation they are less sensitive to the precise location of the edge <xref ref-type="bibr" rid="pcbi.1003468-Hubel1">[3]</xref>. A variety of neural algorithms have been proposed that aim at explaining the response properties of complex cells as components of an invariant representation that is optimized for the spatio-temporal statistics of the visual input <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Krding1">[12]</xref>.</p>
<p>The two main objectives used for the optimization of models of neural representations are <italic>sparseness</italic> and <italic>slowness</italic>. While in the context of unsupervised representation learning the two objectives have been proposed to similarly explain the receptive field properties of complex cells, there are important differences between them that may help to identify the algorithms used in biological vision. Intuitively, the slowness objective can be seen as a measure of approximate invariance or “tolerance”, whereas sparseness is better interpreted as a measure of selectivity. Tolerance and selectivity—or slowness and sparseness, respectively—can be understood as complementary goals which both play an important role for solving the task of object recognition <xref ref-type="bibr" rid="pcbi.1003468-DiCarlo1">[13]</xref>. A prominent view that goes back to Fukushima's proposal of the necognitron (1980) is that these goals are pursued in an alternating fashion by alternating layers of S and C cells where the S cells are optimized for selectivity and the C cells are optimized for tolerance. This idea has been inspired by the finding of simple and complex cells in primary visual cortex which also motivated the terminology of S and C cells.</p>
<p>Thus, based on the strong association between complex cells and invariance, one would expect that slowness rather than sparseness should play a critical role for complex cell representations. In this study, we investigate the differences between slowness and sparseness for shaping the receptive field properties of complex cells.</p>
<p>While for natural signals it may be impossible to find perfectly invariant representations, slowness seeks to find features that at least change as slowly as possible under the appearance transformations exhibited in the data <xref ref-type="bibr" rid="pcbi.1003468-Fldik1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Krding1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Sutton1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Maurer2">[27]</xref>. In contrast to sparse representation learning which is tightly linked to generative modeling, many slow feature learning algorithms follow a discriminative or coarse-graining approach: they do not aim at modeling all variations in the sensory data but rather classify parts of it as noise (or some dimensions as being dominated by noise) and then discard this information. This is most obvious in the case of slow feature analysis (SFA) <xref ref-type="bibr" rid="pcbi.1003468-Wiskott1">[21]</xref>. SFA can be seen as a special case of oriented principal component analysis which seeks to determine the most informative subspace under the assumption that fast changes are noise <xref ref-type="bibr" rid="pcbi.1003468-Bethge1">[28]</xref>. While it is very likely that some information is discarded along the visual pathway, throwing away information in modeling studies requires great caution. For example, if one discards all high spatial frequency information in natural images one would easily obtain a representation which changes more slowly in time. Yet, this improvement in slowness is not productive as high spatial frequency information in natural images cannot be equated with noise but often carries critical information. We therefore compare <italic>complete</italic> sets of filters learned with <italic>slow subspace analysis</italic> (SSA) <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref> and <italic>independent subspace analysis</italic> (ISA) <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen1">[4]</xref>, respectively. The two algorithms are perfectly identical with the only difference that SSA maximizes slowness while ISA maximizes sparsity.</p>
<p>For sparseness it is common to show complete sets of filters, but this is not so in case of slowness. Based on the analysis of a small subset of filters, it has been argued that SSA may generally yield similar results to ISA <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref>. In contrast, we here arrive at quite the opposite conclusion: by looking at the complete representation we find a large discrepancy between the filter shapes derived with SSA and those derived with ISA. Most notably, we find that SSA does not lead to localized receptive fields as has been claimed (<xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Wiskott2">[29]</xref> —but see <xref ref-type="bibr" rid="pcbi.1003468-Bethge1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Sprekeler1">[30]</xref>).</p>
<p><italic>Complete</italic> representations optimizing slowness have previously been studied only for mixed objective functions that combined slowness with sparseness <xref ref-type="bibr" rid="pcbi.1003468-Berkes2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen3">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Cadieu2">[33]</xref> but never when optimizing exclusively for slowness alone. Here we systematically investigate how a complete set of filters changes when varying the objective function from a pure slowness objective to a pure sparsity objective by using a weighted mixture of the two and gradually increasing the ratio of their respective weights. From this analysis we will conclude that the receptive field shapes shown in <xref ref-type="bibr" rid="pcbi.1003468-Berkes2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen3">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Cadieu2">[33]</xref> are mostly determined by the sparsity objective rather than the slowness objective. That is the receptive fields would change relatively little if the slowness objective was dropped but it would change drastically if the sparsity objective was removed. These findings change our view of the effect of slowness and raise new questions that can guide us to a more profound understanding of unsupervised complex cell learning.</p>
</sec><sec id="s2">
<title>Results</title>
<p>The central result of this paper is the observation that the effect of the slowness objective on complex cell learning is substantially different from that of sparseness. Most likely this has gone unnoticed to date because previous work either did not derive complete representations from slowness or combined the slowness objective with a sparsity constraint which masked the genuine effect of slowness. Therefore, we here put a large effort into characterizing the effect of slow subspace learning on the complete set of filter shapes under various conditions. We first study a number of analytically defined transformations such as translations, rotations, and scalings before we turn to natural movies and the comparison between slowness and sparseness.</p>
<p>The general design common to SSA and ISA is illustrated in <xref ref-type="fig" rid="pcbi-1003468-g001">Figure 1</xref>. We apply a set of filters to the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e001" xlink:type="simple"/></inline-formula> and square the filter responses. Two filters form a 2-dimensional subspace (gray box in <xref ref-type="fig" rid="pcbi-1003468-g001">Figure 1</xref>) and the sum of squared filter responses of these two filters yield the subspace energy response. This can be seen as the squared radial component of the projection of the signal into the 2D subspace formed by the two respective filters. For example, if the filters are taken from the Fourier basis and grouped such that the two filters within each subspace have the same spatial frequency and orientation and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e002" xlink:type="simple"/></inline-formula> phase difference, the output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e003" xlink:type="simple"/></inline-formula> at a fixed time instant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e004" xlink:type="simple"/></inline-formula> is the power spectrum of the image <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e005" xlink:type="simple"/></inline-formula>. As input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e006" xlink:type="simple"/></inline-formula> we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e007" xlink:type="simple"/></inline-formula> image patches sampled from the van Hateren image database <xref ref-type="bibr" rid="pcbi.1003468-vanHateren1">[34]</xref> and from the video database <xref ref-type="bibr" rid="pcbi.1003468-vanHateren2">[35]</xref>, vectorized to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e008" xlink:type="simple"/></inline-formula>-dimensions, and applied SSA to all remaining 120 AC components after projecting out the DC component.</p>
<fig id="pcbi-1003468-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g001</object-id><label>Figure 1</label><caption>
<title>Model structure for both independent subspace analysis (ISA) and slow subspace analysis (SSA).</title>
<p>The input signal, e.g. a movie sequence, is applied to several filters. Two filters form a subspace. The output of each filter is passed through a quadratic nonlinearity and summed within each subspace. The output corresponds to the radial component of the 2D subspace. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e009" xlink:type="simple"/></inline-formula> responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e010" xlink:type="simple"/></inline-formula> then form the multidimensional output signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e011" xlink:type="simple"/></inline-formula>. If the filters are the discrete Fourier transform basis where each subspace consists of the two filters which only differ in phase, then the output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e012" xlink:type="simple"/></inline-formula> is the power spectrum of the input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e013" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g001" position="float" xlink:type="simple"/></fig>
<p>In the first part of our study, the input sequence consisted of translations. As time-varying process for the translations, we implemented a two-dimensional random walk of an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e014" xlink:type="simple"/></inline-formula> window over the full image. The shift amplitudes were drawn from a continuous uniform distribution between 0 and 2 pixels, allowing for subpixel shifts. The filters obtained from SSA are shown in <xref ref-type="fig" rid="pcbi-1003468-g002">Figure 2A</xref>. Each row contains the filter pairs of 6 subspaces, sorted by descending slowness from left to right and top to bottom. The filters clearly resemble global sine wave functions. The wave functions differ in spatial frequency and orientation between the different subspaces. Within each subspace, orientation and spatial frequency are almost identical, but phases differ significantly. In fact, the phase difference is close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e015" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e016" xlink:type="simple"/></inline-formula>), resembling quadrature pairs of sine and cosine functions as it is the case for the two-dimensional Fourier basis. Accordingly, the subspace energy output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e017" xlink:type="simple"/></inline-formula> of the resulting SSA representation is very similar to the power spectrum of the image <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e018" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003468-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g002</object-id><label>Figure 2</label><caption>
<title>SSA on translations with open and cyclic boundary conditions.</title>
<p>The complete set of filters learned from translated images with open and cyclic boundary conditions are shown in (A) and (C), respectively. Each row shows the filters of 6 subspaces with 2 dimensions. The subspaces are ordered according to their slowness, with the slowest filter in the upper left corner and decreasing slowness from left to right and top to bottom. The <italic>inverse</italic> slowness <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e019" xlink:type="simple"/></inline-formula> for the individual subspaces after learning (black dots) and for the initial random filters (gray squares) is shown in (B) and (D), respectively. For open boundary conditions (B), the inverse slowness does not converge to 0, hence perfect invariance is not achieved. For cyclic shifts, however, the inverse slowness approaches 0 with arbitrary precision (D), indicating convergence to perfect invariance.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g002" position="float" xlink:type="simple"/></fig>
<p>In fact, one can think of SSA as learning a generalized power spectrum based on a slowness criterion. While the power spectrum is known to be invariant against translations with periodic boundary conditions, perfect invariance—or infinite slowness—is not achieved for the translations with open boundary conditions studied here (see <xref ref-type="fig" rid="pcbi-1003468-g002">Figure 2 B</xref>). The slowness criterion is best understood as a penalty of fast changes since it decomposes into an average over penalties of fast changes for each individual component (see <xref ref-type="sec" rid="s4">methods</xref>). Therefore, we will always show the inverse slowness <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e020" xlink:type="simple"/></inline-formula> for each component such that the <italic>smaller</italic> the area under the curve the <italic>better</italic> the average slowness.</p>
<p>Compared to random subspaces, the decrease in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e021" xlink:type="simple"/></inline-formula>, i.e. the increase in slowness, is substantial: the average inverse slowness <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e022" xlink:type="simple"/></inline-formula> decreases approximately by a factor of three. The low frequency subspaces are clearly the slowest subspaces, and slowness decreases with increasing spatial frequency. At the same time, however, the inverse slowness of all learned subspaces is still larger than 0, i.e. even for the slowest components, perfect invariance is not achieved. This is not surprising, as perfect invariance is impossible whenever unpredictable variations exist as it is the case for open boundary conditions.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003468-g002">Figure 2 C</xref>, we show that SSA can indeed find perfectly invariant filters starting from a random initial filter set if one imposes periodic boundary conditions. To this end, we created <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e023" xlink:type="simple"/></inline-formula> pink noise patches with circulant covariance structure, i.e. the pixels on the left border of the image are correlated with pixels on the right border as if they were direct neighbors. As time-varying process, we implemented a random walk with cyclic shifts where the patches were translated randomly with periodic boundary conditions. As in the previous study, the shift amplitudes were drawn from a continuous uniform distribution between 0 and 2 pixels. Since the Fourier basis is the eigenbasis of the cyclic shift operator it should yield infinite slowness for the cyclic boundary conditions. Indeed, the filters learned from these data recover the Fourier basis with arbitrary precision. Perfect invariance is equivalent with the objective function converging to 0. This means that the response of each subspace is identical for all shifts. <xref ref-type="fig" rid="pcbi-1003468-g002">Figure 2D</xref> shows the inverse slowness <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e024" xlink:type="simple"/></inline-formula> of the individual components. For all filters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e025" xlink:type="simple"/></inline-formula> is very small (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e026" xlink:type="simple"/></inline-formula>), close to perfect invariance and infinite slowness.</p>
<p>Given that the SSA representation learned for translations is very similar to the Fourier basis and since the Fourier basis achieves perfect invariance for cyclic shifts we proceeded to investigate whether the Fourier basis is optimal even for non-cyclic translations as well. We created three different data sets, with random translations as in the first study, but the maximal shift amplitude of the 2D random walk was 1, 2, and 3 pixels, respectively. As initial condition, we used the Fourier basis (<xref ref-type="fig" rid="pcbi-1003468-g003">Figure 3</xref>, ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e027" xlink:type="simple"/></inline-formula>’) instead of a random matrix. The optimized bases are denoted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e028" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e029" xlink:type="simple"/></inline-formula> indicates the maximal shift amplitude. We show the 2D-Fourier amplitude spectrum of the filters rather than the filters in pixel space because it is easier to assess the differences between the different bases. The DC component is located at the center of the spectrum.</p>
<fig id="pcbi-1003468-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g003</object-id><label>Figure 3</label><caption>
<title>Deviations from the Fourier basis for translations with open boundary conditions.</title>
<p>Here, we started the optimization with the Fourier basis (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e030" xlink:type="simple"/></inline-formula>) as initial condition. We used 3 different data sets sampled from the van Hateren image database using 2D translations with a shift amplitude of maximally 1, 2, or 3 pixels. The optimized filters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e031" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e032" xlink:type="simple"/></inline-formula> is the maximal shift amplitude, do not deviate dramatically from the initial condition. The amplitude spectra of all filters are shown in the <italic>upper panel</italic> with the DC component being at the center. The amplitude spectra of the optimized filters blur out towards the lower frequencies except for the lowest frequencies, which blur out towards the higher frequencies. Only the highest frequencies show additional sensitivity at the lowest spatial frequencies which cannot be explained by spatial localization. The slowness of the individual components is shown in the <italic>lower panel</italic>. The black lines indicate the performance of the Fourier basis applied to test data with shift amplitudes of up to 1 (solid), 2 (long dashes), or 3 (short dashes) pixels. The gray lines show the performance of the optimal filters. SSA sacrifices slowness on the slower filters to gain a comparatively larger amount of slowness on the faster filters. In this way, overall SSA achieves better slowness.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g003" position="float" xlink:type="simple"/></fig>
<p>During optimization, the basis slightly departs from the initial condition but remains very localized in the Fourier domain (<xref ref-type="fig" rid="pcbi-1003468-g003">Figure 3</xref>, ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e033" xlink:type="simple"/></inline-formula>’). The low frequency filters become sensitive to higher frequencies while the high frequency filters become also sensitive to lower frequencies as the initial filters blur out towards the border or center, respectively. The objective function is improved for the optimized filters not only on the training but also on the test set (cf. <xref ref-type="table" rid="pcbi-1003468-t001">Table 1</xref>). The slowness of the 60 individual components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e034" xlink:type="simple"/></inline-formula> evaluated on identically created test sets (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e035" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e036" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e037" xlink:type="simple"/></inline-formula>, respectively) is shown in <xref ref-type="fig" rid="pcbi-1003468-g003">Figure 3</xref>. The Fourier filters are slower than the optimized filters for the first 20–30 components, then about equal for 10 components, and significantly faster for the remaining components. Apparently, the SSA objective sacrifices a little bit of the slowness of the low frequency components to get a comparatively larger gain in slowness from modifying the high frequency components. The optimization of average inverse slowness in contrast to searching for a single maximally slow component is a characteristic feature of SSA.</p>
<table-wrap id="pcbi-1003468-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.t001</object-id><label>Table 1</label><caption>
<title>Control for overfitting.</title>
</caption><alternatives><graphic id="pcbi-1003468-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="2" align="left" rowspan="1">Fourier basis</td>
<td colspan="2" align="left" rowspan="1">optimized basis</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">training</td>
<td align="left" rowspan="1" colspan="1">test</td>
<td align="left" rowspan="1" colspan="1">training</td>
<td align="left" rowspan="1" colspan="1">test</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1 pixel shift</td>
<td align="left" rowspan="1" colspan="1">0.17838</td>
<td align="left" rowspan="1" colspan="1">0.17725</td>
<td align="left" rowspan="1" colspan="1">0.13801</td>
<td align="left" rowspan="1" colspan="1">0.15359</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2 pixel shift</td>
<td align="left" rowspan="1" colspan="1">0.29469</td>
<td align="left" rowspan="1" colspan="1">0.29185</td>
<td align="left" rowspan="1" colspan="1">0.24680</td>
<td align="left" rowspan="1" colspan="1">0.27570</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3 pixel shift</td>
<td align="left" rowspan="1" colspan="1">0.41521</td>
<td align="left" rowspan="1" colspan="1">0.41943</td>
<td align="left" rowspan="1" colspan="1">0.36569</td>
<td align="left" rowspan="1" colspan="1">0.40423</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Objective on training and test set for optimized filters and Fourier basis.</p></fn></table-wrap-foot></table-wrap>
<p>Even though we expect changes in natural movies to be dominated by local translations, it is instructive to study other global affine transforms as well. Therefore, we applied SSA to 3 additional data sets: The first data set contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e038" xlink:type="simple"/></inline-formula> patches from the van Hateren image set which were rotated around the center pixel. The second data set consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e039" xlink:type="simple"/></inline-formula> patches from the van Hateren image set which were also rotated around the center pixel but where we kept only the pixels within a predefined circle. Specifically, we reduced the number of dimensions again to 121 pixels by cutting out the corners which left an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e040" xlink:type="simple"/></inline-formula> circular image patch. The patches in the third data set were sampled with sizes ranging from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e041" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e042" xlink:type="simple"/></inline-formula> pixels and then rescaled to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e043" xlink:type="simple"/></inline-formula> pixels, in order to obtain a patch-centered anisotropic scaling transformation. The preprocessing was identical to the previous studies and the initial filter matrix was a random orthonormal matrix. The filters and the objective of the individual subspaces of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e044" xlink:type="simple"/></inline-formula> rotation data are shown in <xref ref-type="fig" rid="pcbi-1003468-g004">Figure 4A</xref>. The filters resemble the rotation filters found with steerable filter theory <xref ref-type="bibr" rid="pcbi.1003468-Bethge1">[28]</xref>. The slowness of all components is significantly larger than for random filters, but with clearly decreasing slowness for the last subspaces. Notably, the last subspaces have no systematic structure. This can be explained by the fact that when rotating a square patch, the pixels in the 4 corners are not predictable unless for multiples of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e045" xlink:type="simple"/></inline-formula> rotations. Therefore the algorithm cannot find meaningful subspaces that would preserve the energy for the pixels in the corners. The filters in <xref ref-type="fig" rid="pcbi-1003468-g004">Figure 4B</xref> from the disc shaped patches do not show these artifacts. Here, all filters nicely resemble angular wave functions as expected from steerable filter theory and also exhibit better slowness. Finally, the scaling filters are shown in <xref ref-type="fig" rid="pcbi-1003468-g004">Figure 4C</xref>. All filters resemble windowed wave functions that are localized towards the boundaries of the patch. This indicates that a scaling can be seen as a combination of local translations which go inward for downscaling and outward for upscaling. All subspaces defined by the learned filters are significantly slower than the random subspaces.</p>
<fig id="pcbi-1003468-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g004</object-id><label>Figure 4</label><caption>
<title>SSA filters for local rotation and scaling.</title>
<p>Illustration of the filters obtained from patch-centered rotation sequences (A,B) and patch-centered scaling sequences (C) with the slowness of the individual filter subspaces before (<italic>random</italic>) and after the optimization (<italic>learned</italic>). The filters are ordered in ascending inverse slowness <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e046" xlink:type="simple"/></inline-formula> (row-wise) with the slowest feature in the upper left and the fastest feature in the lower right corner. The data in (A) and (C) consist of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e047" xlink:type="simple"/></inline-formula> square patches from the van Hateren data set while the data for (B) consist of 121-dimensional round patches which are, for visualization, embedded in a 14×14 square patch. The rotation filters match those found in steerable filter theory <xref ref-type="bibr" rid="pcbi.1003468-Bethge1">[28]</xref>. The filters of the patch-centered anisotropic scaling exhibit localized edge filters centered towards the patch boundaries.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g004" position="float" xlink:type="simple"/></fig>
<p>After characterizing the result of slow subspace learning for analytically defined transformations we now turn to natural movies and the comparison between slowness and sparseness. Specifically, we compare slow subspace analysis (SSA) to independent subspace analysis (ISA) in order to show how the slowness and the sparsity objective have different effects on the receptive field shapes learned. To this end, we combine the two objectives to obtain a weighted mixture of them for which we can gradually tune the trade-off between the slowness and the sparseness objective. In this way, we obtain a 1-parametric family of objective functions<disp-formula id="pcbi.1003468.e048"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e048" xlink:type="simple"/><label>(1)</label></disp-formula>for which the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e049" xlink:type="simple"/></inline-formula> determines the trade-off between slowness and sparseness. Specifically, we obtain SSA in case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e050" xlink:type="simple"/></inline-formula> and ISA for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e051" xlink:type="simple"/></inline-formula>. As one can see in <xref ref-type="fig" rid="pcbi-1003468-g005">Figures 5</xref> the filters learned with SSA (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e052" xlink:type="simple"/></inline-formula>) look very different from those learned with ISA (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e053" xlink:type="simple"/></inline-formula>). This finding contradicts earlier claims that the filters learned with SSA are comparable to those learned with ISA. The most obvious difference is that the slowness objective works against the localization of filters that is brought forward by the sparsity objective.</p>
<fig id="pcbi-1003468-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g005</object-id><label>Figure 5</label><caption>
<title>Filters of slowness, independence and mixture objective learned on movies.</title>
<p>The lower panel shows the performance with respect to both the slowness objective <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e054" xlink:type="simple"/></inline-formula> (blue) and the sparsity objective <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e055" xlink:type="simple"/></inline-formula> (red) and the upper panel displays four sets of filters as obtained for different values for the trade-off parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e056" xlink:type="simple"/></inline-formula>: The leftmost case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e057" xlink:type="simple"/></inline-formula>) is equivalent to SSA and the rightmost case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e058" xlink:type="simple"/></inline-formula>) is equivalent to ISA. There is a large difference between the two that can easily be grasped by eye. The example for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e059" xlink:type="simple"/></inline-formula> reflects the crossing point in performance (see lower panel) meaning that the representation performs slightly better than 80% of its maximal performance with respect to both objectives simultaneously. The case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e060" xlink:type="simple"/></inline-formula> was hand-picked to represent the point where the filters perceptually look similarly close to ISA and SSA.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g005" position="float" xlink:type="simple"/></fig>
<p>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e061" xlink:type="simple"/></inline-formula> we will refer to the resulting algorithm as <italic>independent slow subspace analysis</italic> (ISSA). If a representation is optimized for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e062" xlink:type="simple"/></inline-formula> its performance with respect to the slowness objective <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e063" xlink:type="simple"/></inline-formula> decreases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e064" xlink:type="simple"/></inline-formula>. At the same time, its performance with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e065" xlink:type="simple"/></inline-formula> increases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e066" xlink:type="simple"/></inline-formula>. The percentages shown indicate the increase in slowness and sparseness relative to the maximal gain that can be achieved if one optimizes solely for one of the two objectives. Note that the shapes of these curves depend on the objective functions used and are not invariant under pointwise nonlinear transformations. The values shown here are determined directly by the objective functions without any additional transformation (see <xref ref-type="disp-formula" rid="pcbi.1003468.e083">Eqs. 3</xref>,<xref ref-type="disp-formula" rid="pcbi.1003468.e113">11</xref>). Remarkably, it is possible to derive a representation which performs reasonably well with respect to both sparseness and slowness simultaneously. At an intermediate point where both objectives, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e067" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e068" xlink:type="simple"/></inline-formula>, are reduced by the same factor in our units, the performance is still larger than 80% for each. Interestingly, for this trade-off the receptive fields look quite similar to those obtained with ISA. This may explain why previous work on unsupervised learning with combinations of sparseness and slowness did not reveal that the two objectives drive the receptive fields towards very different shapes.</p>
<p>The trade-off in performance with respect to slowness and sparsity for natural movies, translation, rotation, and scaling is summarized in <xref ref-type="fig" rid="pcbi-1003468-g006">Figure 6</xref>. It shows the ISA filters (A), the ISSA filters at the intermediate point of slowness and sparsity for natural movies (B), translation (C), rotation (D), and scaling (E) and in the same order the SSA filters in (F,G,H,I). The concave shape of the curves (upper left) indicates that the trade-off between the two objectives is rather graceful such that it is possible to achieve a reasonably good performance for both objectives at the same time.</p>
<fig id="pcbi-1003468-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003468.g006</object-id><label>Figure 6</label><caption>
<title>Trade-off in the performance with respect to slowness and sparsity.</title>
<p>When optimizing the filter set for a weighted superposition of the slowness and sparsity objectives the performance with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e069" xlink:type="simple"/></inline-formula> decreases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e070" xlink:type="simple"/></inline-formula> (<italic>upper left</italic>). The steepness of decay indicates the impact of the trade-off. The different colors correspond to different datasets (see legend). While the performance with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e071" xlink:type="simple"/></inline-formula> for the rotation data falls off quickly (green), the differences between scaling, translation and movie data (cyan, blue, red) are not significant. The concave shapes of the curves indicate a rather gentle trade-off. The dashed diagonal line indicates an intermediate point for this trade-off. We chose it such that both objectives are reduced by the same factor relative to their optimal performance in the units used here. The corresponding filters are shown in the adjacent panels: The ISA filters are shown in (A) which are independent of the temporal statistics. The ISSA filters at the break even point are shown in (B) for movies, in (C) for translations, in (D) for rotations, and in (E) for scalings. The last row shows the SSA filters in the same order: (F) for movies, in (G) for translations, in (H) for rotations, and in (I) for scalings.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003468.g006" position="float" xlink:type="simple"/></fig></sec><sec id="s3">
<title>Discussion</title>
<p>Unsupervised learning algorithms are a widespread approach to study candidate computational principles that may underly the formation of neural representations in sensory systems. Slowness and sparsity both have been suggested as objectives driving the formation of complex cell representations. More specifically, it has been claimed that the filter properties obtained from slow subspace analysis would resemble those obtained with independent subspace analysis <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref> and that the optimal stimulus for SFA is localized <xref ref-type="bibr" rid="pcbi.1003468-Wiskott2">[29]</xref>. Here, we showed that there is a striking difference between the sets of SSA and ISA filters: While the sparsity objective of ISA facilitates localized filter shapes, maximal slowness can be achieved only with global receptive fields as found by SSA.</p>
<p>The different implications of slowness and sparseness are most notable in filters containing high spatial frequencies. For low spatial frequency filters the number of cycles is small simply because it is constrained to be smaller than the product of spatial frequency and simulation window size. Since previous studies have inspected only low spatial frequency filters the different effect of sparseness and slowness has gone unnoticed or at least not been sufficiently appreciated <xref ref-type="bibr" rid="pcbi.1003468-Berkes1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Wiskott2">[29]</xref>. A signature of the drive towards global filters generated by slowness can be found in the bandwidth statistics presented in <xref ref-type="bibr" rid="pcbi.1003468-Berkes1">[6]</xref>. Global filter shapes correspond to small bandwidth. While the authors mention that the fraction of small bandwidth filters exceeds that found for physiological receptive fields they rather suggested that this may be an artifact of their preprocessing, specifically referring to dimensionality reduction based on principal component analysis. However, the opposite is the case: the preprocessing rather leads to an <italic>underestimation</italic> of the fraction of small bandwidth filters. Principal component analysis will always select for low spatial frequency components and thus reduce the fraction of small bandwidth filters because it is the high spatial frequency components which have the smallest bandwidth.</p>
<p>While it is difficult to make rigorous statements that are model-independent, there are general arguments why the lack of localization is likely a generic consequence of slowness rather than a spurious property that was specific to SSA only: By definition a neuron cannot be driven by stimuli outside of its receptive field (RF). Therefore, whenever a stimulus is presented that drives the neuron inside its RF, the neuron must stop firing when the stimulus is shifted outside the RF. This suggests very generally, that in the presence of motion the objective of slowness or invariance necessarily requires large RFs. Sparsity, in contrast, encourages neurons to respond as selectively as possible. One obvious way to achieve this is to become selective for location which directly translates into small RF sizes.</p>
<p>In addition, analytical considerations suggest that slowness is likely to generate global filters with small bandwidth. For small image patches it is reasonable to assume that the spatio-temporal statistics are dominated by translational motion. Thus, it is not surprising that the filter properties of SSA found for natural movies resemble those for translations. In computer vision, there is a large number of studies which derive features that are invariant under specific types of transformations such as translations, scalings and rotations. An analytical approach to invariance is provided by steerable filter theory <xref ref-type="bibr" rid="pcbi.1003468-Knutsson1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Freeman1">[37]</xref> which allows one to design perfectly invariant filters for any compact Lie group transformation <xref ref-type="bibr" rid="pcbi.1003468-HelOr1">[38]</xref>. The best known example is the power spectrum which is perfectly invariant under translations with periodic boundary conditions <xref ref-type="bibr" rid="pcbi.1003468-Bethge1">[28]</xref>. For the other Lie group transformations studied in this paper, the symmetry was broken due to discretization and boundary effects. In these cases the representations found with SSA can be seen as a generalization of the Fourier transform whose subspace energies are not perfectly invariant anymore but at least maximally stable under the given spatio-temporal statistics. A very similar argument has also been made for SFA <xref ref-type="bibr" rid="pcbi.1003468-Sprekeler1">[30]</xref>.</p>
<p>The receptive fields of complex cells determined from physiological experiments rarely exhibit multiple cycles as predicted by SSA. This indicates that complex cells in the brain are not fully optimized for slowness. It may still be possible though that slowness plays some role in the formation of complex cells. The trade-off analysis with the mixed objective has shown that giving up some sparsity allows one to achieve both relatively large sparsity and slowness at the same time with localized receptive fields.</p>
<p>Having established how exactly sparseness and slowness differ in their implied receptive fields also helps to address the roles of sparseness and slowness experimentally. Li &amp; DiCarlo <xref ref-type="bibr" rid="pcbi.1003468-Li1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Li2">[40]</xref> found neural correlates of the learning of invariances by manipulating the statistics of the presented stimuli. Since their recordings were from area IT where receptive fields are known to be very large, it would be very interesting to see the effect of similar experiments, made during the critical period, on complex cells in primary visual cortex. To distinguish between slowness and sparseness it might also be instructive to vary the temporal continuity of the training stimuli, e.g. by comparing the effect of smooth translations with discrete jumps on the learnt receptive fields. Another, possibly more direct approach to distinguish between sparseness and slowness might be to compute the respective objective functions directly on the sensory responses over development. While such an experiment has already been done for sparseness by <xref ref-type="bibr" rid="pcbi.1003468-Berkes2">[8]</xref> who interestingly found that sparseness <italic>decreases</italic> throughout development, we are not aware of the equivalent evaluation of any change in neuronal slowness.</p>
<p>Independent of what happens during development, the comparison of slowness and sparseness raises questions about how we should view the role of complex cells with respect to the tolerance-selectivity trade-off. Given that large receptive fields are advantageous for invariance or slowness, the small receptive field size of complex cells suggests that complex cells do not aim at achieving maximal tolerance but rather lean towards preserving a high degree of selectivity. For both ISA and SSA some degree of invariance is already built into the architecture which resembles the energy model of complex cells and will always find two-dimensional invariant subspaces. Instead of prescribing the invariant subspace dimensionality we wanted to know what happens if the subspace dimensionality is learned as well. This can be done by learning complex cells with SFA on the full quadratic feature space and then investigating the spectrum of the resulting quadratic forms. Comparing the number of subspaces employed by SFA to maximize slowness to empirical measurements in V1 <xref ref-type="bibr" rid="pcbi.1003468-Chen1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Rust1">[42]</xref> it turns out that the number of subspaces employed by real neurons, and therefore the degree of invariance is smaller than predicted by slowness (see <xref ref-type="supplementary-material" rid="pcbi.1003468.s001">Figure S1</xref>).</p>
<p>The deeper principle underlying both sparsity and slowness is the idea of generative modeling <xref ref-type="bibr" rid="pcbi.1003468-Turner1">[25]</xref>. From a generative modeling perspective, one is most concerned about modeling the precise shape of all variations in the data rather than just optimizing some fixed architecture or feature space to be as invariant or sparse as possible. More specifically, in a generative modeling framework all ingredients of the model are formalized by a density model and thus the likelihood becomes the natural objective function. This holds also true for the studies which combined the slowness objective with a sparsity objective in the past <xref ref-type="bibr" rid="pcbi.1003468-Berkes2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen3">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003468-Cadieu2">[33]</xref>. The generative power of these models, however, still needs to be significantly improved in order to be able to explain object recognition performance of humans and animals. A better understanding of the partially opposing demands of slowness and sparseness on the response properties of visual neurons will help us understand the computational strategy employed by the visual system in reaching that performance.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Slow Subspace Analysis</title>
<p>The algorithm of slow subspace analysis (SSA) has previously been described by Kayser et al <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref>. Just like in independent subspace analysis <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen1">[4]</xref> also in SSA the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e072" xlink:type="simple"/></inline-formula>-dimensional input space is separated into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e073" xlink:type="simple"/></inline-formula> independent subspaces of dimensionality <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e074" xlink:type="simple"/></inline-formula> and the (squared) norm of each subspace should vary as slowly as possible. The output function of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e075" xlink:type="simple"/></inline-formula>-th subspace is then defined as<disp-formula id="pcbi.1003468.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e076" xlink:type="simple"/><label>(2)</label></disp-formula>where K is the dimensionality of the subspace, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e077" xlink:type="simple"/></inline-formula> the number of the subspace, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e078" xlink:type="simple"/></inline-formula> is the orthonormal filter matrix. It is important to notice that, for an input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e079" xlink:type="simple"/></inline-formula> with zero mean and unit variance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e080" xlink:type="simple"/></inline-formula> has mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e081" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e082" xlink:type="simple"/></inline-formula>, the set of squared subspace norms corresponds to the power spectrum of the Fourier transform if the set of filters are the discrete Fourier transform.</p>
<p>The objective function of SSA has been called “temporal smoothness” objective by Kayser <italic>et al.</italic> <xref ref-type="bibr" rid="pcbi.1003468-Kayser1">[9]</xref> and is given by<disp-formula id="pcbi.1003468.e083"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e083" xlink:type="simple"/><label>(3)</label></disp-formula>Note, however, that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e084" xlink:type="simple"/></inline-formula> increases with the amount of rapid changes and is minimized subject to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e085" xlink:type="simple"/></inline-formula>. To find the optimal set of filters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e086" xlink:type="simple"/></inline-formula> under the given constraints we use a variant of the gradient projection method of Rosen <xref ref-type="bibr" rid="pcbi.1003468-Luenberger1">[43]</xref> which was successfully used for simple cell learning before <xref ref-type="bibr" rid="pcbi.1003468-Hurri1">[22]</xref>.</p>
<p>In order to compute the gradient of the objective function we have to compute the temporal derivative of the output signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e087" xlink:type="simple"/></inline-formula> first, using the difference quotient as approximation:<disp-formula id="pcbi.1003468.e088"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e088" xlink:type="simple"/><label>(4)</label></disp-formula>As we use discrete time steps, we can set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e089" xlink:type="simple"/></inline-formula> which leads to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e090" xlink:type="simple"/></inline-formula>. This simplifies the objective function (3) as the temporal difference mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e091" xlink:type="simple"/></inline-formula>. The objective function can be further simplified by using the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e092" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e093" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e094" xlink:type="simple"/></inline-formula> having zero mean and unit variance, which leads to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e095" xlink:type="simple"/></inline-formula>. The complete objective function is then<disp-formula id="pcbi.1003468.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e096" xlink:type="simple"/><label>(5)</label></disp-formula>For every iteration, the gradient of the objective function is computed, scaled by the step length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e097" xlink:type="simple"/></inline-formula>, and subtracted from the current filter set<disp-formula id="pcbi.1003468.e098"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e098" xlink:type="simple"/><label>(6)</label></disp-formula>The partial gradient with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e099" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003468.e100"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e100" xlink:type="simple"/><label>(7)</label></disp-formula>with<disp-formula id="pcbi.1003468.e101"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e101" xlink:type="simple"/><label>(8)</label></disp-formula>The matrix containing the resulting filter set is then projected onto the orthogonal group using symmetric orthogonalization <xref ref-type="bibr" rid="pcbi.1003468-Lwdin1">[44]</xref><disp-formula id="pcbi.1003468.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e102" xlink:type="simple"/><label>(9)</label></disp-formula>yielding the closest orthonormal matrix with respect to the Frobenius norm <xref ref-type="bibr" rid="pcbi.1003468-Fan1">[45]</xref>. Along this gradient a line search is performed where the initial step length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e103" xlink:type="simple"/></inline-formula> is reduced until the objective function on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e104" xlink:type="simple"/></inline-formula> is smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e105" xlink:type="simple"/></inline-formula> before the iteration proceeds.</p>
<p>The optimization is initialized with a random orthonormal matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e106" xlink:type="simple"/></inline-formula>. As stopping criterion the optimization terminates when the change in the objective function is smaller than the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e107" xlink:type="simple"/></inline-formula>. In all our simulations we used a subspace dimension of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e108" xlink:type="simple"/></inline-formula>. A python implementation of the algorithm can be found as part of the natter toolbox <ext-link ext-link-type="uri" xlink:href="http://bethgelab.org/software/natter/" xlink:type="simple">http://bethgelab.org/software/natter/</ext-link>.</p>
</sec><sec id="s4b">
<title>Independent Subspace Analysis</title>
<p>Independent subspace analysis (ISA) has originally been proposed by Hyvärinen and Hoyer <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen1">[4]</xref>. The only difference between SSA and ISA is the objective function. Generally speaking, ISA is characterized by a density model for which the density factorizes over a decomposition of linear subspaces. In most cases the subspaces all have the same dimension, and in case of natural images the marginal distributions over the individual subspaces are modeled as sparse spherically symmetric distributions. Like Hyvärinen and Hoyer <xref ref-type="bibr" rid="pcbi.1003468-Hyvrinen1">[4]</xref> we chose the spherical exponential distribution<disp-formula id="pcbi.1003468.e109"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e109" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e110" xlink:type="simple"/></inline-formula> is the subspace response as defined in <xref ref-type="disp-formula" rid="pcbi.1003468.e076">Equation 2</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e111" xlink:type="simple"/></inline-formula> is a scaling constant and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e112" xlink:type="simple"/></inline-formula> the normalization constant. Correspondingly, the objective function reads<disp-formula id="pcbi.1003468.e113"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e113" xlink:type="simple"/><label>(11)</label></disp-formula>The scaling and normalization constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e115" xlink:type="simple"/></inline-formula> can be omitted. This leads to the gradient<disp-formula id="pcbi.1003468.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003468.e116" xlink:type="simple"/><label>(12)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e117" xlink:type="simple"/></inline-formula> as defined in <xref ref-type="disp-formula" rid="pcbi.1003468.e101">Equation 8</xref>. The optimization is identical to SSA where only objective and gradient are replaced. For the numerical implementation of ISA we used a python translation of the code provided by the original authors at <ext-link ext-link-type="uri" xlink:href="http://research.ics.aalto.fi/ica/imageica/" xlink:type="simple">http://research.ics.aalto.fi/ica/imageica/</ext-link>.</p>
</sec><sec id="s4c">
<title>Data Collection</title>
<p>The time-varying input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e118" xlink:type="simple"/></inline-formula> was derived from the van Hateren image database <xref ref-type="bibr" rid="pcbi.1003468-vanHateren1">[34]</xref> for translations, rotations and scalings and the van Hateren movie database <xref ref-type="bibr" rid="pcbi.1003468-vanHateren2">[35]</xref> for movie sequences. The image database contains over 4000 calibrated monochrome images of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e119" xlink:type="simple"/></inline-formula> pixels, where each pixel corresponds to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e120" xlink:type="simple"/></inline-formula> of visual angle. We created a temporal sequence by sliding a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e121" xlink:type="simple"/></inline-formula> window over the image. Step length and direction for translation, angle for rotation and anisotropic scaling factors were sampled from a uniform random process. If not stated otherwise, the translation was sampled independently for x- and y direction from a uniform distribution on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e122" xlink:type="simple"/></inline-formula>, the rotation angle from a uniform distribution on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e123" xlink:type="simple"/></inline-formula> and the scaling factors independently for x- and y-direction from a uniform distribution on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e124" xlink:type="simple"/></inline-formula>. The movie database consists of 216 movies of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e125" xlink:type="simple"/></inline-formula> pixels with a duration of 192 s and 25 frames per second. The images were taken in Holland and show the landscape consisting mostly of bushes, trees and lakes with the occasional streets and houses. The video clips were recorded from Dutch, German and British television with mostly wildlife scenes but also sports and movies. For each stimulus set we sampled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e126" xlink:type="simple"/></inline-formula> patches.</p>
</sec><sec id="s4d">
<title>Preprocessing</title>
<p>The extracted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e127" xlink:type="simple"/></inline-formula> image patches were treated as vectors by stacking up the columns of the image patches, resulting in a 121-dimensional input vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e128" xlink:type="simple"/></inline-formula>. We projected out the DC component, i.e. removed the mean from the patches, and applied symmetric whitening to the remaining 120 AC components. No low pass filtering or further dimensionality reduction was applied. All computations were done in the 120-dimensional whitened space and the optimized filters then projected back into the original pixel space.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003468.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003468.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Model complex cells derived with SFA fail to reproduce the small numbers of significant eigenvalues found empirically with STC analysis.</bold> We computed SFA filters on the quadratic feature space of the 100 lowest Fourier components of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e129" xlink:type="simple"/></inline-formula> image patches sampled from the van Hateren image database <xref ref-type="bibr" rid="pcbi.1003468-vanHateren1">[34]</xref>. As temporal transformation we applied a 2D translation with shift amplitudes drawn from a 2D uniform continuous distribution on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e130" xlink:type="simple"/></inline-formula> to the data. We then applied the same analysis to the SFA filters as used in <xref ref-type="bibr" rid="pcbi.1003468-Rust1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1003468-Chen1">[41]</xref>. We applied a sequence of 50000 Gaussian white noise pattern to the SFA filter. The filter responses were centered at their respective median and split in two firing rate sets, the excitatory from all positive responses (i.e. larger than median) and the inhibitory from the absolute value of all negative responses (i.e. smaller than median). The firing rates were then used to generate Poisson spike counts. Given spike counts and stimuli, we computed the spike triggered covariance (STC) for 100 different noise stimulus sets per SFA filter. The spectrum of eigenvalues (eigenspectrum) of the STC matrix of one cell recorded from V1 in an awake monkey <xref ref-type="bibr" rid="pcbi.1003468-Chen1">[41]</xref> is shown in (A), the eigenspectrum of the STC of one SFA filter is shown in (B). To determine which eigenvectors are significant, we computed the STC with shuffled spike counts as control. The dashed lines correspond to mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e131" xlink:type="simple"/></inline-formula> 4.4 SD, which corresponds to a confidence interval of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003468.e132" xlink:type="simple"/></inline-formula> for Gaussian distributed eigenvalues. One clear difference is the number of significant eigenvectors. While for the V1 cell, only a few eigenvectors are significant, for the SFA model almost all eigenvectors are significant. The histogram of the number of significant excitatory and inhibitory eigenvectors is shown in (C) for the physiological data and in (D) for the SFA model. While the V1 cells have only few significant eigenvectors for all 130 recorded cells, the 980000 cells of the SFA model have on average 80 significant excitatory and inhibitory eigenvectors out of the 100 dimensions. The histogram bins with 0 entries were not plotted for clarity of the figure.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Philipp Berens and Alexander Ecker for helpful discussions and comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003468-Burges1"><label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">Burges CJC (2005) Geometric Methods for Feature Extraction and Dimensional Reduction. In: Maimon O, Rokach L, editors, Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers, Kluwer Academic Publishers. pp. 59–92.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Riesenhuber1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>25</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Hubel1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title>. <source>The Journal of Physiology</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>154</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Hyvrinen1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hoyer</surname><given-names>P</given-names></name> (<year>2000</year>) <article-title>Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>1705</fpage>–<lpage>1720</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Hyvrinen2"><label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">Hyvärinen A, Karhunen J, Oja E (2001) Independent Component Analysis. New York, NY, USA: John Wiley &amp; Sons, Inc., 481 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Berkes1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name> (<year>2005</year>) <article-title>Slow feature analysis yields a rich repertoire of complex cell properties</article-title>. <source>Journal of Vision</source> <volume>5</volume>: <fpage>579</fpage>–<lpage>602</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Karklin1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karklin</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name> (<year>2009</year>) <article-title>Emergence of complex cell properties by learning to generalize in natural scenes</article-title>. <source>Nature</source> <volume>457</volume>: <fpage>83</fpage>–<lpage>86</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Berkes2"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Turner</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A Structured Model of Video Reproduces Primary Visual Cortical Organisation</article-title>. <source>PLoS Computational Biology</source> <volume>5</volume>: <fpage>16</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Kayser1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Kayser C, Einhäuser W, Dümmer O, König P, Körding KP (2001) Extracting Slow Subspaces from Natural Videos Leads to Complex Cells. In: Artificial Neural Networks - ICANN 2001. Austrian Res Inst Artifical Intelligence, volume 2130, pp. 1075–1080. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/3-540-44668-0149" xlink:type="simple">10.1007/3-540-44668-0149</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Einhuser1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>König</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name> (<year>2002</year>) <article-title>Learning the invariance properties of complex cells from their responses to natural stimuli</article-title>. <source>European Journal of Neuroscience</source> <volume>15</volume>: <fpage>475</fpage>–<lpage>486</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Kayser2"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>König</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>Learning the nonlinearity of neurons from natural visual stimuli</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>1751</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Krding1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name>, <name name-style="western"><surname>König</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>How are complex cell properties adapted to the statistics of natural stimuli</article-title>? <source>Journal of Neurophysiology</source> <volume>91</volume>: <fpage>206</fpage>–<lpage>212</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-DiCarlo1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name> (<year>2012</year>) <article-title>How does the brain solve visual object recognition</article-title>? <source>Neuron</source> <volume>73</volume>: <fpage>415</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Sutton1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name> (<year>1981</year>) <article-title>An adaptive network that constructs and uses an internal model of its world</article-title>. <source>Cognition and Brain Theory</source> <volume>4</volume>: <fpage>217</fpage>–<lpage>246</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Klopf1"><label>15</label>
<mixed-citation publication-type="book" xlink:type="simple">Klopf AH (1982) The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence. Washington DC: Hemisphere Publishing Corporation, 140 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Fldik1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name> (<year>1991</year>) <article-title>Learning Invariance from Transformation Sequences</article-title>. <source>Neural Computation</source> <volume>3</volume>: <fpage>194</fpage>–<lpage>200</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Mitchison1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchison</surname><given-names>G</given-names></name> (<year>1991</year>) <article-title>Removing Time Variation with the Anti-Hebbian Differential Synapse</article-title>. <source>Neural Computation</source> <volume>3</volume>: <fpage>312</fpage>–<lpage>320</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Stone1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stone</surname><given-names>JV</given-names></name>, <name name-style="western"><surname>Bray</surname><given-names>A</given-names></name> (<year>1995</year>) <article-title>A learning rule for extracting spatio-temporal invariances</article-title>. <source>Network Computation in Neural Systems</source> <volume>6</volume>: <fpage>429</fpage>–<lpage>436</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Stone2"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stone</surname><given-names>JV</given-names></name> (<year>1996</year>) <article-title>Learning Perceptually Salient Visual Parameters Using Spatiotemporal Smoothness Constraints</article-title>. <source>Neural Computation</source> <volume>8</volume>: <fpage>1463</fpage>–<lpage>1492</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Wallis1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>1997</year>) <article-title>A model of invariant object recognition in the visual system</article-title>. <source>Progress in Neurobiology</source> <volume>51</volume>: <fpage>167</fpage>–<lpage>194</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Wiskott1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2002</year>) <article-title>Slow feature analysis: Unsupervised learning of invariances</article-title>. <source>Neural computation</source> <volume>14</volume>: <fpage>715</fpage>–<lpage>770</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Hurri1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hurri</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Simple-cell-like receptive fields maximize temporal coherence in natural video</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>663</fpage>–<lpage>91</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Spratling1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spratling</surname><given-names>MW</given-names></name> (<year>2005</year>) <article-title>Learning viewpoint invariant perceptual representations from cluttered images</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source> <volume>27</volume>: <fpage>753</fpage>–<lpage>61</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Maurer1"><label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">Maurer A (2006) Unsupervised slow subspace-learning from stationary processes. In: Proceedings of the 17th international conference on Algorithmic Learning Theory. Berlin, Heidelberg: SpringerVerlag, volume 4264 of <italic>Lecture Notes in Computer Science</italic>, pp. 363–377.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Turner1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turner</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>A Maximum-Likelihood Interpretation for Slow Feature Analysis</article-title>. <source>Neural Computation</source> <volume>19</volume>: <fpage>1022</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Masquelier1"><label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Masquelier T, Serre T, Poggio T (2007) Learning complex cell invariance from natural videos: A plausibility proof. Technical report, Massachusetts Institute of Technology Computer Science and Artifificial Intelligence Laboratory.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Maurer2"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maurer</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>Unsupervised slow subspace-learning from stationary processes</article-title>. <source>Theoretical Computer Science</source> <volume>405</volume>: <fpage>237</fpage>–<lpage>255</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Bethge1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Bethge M, Gerwinn S, Macke JH (2007) Unsupervised learning of a steerable basis for invariant image representations. In: Proceedings of SPIE Human Vision and Electronic Imaging XII (EI105). volume 6492, p. 12.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Wiskott2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Franzius</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Wilbert</surname><given-names>N</given-names></name> (<year>2011</year>) <article-title>Slow feature analysis</article-title>. <source>Scholarpedia</source> <volume>6</volume>: <fpage>5282, revision #136882</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Sprekeler1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>A theory of slow feature analysis for transformation-based input signals with an application to complex cells</article-title>. <source>Neural Computation</source> <volume>23</volume>: <fpage>303</fpage>–<lpage>335</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Hyvrinen3"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hurri</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Väyrynen</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Bubbles: a unifying framework for low-level statistical properties of natural image sequences</article-title>. <source>Journal of the Optical Society of America A</source> <volume>20</volume>: <fpage>1237</fpage>–<lpage>1252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Cadieu1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name> (<year>2009</year>) <article-title>Learning transformational invariants from natural movies</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>21</volume>: <fpage>209</fpage>–<lpage>216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Cadieu2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name> (<year>2012</year>) <article-title>Learning intermediate-level representations of form and motion from natural movies</article-title>. <source>Neural Computation</source> <volume>24</volume>: <fpage>827</fpage>–<lpage>66</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-vanHateren1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>van der Schaaf</surname><given-names>A</given-names></name> (<year>1998</year>) <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>265</volume>: <fpage>359</fpage>–<lpage>366</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-vanHateren2"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Ruderman</surname><given-names>DL</given-names></name> (<year>1998</year>) <article-title>Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>265</volume>: <fpage>2315</fpage>–<lpage>20</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Knutsson1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Knutsson H, Granlund GH (1983) Texture Analysis Using Two-Dimensional Quadrature Filters. In: IEEE Computer Society Workshop on Computer Architecture for Pattern Analysis and Image Database Management. pp. 206–213.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Freeman1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freeman</surname><given-names>WT</given-names></name>, <name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name> (<year>1991</year>) <article-title>The design and use of steerable filters</article-title>. <source>IEEE Transactions on Pattern analysis and machine intelligence</source> <volume>13</volume>: <fpage>891</fpage>–<lpage>906</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-HelOr1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hel-Or</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Teo</surname><given-names>PC</given-names></name> (<year>1998</year>) <article-title>Canonical decomposition of steerable functions</article-title>. <source>Journal of Mathematical Imaging and Vision</source> <volume>9</volume>: <fpage>83</fpage>–<lpage>95</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Li1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2008</year>) <article-title>Unsupervised natural experience rapidly alters invariant object representation in visual cortex</article-title>. <source>Science</source> <volume>321</volume>: <fpage>1502</fpage>–<lpage>1507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Li2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>1062</fpage>–<lpage>1075</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Chen1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2007</year>) <article-title>Excitatory and suppressive receptive field subunits in awake monkey primary visual cortex (V1)</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>104</volume>: <fpage>19120</fpage>–<lpage>19125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Rust1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2005</year>) <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>945</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Luenberger1"><label>43</label>
<mixed-citation publication-type="book" xlink:type="simple">Luenberger DG (1969) Optimization by vector space methods. New York, NY, USA: John Wiley &amp; Sons, Inc., 326 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Lwdin1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Löwdin</surname><given-names>PO</given-names></name> (<year>1950</year>) <article-title>On the Non-Orthogonality Problem Connected with the Use of Atomic Wave Functions in the Theory of Molecules and Crystals</article-title>. <source>The Journal of Chemical Physics</source> <volume>18</volume>: <fpage>365</fpage>–<lpage>375</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003468-Fan1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hoffman</surname><given-names>AJ</given-names></name> (<year>1955</year>) <article-title>Some Metric Inequalities in the Space of Matrices</article-title>. <source>Proceedings of the American Mathematical Society</source> <volume>6</volume>: <fpage>111</fpage>–<lpage>116</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>